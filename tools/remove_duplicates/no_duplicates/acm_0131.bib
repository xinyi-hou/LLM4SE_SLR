@inproceedings{10.1145/3611643.3616339,
author = {Lin, Bo and Wang, Shangwen and Liu, Zhongxin and Liu, Yepang and Xia, Xin and Mao, Xiaoguang},
title = {CCT5: A Code-Change-Oriented Pre-trained Model},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616339},
doi = {10.1145/3611643.3616339},
abstract = {Software is constantly changing, requiring developers to perform several derived tasks in a timely manner, such as writing a description for the intention of the code change, or identifying the defect-prone code changes. Considering that the cost of dealing with these tasks can account for a large proportion (typically around 70 percent) of the total development expenditure, automating such processes will significantly lighten the burdens of developers. To achieve such a target, existing approaches mainly rely on training deep learning models from scratch or fine-tuning existing pre-trained models on such tasks, both of which have weaknesses. Specifically, the former uses comparatively small-scale labelled data for training, making it difficult to learn and exploit the domain knowledge of programming language hidden in the large-amount unlabelled code in the wild; the latter is hard to fully leverage the learned knowledge of the pre-trained model, as existing pre-trained models are designed to encode a single code snippet rather than a code change (the difference between two code snippets). We propose to pre-train a model specially designed for code changes to better support developers in software maintenance. To this end, we first collect a large-scale dataset containing 1.5M+ pairwise data of code changes and commit messages. Based on these data, we curate five different tasks for pre-training, which equip the model with diverse domain knowledge about code changes. We fine-tune the pre-trained model, CCT5, on three widely-studied tasks incurred by code changes and two tasks specific to the code review process. Results show that CCT5 outperforms both conventional deep learning approaches and existing pre-trained models on these tasks.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1509–1521},
numpages = {13},
keywords = {Pre-Training, Deep Learning, Code Change},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616338,
author = {Du, Yali and Yu, Zhongxing},
title = {Pre-training Code Representation with Semantic Flow Graph for Effective Bug Localization},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616338},
doi = {10.1145/3611643.3616338},
abstract = {Enlightened by the big success of pre-training in natural language processing, pre-trained models for programming languages have been widely used to promote code intelligence in recent years. In particular, BERT has been used for bug localization tasks and impressive results have been obtained. However, these BERT-based bug localization techniques suffer from two issues. First, the pre-trained BERT model on source code does not adequately capture the deep semantics of program code. Second, the overall bug localization models neglect the necessity of large-scale negative samples in contrastive learning for representations of changesets and ignore the lexical similarity between bug reports and changesets during similarity estimation. We address these two issues by 1) proposing a novel directed, multiple-label code graph representation named Semantic Flow Graph (SFG), which compactly and adequately captures code semantics, 2) designing and training SemanticCodeBERT based on SFG, and 3) designing a novel Hierarchical Momentum Contrastive Bug Localization technique (HMCBL). Evaluation results show that our method achieves state-of-the-art performance in bug localization.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {579–591},
numpages = {13},
keywords = {type, semantic flow graph, pre-trained model, contrastive learning, computation role, bug localization},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3580305.3599790,
author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie},
title = {CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599790},
doi = {10.1145/3580305.3599790},
abstract = {Large pre-trained code generation models, such as OpenAI Codex, can generate syntax-and function-correct code, making the coding of programmers more productive. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 8 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible since Sep. 2022, we open-sourced its code, model weights, API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5673–5684},
numpages = {12},
keywords = {pre-trained model, large language model, code generation},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3593663.3593695,
author = {Dobslaw, Felix and Bergh, Peter},
title = {Experiences with Remote Examination Formats in Light of GPT-4},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593695},
doi = {10.1145/3593663.3593695},
abstract = {Sudden access to the rapidly improving large language model GPT by OpenAI forces educational institutions worldwide to revisit their exam procedures. In the pre-GPT era, we successfully applied oral and open-book home exams for two courses in the third year of our predominantly remote Software Engineering BSc program. We ask in this paper whether our current open-book exams are still viable or whether a move back to a legally compliant but less scalable oral exam is the only workable alternative. We further compare work-effort estimates between oral and open-book exams and report on differences in throughput and grade distribution over eight years to better understand the impact of examination format on the outcome. Examining GPT-4 on the most recent open-book exams showed that our current Artificial Intelligence and Reactive Programming exams are not GPT v4 proof. Three potential weaknesses of GPT are outlined. We also found that grade distributions have largely been unaffected by the examination format, opening up for a move to oral examinations only if needed. Throughput was higher for open-book exam course instances (73% vs 64%), while fail rates were too (12% vs 7%), with teacher workload increasing even for smaller classes. We also report on our experience regarding effort. Oral examinations are efficient for smaller groups but come with caveats regarding intensity and stress.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {220–225},
numpages = {6},
keywords = {Software Engineering Education, Oral Examinations, Examination Formats, ChatGPT},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1007/978-981-99-7584-6_21,
author = {Yang, Yilin},
title = {IoT Software Vulnerability Detection Techniques through Large Language Model},
year = {2023},
isbn = {978-981-99-7583-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-7584-6_21},
doi = {10.1007/978-981-99-7584-6_21},
abstract = {The explosion of IoT usage provides efficiency and convenience in various fields including daily life, business and information technology. However, there are potential risks in large-scale IoT systems and vulnerability detection plays a significant role in the application of IoT. Besides, traditional approaches like routine security audits are expensive. Thus, substitution methods with lower costs are needed to achieve IoT system vulnerability detection. LLMs, as new tools, show exceptional natural language processing capabilities, meanwhile, static code analysis offers low-cost software analysis avenues. The paper aims at the combination of LLMs and static code analysis, implemented by prompt engineering, which not only expands the application of LLMs but also provides a probability of accomplishing cost-effective IoT vulnerability software detection.},
booktitle = {Formal Methods and Software Engineering: 24th International Conference on Formal Engineering Methods, ICFEM 2023, Brisbane, QLD, Australia, November 21–24, 2023, Proceedings},
pages = {285–290},
numpages = {6},
keywords = {Prompt Engineering, Large Language Model, Vulnerability Detection},
location = {<conf-loc content-type="InPerson">Brisbane, QLD, Australia</conf-loc>}
}

@inproceedings{10.1007/978-3-031-46002-9_23,
author = {Belzner, Lenz and Gabor, Thomas and Wirsing, Martin},
title = {Large Language Model Assisted Software Engineering: Prospects, Challenges, and&nbsp;a&nbsp;Case Study},
year = {2023},
isbn = {978-3-031-46001-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-46002-9_23},
doi = {10.1007/978-3-031-46002-9_23},
abstract = {Large language models such as OpenAI’s GPT and Google’s Bard offer new opportunities for supporting software engineering processes. Large language model assisted software engineering promises to support developers in a conversational way with expert knowledge over the whole software lifecycle. Current applications range from requirements extraction, ambiguity resolution, code and test case generation, code review and translation to verification and repair of software vulnerabilities. In this paper we present our position on the potential benefits and challenges associated with the adoption of language models in software engineering. In particular, we focus on the possible applications of large language models for requirements engineering, system design, code and test generation, code quality reviews, and software process management. We also give a short review of the state-of-the-art of large language model support for software construction and illustrate our position by a case study on the object-oriented development of a simple “search and rescue” scenario.},
booktitle = {Bridging the Gap Between AI and Reality: First International Conference, AISoLA 2023, Crete, Greece, October 23–28, 2023, Proceedings},
pages = {355–374},
numpages = {20},
keywords = {verification, validation, design, requirements, challenges, state-of-the-art, software engineering, LLM-assisted, Bard, GPT, large language model},
location = {<conf-loc content-type="InPerson">Crete, Greece</conf-loc>}
}

@inproceedings{10.1145/3576915.3623120,
author = {Li, Zongjie and Wang, Chaozheng and Wang, Shuai and Gao, Cuiyun},
title = {Protecting Intellectual Property of Large Language Model-Based Code Generation APIs via Watermarks},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623120},
doi = {10.1145/3576915.3623120},
abstract = {The rise of large language model-based code generation (LLCG) has enabled various commercial services and APIs. Training LLCG models is often expensive and time-consuming, and the training data are often large-scale and even inaccessible to the public. As a result, the risk of intellectual property (IP) theft over the LLCG models (e.g., via imitation attacks) has been a serious concern. In this paper, we propose the first watermark (WM) technique to protect LLCG APIs from remote imitation attacks. Our proposed technique is based on replacing tokens in an LLCG output with their "synonyms" available in the programming language. A WM is thus defined as the stealthily tweaked distribution among token synonyms in LLCG outputs. We design six WM schemes (instantiated into over 30 WM passes) which rely on conceptually distinct token synonyms available in programming languages. Moreover, to check the IP of a suspicious model (decide if it is stolen from our protected LLCG API), we propose a statistical tests-based procedure that can directly check a remote, suspicious LLCG API.We evaluate our WM technique on LLCG models fine-tuned from two popular large language models, CodeT5 and CodeBERT. The evaluation shows that our approach is effective in both WM injection and IP check. The inserted WMs do not undermine the usage of normal users (i.e., high fidelity) and incur negligible extra cost. Moreover, our injected WMs exhibit high stealthiness and robustness against powerful attackers; even if they know all WM schemes, they can hardly remove WMs without largely undermining the accuracy of their stolen models.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2336–2350},
numpages = {15},
keywords = {code generation, large language model, watermark},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@inproceedings{10.1109/ICSE48619.2023.00206,
author = {Jiang, Wenxin and Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R. and Sethi, Rohan and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
title = {An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00206},
doi = {10.1109/ICSE48619.2023.00206},
abstract = {Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2463–2475},
numpages = {13},
keywords = {trust, cybersecurity, engineering decision making, software supply chain, deep learning, machine learning, empirical software engineering, software reuse},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3618305.3623587,
author = {Ribeiro, Francisco},
title = {Large Language Models for Automated Program Repair},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623587},
doi = {10.1145/3618305.3623587},
abstract = {This paper introduces two methods for automated program repair (APR) utilizing pre-trained language models. The first method demonstrates program repair as a code completion task and is validated on a dataset of Java programs. The second method, Mentat, leverages OCaml’s parser and type system as fault localization techniques to generate prompts for GPT-3, producing candidate patches. Evaluation results show promising repair rates, with 27% and 39.2% effectiveness, respectively. For OCaml, a comparative study employing an automated validation strategy is presented in which the technique outperforms other tools. Language models are effective at APR, enhancing bug fixing and freeing developers to focus on other critical aspects of software engineering.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {7–9},
numpages = {3},
keywords = {type systems, fault localization, code generation, automated program repair},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@article{10.1016/j.jss.2023.111746,
author = {Yang, Kaiyuan and Wang, Junfeng and Song, Zihua},
title = {Learning a holistic and comprehensive code representation for code summarization},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {203},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111746},
doi = {10.1016/j.jss.2023.111746},
journal = {J. Syst. Softw.},
month = {sep},
numpages = {12},
keywords = {Program comprehension, Deep learning, API, Code summarization}
}

@article{10.1016/j.neucom.2023.126777,
author = {Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Zhou, Ziyi and Huang, Zijie},
title = {Enhancing code summarization with action word prediction},
year = {2024},
issue_date = {Jan 2024},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {563},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2023.126777},
doi = {10.1016/j.neucom.2023.126777},
journal = {Neurocomput.},
month = {jan},
numpages = {19},
keywords = {Code summarization, Action word prediction, Multi-task learning, Deep learning}
}

@inproceedings{10.1109/ICSE48619.2023.00203,
author = {Tufano, Rosalia and Pascarella, Luca and Bavota, Gabriele},
title = {Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00203},
doi = {10.1109/ICSE48619.2023.00203},
abstract = {Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of transformers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2425–2437},
numpages = {13},
keywords = {code recommenders, pre-training},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3613892,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {static analyses, prompt augmentation, finetuning, Program repair},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00126,
author = {Zhu, Qihao and Sun, Zeyu and Zhang, Wenjie and Xiong, Yingfei and Zhang, Lu},
title = {Tare: Type-Aware Neural Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00126},
doi = {10.1109/ICSE48619.2023.00126},
abstract = {Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules.In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar.The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1443–1445},
numpages = {3},
keywords = {neural networks, program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3631974,
author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
title = {A Survey of Learning-based Automated Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631974},
doi = {10.1145/3631974},
abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {55},
numpages = {69},
keywords = {AI and software engineering, neural machine translation, deep learning, Automatic program repair}
}

@inproceedings{10.1145/3611643.3616271,
author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616271},
doi = {10.1145/3611643.3616271},
abstract = {During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI “copilots” (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {172–184},
numpages = {13},
keywords = {Program Repair, Large Language Model, Completion Engine},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3632742,
author = {Sun, Weisong and Fang, Chunrong and Chen, Yuchen and Zhang, Quanjun and Tao, Guanhong and You, Yudu and Han, Tingxu and Ge, Yifei and Hu, Yuling and Luo, Bin and Chen, Zhenyu},
title = {An Extractive-and-Abstractive Framework for Source Code Summarization},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3632742},
doi = {10.1145/3632742},
abstract = {(Source) Code summarization aims to automatically generate summaries/comments for given code snippets in the form of natural language. Such summaries play a key role in helping developers understand and maintain source code. Existing code summarization techniques can be categorized into extractive methods and abstractive methods. The extractive methods extract a subset of important statements and keywords from the code snippet using retrieval techniques and generate a summary that preserves factual details in important statements and keywords. However, such a subset may miss identifier or entity naming, and consequently, the naturalness of the generated summary is usually poor. The abstractive methods can generate human-written-like summaries leveraging encoder-decoder models. However, the generated summaries often miss important factual details. To generate human-written-like summaries with preserved factual details, we propose a novel extractive-and-abstractive framework. The extractive module in the framework performs the task of extractive code summarization, which takes in the code snippet and predicts important statements containing key factual details. The abstractive module in the framework performs the task of abstractive code summarization, which takes in the code snippet and important statements in parallel and generates a succinct and human-written-like natural language summary. We evaluate the effectiveness of our technique, called EACS, by conducting extensive experiments on three datasets involving six programming languages. Experimental results show that EACS significantly outperforms state-of-the-art techniques for all three widely used metrics, including BLEU, METEOR, and ROUGH-L. In addition, the human evaluation demonstrates that the summaries generated by EACS have higher naturalness and informativeness and are more relevant to given code snippets.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {Abstractive Code Summarization Program Comprehension, Extractive Code Summarization, Code Summarization}
}

@inproceedings{10.1145/3583780.3614744,
author = {Koreeda, Yuta and Morishita, Terufumi and Imaichi, Osamu and Sogawa, Yasuhiro},
title = {LARCH: Large Language Model-based Automatic Readme Creation with Heuristics},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614744},
doi = {10.1145/3583780.3614744},
abstract = {Writing a readme is a crucial aspect of software development as it plays a vital role in managing and reusing program code. Though it is a pain point for many developers, automatically creating one remains a challenge even with the recent advancements in large language models (LLMs), because it requires generating an abstract description from thousands of lines of code. In this demo paper, we show that LLMs are capable of generating a coherent and factually correct readmes if we can identify a code fragment that is representative of the repository. Building upon this finding, we developed LARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages representative code identification with heuristics and weak supervision. Through human and automated evaluations, we illustrate that LARCH can generate coherent and factually correct readmes in the majority of cases, outperforming a baseline that does not rely on representative code identification. We have made LARCH open-source and provided a cross-platform Visual Studio Code interface and command-line interface, accessible at https://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's capabilities is available at https://youtu.be/ZUKkh5ED-O4.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5066–5070},
numpages = {5},
keywords = {weak supervision, software development, large language model},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1109/ICSE48619.2023.00125,
author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
title = {Impact of Code Language Models on Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00125},
doi = {10.1109/ICSE48619.2023.00125},
abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task.Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%--1,267% improvement to CLMs and enables them to fix 46%--164% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs.This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1430–1442},
numpages = {13},
keywords = {deep learning, fine-tuning, code language model, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00112,
author = {Parasaram, Nikhil and Barr, Earl T. and Mechtaev, Sergey},
title = {Rete: Learning Namespace Representation for Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00112},
doi = {10.1109/ICSE48619.2023.00112},
abstract = {A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, defuse chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that Rete generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31% and 59% over the existing state of the art.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1264–1276},
numpages = {13},
keywords = {variable representation, patch prioritisation, deep learning, program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3568813.3600130,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Haaranen, Lassi and Hellas, Arto},
title = {Evaluating Distance Measures for Program Repair},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600130},
doi = {10.1145/3568813.3600130},
abstract = {Background and Context: Struggling with programming assignments while learning to program is a common phenomenon in programming courses around the world. Supporting struggling students is a common theme in Computing Education Research (CER), where a wide variety of support methods have been created and evaluated. An important stream of research here focuses on program repair, where methods for automatically fixing erroneous code are used for supporting students as they debug their code. Work in this area has so far assessed the performance of the methods by evaluating the closeness of the proposed fixes to the original erroneous code. The evaluations have mainly relied on the use of edit distance measures such as the sequence edit distance and there is a lack of research on which distance measure is the most appropriate. Objectives: Provide insight into measures for quantifying the distance between erroneous code written by a student and a proposed change. We conduct the evaluation in an introductory programming context, where insight into the distance measures can provide help in choosing a suitable metric that can inform which fixes should be suggested to novices. Method: A team of five experts annotated a subset of the Dublin dataset, creating solutions for over a thousand erroneous programs written by students. We evaluated how the prominent edit distance measures from the CER literature compare against measures used in Natural Language Processing (NLP) tasks for retrieving the experts’ solutions from a pool of proposed solutions. We also evaluated how the expert-generated solutions compare against the solutions proposed by common program repair algorithms. The annotated dataset and the evaluation code are published as part of the work. Findings: Our results highlight that the ROUGE score, classically used for evaluating the performance of machine summarization tasks, performs well as an evaluation and selection metric for program repair. We also highlight the practical utility of NLP metrics, which allow an easier interpretation and comparison of the performance of repair techniques when compared to the classic methods used in the CER literature. Implications: Our study highlights the variety of distance metrics used for comparing source codes. We find issues with the classically used distance measures that can be combated by using NLP metrics. Based on our findings, we recommend including NLP metrics, and in particular, the ROUGE metric, in evaluations when considering new program repair methodologies. We also suggest incorporating NLP metrics into other areas where source codes are compared, including plagiarism detection.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {495–507},
numpages = {13},
keywords = {program repair, natural language processing, feedback, educational data mining, distance metrics, distance measures, dataset, computing education, bug fixing, automatic program repair, automated program repair, ROUGE, BLEU},
location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
series = {ICER '23}
}

@inproceedings{10.5555/3618408.3618894,
author = {Guo, Daya and Xu, Canwen and Duan, Nan and Yin, Jian and McAuley, Julian},
title = {LongCoder: a long-range pre-trained language model for code completion},
year = {2023},
publisher = {JMLR.org},
abstract = {In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. Long-Coder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens --bridge tokens and memory tokens -- to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {486},
numpages = {10},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.1145/3631972,
author = {Zirak, Armin and Hemmati, Hadi},
title = {Improving Automated Program Repair with Domain Adaptation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631972},
doi = {10.1145/3631972},
abstract = {Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set’s (“Domain Shift”). In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning and two APR models on 2672 bugs from 12 projects. The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05% and CodeXGLUE by 48.78%, in terms of “Exact Match”. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of “Exposure Bias”). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76% and 17.62% for TFix and CodeXGLUE, respectively.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {Domain Adaptation, CodeBERT, Transformers, Neural Machine Translation, Deep Learning, Automated Program Repair}
}

@inproceedings{10.1145/3624062.3624280,
author = {Zahra, Zaynab and Li, Zihao and Filgueira, Rosa},
title = {Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624280},
doi = {10.1145/3624062.3624280},
abstract = {This paper introduces Laminar, a novel serverless framework based on dispel4py, a parallel stream-based dataflow library. Laminar efficiently manages streaming workflows and components through a dedicated registry, offering a seamless serverless experience. Leveraging large lenguage models, Laminar enhances the framework with semantic code search, code summarization, and code completion. This contribution enhances serverless computing by simplifying the execution of streaming computations, managing data streams more efficiently, and offering a valuable tool for both researchers and practitioners.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {2009–2020},
numpages = {12},
keywords = {transformers, streaming applications, serverless computing, semantic code search, dispel4py, code summarization, code completion},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC-W '23}
}

@inproceedings{10.1007/978-3-031-36272-9_74,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Hellas, Arto and Denny, Paul},
title = {Automated Program Repair Using Generative Models for&nbsp;Code Infilling},
year = {2023},
isbn = {978-3-031-36271-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36272-9_74},
doi = {10.1007/978-3-031-36272-9_74},
abstract = {In educational settings, automated program repair techniques serve as a feedback mechanism to guide students working on their programming assignments. Recent work has investigated using large language models (LLMs) for program repair. In this area, most of the attention has been focused on using proprietary systems accessible through APIs. However, the limited access and control over these systems remain a block to their adoption and usage in education. The present work studies the repairing capabilities of open large language models. In particular, we focus on a recent family of generative models, which, on top of standard left-to-right program synthesis, can also predict missing spans of code at any position in a program. We experiment with one of these models on four programming datasets and show that we can obtain good repair performance even without additional training.},
booktitle = {Artificial Intelligence in Education: 24th International Conference, AIED 2023, Tokyo, Japan, July 3–7, 2023, Proceedings},
pages = {798–803},
numpages = {6},
keywords = {Computer Science Education, Large Language Models, Program repair},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3617555.3617877,
author = {Amasaki, Sousuke},
title = {On Effectiveness of Further Pre-training on BERT Models for Story Point Estimation},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617877},
doi = {10.1145/3617555.3617877},
abstract = {CONTEXT: Recent studies on story point estimation used deep learning-based language models.  
These language models were pre-trained on general corpora.  
However, using language models further pre-trained with specific corpora might be effective.  
OBJECTIVE: To examine the effectiveness of further pre-trained language models for the predictive performance of story point estimation.  
METHOD: Two types of further pre-trained language models, namely, domain-specific and repository-specific models, were compared with off-the-shelf models and Deep-SE. The estimation performance was evaluated with 16 project data.  
RESULTS: The effectiveness of domain-specific and repository-specific models were limited though they were better than the base model they further pre-trained.  
CONCLUSION: The effect of further pre-training was small. Large off-the-shelf models might be better to be chosen.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {49–53},
numpages = {5},
keywords = {story points, further pre-training, effort estimation, BERT},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {PROMISE 2023}
}

@inproceedings{10.1145/3611643.3616256,
author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
title = {RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616256},
doi = {10.1145/3611643.3616256},
abstract = {Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70% to 54.15% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {146–158},
numpages = {13},
keywords = {Retrieval-augmented generation, Pretrained language models, Neural networks, Automated program repair},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1007/s10664-023-10384-x,
author = {Gao, Yuexiu and Zhang, Hongyu and Lyu, Chen},
title = {EnCoSum: enhanced semantic features for multi-scale multi-modal source code summarization},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10384-x},
doi = {10.1007/s10664-023-10384-x},
abstract = {Code summarization aims to generate concise natural language descriptions for a piece of code, which can help developers comprehend the source code. Analysis of current work shows that the extraction of syntactic and semantic features of source code is crucial for generating high-quality summaries. To provide a more comprehensive feature representation of source code from different perspectives, we propose an approach named EnCoSum, which enhances semantic features for the multi-scale multi-modal code summarization method. This method complements our previously proposed M2TS approach (multi-scale multi-modal approach based on Transformer for source code summarization), which uses the multi-scale method to capture Abstract Syntax Trees (ASTs) structural information more completely and accurately at multiple local and global levels. In addition, we devise a new cross-modal fusion method to fuse source code and AST features, which can highlight key features in each modality that help generate summaries. To obtain richer semantic information, we improve M2TS. First, we add data flow and control flow to ASTs, and added-edge ASTs, called Enhanced-ASTs (E-ASTs). In addition, we introduce method name sequences extracted in the source code, which exist more knowledge about critical tokens in the corresponding summaries and can help the model generate higher-quality summaries. We conduct extensive experiments on processed Java and Python datasets and evaluate our approach via the four most commonly used machine translation metrics. The experimental results demonstrate that EnCoSum is effective and outperforms current state-of-the-art methods. Further, we perform ablation experiments on each of the model’s key components, and the results show that they all contribute to the performance of EnCoSum.},
journal = {Empirical Softw. Engg.},
month = {sep},
numpages = {43},
keywords = {Deep learning, Cross-modal fusion, Method name sequences, Abstract syntax trees, Code summarization}
}

@inproceedings{10.1145/3617553.3617887,
author = {Fulcini, Tommaso and Torchiano, Marco},
title = {Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks?},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617553.3617887},
doi = {10.1145/3617553.3617887},
abstract = {Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-the-art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies.  
In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers.  
To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-the-art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience.  
The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution.},
booktitle = {Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
pages = {22–28},
numpages = {7},
keywords = {Software Lifecycle, Software Engineering, Large Language Model, Gamification, Artificial Intelligence},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {Gamify 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00073,
author = {Mu, Fangwen and Chen, Xiao and Shi, Lin and Wang, Song and Wang, Qing},
title = {Developer-Intent Driven Code Comment Generation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00073},
doi = {10.1109/ICSE48619.2023.00073},
abstract = {Existing automatic code comment generators mainly focus on producing a general description of functionality for a given code snippet without considering developer intentions. However, in real-world practice, comments are complicated, which often contain information reflecting various intentions of developers, e.g., functionality summarization, design rationale, implementation details, code properties, etc. To bridge the gap between automatic code comment generation and real-world comment practice, we define Developer-Intent Driven Code Comment Generation, which can generate intent-aware comments for the same source code with different intents. To tackle this challenging task, we propose DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents. Our approach is evaluated on two real-world Java datasets, and the experimental results show that our approach outperforms the state-of-the-art baselines. A human evaluation also confirms the significant potential of applying DOME in practical usage, enabling developers to comment code effectively according to their own needs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {768–780},
numpages = {13},
keywords = {automated comment-intent labeling, intent-controllable comment generation, code comment generation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00111,
author = {Jiang, Nan and Lutellier, Thibaud and Lou, Yiling and Tan, Lin and Goldwasser, Dan and Zhang, Xiangyu},
title = {KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00111},
doi = {10.1109/ICSE48619.2023.00111},
abstract = {Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases.We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1251–1263},
numpages = {13},
keywords = {deep learning, abstract syntax tree, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3631975,
author = {Zhu, Tingwei and Li, Zhong and Pan, Minxue and Shi, Chaoxuan and Zhang, Tian and Pei, Yu and Li, Xuandong},
title = {Deep is Better? An Empirical Comparison of Information Retrieval and Deep Learning Approaches to Code Summarization},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631975},
doi = {10.1145/3631975},
abstract = {Code summarization aims to generate short functional descriptions for source code to facilitate code comprehension. While Information Retrieval (IR) approaches that leverage similar code snippets and corresponding summaries have led the early research, Deep Learning (DL) approaches that use neural models to capture statistical properties between code and summaries are now mainstream. Although some preliminary studies suggest that IR approaches are more effective in some cases, it is currently unclear how effective the existing approaches can be in general, where and why IR/DL approaches perform better, and whether the integration of IR and DL can achieve better performance. Consequently, there is an urgent need for a comprehensive study of the IR and DL code summarization approaches to provide guidance for future development in this area. This paper presents the first large-scale empirical study of 18 IR, DL, and hybrid code summarization approaches on five benchmark datasets. We extensively compare different types of approaches using automatic metrics, we conduct quantitative and qualitative analyses of where and why IR and DL approaches perform better, respectively, and we also study hybrid approaches for assessing the effectiveness of integrating IR and DL. The study shows that the performance of IR approaches should not be underestimated, that while DL models perform better in predicting tokens from method signatures and capturing structural similarities in code, simple IR approaches tend to perform better in the presence of code with high similarity or long reference summaries, and that existing hybrid approaches do not perform as well as individual approaches in their respective areas of strength. Based on our findings, we discuss future research directions for better code summarization.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {information retrieval, deep learning, code summarization}
}

@inproceedings{10.1145/3555776.3577762,
author = {Kim, Jisung and Lee, Byungjeong},
title = {MCRepair: Multi-Chunk Program Repair via Patch Optimization with Buggy Block},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577762},
doi = {10.1145/3555776.3577762},
abstract = {Automated program repair (APR) is a technology that identifies and repairs bugs automatically. However, repairing multi-chunk bugs remains a long-standing and challenging problem because an APR technique must consider dependencies and then reduce the large patch space. In addition, little is known about how to combine individual candidate patches even though multi-chunk bugs require combinations. Therefore, we propose a novel APR technique called multi-code repair (MCRepair), which applies a buggy block, patch optimization, and CodeBERT to target multi-chunk bugs. A buggy block is a novel method that binds buggy chunks into a multi-buggy chunk and preprocesses the chunk with its buggy contexts for patch space reduction and dependency problems. Patch optimization is a novel strategy that effectively combines the generated candidate patches with patch space reduction. In addition, CodeBERT, a BERT for source code datasets, is fine-tuned to address the lack of datasets and out-of-vocabulary problems. We conducted several experiments to evaluate our approach on six project modules of Defects4J. In the experiments using Defects4J, MCRepair repaired 66 bugs, including 22 multi-chunk bugs. Moreover, it fixed 19 unique bugs, including nine multi-chunk bugs, and improved 45--266% performance than the baselines.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1508–1515},
numpages = {8},
keywords = {deep learning, patch optimization, buggy block, automated program repair},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00091,
author = {Zhu, Tingwei and Li, Zhong and Pan, Minxue and Shi, Chaoxuan and Zhang, Tian and Pe, Yu and Li, Xuandong},
title = {Revisiting Information Retrieval and Deep Learning Approaches for Code Summarization},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00091},
doi = {10.1109/ICSE-Companion58688.2023.00091},
abstract = {Code summarization refers to the procedure of creating short descriptions that outline the semantics of source code snippets. Existing code summarization approaches can be broadly classified into Information Retrieval (IR)-based and Deep Learning (DL)-based approaches. However, their effectiveness, and especially their strengths and weaknesses, remain largely understudied. Existing evaluations use different benchmarks and metrics, making performance comparisons of these approaches susceptible to bias and potentially yielding misleading results. For example, the DL-based approaches typically show better code summarization performance in their original papers [1], [2]. However, Gros et al. [3] report that a naive IR approach could achieve comparable (or even better) performance to the DL-based ones. In addition, some recent work [4], [5] suggests that incorporating IR techniques can improve the DL-based approaches. To further advance code summarization techniques, it is critical that we have a good understanding of how IR-based and DL-based approaches perform on different datasets and in terms of different metrics.Prior works have studied some aspects of code summarization, such as the factors affecting performance evaluation [6] and the importance of data preprocessing [7], etc. In this paper, we focus on the study of the IR-based and DL-based code summarization approaches to enhance the understanding and design of more advanced techniques. We first compare the IR-based and DL-based approaches under the same experimental settings and benchmarks, then study their strengths and limitations through quantitative and qualitative analyses. Finally, we propose a simpler but effective strategy to combine IR and DL to further improve code summarization.Four IR-based approaches and two DL-based approaches are investigated with regard to representativeness and diversity. For IR-based approaches, we select three BM25-based approaches (i.e., BM25-spl, BM25-ast, BM25-alpha) and one nearest neighbor-based approach NNGen [8], which are often compared as baselines in prior works. They retrieve the most similar code from the database and directly output the corresponding summary. BM25-based approaches are implemented by Lucene [9]. Taking code forms as input, BM25-spl splits the CamelCase and snake_case in original source code tokens, BM25-ast obtains sequence representations using pre-order Abstract Syntax Tree (AST) traversal, and BM25-alpha keeps only the alpha tokens in the code. For DL-based approaches, we choose the state-of-the-art pre-trained model PLBART [2] and the trained-from-scratch model SiT[1].We adopt four widely used Java datasets, namely TLC [10], CSN [11], HDC [12], and FCM [13] as our subject datasets. TLC and HDC are method-split datasets, where methods in the same project are randomly split into training/validation/test sets. CSN and FCM are project-split datasets, where examples from the same project exist in only one partition. We further process the four datasets to build cleaner datasets by removing examples that have syntax errors, empty method bodies, and too long or too short sequence lengths, etc. We also remove the duplicate examples in the validation and test sets.To comprehensively and systematically evaluate the performance of the code summarization approaches, we adopt three widely used metrics, i.e., BLEU (both C-BLEU and S-BLEU are included), ROUGE, and METEOR, in our experiments. Effectiveness. We conduct a comprehensive comparison of the six studied approaches under exactly the same settings and datasets. Table I shows the experimental results obtained on four subject datasets in terms of four metrics.Comparing by metrics from Table I, we can observe that overall there are large variations in the approach rankings and score gaps when using different metrics for evaluation. DL-based approaches generally achieve better performance than IR-based approaches in terms of METEOR and ROUGE-L. IR-based approaches achieve comparable or even better C-BLEU scores, but have lower S-BLEU scores than the DL-based approaches. This shows that different metrics have a large impact on the results of the approach evaluation. Different metrics are needed to evaluate the code summarization approaches.Considering different datasets, the pre-trained DL-based approach PLBART performs best among the six approaches studied. On the other hand, we notice that the IR-based approaches, despite their simplicity, also achieve comparable or even better performance, especially on method-split datasets. For example, the C-BLEU scores of BM25-spl on TLC and HDC are the highest among all approaches. Therefore, although DL-based methods usually show better performance for code summarization, we should not overlook the capabilities of IR-based methods.Strengths. To evaluate how the similarity between the training and test codes affects the performance of the approaches, we use the Retrieval-Similarity metric, as Rencos [4] did, to measure the token-level similarity between a test code and its most similar training code. Based on this, we examine how the BLEU score of each approach varies as the Retrieval-Similarity value changes on the four subject datasets.Figure 1 shows the results, from which we observe that IR-based approaches perform better than DL-based ones when the Retrieval-Similarity values are higher. Through qualitative analysis of examples with high retrieval similarity, we find that due to the cloning phenomenon, similar codes have similar summaries, so IR-based approaches tend to perform better on examples with high Retrieval-Similarity values.Integration. Based on previous findings, we are motivated to design a simpler integration approach. We propose to take advantage of Retrieval-Similarity to decide whether to use the IR or DL approach to generate a summary for the input code. Specifically, we first use Lucene to retrieve a similar code for the input and compute a Retrieval-Similarity value between them. If the value is higher than a similarity threshold, we directly use the IR summary. Otherwise, we choose the DL model to get the output. To determine the similarity threshold, we conduct grid search on the validation set. The similarity achieving the highest metric score on validation set is set as the final threshold.We choose the best DL model PLBART and the best IR approach BM25-spl for integration and evaluate our approach on all four cleaned datasets. The effectiveness results are shown in the 'Ours' row of Table I. From the table, we can see that our integration is effective and achieves state-of-the-art results. Not only does it outperform a single approach, but the scores it achieves are higher than all the previous highest scores in our experiments on all metrics across all datasets.In summary, our study shows that the IR and DL approaches have their own strengths in terms of performance using different metrics on different datasets. Although IR-based approaches are simpler, they can still achieve comparable or even better performance in some cases, especially in the presence of high-similarity code. Based on the results, we propose a simple integration approach that achieves state-of-the-art results. Our study shows that it is not enough to focus on the DL model alone. Taking advantage of IR approaches is a promising direction for improving code summarization. Future work should explore the incorporation of more types of information and more advanced integration methods.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {328–329},
numpages = {2},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.neucom.2023.126385,
author = {Song, Zixuan and Zeng, Hui and Shang, Xiuwei and Li, Guanxi and Li, Hui and Guo, Shikai},
title = {An data augmentation method for source code summarization},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {549},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2023.126385},
doi = {10.1016/j.neucom.2023.126385},
journal = {Neurocomput.},
month = {sep},
numpages = {15},
keywords = {Word replacement, Clustering, Data augmentation, Code summary}
}

@article{10.1016/j.jpdc.2023.104747,
author = {Zhang, Xiangping and Liu, Jianxun and Shi, Min},
title = {A parallel deep learning-based code clone detection model},
year = {2023},
issue_date = {Nov 2023},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2023.104747},
doi = {10.1016/j.jpdc.2023.104747},
journal = {J. Parallel Distrib. Comput.},
month = {nov},
numpages = {12},
keywords = {Temporal convolutional network, Code representation, Abstract syntax tree, Code clone detection}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00098,
author = {Chimalakonda, Sridhar and Das, Debeshee and Mathai, Alex and Tamilselvam, Srikanth and Kumar, Atul},
title = {The Landscape of Source Code Representation Learning in AI-Driven Software Engineering Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00098},
doi = {10.1109/ICSE-Companion58688.2023.00098},
abstract = {Appropriate representation of source code and its relevant properties form the backbone of Artificial Intelligence (AI)/ Machine Learning (ML) pipelines for various software engineering (SE) tasks such as code classification, bug prediction, code clone detection, and code summarization. In the literature, researchers have extensively experimented with different kinds of source code representations (syntactic, semantic, integrated, customized) and ML techniques such as pre-trained BERT models. In addition, it is common for researchers to create hand-crafted and customized source code representations for an appropriate SE task. In a 2018 survey [1], Allamanis et al. listed nearly 35 different ways of of representing source code for different SE tasks like Abstract Syntax Trees (ASTs), customized ASTs, Control Flow Graphs (CFGs), Data Flow Graphs (DFGs) and so on. The main goal of this tutorial is two-fold (i) Present an overview of the state-of-the-art of source code representations and corresponding ML pipelines with an explicit focus on the merits and demerits of each of the representations (ii) Practical challenges in infusing different code-views in the state-of-the-art ML models and future research directions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {342–343},
numpages = {2},
keywords = {machine learning, code representation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3622815,
author = {Wang, Shangwen and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Lei, Yan and Mao, Xiaoguang},
title = {Two Birds with One Stone: Boosting Code Generation and Code Search via a Generative Adversarial Network},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622815},
doi = {10.1145/3622815},
abstract = {Automatically transforming developers' natural language descriptions into source code has been a longstanding goal in software engineering research.  
Two types of approaches have been proposed in the literature to achieve this: code generation, which involves generating a new code snippet, and code search, which involves reusing existing code.  
However, despite existing efforts, the effectiveness of the state-of-the-art techniques remains limited.  
To seek for further advancement, our insight is that code generation and code search can help overcome the limitation of each other:  
the code generator can benefit from feedback on the quality of its generated code, which can be provided by the code searcher, while the code searcher can benefit from the additional training data augmented by the code generator to better understand code semantics.  
Drawing on this insight, we propose a novel approach that combines code generation and code search techniques using a generative adversarial network (GAN), enabling mutual improvement through the adversarial training.  
Specifically, we treat code generation and code search as the generator and discriminator in the GAN framework, respectively, and incorporate several customized designs for our tasks.  
We evaluate our approach in eight different settings, and consistently observe significant performance improvements for both code generation and code search.  
For instance, when using NatGen, a state-of-the-art code generator, as the generator and GraphCodeBERT, a state-of-the-art code searcher, as the discriminator, we achieve a 32% increase in CodeBLEU score for code generation, and a 12% increase in mean reciprocal rank for code search on a large-scale Python dataset, compared to their original performances.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {239},
numpages = {30},
keywords = {Generative Adversarial Network, Code Search, Code Generation}
}

@article{10.1145/3625293,
author = {Ismayilzada, Elkhan and Rahman, Md Mazba Ur and Kim, Dongsun and Yi, Jooyong},
title = {Poracle: Testing Patches under Preservation Conditions to Combat the Overfitting Problem of Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3625293},
doi = {10.1145/3625293},
abstract = {To date, the users of test-driven program repair tools suffer from the overfitting problem; a generated patch may pass all available tests without being correct. In the existing work, users are treated as merely passive consumers of the tests. However, what if they are willing to modify the test to better assess the patches obtained from a repair tool? In this work, we propose a novel semi-automatic patch-classification methodology named Poracle. Our key contributions are three-fold. First, we design a novel lightweight specification method that reuses the existing test. Specifically, the users extend the existing failing test with a preservation condition—the condition under which the patched and pre-patched versions should produce the same output. Second, we develop a fuzzer that performs differential fuzzing with a test containing a preservation condition. Once we find an input that satisfies a specified preservation condition but produces different outputs between the patched and pre-patched versions, we classify the patch as incorrect with high confidence. We show that our approach is more effective than the four state-of-the-art patch classification approaches. Last, we show through a user study that the users find our semi-automatic patch assessment method more effective and preferable than the manual assessment.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {44},
numpages = {39},
keywords = {preservation condition, patch classification, patch validation, overfitting problem, Automated program repair}
}

@inproceedings{10.1109/ICSE48619.2023.00129,
author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
title = {Automated Program Repair in the Era of Large Pre-Trained Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00129},
doi = {10.1109/ICSE48619.2023.00129},
abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1482–1494},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00109,
author = {Motwani, Manish and Brun, Yuriy},
title = {Better Automatic Program Repair by Using Bug Reports and Tests Together},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00109},
doi = {10.1109/ICSE48619.2023.00109},
abstract = {Automated program repair is already deployed in industry, but concerns remain about repair quality. Recent research has shown that one of the main reasons repair tools produce incorrect (but seemingly correct) patches is imperfect fault localization (FL). This paper demonstrates that combining information from natural-language bug reports and test executions when localizing faults can have a significant positive impact on repair quality. For example, existing repair tools with such FL are able to correctly repair 7 defects in the Defects4J benchmark that no prior tools have repaired correctly.We develop, Blues, the first information-retrieval-based, statement-level FL technique that requires no training data. We further develop RAFL, the first unsupervised method for combining multiple FL techniques, which outperforms a supervised method. Using RAFL, we create SBIR by combining Blues with a spectrum-based (SBFL) technique. Evaluated on 815 real-world defects, SBIR consistently ranks buggy statements higher than its underlying techniques.We then modify three state-of-the-art repair tools, Arja, SequenceR, and SimFix, to use SBIR, SBFL, and Blues as their internal FL. We evaluate the quality of the produced patches on 689 real-world defects. Arja and SequenceR significantly benefit from SBIR: Arja using SBIR correctly repairs 28 defects, but only 21 using SBFL, and only 15 using Blues; SequenceR using SBIR correctly repairs 12 defects, but only 10 using SBFL, and only 4 using Blues. SimFix, (which has internal mechanisms to overcome poor FL), correctly repairs 30 defects using SBIR and SBFL, but only 13 using Blues. Our work is the first investigation of simultaneously using multiple software artifacts for automated program repair, and our promising findings suggest future research in this directions is likely to be fruitful.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1225–1237},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3630010,
author = {Yang, Guang and Zhou, Yu and Yang, Wenhua and Yue, Tao and Chen, Xiang and Chen, Taolue},
title = {How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630010},
doi = {10.1145/3630010},
abstract = {Pre-trained code generation models (PCGMs) have been widely applied in neural code generation which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this paper, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs, from a model robustness perspective. Specifically, we propose a novel approach, named RADAR (neuRAl coDe generAtor Robustifier). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input, but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task, and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
keywords = {Pre-trained model, Passive defense, Robustness, Adversarial examples, Code generation}
}

@article{10.1145/3628161,
author = {Xie, Yutao and Lin, Jiayi and Dong, Hande and Zhang, Lei and Wu, Zhonghai},
title = {Survey of Code Search Based on Deep Learning},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628161},
doi = {10.1145/3628161},
abstract = {Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given natural language query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework that maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-step process: query semantics modeling, code semantics modeling, and matching modeling, which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {54},
numpages = {42},
keywords = {pre-training, deep learning, natural language processing, code understanding, Code search}
}

@inproceedings{10.1145/3623476.3623522,
author = {Ribeiro, Francisco and de Macedo, José Nuno Castro and Tsushima, Kanae and Abreu, Rui and Saraiva, João},
title = {GPT-3-Powered Type Error Debugging: Investigating the Use of Large Language Models for Code Repair},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623522},
doi = {10.1145/3623476.3623522},
abstract = {Type systems are responsible for assigning types to terms in programs. That way, they enforce the actions that can be taken and can, consequently, detect type errors during compilation. However, while they are able to flag the existence of an error, they often fail to pinpoint its cause or provide a helpful error message. Thus, without adequate support, debugging this kind of errors can take a considerable amount of effort. Recently, neural network models have been developed that are able to understand programming languages and perform several downstream tasks. We argue that type error debugging can be enhanced by taking advantage of this deeper understanding of the language’s structure. In this paper, we present a technique that leverages GPT-3’s capabilities to automatically fix type errors in OCaml programs. We perform multiple source code analysis tasks to produce useful prompts that are then provided to GPT-3 to generate potential patches. Our publicly available tool, Mentat, supports multiple modes and was validated on an existing public dataset with thousands of OCaml programs. We automatically validate successful repairs by using Quickcheck to verify which generated patches produce the same output as the user-intended fixed version, achieving a 39% repair rate. In a comparative study, Mentat outperformed two other techniques in automatically fixing ill-typed OCaml programs.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {111–124},
numpages = {14},
keywords = {GPT-3, Fault Localization, Code Generation, Automated Program Repair},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inproceedings{10.1145/3611643.3613078,
author = {Li, Haonan and Hao, Yu and Zhai, Yizhuo and Qian, Zhiyun},
title = {Assisting Static Analysis with Large Language Models: A ChatGPT Experiment},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613078},
doi = {10.1145/3611643.3613078},
abstract = {Recent advances of Large Language Models (LLMs), e.g., ChatGPT, exhibited strong capabilities of comprehending and responding to questions across a variety of domains. Surprisingly, ChatGPT even possesses a strong understanding of program code. In this paper, we investigate where and how LLMs can assist static analysis by asking appropriate questions. In particular, we target a specific bug-finding tool, which produces many false positives from the static analysis. In our evaluation, we find that these false positives can be effectively pruned by asking carefully constructed questions about function-level behaviors or function summaries. Specifically, with a pilot study of 20 false positives, we can successfully prune 8 out of 20 based on GPT-3.5, whereas GPT-4 had a near-perfect result of 16 out of 20, where the four failed ones are not currently considered/supported by our questions, e.g., involving concurrency. Additionally, it also identified one false negative case (a missed bug). We find LLMs a promising tool that can enable a more effective and efficient program analysis.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2107–2111},
numpages = {5},
keywords = {static analysis, large language model, bug detection},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.24963/ijcai.2023/249,
author = {Ma, Yi-Fan and Du, Yali and Li, Ming},
title = {Capturing the long-distance dependency in the control flow graph via structural-guided attention for bug localization},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/249},
doi = {10.24963/ijcai.2023/249},
abstract = {To alleviate the burden of software maintenance, bug localization, which aims to automatically locate the buggy source files based on the bug report, has drawn significant attention in the software mining community. Recent studies indicate that the program structure in source code carries more semantics reflecting the program behavior, which is beneficial for bug localization. Benefiting from the rich structural information in the Control Flow Graph (CFG), CFG-based bug localization methods have achieved the state-of-the-art performance. Existing CFG-based methods extract the semantic feature from the CFG via the graph neural network. However, the step-wise feature propagation in the graph neural network suffers from the problem of information loss when the propagation distance is long, while the long-distance dependency is rather common in the CFG. In this paper, we argue that the long-distance dependency is crucial for feature extraction from the CFG, and propose a novel bug localization model named sgAttention. In sgAttention, a particularly designed structural-guided attention is employed to globally capture the information in the CFG, where features of irrelevant nodes are masked for each node to facilitate better feature extraction from the CFG. Experimental results on four widely-used open-source software projects indicate that sgAttention averagely improves the state-of-the-art bug localization methods by 32.9% and 29.2% and the state-of-the-art pre-trained models by 5.8% and 4.9% in terms of MAP and MRR, respectively.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {249},
numpages = {9},
location = {<conf-loc>, <city>Macao</city>, <country>P.R.China</country>, </conf-loc>},
series = {IJCAI '23}
}

@inproceedings{10.1145/3583780.3615047,
author = {Hoq, Muntasir and Chilla, Sushanth Reddy and Ahmadi Ranjbar, Melika and Brusilovsky, Peter and Akram, Bita},
title = {SANN: Programming Code Representation Using Attention Neural Network with Optimized Subtree Extraction},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615047},
doi = {10.1145/3583780.3615047},
abstract = {Automated analysis of programming data using code representation methods offers valuable services for programmers, from code completion to clone detection to bug detection. Recent studies show the effectiveness of Abstract Syntax Trees (AST), pre-trained Transformer-based models, and graph-based embeddings in programming code representation. However, pre-trained large language models lack interpretability, while other embedding-based approaches struggle with extracting important information from large ASTs. This study proposes a novel Subtree-based Attention Neural Network (SANN) to address these gaps by integrating different components: an optimized sequential subtree extraction process using Genetic algorithm optimization, a two-way embedding approach, and an attention network. We investigate the effectiveness of SANN by applying it to two different tasks: program correctness prediction and algorithm detection on two educational datasets containing both small and large-scale code snippets written in Java and C, respectively. The experimental results show SANN's competitive performance against baseline models from the literature, including code2vec, ASTNN, TBCNN, CodeBERT, GPT-2, and MVG, regarding accurate predictive power. Finally, a case study is presented to show the interpretability of our model prediction and its application for an important human-centered computing application, student modeling. Our results indicate the effectiveness of the SANN model in capturing important syntactic and semantic information from students' code, allowing the construction of accurate student models, which serve as the foundation for generating adaptive instructional support such as individualized hints and feedback.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {783–792},
numpages = {10},
keywords = {static analysis, program correctness prediction, program analysis, code representation, algorithm detection},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1145/3587102.3588814,
author = {Cipriano, Bruno Pereira and Alves, Pedro},
title = {GPT-3 vs Object Oriented Programming Assignments: An Experience Report},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588814},
doi = {10.1145/3587102.3588814},
abstract = {Recent studies show that AI-driven code generation tools, such as Large Language Models, are able to solve most of the problems usually presented in introductory programming classes. However, it is still unknown how they cope with Object Oriented Programming assignments, where the students are asked to design and implement several interrelated classes (either by composition or inheritance) that follow a set of best-practices. Since the majority of the exercises in these tools' training dataset are written in English, it is also unclear how well they function with exercises published in other languages.In this paper, we report our experience using GPT-3 to solve 6 real-world tasks used in an Object Oriented Programming course at a Portuguese University and written in Portuguese. Our observations, based on an objective evaluation of the code, performed by an open-source Automatic Assessment Tool, show that GPT-3 is able to interpret and handle direct functional requirements, however it tends not to give the best solution in terms of object oriented design. We perform a qualitative analysis of GPT-3's output, and gather a set of recommendations for computer science educators, since we expect students to use and abuse this tool in their academic work.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {61–67},
numpages = {7},
keywords = {teaching, programming assignments, object oriented programming, large language models, GPT-3},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@article{10.1016/j.infsof.2023.107336,
author = {Takerngsaksiri, Wannita and Tantithamthavorn, Chakkrit and Li, Yuan-Fang},
title = {Syntax-aware on-the-fly code completion},
year = {2024},
issue_date = {Jan 2024},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {165},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107336},
doi = {10.1016/j.infsof.2023.107336},
journal = {Inf. Softw. Technol.},
month = {jan},
numpages = {15},
keywords = {Code completion, Multi-task learning}
}

@article{10.1145/3637230,
author = {Biringa, Chidera and Kul, Gökhan},
title = {PACE: A Program Analysis Framework for Continuous Performance Prediction},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637230},
doi = {10.1145/3637230},
abstract = {Software development teams establish elaborate continuous integration pipelines containing automated test cases to accelerate the development process of software. Automated tests help to verify the correctness of code modifications decreasing the response time to changing requirements. However, when the software teams do not track the performance impact of pending modifications, they may need to spend considerable time refactoring existing code. This paper presents PACE, a program analysis framework that provides continuous feedback on the performance impact of pending code updates. We design performance microbenchmarks by mapping the execution time of functional test cases given a code update. We map microbenchmarks to code stylometry features and feed them to predictors for performance predictions. Our experiments achieved significant performance in predicting code performance, outperforming current state-of-the-art by 75% on neural-represented code stylometry features.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec}
}

@inproceedings{10.1007/978-3-031-49266-2_18,
author = {Kaichi, Ryunosuke and Matsumoto, Shinsuke and Kusumoto, Shinji},
title = {Automatic Fixation of&nbsp;Decompilation Quirks Using Pre-trained Language Model},
year = {2023},
isbn = {978-3-031-49265-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49266-2_18},
doi = {10.1007/978-3-031-49266-2_18},
abstract = {Decompiler is a system for recovering the original code from bytecode. A critical challenge in decompilers is that the decompiled code contains differences from the original code. These differences not only reduce the readability of the source code but may also change the program’s behavior. In this study, we propose a deep learning-based quirk fixation method that adopts grammatical error correction. One advantage of the proposed method is that it can be applied to any decompiler and programming language. Our experimental results show that the proposed method removes 55% of identifier quirks and 91% of structural quirks. In some cases, however, the proposed method injected a small amount of new quirks.},
booktitle = {Product-Focused Software Process Improvement: 24th International Conference, PROFES 2023, Dornbirn, Austria, December 10–13, 2023, Proceedings, Part I},
pages = {259–266},
numpages = {8},
keywords = {grammatical error correction, quirk, deep learning, fine-tuning, decompiler},
location = {<conf-loc content-type="InPerson">Dornbirn, Austria</conf-loc>}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00053,
author = {Tufano, Rosalia},
title = {Automating Code Review},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00053},
doi = {10.1109/ICSE-Companion58688.2023.00053},
abstract = {Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, code review comes at the cost of spending developers' time on reviewing their teammates' code. The goal of this research is to investigate the possibility of using Deep Learning (DL) to automate specific code review tasks. We started by training vanilla Transformer models to learn code changes performed by developers during real code review activities. This gives the models the possibility to automatically (i) revise the code submitted for review without any input from the reviewer; and (ii) implement changes required to address a specific reviewer's comment. While the preliminary results were encouraging, in this first work we tested DL models in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. Thus, in a subsequent work, we exploited a pre-trained Text-To-Text-Transfer-Transformer (T5) to overcome some of these limitations and experiment DL models for code review automation in more realistic and challenging scenarios. The achieved results show the improvements brought by T5 both in terms of applicability (i.e., scenarios in which it can be applied) and performance. Despite this, we are still far from performance levels making these techniques deployable in practice, thus calling for additional research in this area, as we discuss in our future work agenda.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {192–196},
numpages = {5},
keywords = {deep learning, code review},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-41181-6_28,
author = {Rahaman, Sazzadur and Frantz, Miles and Miller, Barton and Yao, Danfeng (Daphne)},
title = {SpanL: Creating Algorithms for Automatic API Misuse Detection with Program Analysis Compositions},
year = {2023},
isbn = {978-3-031-41180-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-41181-6_28},
doi = {10.1007/978-3-031-41181-6_28},
abstract = {High-level language platforms provide APIs to aid developers in easily integrating security-relevant features in their code. Prior research shows that improper use of these APIs is a major source of insecurity in various application domains. Automatic code screening holds lots of potential to enable secure coding. However, building domain-specific security analysis tools requires both application domain and program analysis expertise. Interestingly, most of the prior works in developing domain-specific security analysis tools leverage some form of data flow analysis in the core. We leverage this insight to build a specification language named SpanL1 for domain-specific security screening. The expressiveness analysis shows that a rule requiring any composition of dataflow analysis can be modeled in our language. Our evaluation on four cryptographic API misuse problems shows that our prototype implementation of SpanL does not introduce any imprecision due to the expressiveness of the language(1SpanL stands for Security sPecificAtioN Language.).},
booktitle = {Applied Cryptography and Network Security Workshops: ACNS 2023 Satellite Workshops, ADSC, AIBlock, AIHWS, AIoTS, CIMSS, Cloud S&amp;P, SCI, SecMT, SiMLA, Kyoto, Japan, June 19–22, 2023, Proceedings},
pages = {515–529},
numpages = {15},
keywords = {API Misuse, Specification Language, Program Analysis},
location = {Kyoto, Japan}
}

@inproceedings{10.1145/3587102.3588805,
author = {Reeves, Brent and Sarsa, Sami and Prather, James and Denny, Paul and Becker, Brett A. and Hellas, Arto and Kimmel, Bailey and Powell, Garrett and Leinonen, Juho},
title = {Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588805},
doi = {10.1145/3587102.3588805},
abstract = {The recent emergence of code generation tools powered by large language models has attracted wide attention. Models such as OpenAI Codex can take natural language problem descriptions as input and generate highly accurate source code solutions, with potentially significant implications for computing education. Given the many complexities that students face when learning to write code, they may quickly become reliant on such tools without properly understanding the underlying concepts. One popular approach for scaffolding the code writing process is to use Parsons problems, which present solution lines of code in a scrambled order. These remove the complexities of low-level syntax, and allow students to focus on algorithmic and design-level problem solving. It is unclear how well code generation models can be applied to solve Parsons problems, given the mechanics of these models and prior evidence that they underperform when problems include specific restrictions. In this paper, we explore the performance of the Codex model for solving Parsons problems over various prompt variations. Using a corpus of Parsons problems we sourced from the computing education literature, we find that Codex successfully reorders the problem blocks about half of the time, a much lower rate of success when compared to prior work on more free-form programming tasks. Regarding prompts, we find that small variations in prompting have a noticeable effect on model performance, although the effect is not as pronounced as between different problems.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {299–305},
numpages = {7},
keywords = {openAI, novice programming, neural networks, natural language processing, machine learning, large language models, introductory programming, generative ai, deep learning, copilot, computer programming, codex, code writing, code generation, chatgpt, artificial intelligence, ai, academic integrity, ML, GitHub, GPT-3, CS1},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3611643.3616280,
author = {Wang, Chaozheng and Hu, Junhao and Gao, Cuiyun and Jin, Yu and Xie, Tao and Huang, Hailiang and Lei, Zhenyu and Deng, Yuetang},
title = {How Practitioners Expect Code Completion?},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616280},
doi = {10.1145/3611643.3616280},
abstract = {Code completion has become a common practice for programmers during their daily programming activities. It automatically predicts the next tokens or statements that the programmers may use. Code completion aims to substantially save keystrokes and improve the programming efficiency for programmers. Although there exists substantial research on code completion, it is still unclear what practitioner expectations are on code completion and whether these expectations are met by the existing research. To address these questions, we perform a study by first interviewing 15 professionals and then surveying 599 practitioners from 18 IT companies about their expectations on code completion. We then compare the practitioner expectations with the existing research by conducting a literature review of papers on code completion published in major publication venues from 2012 to 2022. Based on the comparison, we highlight the directions desirable for researchers to invest efforts toward developing code completion techniques for meeting practitioner expectations.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1294–1306},
numpages = {13},
keywords = {practitioners expectations, empirical study, Code completion},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3568812.3603476,
author = {Phung, Tung and Pădurean, Victor-Alexandru and Cambronero, José and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
title = {Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603476},
doi = {10.1145/3568812.3603476},
abstract = {Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies. State-of-the-art models like OpenAI’s ChatGPT&nbsp;[8] and GPT-4&nbsp;[9] could enhance programming education in various roles, e.g., by acting as a personalized digital tutor for a student, a digital assistant for an educator, and a digital peer for collaborative learning&nbsp;[1, 2, 7]. In our work, we seek to comprehensively evaluate and benchmark state-of-the-art large language models for various scenarios in programming education. Recent works have evaluated several large language models in the context of programming education&nbsp;[4, 6, 10, 11, 12]. However, these works are limited for several reasons: they have typically focused on evaluating a specific model for a specific education scenario (e.g., generating explanations), or have considered models that are already outdated (e.g., OpenAI’s Codex&nbsp;[3] is no longer publicly available since March 2023). Consequently, there is a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios in programming education. These scenarios are designed to capture distinct roles these models could play, namely digital tutors, assistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (1) program repair, i.e., fixing a student’s buggy program; (2) hint generation, i.e., providing a natural language hint to the student to help resolve current issues; (3) grading feedback, i.e., grading a student’s program w.r.t. a given rubric; (4) peer programming, i.e., completing a partially written program or generating a sketch for the solution program; (5) task creation, i.e., generating new tasks that exercise specific types of concepts or bugs; (6) contextualized explanation, i.e., explaining specific concepts or functions in the context of a given program. Our study uses a mix of quantitative and qualitative evaluation to compare the performance of these models with the performance of human tutors. We conduct our evaluation based on 5 introductory Python programming problems with a diverse set of input/output specifications. For each of these problems, we consider 5 buggy programs based on publicly accessible submissions from geeksforgeeks.org &nbsp;[5] (see Figure&nbsp;1); these buggy programs are picked to capture different types of bugs for each problem. We will provide a detailed analysis of the data and results in a longer version of this poster. Our preliminary results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors’ performance for several scenarios.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {41–42},
numpages = {2},
keywords = {large language models, introductory programming education, generative AI, ChatGPT},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3611643.3617850,
author = {Gu, Qiuhan},
title = {LLM-Based Code Generation Method for Golang Compiler Testing},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3617850},
doi = {10.1145/3611643.3617850},
abstract = {Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44%. Moreover, among all the generated testcases, only 2.79% exhibited syntax errors, and none displayed undefined behavior.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2201–2203},
numpages = {3},
keywords = {Large model, Go language, Compiler testing, Code generation},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616306,
author = {Mao, Yuetian and Wan, Chengcheng and Jiang, Yuze and Gu, Xiaodong},
title = {Self-Supervised Query Reformulation for Code Search},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616306},
doi = {10.1145/3611643.3616306},
abstract = {Automatic query reformulation is a widely utilized technology for enriching user requirements and enhancing the outcomes of code search. It can be conceptualized as a machine translation task, wherein the objective is to rephrase a given query into a more comprehensive alternative. While showing promising results, training such a model typically requires a large parallel corpus of query pairs (i.e., the original query and a reformulated query) that are confidential and unpublished by online code search engines. This restricts its practicality in software development processes. In this paper, we propose SSQR, a self-supervised query reformulation method that does not rely on any parallel query corpus. Inspired by pre-trained models, SSQR treats query reformulation as a masked language modeling task conducted on an extensive unannotated corpus of queries. SSQR extends T5 (a sequence-to-sequence model based on Transformer) with a new pre-training objective named corrupted query completion (CQC), which randomly masks words within a complete query and trains T5 to predict the masked content. Subsequently, for a given query to be reformulated, SSQR identifies potential locations for expansion and leverages the pre-trained T5 model to generate appropriate content to fill these gaps. The selection of expansions is then based on the information gain associated with each candidate. Evaluation results demonstrate that SSQR outperforms unsupervised baselines significantly and achieves competitive performance compared to supervised methods.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {363–374},
numpages = {12},
keywords = {Self-supervised Learning, Query Reformulation, Code Search},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-3-031-48796-5_11,
author = {Gay, Gregory},
title = {Improving the&nbsp;Readability of&nbsp;Generated Tests Using GPT-4 and&nbsp;ChatGPT Code Interpreter},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48796-5_11},
doi = {10.1007/978-3-031-48796-5_11},
abstract = {A major challenge in automated test generation is the readability of generated tests. Emerging large language models (LLMs) excel at language analysis and transformation tasks. We propose that improving test readability is such a task and explore the capabilities of the GPT-4 LLM in improving readability of tests generated by the Pynguin search-based generation framework. Our initial results are promising. However, there are remaining research and technical challenges.},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {140–146},
numpages = {7},
keywords = {Generative AI, Large Language Models, Readability, Search-Based Test Generation, Automated Test Generation},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@inproceedings{10.1109/ICSE48619.2023.00014,
author = {Liu, Zhongxin and Tang, Zhijie and Xia, Xin and Yang, Xiaohu},
title = {CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00014},
doi = {10.1109/ICSE48619.2023.00014},
abstract = {Representing code changes as numeric feature vectors, i.e., code change representations, is usually an essential step to automate many software engineering tasks related to code changes, e.g., commit message generation and just-in-time defect prediction. Intuitively, the quality of code change representations is crucial for the effectiveness of automated approaches. Prior work on code changes usually designs and evaluates code change representation approaches for a specific task, and little work has investigated code change encoders that can be used and jointly trained on various tasks. To fill this gap, this work proposes a novel Code Change Representation learning approach named CCRep, which can learn to encode code changes as feature vectors for diverse downstream tasks. Specifically, CCRep regards a code change as the combination of its before-change and after-change code, leverages a pre-trained code model to obtain high-quality contextual embeddings of code, and uses a novel mechanism named query back to extract and encode the changed code fragments and make them explicitly interact with the whole code change. To evaluate CCRep and demonstrate its applicability to diverse code-change-related tasks, we apply it to three tasks: commit message generation, patch correctness assessment, and just-in-time defect prediction. Experimental results show that CCRep outperforms the state-of-the-art techniques on each task.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {17–29},
numpages = {13},
keywords = {just-in-time defect prediction, patch correctness assessment, commit message generation, representation learning, code change},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3587102.3588815,
author = {Daun, Marian and Brings, Jennifer},
title = {How ChatGPT Will Change Software Engineering Education},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588815},
doi = {10.1145/3587102.3588815},
abstract = {This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {110–116},
numpages = {7},
keywords = {software engineering education, generative AI, ChatGPT},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00110,
author = {Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun},
title = {CCTest: Testing and Repairing Code Completion Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00110},
doi = {10.1109/ICSE48619.2023.00110},
abstract = {Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the "average" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1238–1250},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3568813.3600142,
author = {Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
title = {Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600142},
doi = {10.1145/3568813.3600142},
abstract = {This paper studies recent developments in large language models’ (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class’ assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4’s handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {78–92},
numpages = {15},
keywords = {programming knowledge assessment, introductory and intermediate programming, generative pre-trained transformers, coding exercises, Python course, Multiple-choice question answering, MCQ, GitHub Copilot, GPT, Codex, ChatGPT, AlphaCode, AI code generation},
location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
series = {ICER '23}
}

@article{10.1016/j.jss.2023.111753,
author = {Liu, Jingyu and Ai, Jun and Lu, Minyan and Wang, Jie and Shi, Haoxiang},
title = {Semantic feature learning for software defect prediction from source code and external knowledge},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {204},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111753},
doi = {10.1016/j.jss.2023.111753},
journal = {J. Syst. Softw.},
month = {oct},
numpages = {11},
keywords = {Transformer, Semantic features, Software defect prediction}
}

@inproceedings{10.1145/3613372.3614189,
author = {Albonico, Michel and Varela, Paulo Júnior},
title = {A Report on the Use of ChatGPT in Software Engineering and Systems Analysis Courses},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3614189},
doi = {10.1145/3613372.3614189},
abstract = {ChatGPT is a natural language model that works as a virtual chat assistant. It has the potential to be used for fostering classroom discussions and addressing student needs when the professor is not accessible. Although it is still early to assess the impact of ChatGPT and similar technologies, there is a considerable discussion on social media and blogs regarding the aspirations and opportunities of utilizing ChatGPT in the software industry and education. The main perception is that ChatGPT can serve as a support tool but should not completely replace interpersonal interaction, as face-to-face dialogue remains crucial for the development of interpersonal skills and a deeper understanding of concepts. This article reports a recent classroom experience in the subjects of Software Engineering and Systems Analysis, while also analyzing ChatGPT’s responses to student inquiries.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {303–311},
numpages = {9},
keywords = {System Analysis, Student Support, Software Engineering, ChatGPT},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@inproceedings{10.1007/978-3-031-49011-8_37,
author = {Ahmed, Areeg and Azab, Shahira and Abdelhamid, Yasser},
title = {Source-Code Generation Using Deep Learning: A Survey},
year = {2023},
isbn = {978-3-031-49010-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49011-8_37},
doi = {10.1007/978-3-031-49011-8_37},
abstract = {In recent years, the need for writing effective, reusable, and high-quality source code has grown exponentially. Writing source code is an integral part of building any software system; the development phase of the software lifecycle contains code implementation, refactoring, maintenance, and fixing bugs. Software developers implement the desired solution by turning the system requirements into viable software products. For the most part, the implementation phase can be challenging as it requires a certain level of problem-solving skills and the ability to produce high-quality outcomes without decreasing productivity rates or not meeting the business plans and deadlines. Programmers’ daily tasks might also include writing large amounts of repetitive boilerplate code, which can be tedious, not to mention the potential bugs that could arise from human errors during the development process. The ability to automatically generate source code will save significant time and effort invested in the software development process by increasing the speed and efficiency of software development teams. In this survey, we review and summarize the recent studies on deep learning approaches used to generate source code in different programming languages such as Java, Python, and SQL (Structured Query Language). We categorize the surveyed work into two groups, Natural Language-based solutions for approaches that use natural text as input and Computer Vision-based solutions which generate code based on images as input.},
booktitle = {Progress in Artificial Intelligence: 22nd EPIA Conference on Artificial Intelligence, EPIA 2023, Faial Island, Azores, September 5–8, 2023, Proceedings, Part II},
pages = {467–482},
numpages = {16},
keywords = {Computer vision, Natural language, Machine learning, Transformers, Deep learning, Code generation},
location = {<conf-loc content-type="InPerson">Faial Island, Portugal</conf-loc>}
}

@article{10.1145/3632745,
author = {Li, Zhihao and Li, Chuanyi and Tang, Ze and Huang, Wanhong and Ge, Jidong and Luo, Bin and Ng, Vincent and Wang, Ting and Hu, Yucheng and Zhang, Xiaopeng},
title = {PTM-APIRec: Leveraging Pre-trained Models of Source Code in API Recommendation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3632745},
doi = {10.1145/3632745},
abstract = {Recommending APIs is a practical and essential feature of IDEs. Improving the accuracy of API recommendations is an effective way to improve coding efficiency. With the success of deep learning in software engineering, the state-of-the-art (SOTA) performance of API recommendation is also achieved by deep learning-based approaches. However, existing SOTAs either only consider the API sequences in the code snippets, or rely on complex operations for extracting hand-crafted features, all of which have potential risks in under-encoding the input code snippets and further resulting in sub-optimal recommendation performance. To this end, this paper proposes to utilize the code understanding ability of existing general code Pre-Training Models to fully encode the input code snippet to improve the accuracy of API Recommendation, namely, PTM-APIRec. To ensure that the code semantics of the input are fully understood and the API recommended actually exists, we use separate vocabularies for the input code snippet and the APIs to be predicted. The experimental results on the JDK and Android datasets show that PTM-APIRec surpasses existing approaches. Besides, an effective way to improve the performance of PTM-APIRec is to enhance the pre-trained model with more pre-training data (which is easier to obtain than API recommendation datasets).},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {Code Completion, Code Pre-training, API recommendation}
}

@article{10.1145/3593230,
author = {Brie, Paul and Burny, Nicolas and Sluÿters, Arthur and Vanderdonckt, Jean},
title = {Evaluating a Large Language Model on Searching for GUI Layouts},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593230},
doi = {10.1145/3593230},
abstract = {The field of generative artificial intelligence has seen significant advancements in recent years with the advent of large language models, which have shown impressive results in software engineering tasks but not yet in engineering user interfaces. Thus, we raise a specific research question: would an LLM-based system be able to search for relevant GUI layouts? To address this question, we conducted a controlled study evaluating how Instigator, an LLM-based system for searching GUI layouts of web pages by generative pre-trained training, would return GUI layouts that are relevant to a given instruction and what would be the user experience of (N =34) practitioners interacting with Instigator. Our results identify a very high similarity and a moderate correlation between the rankings of the GUI layouts generated by Instigator and the rankings of the practitioners with respect to their relevance to a given design instruction. We highlight the results obtained through thirteen UEQ+ scales that characterize the user experience of the practitioner with Instigator, which we use to discuss perspectives for improving such future tools.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {jun},
articleno = {178},
numpages = {37},
keywords = {web pages, large language model, gui layout, gui design, generative pre-training}
}

@inproceedings{10.1145/3607199.3607242,
author = {Chen, Yizheng and Ding, Zhoujie and Alowain, Lamya and Chen, Xinyun and Wagner, David},
title = {DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607242},
doi = {10.1145/3607199.3607242},
abstract = {We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined. Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonstrate an important generalization challenge for the deployment of deep learning-based models. We show that increasing the volume of training data may not further improve the performance of deep learning models for vulnerability detection, but might be useful to improve the generalization ability to unseen projects. We also identify hopeful future research directions. We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments. Moreover, developing source code specific pre-training objectives is a promising research direction to improve the vulnerability detection performance.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {654–668},
numpages = {15},
keywords = {vulnerability detection, large language models, deep learning, datasets},
location = {<conf-loc>, <city>Hong Kong</city>, <country>China</country>, </conf-loc>},
series = {RAID '23}
}

@inproceedings{10.1007/978-3-031-44207-0_26,
author = {Zhang, Dongping and Xian, Hequn and Chen, Jiyang and Xu, Zhiguo},
title = {VDCNet: A Vulnerability Detection and&nbsp;Classification System in&nbsp;Cross-Project Scenarios},
year = {2023},
isbn = {978-3-031-44206-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44207-0_26},
doi = {10.1007/978-3-031-44207-0_26},
abstract = {The existence of software vulnerabilities is the primary cause of most security incidents in cyberspace. Timely detection of potential vulnerabilities from source code during the software development stage is a critical issue for developers. With the increasing scale of open-source projects, traditional static analysis tools are becoming more and more unreliable and stagnant in their development. Meanwhile, approaches for vulnerability detection based on deep learning are being investigated. This paper introduces a novel deep learning-based vulnerability detection system, VDCNet, to identify and classify multiple vulnerabilities more effectively. We extract advanced semantic information from AST representations of source code and capture patterns of vulnerable functions by training neural networks. VDCNet constructs a BERT model for embedding and a Bi-LSTM network for prediction. The experimental results on a comprehensive dataset demonstrate that our method is more efficient in binary vulnerability detection than other deep learning-based methods, with outstanding multi-classification performance in cross-project scenarios.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2023: 32nd International Conference on Artificial Neural Networks, Heraklion, Crete, Greece, September 26–29, 2023, Proceedings, Part I},
pages = {305–316},
numpages = {12},
keywords = {Feature extraction, Deep learning, Code representation, Vulnerability detection},
location = {Heraklion, Greece}
}

@inproceedings{10.1145/3611643.3616302,
author = {Wei, Xiaokai and Gonugondla, Sujan Kumar and Wang, Shiqi and Ahmad, Wasi and Ray, Baishakhi and Qian, Haifeng and Li, Xiaopeng and Kumar, Varun and Wang, Zijian and Tian, Yuchen and Sun, Qing and Athiwaratkun, Ben and Shang, Mingyue and Ramanathan, Murali Krishna and Bhatia, Parminder and Xiang, Bing},
title = {Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616302},
doi = {10.1145/3611643.3616302},
abstract = {ML-powered code generation aims to assist developers to write code in a more productive manner by intelligently generating code blocks based on natural language prompts. Recently, large pretrained deep learning models have pushed the boundary of code generation and achieved impressive performance. However, the huge number of model parameters poses a significant challenge to their adoption in a typical software development environment, where a developer might use a standard laptop or mid-size server to develop code. Such large models cost significant resources in terms of memory, latency, dollars, as well as carbon footprint. Model compression is a promising approach to address these challenges. We have identified quantization as one of the most promising compression techniques for code-generation as it avoids expensive retraining costs. As quantization represents model parameters with lower-bit integer (e.g., int8), the model size and runtime latency would both benefit. We empirically evaluate quantized models on code generation tasks across different dimensions: (i) resource usage and carbon footprint, (ii) accuracy, and (iii) robustness. Through systematic experiments we find a code-aware quantization recipe that could run even a 6-billion-parameter model in a regular laptop without significant accuracy or robustness degradation. We find that the recipe is readily applicable to code summarization task as well.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {224–236},
numpages = {13},
keywords = {Quantization, Model Hosting, Large Language Models, Generative AI, Code Generation},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00089,
author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Ni, Mingze and Li, Li},
title = {Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00089},
doi = {10.1109/ICSE-Companion58688.2023.00089},
abstract = {Currently, large pre-trained language models are widely applied in neural code completion systems. Though large code models significantly outperform their smaller counterparts, around 70% of displayed code completions from Copilot are not accepted by developers. Being reviewed but not accepted, their help to developer productivity is considerably limited. Even worse, considering the high cost of the large code models, it is a huge waste of computing resources and energy. To fill this significant gap, we propose an early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities without sending them to the code completion system. Furthermore, we propose a lightweight Transformer-based estimator to demonstrate the feasibility of the mechanism. The experimental results show that the proposed estimator helps save 23.3% of computational cost measured in floating-point operations for the code completion systems, and 80.2% of rejected prompts lead to unhelpful completion.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {324–325},
numpages = {2},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.5555/3615924.3615947,
author = {Paria, Shirani and Sagar, Bhatt and Asmaa, Hailane and Guy-Vincent, Jourdan},
title = {Towards Cross-Architecture Binary Code Vulnerability Detection},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Today’s Internet of Things (IoT) environments are heterogeneous as they are typically comprised of devices equipped with various CPU architectures and software platforms. Therefore, in defending IoT environments against security threats, the capability of crossarchitecture vulnerability detection is of paramount importance. In this paper, we propose BinX, a deep learning-based approach for code similarity detection in binaries that are obtained through different compilers and optimization levels for various architectures. Our research is guided by a key idea that involves leveraging the Ghidra decompiler to generate the decompiled C code and the high p-code intermediate representation and pre-train transformerbased model, specifically BERT and CodeBERT, to accurately generate semantic embeddings. These embeddings are then utilized as inputs to an RNN Siamese neural network, enhancing the learning process for code similarity detection. The effectiveness of our approach is demonstrated through several experiments and comparisons with existing methods. Our results showcase the potential of BinX in enabling cross-architecture vulnerability detection in cross-architecture cross-compiled binaries, contributing to the advancement of security in IoT environments.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {191–196},
numpages = {6},
keywords = {machine learning., vulnerability detection, Binary code analysis},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@article{10.1145/3641542,
author = {Fan, Guodong and Chen, Shizhan and Gao, Cuiyun and Xiao, Jianmao and Zhang, Tao and Feng, Zhiyong},
title = {Rapid: Zero-shot Domain Adaptation for Code Search with Pre-trained Models},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641542},
doi = {10.1145/3641542},
abstract = {Code search, which refers to the process of identifying the most relevant code snippets for a given natural language query, plays a crucial role in software maintenance. However, current approaches heavily rely on labeled data for training, which results in performance decreases when confronted with cross-domain scenarios including domain-specific or project-specific situations. This decline can be attributed to their limited ability to effectively capture the semantics associated with such scenarios. To tackle the aforementioned problem, we propose a zeRo-shot domAin adaPtion with pre-traIned moDels framework for code search named RAPID. The framework first generates synthetic data by pseudo labeling, then trains the CodeBERT with sampled synthetic data. To avoid the influence of noisy synthetic data and enhance the model performance, we propose a mixture sampling strategy to obtain hard negative samples during training. Specifically, the mixture sampling strategy considers both relevancy and diversity to select the data that are hard to be distinguished by the models. To validate the effectiveness of our approach in zero-shot settings, we conduct extensive experiments and find that RAPID outperforms the CoCoSoDa and UniXcoder model by an average of 15.7% and 10%, respectively, as measured by the MRR metric. When trained on full data, our approach results in an average improvement of 7.5% under the MRR metric using CodeBERT. We observe that as the model’s performance in zero-shot tasks improves, the impact of hard negatives diminishes. Our observation also indicates that fine-tuning CodeT5 for generating pseudo labels can enhance the performance of the code search model, and using only 100-shot samples can yield comparable results to the supervised baseline. Furthermore, we evaluate the effectiveness of RAPID in real-world code search tasks in three GitHub projects through both human and automated assessments. Our findings reveal RAPID exhibits superior performance, e.g., an average improvement of 18% under the MRR metric over the top-performing model.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Code Search, Zero-shot Learning, Software Maintenance, Pre-trained Models, Domain Adaption}
}

@inproceedings{10.1145/3611643.3616297,
author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Li, Li},
title = {CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616297},
doi = {10.1145/3611643.3616297},
abstract = {Code datasets are of immense value for training neural-network-based code completion models, where companies or organizations have made substantial investments to establish and process these datasets.  
Unluckily, these datasets, either built for proprietary or public usage, face the high risk of unauthorized exploits, resulting from data leakages, license violations, etc.  
Even worse, the "black-box" nature of neural models sets a high barrier for externals to audit their training datasets, which further connives these unauthorized usages.  
Currently, watermarking methods have been proposed to prohibit inappropriate usage of image and natural language datasets.  
However, due to domain specificity, they are not directly applicable to code datasets, leaving the copyright protection of this emerging and important field of code data still exposed to threats.  
To fill this gap, we propose a method, named CodeMark, to embed user-defined imperceptible watermarks into code datasets to trace their usage in training neural code completion models.  
CodeMark is based on adaptive semantic-preserving transformations, which preserve the exact functionality of the code data and keep the changes covert against rule-breakers.  
We implement CodeMark in a toolkit and conduct an extensive evaluation of code completion models.  
CodeMark is validated to fulfill all desired properties of practical watermarks, including  
harmlessness to model accuracy, verifiability, robustness, and imperceptibility.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1561–1572},
numpages = {12},
keywords = {Watermarking, Neural code completion models, Code dataset},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616323,
author = {Wang, Shangwen and Geng, Mingyang and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Li, Li and Bissyandé, Tegawendé F. and Mao, Xiaoguang},
title = {Natural Language to Code: How Far Are We?},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616323},
doi = {10.1145/3611643.3616323},
abstract = {A longstanding dream in software engineering research is to devise effective approaches for automating development tasks based on developers' informally-specified intentions. Such intentions are generally in the form of natural language descriptions. In recent literature, a number of approaches have been proposed to automate tasks such as code search and even code generation based on natural language inputs. While these approaches vary in terms of technical designs, their objective is the same: transforming a developer's intention into source code. The literature, however, lacks a comprehensive understanding towards the effectiveness of existing techniques as well as their complementarity to each other. We propose to fill this gap through a large-scale empirical study where we systematically evaluate natural language to code techniques. Specifically, we consider six state-of-the-art techniques targeting code search, and four targeting code generation. Through extensive evaluations on a dataset of 22K+ natural language queries, our study reveals the following major findings: (1) code search techniques based on model pre-training are so far the most effective while code generation techniques can also provide promising results; (2) complementarity widely exists among the existing techniques; and (3) combining the ten techniques together can enhance the performance for 35% compared with the most effective standalone technique. Finally, we propose a post-processing strategy to automatically integrate different techniques based on their generated code. Experimental results show that our devised strategy is both effective and extensible.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {375–387},
numpages = {13},
keywords = {Pre-Training Technique, Code Search, Code Generation},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616245,
author = {Yang, Lanxin and Xu, Jinwei and Zhang, Yifan and Zhang, He and Bacchelli, Alberto},
title = {EvaCRC: Evaluating Code Review Comments},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616245},
doi = {10.1145/3611643.3616245},
abstract = {In code reviews, developers examine code changes authored by peers and provide feedback through comments. Despite the importance of these comments, no accepted approach currently exists for assessing their quality. Therefore, this study has two main objectives: (1) to devise a conceptual model for an explainable evaluation of review comment quality, and (2) to develop models for the automated evaluation of comments according to the conceptual model. To do so, we conduct mixed-method studies and propose a new approach: EvaCRC (Evaluating Code Review Comments). To achieve the first goal, we collect and synthesize quality attributes of review comments, by triangulating data from both authoritative documentation on code review standards and academic literature. We then validate these attributes using real-world instances. Finally, we establish mappings between quality attributes and grades by inquiring domain experts, thus defining our final explainable conceptual model. To achieve the second goal, EvaCRC leverages multi-label learning. To evaluate and refine EvaCRC, we conduct an industrial case study with a global ICT enterprise. The results indicate that EvaCRC can effectively evaluate review comments while offering reasons for the grades. Data and materials: https://doi.org/10.5281/zenodo.8297481},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {275–287},
numpages = {13},
keywords = {review comments, quality evaluation, Code review},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00179,
author = {Li, Jia and Li, Yongmin and Li, Ge and Jin, Zhi and Hao, Yiyang and Hu, Xing},
title = {SKCODER: A Sketch-Based Approach for Automatic Code Generation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00179},
doi = {10.1109/ICSE48619.2023.00179},
abstract = {Recently, deep learning techniques have shown great success in automatic code generation. Inspired by the code reuse, some researchers propose copy-based approaches that can copy the content from similar code snippets to obtain better performance. Practically, human developers recognize the content in the similar code that is relevant to their needs, which can be viewed as a code sketch. The sketch is further edited to the desired code. However, existing copy-based approaches ignore the code sketches and tend to repeat the similar code without necessary modifications, which leads to generating wrong results.In this paper, we propose a sketch-based code generation approach named SKCODER to mimic developers' code reuse behavior. Given a natural language requirement, SKCODER retrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code. Our motivations are that the extracted sketch provides a well-formed pattern for telling models "how to write". The post-editing further adds requirement-specific details into the sketch and outputs the complete code. We conduct experiments on two public datasets and a new dataset collected by this work. We compare our approach to 20 baselines using 5 widely used metrics. Experimental results show that (1) SKCODER can generate more correct programs, and outperforms the state-of-the-art - CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets. (2) Our approach is effective to multiple code generation models and improves them by up to 120.1% in Pass@1. (3) We investigate three plausible code sketches and discuss the importance of sketches. (4) We manually evaluate the generated code and prove the superiority of our SKCODER in three aspects.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2124–2135},
numpages = {12},
keywords = {deep learning, code generation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00185,
author = {Shi, Ensheng and Wang, Yanlin and Gu, Wenchao and Du, Lun and Zhang, Hongyu and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
title = {CoCoSoDa: Effective Contrastive Learning for Code Search},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00185},
doi = {10.1109/ICSE48619.2023.00185},
abstract = {Code search aims to retrieve semantically relevant code snippets for a given natural language query. Recently, many approaches employing contrastive learning have shown promising results on code representation learning and greatly improved the performance of code search. However, there is still a lot of room for improvement in using contrastive learning for code search. In this paper, we propose CoCoSoDa to effectively utilize contrastive learning for code search via two key factors in contrastive learning: data augmentation and negative samples. Specifically, soft data augmentation is to dynamically masking or replacing some tokens with their types for input sequences to generate positive samples. Momentum mechanism is used to generate large and consistent representations of negative samples in a mini-batch through maintaining a queue and a momentum encoder. In addition, multimodal contrastive learning is used to pull together representations of code-query pairs and push apart the unpaired code snippets and queries. We conduct extensive experiments to evaluate the effectiveness of our approach on a large-scale dataset with six programming languages. Experimental results show that: (1) CoCoSoDa outperforms 18 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 13.3%, 10.5%, and 5.9% on average MRR scores, respectively. (2) The ablation studies show the effectiveness of each component of our approach. (3) We adapt our techniques to several different pre-trained models such as RoBERTa, CodeBERT, and GraphCodeBERT and observe a significant boost in their performance in code search. (4) Our model performs robustly under different hyper-parameters. Furthermore, we perform qualitative and quantitative analyses to explore reasons behind the good performance of our model.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2198–2210},
numpages = {13},
keywords = {momentum mechanism, soft data augmentation, contrastive learning, code search},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3636243.3636245,
author = {Macneil, Stephen and Denny, Paul and Tran, Andrew and Leinonen, Juho and Bernstein, Seth and Hellas, Arto and Sarsa, Sami and Kim, Joanne},
title = {Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636245},
doi = {10.1145/3636243.3636245},
abstract = {Identifying and resolving logic errors can be one of the most frustrating challenges for novices programmers. Unlike syntax errors, for which a compiler or interpreter can issue a message, logic errors can be subtle. In certain conditions, buggy code may even exhibit correct behavior – in other cases, the issue might be about how a problem statement has been interpreted. Such errors can be hard to spot when reading the code, and they can also at times be missed by automated tests. There is great educational potential in automatically detecting logic errors, especially when paired with suitable feedback for novices. Large language models (LLMs) have recently demonstrated surprising performance for a range of computing tasks, including generating and explaining code. These capabilities are closely linked to code syntax, which aligns with the next token prediction behavior of LLMs. On the other hand, logic errors relate to the runtime performance of code and thus may not be as well suited to analysis by LLMs. To explore this, we investigate the performance of two popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly explanation of logic errors. We compare LLM performance with a large cohort of introductory computing students (n = 964) solving the same error detection task. Through a mixed-methods analysis of student and model responses, we observe significant improvement in logic error identification between the previous and current generation of LLMs, and find that both LLM generations significantly outperform students. We outline how such models could be integrated into computing education tools, and discuss their potential for supporting students when learning programming.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {11–18},
numpages = {8},
keywords = {bug detection, computing education, generative AI, large language models, programming errors},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {ACE '24}
}

@article{10.14778/3611540.3611630,
author = {Trummer, Immanuel},
title = {Demonstrating GPT-DB: Generating Query-Specific and Customizable Code for SQL Processing with GPT-4},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611630},
doi = {10.14778/3611540.3611630},
abstract = {GPT-DB generates code for SQL processing in general-purpose programming languages such as Python. Generated code can be freely customized using user-provided natural language instructions. This enables users, for instance, to try out specific libraries for SQL processing or to generate non-standard output while processing.GPT-DB is based on OpenAI's GPT model series, neural networks capable of translating natural language instructions into code. By default, GPT-DB exploits the most recently released GPT-4 model whereas visitors may also select prior versions for comparison. GPT-DB automatically generates query-specific prompts, instructing GPT on code generation. These prompts include a description of the target database, as well as logical query plans described as natural language text, and instructions for customization. GPT-DB automatically verifies, and possibly re-generates, code using a reference database system for result comparisons. It enables users to select code samples for training, thereby increasing accuracy for future queries. The proposed demonstration showcases code generation for various queries and with varying instructions for code customization.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {4098–4101},
numpages = {4}
}

@article{10.1145/3636430,
author = {Tipirneni, Sindhu and Zhu, Ming and Reddy, Chandan K.},
title = {StructCoder: Structure-Aware Transformer for Code Generation},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3636430},
doi = {10.1145/3636430},
abstract = {There has been a recent surge of interest in automating software engineering tasks using deep learning. This article addresses the problem of code generation, in which the goal is to generate target code given source code in a different language or a natural language description. Most state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model in which both the encoder and decoder are explicitly trained to recognize the syntax and dataflow in the source and target codes, respectively. We not only make the encoder structure aware by leveraging the source code’s syntax tree and dataflow graph, but we also support the decoder in preserving the syntax and dataflow of the target code by introducing two novel auxiliary tasks: Abstract Syntax Tree (AST) path prediction and dataflow prediction. To the best of our knowledge, this is the first work to introduce a structure-aware Transformer decoder that models both syntax and dataflow to enhance the quality of generated code. The proposed StructCoder model achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark and improves over baselines of similar size on the APPS code generation benchmark. Our code is publicly available at .},
journal = {ACM Trans. Knowl. Discov. Data},
month = {jan},
articleno = {70},
numpages = {20},
keywords = {Transformer, code generation, language models, Deep learning}
}

@article{10.1016/j.ins.2023.03.118,
author = {Jiang, Wenchao and Qiu, Shaojian and Liang, Tiancai and Zhang, Fanlong},
title = {Cross-project clone consistent-defect prediction via transfer-learning method},
year = {2023},
issue_date = {Jul 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {635},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2023.03.118},
doi = {10.1016/j.ins.2023.03.118},
journal = {Inf. Sci.},
month = {jul},
pages = {138–150},
numpages = {13},
keywords = {Transfer learning, Cross-project, Defect prediction, Clone consistent-defect, Code clones}
}

@article{10.1145/3597204,
author = {Liu, Xuanzhe and Gu, Diandian and Chen, Zhenpeng and Wen, Jinfeng and Zhang, Zili and Ma, Yun and Wang, Haoyu and Jin, Xin},
title = {Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597204},
doi = {10.1145/3597204},
abstract = {Deep learning (DL) has become a key component of modern software. In the “big model” era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers’ issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers’ issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {156},
numpages = {26},
keywords = {software engineering, distributed training, Empirical study}
}

@inproceedings{10.1007/978-981-99-8664-4_13,
author = {Sun, Hao and Bu, Zhe and Xiao, Yang and Zhou, Chengsheng and Hao, Zhiyu and Zhu, Hongsong},
title = {Software Vulnerability Detection Using an&nbsp;Enhanced Generalization Strategy},
year = {2023},
isbn = {978-981-99-8663-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8664-4_13},
doi = {10.1007/978-981-99-8664-4_13},
abstract = {Detecting vulnerabilities in software is crucial for preventing cybersecurity attacks, and current machine learning-based methods rely on large amounts of labeled data to train detection models. On the one hand, a major assumption is that the training and test data follow an identical distribution. However, vulnerabilities in different software projects may exhibit various distributions due to their application scenarios, coding habits, and other factors. On the other hand, when detecting vulnerabilities in new projects, it is time-consuming to retrain and test the models. Especially for new projects being developed, it has few or no instances of vulnerabilities. Therefore, how to leverage previous learning experience to learn new projects faster is important. To address these issues, we propose VulGML, a vulnerability detection approach using graph embedding and meta-learning. The goal is to establish a model with enhanced generalization, so that the model trained on multiple known projects can detect vulnerabilities in new projects. To further illustrate the strong generalization of VulGML, we also choose multiple known vulnerability types to train the meta-learning model and a new vulnerability type for vulnerability detection. Experimental results show that VulGML outperforms the state-of-the-art methods by 6.44–39.61% in detecting new projects, achieves an accuracy higher than 77.80% when detecting vulnerabilities in new vulnerability types, and its modules have greatly improved detection performance, demonstrating that VulGML is potentially valuable in practical usage.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 9th International Symposium, SETTA 2023, Nanjing, China, November 27–29, 2023, Proceedings},
pages = {226–242},
numpages = {17},
keywords = {Meta-learning, Graph embedding, Enhanced generalization, Vulnerability detection, Cybersecurity attacks},
location = {<conf-loc content-type="InPerson">Nanjing, China</conf-loc>}
}

@inproceedings{10.1145/3597926.3598048,
author = {Dong, Yihong and Li, Ge and Jin, Zhi},
title = {CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598048},
doi = {10.1145/3597926.3598048},
abstract = {General-purpose code generation aims to automatically convert the natural language description to code snippets in a general-purpose programming language (GPL) such as Python. In the process of code generation, it is essential to guarantee the generated code satisfies grammar constraints of GPL. However, existing sequence-to-sequence (Seq2Seq) approaches neglect grammar rules when generating GPL code. In this paper, we devise a pushdown automaton (PDA)-based methodology to make the first attempt to consider grammatical Seq2Seq models for general-purpose code generation, exploiting the principle that PL is a subset of PDA recognizable language and code accepted by PDA is grammatical. Specifically, we construct a PDA module and design an algorithm to constrain the generation of Seq2Seq models to ensure grammatical correctness. Guided by this methodology, we further propose CODEP, a code generation framework equipped with a PDA module, to integrate the deduction of PDA into deep learning. This framework leverages the state of PDA deduction (including state representation, state prediction task, and joint prediction with state) to assist models in learning PDA deduction. To comprehensively evaluate CODEP, we construct a PDA for Python and conduct extensive experiments on four public benchmark datasets. CODEP can employ existing sequence-based models as base models, and we show that it achieves 100% grammatical correctness percentage on these benchmark datasets. Consequently, CODEP relatively improves 17% CodeBLEU on CONALA, 8% EM on DJANGO, and 15% CodeBLEU on JUICE-10K compared to base models. Moreover, PDA module also achieves significant improvements on the pre-trained models.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {188–198},
numpages = {11},
keywords = {Seq2Seq, PL, PDA, Code intelligence, Code generation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1007/978-981-99-6222-8_30,
author = {Li, Wei and Li, Xiang and Feng, Wanzheng and Jin, Guanglu and Liu, Zhihan and Jia, Jing},
title = {Vulnerability Detection Based on Unified Code Property Graph},
year = {2023},
isbn = {978-981-99-6221-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-6222-8_30},
doi = {10.1007/978-981-99-6222-8_30},
abstract = {As the number of source codes grows rapidly, detecting source code vulnerabilities in current software has become an important study. Most current deep learning-based vulnerability detection technologies treat source code as a sequence, which loses the source code’s structural information, leading to many false positives in the detection results. We propose a novel source code vulnerability detection model, named UCPGVul, based on the Unified Code Property Graph (UCPG). A new graph representation, UCPG, is proposed to extract semantic features from the source code. By extracting features from UCPG, our proposed UCPGVul model can capture more vulnerability features. Experimental results on a publicly available dataset show that UCPGVul can achieve more accurate and stable detection results compared to five state-of-the-art methods.},
booktitle = {Web Information Systems and Applications: 20th International Conference, WISA 2023,  Chengdu, China, September 15–17, 2023,  Proceedings},
pages = {359–370},
numpages = {12},
keywords = {Graph neural network, Source code analysis, Vulnerability detection},
location = {Chengdu, China}
}

@inproceedings{10.1007/978-3-031-49266-2_16,
author = {Piereder, Christina and Fleck, Günter and Geist, Verena and Moser, Michael and Pichler, Josef},
title = {Using AI-Based Code Completion for&nbsp;Domain-Specific Languages},
year = {2023},
isbn = {978-3-031-49265-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49266-2_16},
doi = {10.1007/978-3-031-49266-2_16},
abstract = {Code completion is a very important feature of modern integrated development environments. Research has been done for years to improve code completion systems for general-purpose languages. However, only little literature can be found for (AI-based) code completion for domain specific languages (DSLs). A DSL is a special-purpose programming language tailored for a specific application domain. In this paper, we investigate whether AI-based state-of-the-art code completion approaches can also be applied for DSLs. This is demonstrated using the domain-specific language TTI (Thermal Text Input). TTI is used for power transformer design specification in an industrial context, where an existing code completion shall be replaced by an advanced machine learning approach. For this purpose, implementations of two code completion systems are adapted to our needs. One of them shows very promising results and achieves a top-5 accuracy of 97%. To evaluate the practical applicability, the approach is integrated into an existing editor of a power transformer manufacturer.},
booktitle = {Product-Focused Software Process Improvement: 24th International Conference, PROFES 2023, Dornbirn, Austria, December 10–13, 2023, Proceedings, Part I},
pages = {227–242},
numpages = {16},
keywords = {Applied Research, Experiences and Lessons Learned, Industrial Transformer Construction Domain, Domain-Specific Languages, AI-based Code Completion},
location = {<conf-loc content-type="InPerson">Dornbirn, Austria</conf-loc>}
}

@inproceedings{10.1145/3609437.3609456,
author = {Li, Jia and Liu, Fang and Li, Jia and Zhao, Yunfei and Li, Ge and Jin, Zhi},
title = {MCodeSearcher: Multi-View Contrastive Learning for Code Search},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609456},
doi = {10.1145/3609437.3609456},
abstract = {Code search has been a critical software development activity in facilitating developers to retrieve a proper code snippet from open-source repositories given a user intent. In recent years, large-scale pre-trained models have shown impressive performance on code representation learning and have achieved state-of-the-art performance on code search task. However, it is challenging for these models to distinguish the functionally equivalent code snippets with dissimilar implementations or the non-equivalent code snippets that look similar. Due to the diversity of the code implementations, it is necessary for the code search engines to identify the functional similarities or dissimilarities of source code so as to return the functionally matched source code for a given query. Besides, existing pre-trained models mainly focus on learning the semantic representations of code snippets. The semantic correlation between the code snippet and natural language query is not sufficiently exploited. An effective code search tool not only needs to understand the relationship between queries and code snippets but also needs to identify the relationship between diversified code snippets. To address these limitations, we propose a novel multi-view contrastive learning model MCodeSearcher for code retrieval, aiming at sufficiently exploiting (1) the semantic correlation between queries and code snippets, and (2) the relationship between functionally equivalent code snippets. To achieve this, we design contrastive training objectives from three views and pre-train our model with these objectives. The experimental results on five representative code search datasets show that our approach significantly outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {270–280},
numpages = {11},
keywords = {deep neural network, contrastive learning, Code search},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1109/ICSE48619.2023.00013,
author = {Wang, Deze and Chen, Boxing and Li, Shanshan and Luo, Wei and Peng, Shaoliang and Dong, Wei and Liao, Xiangke},
title = {One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00013},
doi = {10.1109/ICSE48619.2023.00013},
abstract = {As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned with the entire dataset on code summarization. Our experiments on three probing tasks show that adapter tuning significantly outperforms full-model fine-tuning and effectively overcomes catastrophic forgetting.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {5–16},
numpages = {12},
keywords = {multilingual task, adapter, transfer learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00060,
author = {Xue, Qiaomu},
title = {Automating Code Generation for MDE Using Machine Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00060},
doi = {10.1109/ICSE-Companion58688.2023.00060},
abstract = {The overall aim of our research is to improve the techniques for synthesizing code generators in the Model-Driven Engineering (MDE) context. Code generation is one of the main elements of Model-Driven Engineering, involving transformation from specification models to produce executable code. A code generator is designed to reduce the manual program construction work used to implement a software system, but building a code generator itself still currently needs much manual effort. Meanwhile, existing code generators are typically not flexible to adjust for changing development requirements and are hard to reuse for different target languages.Therefore, we aim to provide techniques to improve the process of building code generators, and let them be more reusable.Currently, we researched the related new and traditional approaches for generating code and projects using AI for program translation, code completion or program generation. Based on this research we decided to focus on a symbolic machine learning method related to the programming-by-example concept to build code generators. We use this "Code Generation By Example" (CGBE) concept with tree-to-tree structure mappings as the information format. CGBE has good performance in terms of training dataset size and time when applied to learning a UML-to-Java code generator, but further work is needed to extend it to generate different programming languages and to evaluate these cases, and to handle the optimisation of generated code.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {221–223},
numpages = {3},
keywords = {symbolic machine learning, model-driven engineering, model transformation by example, code generation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-51482-1_19,
author = {Guo, Yuejun and Hu, Qiang and Tang, Qiang and Traon, Yves Le},
title = {An Empirical Study of&nbsp;the&nbsp;Imbalance Issue in&nbsp;Software Vulnerability Detection},
year = {2024},
isbn = {978-3-031-51481-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-51482-1_19},
doi = {10.1007/978-3-031-51482-1_19},
abstract = {Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.},
booktitle = {Computer Security – ESORICS 2023: 28th European Symposium on Research in Computer Security, The Hague, The Netherlands, September 25–29, 2023, Proceedings, Part IV},
pages = {371–390},
numpages = {20},
keywords = {Software security, Vulnerability detection, Deep learning, Imbalance},
location = {<conf-loc content-type="InPerson">The Hague, The Netherlands</conf-loc>}
}

@article{10.1145/3624735,
author = {Guo, Yuejun and Hu, Qiang and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Le Traon, Yves},
title = {KAPE: kNN-based Performance Testing for Deep Code Search},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624735},
doi = {10.1145/3624735},
abstract = {Code search is a common yet important activity of software developers. An efficient code search model can largely facilitate the development process and improve the programming quality. Given the superb performance of learning the contextual representations, deep learning models, especially pre-trained language models, have been widely explored for the code search task. However, studies mainly focus on proposing new architectures for ever-better performance on designed test sets but ignore the performance on unseen test data where only natural language queries are available. The same problem in other domains, e.g., CV and NLP, is usually solved by test input selection that uses a subset of the unseen set to reduce the labeling effort. However, approaches from other domains are not directly applicable and still require labeling effort. In this article, we propose the kNN-based performance testing (KAPE) to efficiently solve the problem without manually matching code snippets to test queries. The main idea is to use semantically similar training data to perform the evaluation. Extensive experiments on six programming language datasets, three state-of-the-art pre-trained models, and seven baseline methods demonstrate that KAPE can effectively assess the model performance (e.g., CodeBERT achieves MRR 0.5795 on JavaScript) with a slight difference (e.g., 0.0261).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {48},
numpages = {24},
keywords = {test selection, deep learning testing, software testing, Deep code search}
}

@article{10.1016/j.eswa.2023.120978,
author = {Yu, Qingchen and Liu, Xin and Zhou, Qingguo and Zhuge, Jianwei and Wu, Chunming},
title = {Code classification with graph neural networks: Have you ever struggled to make it work?},
year = {2023},
issue_date = {Dec 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {233},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.120978},
doi = {10.1016/j.eswa.2023.120978},
journal = {Expert Syst. Appl.},
month = {dec},
numpages = {15},
keywords = {Software vulnerability, Code classification, Deep learning, Graph neural network}
}

@inproceedings{10.1145/3611643.3616304,
author = {Grishina, Anastasiia and Hort, Max and Moonen, Leon},
title = {The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616304},
doi = {10.1145/3611643.3616304},
abstract = {The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  

We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  

Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection accuracy on Devign with only 3 out of 12 layers of CodeBERT and a 3.3x speed-up of fine-tuning. These findings show that early layers can be used to obtain better results using the same resources, as well as to reduce resource usage during fine-tuning and inference.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {895–907},
numpages = {13},
keywords = {vulnerability detection, transformer, sustainability, model optimization, code classification, ML4SE, AI4SE, AI4Code},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00190,
author = {Yuan, Bin and Lu, Yifan and Fang, Yilin and Wu, Yueming and Zou, Deqing and Li, Zhen and Li, Zhi and Jin, Hai},
title = {Enhancing Deep Learning-Based Vulnerability Detection by Building Behavior Graph Model},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00190},
doi = {10.1109/ICSE48619.2023.00190},
abstract = {Software vulnerabilities have posed huge threats to the cyberspace security, and there is an increasing demand for automated vulnerability detection (VD). In recent years, deep learning-based (DL-based) vulnerability detection systems have been proposed for the purpose of automatic feature extraction from source code. Although these methods can achieve ideal performance on synthetic datasets, the accuracy drops a lot when detecting real-world vulnerability datasets. Moreover, these approaches limit their scopes within a single function, being not able to leverage the information between functions. In this paper, we attempt to extract the function's abstract behaviors, figure out the relationships between functions, and use this global information to assist DL-based VD to achieve higher performance. To this end, we build a Behavior Graph Model and use it to design a novel framework, namely VulBG. To examine the ability of our constructed Behavior Graph Model, we choose several existing DL-based VD models (e.g., TextCNN, ASTGRU, CodeBERT, Devign, and VulCNN) as our baseline models and conduct evaluations on two real-world datasets: the balanced FFMpeg+Qemu dataset and the unbalanced Chrome+Debian dataset. Experimental results indicate that VulBG enables all baseline models to detect more real vulnerabilities, thus improving the overall detection performance.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2262–2274},
numpages = {13},
keywords = {deep learning, behavior graph, vulnerability detection},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-48639-5_1,
author = {(Tony) Wasserman, Anthony I.},
title = {Specializations in Software Engineering Education},
year = {2023},
isbn = {978-3-031-48638-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48639-5_1},
doi = {10.1007/978-3-031-48639-5_1},
abstract = {This paper describes the origins and evolution of software engineering education as it has developed independently of computer science and electrical engineering programs. The rapid growth of software technology and development processes has led to the emergence of subdisciplines in software engineering, to the extent that it is no longer feasible for software engineers to remain knowledgeable about all of the relevant topics. As a result, it seems likely that software engineering education will follow the path taken in other fields, such as law and medicine, where students receive foundational education in software engineering, followed by additional education and practice in one or more specialized areas.},
booktitle = {Frontiers in Software Engineering Education: Second International Workshop, FISEE 2023, Villebrumier, France, January 23–25, 2023, Invited Papers},
pages = {1–13},
numpages = {13},
keywords = {education, Software engineering},
location = {<conf-loc content-type="InPerson">Villebrumier, France</conf-loc>}
}

@article{10.1007/s10664-023-10372-1,
author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Xu, Yiran and Han, Tingting and Chen, Taolue},
title = {A syntax-guided multi-task learning approach for Turducken-style code generation},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10372-1},
doi = {10.1007/s10664-023-10372-1},
abstract = {Due to the development of pre-trained language models, automated code generation techniques have shown great promise in recent years. However, the generated code will not always adhere to syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize three significant challenges in regards to syntactic constraints: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach TurduckenGen. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.},
journal = {Empirical Softw. Engg.},
month = {oct},
numpages = {35},
keywords = {Abstract syntax tree, CodeT5, Multi-task learning, Turducken-style code, Syntactically-constrained code generation}
}

@inproceedings{10.1145/3597926.3598037,
author = {Nie, Xu and Li, Ningke and Wang, Kailong and Wang, Shangguang and Luo, Xiapu and Wang, Haoyu},
title = {Understanding and Tackling Label Errors in Deep Learning-Based Vulnerability Detection (Experience Paper)},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598037},
doi = {10.1145/3597926.3598037},
abstract = {Software system complexity and security vulnerability diversity are plausible sources of the persistent challenges in software vulnerability research. Applying deep learning methods for automatic vulnerability detection has been proven an effective means to complement traditional detection approaches. Unfortunately, lacking well-qualified benchmark datasets could critically restrict the effectiveness of deep learning-based vulnerability detection techniques. Specifically, the long-term existence of erroneous labels in the existing vulnerability datasets may lead to inaccurate, biased, and even flawed results. In this paper, we aim to obtain an in-depth understanding and explanation of the label error causes. To this end, we systematically analyze the diversified datasets used by state-of-the-art learning-based vulnerability detection approaches, and examine their techniques for collecting vulnerable source code datasets. We find that label errors heavily impact the mainstream vulnerability detection models, with a worst-case average F1 drop of 20.7%. As mitigation, we introduce two approaches to dataset denoising, which will enhance the model performance by an average of 10.4%. Leveraging dataset denoising methods, we provide a feasible solution to obtain high-quality labeled datasets.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {52–63},
numpages = {12},
keywords = {vulnerability detection, denoising, deep learning},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00192,
author = {Yang, Xu and Wang, Shaowei and Li, Yi and Wang, Shaohua},
title = {Does Data Sampling Improve Deep Learning-Based Vulnerability Detection? Yeas! and Nays!},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00192},
doi = {10.1109/ICSE48619.2023.00192},
abstract = {Recent progress in Deep Learning (DL) has sparked interest in using DL to detect software vulnerabilities automatically and it has been demonstrated promising results at detecting vulnerabilities. However, one prominent and practical issue for vulnerability detection is data imbalance. Prior study observed that the performance of state-of-the-art (SOTA) DL-based vulnerability detection (DLVD) approaches drops precipitously in real world imbalanced data and a 73% drop of F1-score on average across studied approaches. Such a significant performance drop can disable the practical usage of any DLVD approaches. Data sampling is effective in alleviating data imbalance for machine learning models and has been demonstrated in various software engineering tasks. Therefore, in this study, we conducted a systematical and extensive study to assess the impact of data sampling for data imbalance problem in DLVD from two aspects: i) the effectiveness of DLVD, and ii) the ability of DLVD to reason correctly (making a decision based on real vulnerable statements). We found that in general, oversampling outperforms undersampling, and sampling on raw data outperforms sampling on latent space, typically random oversampling on raw data performs the best among all studied ones (including advanced one SMOTE and OSS). Surprisingly, OSS does not help alleviate the data imbalance issue in DLVD. If the recall is pursued, random undersampling is the best choice. Random oversampling on raw data also improves the ability of DLVD approaches for learning real vulnerable patterns. However, for a significant portion of cases (at least 33% in our datasets), DVLD approach cannot reason their prediction based on real vulnerable statements. We provide actionable suggestions and a roadmap to practitioners and researchers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2287–2298},
numpages = {12},
keywords = {interpretable AI, data sampling, deep learning, vulnerability detection},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3609437.3609459,
author = {Pan, Yue and Lyu, Chen},
title = {Measuring Efficient Code Generation with GEC},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609459},
doi = {10.1145/3609437.3609459},
abstract = {Although efficiency is one of the core metrics in programming, recent large-scale language models often face the issue of “inefficient code” generation, which struggles to meet the real-time requirements of algorithms. However, there is relatively little research on evaluating the selection of efficient algorithms, and it is not easy to rigorously assess a model’s ability to correctly choose efficient algorithm solutions. Furthermore, the selection of efficient algorithm solutions often relies on the appropriate application of problem-solving skills, necessitating more in-depth research on algorithm reasoning. To address this challenge, we introduce the Generation of Efficient Code (GEC) benchmark, which aims to evaluate the ability to select efficient algorithm solutions. Unlike code generation, our benchmark focuses on a model’s ability to generate satisfactory efficient code when given a natural language description and inefficient code. We propose two novel metrics to examine the efficiency of the generated code and assess the model’s ability to generate efficient code. Our benchmark includes 3,712 problems, 31,577 combinations of efficient and inefficient code pairs, and 13,092 alternative efficient codes. We evaluate the performance of mainstream code generation models on the GEC benchmark. As the societal importance of code efficiency increases in the coming years, our benchmark will provide an essential measurement standard for tracking research progress. Our dataset and models are open-source and can be accessed at https://github.com/CodeGeneration2/Efficient-Code-Generation-with-GEC.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {249–258},
numpages = {10},
keywords = {Code Generation, Code Efficiency Optimization, Benchmark Datasets},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3609437.3609451,
author = {Zhang, Xuejun and Zhang, Fenghe and Zhao, Bo and Zhou, Bo and Xiao, Boyang},
title = {VulD-Transformer: Source Code Vulnerability Detection via Transformer},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609451},
doi = {10.1145/3609437.3609451},
abstract = {The detection of software vulnerability is an important and challenging problem. Existing studies have shown that deep learning-based approaches can significantly improve the performance of vulnerability detection due to their powerful capabilities of automatic learning semantically rich code representation. However, the deep learning-based source code vulnerability detection methods still have limited learning ability for remote contextual dependency information between code statements. In this paper, we propose a deep learning-based code slice-level vulnerability detection via Transformer, dubbed VulD-Transformer, which is designed to detect vulnerabilities more effectively. In VulD-Transformer, transformer model is used to capture the critical features of vulnerabilities of long code slices. Especially, we firstly obtain code slices containing data dependencies and control dependencies by extracting the vulnerability syntax features and programs’ Program Dependency Graphs. Moreover, in order to improve the feature learning capability of the model for remote code statements, we design a Transformer-based vulnerability detection model. The experimental results on four synthetic datasets show that, compared to the VulDeePecker, SySeVR-BGRU, SySeVR-ABGRU and Russell approaches, VulD-Transformer achieves 6.12%, 8.01%, and 7.63% improvement on average in accuracy, recall and F1-measure respectively, when the code slices are more than 256 tokens. In addition, compared with these baselines, VulD-Transformer achieves 9.01%, 38.51%, and 20.98% improvement on average in accuracy, recall and F1-measure respectively on two real source code vulnerability datasets, Devign and REVEAL respectively, which are significantly higher than those of the comparison methods.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {185–193},
numpages = {9},
keywords = {transformer, deep learning, data dependencies, control dependencies, Vulnerabilities detection},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3597926.3598035,
author = {Ding, Yangruibo and Chakraborty, Saikat and Buratti, Luca and Pujar, Saurabh and Morari, Alessandro and Kaiser, Gail and Ray, Baishakhi},
title = {CONCORD: Clone-Aware Contrastive Learning for Source Code},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598035},
doi = {10.1145/3597926.3598035},
abstract = {Deep Learning (DL) models to analyze source code have shown immense promise during the past few years.  
More recently, self-supervised pre-training has gained traction for learning generic code representations valuable for many downstream SE tasks, such as clone and bug detection.  

While previous work successfully learned from different code abstractions (e.g., token, AST, graph), we argue that it is also essential to factor in how developers code day-to-day for learning general-purpose representation. On the one hand, human developers tend to write repetitive programs referencing existing code snippets from the current codebase or online resources (e.g., Stack Overflow website) rather than implementing functions from scratch; such behaviors result in a vast number of code clones. In contrast, a deviant clone by mistake might trigger malicious program behaviors.  

Thus, as a proxy to incorporate developers' coding behavior into the pre-training scheme, we propose to include code clones and their deviants. In particular, we propose CONCORD, a self-supervised pre-training strategy to place benign clones closer in the representation space while moving deviants further apart. We show that CONCORD's clone-aware pre-training drastically reduces the need for expensive pre-training resources while improving the performance of downstream SE tasks. We also empirically demonstrate that CONCORD can improve existing pre-trained models to learn better representations that consequently become more efficient in both identifying semantically equivalent programs and differentiating buggy from non-buggy code.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {26–38},
numpages = {13},
keywords = {Source Code Pre-training, Code Clone, Bug Detection},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1016/j.compeleceng.2023.108839,
author = {Yijing, Huang and Wei, Wanyue and He, Yang and Qihong, Wu and Kaiming, Xu},
title = {Intelligent algorithms for incident detection and management in smart transportation systems},
year = {2023},
issue_date = {Sep 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2023.108839},
doi = {10.1016/j.compeleceng.2023.108839},
journal = {Comput. Electr. Eng.},
month = {sep},
numpages = {15},
keywords = {Vehicle-to-infrastructure communication, Roadside infrastructure units, Traffic congestion, GAN, TSSAE, Deep learning, Intelligent transport systems, Incident detection}
}

@inproceedings{10.1145/3594315.3594371,
author = {Tang, Fanggeng and He, Pan},
title = {Software Defect Prediction using Multi-scale Structural Information},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594371},
doi = {10.1145/3594315.3594371},
abstract = {In recent years, most researches have used the sequence of nodes in the abstract syntax tree (AST) of code to extract features for software defect prediction (SDP). While the AST is a kind of graph data, it may ignore some part of the structural information to use the original graph data as a sequence for input. Thus, Graph neural network (GNN) has been used to extract structural information in SDP. However, existing researches ignore that GNN learning is inherently local. It is difficult to interact between remote nodes and to capture long-term dependencies in source code. We apply a combination model of GNN Transformer to predict the software defects. Using an AST directly as the input, GNN extracts local features and structural information between the node and its neighbors. We then encode the relative and absolute positions of the nodes in the AST. The position encodings are passed into the Transformer along with the feature information extracted by GNN to extract the global features, which are the long-term dependencies between nodes. Finally, the extracted fused features are used in the SDP. Experiments on the PROMISE dataset have shown that our method achieves higher F-measure and better identification of defective features in source code than the state-of-the-art SDP method.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {548–556},
numpages = {9},
keywords = {deep learning, Transformer, Software defect prediction, Graph neural network},
location = {<conf-loc>, <city>Tianjin</city>, <country>China</country>, </conf-loc>},
series = {ICCAI '23}
}

@article{10.1007/s10515-023-00407-8,
author = {Bano, Muneera and Hoda, Rashina and Zowghi, Didar and Treude, Christoph},
title = {Large language models for qualitative research in software engineering: exploring opportunities and challenges},
year = {2023},
issue_date = {May 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00407-8},
doi = {10.1007/s10515-023-00407-8},
abstract = {The recent surge in the integration of Large Language Models (LLMs) like ChatGPT into qualitative research in software engineering, much like in other professional domains, demands a closer inspection. This vision paper seeks to explore the opportunities of using LLMs in qualitative research to address many of its legacy challenges as well as potential new concerns and pitfalls arising from the use of LLMs. We share our vision for the evolving role of the qualitative researcher in the age of LLMs and contemplate how they may utilize LLMs at various stages of their research experience.},
journal = {Automated Software Engg.},
month = {dec},
numpages = {12},
keywords = {Software engineering, Qualitative research, LLMs, Large language models}
}

@article{10.1016/j.jss.2023.111763,
author = {Fan, Guodong and Chen, Shizhan and Wu, Hongyue and Gao, Cuiyun and Xiao, Jianmao and Xue, Xiao and Feng, Zhiyong},
title = {Dialog summarization for software collaborative platform via tuning pre-trained models},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {204},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111763},
doi = {10.1016/j.jss.2023.111763},
journal = {J. Syst. Softw.},
month = {oct},
numpages = {15},
keywords = {Live chat, Prompt tuning, Pre-trained language model, Text summarization, Software maintenance}
}

@article{10.1145/3630009,
author = {Shin, Jiho and Wei, Moshi and Wang, Junjie and Shi, Lin and Wang, Song},
title = {The Good, the Bad, and the Missing: Neural Code Generation for Machine Learning Tasks},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630009},
doi = {10.1145/3630009},
abstract = {Machine learning (ML) has been increasingly used in a variety of domains, while solving ML programming tasks poses unique challenges due to the fundamental difference in the nature and the construct of general programming tasks, especially for developers who do not have ML backgrounds. Automatic code generation that produces a code snippet from a natural language description can be a promising technique to accelerate ML programming tasks. In recent years, although many deep learning-based neural code generation models have been proposed with high accuracy, the fact that most of them are mainly evaluated on general programming tasks calls into question their effectiveness and usefulness in ML programming tasks. In this article, we set out to investigate the effectiveness of existing neural code generation models on ML programming tasks. For our analysis, we select six state-of-the-art neural code generation models and evaluate their performance on four widely used ML libraries, with newly created 83K pairs of natural-language described ML programming tasks. Our empirical study reveals some good, bad, and missing aspects of neural code generation models on ML tasks, with a few major ones listed below. (Good) Neural code generation models perform significantly better on ML tasks than on non-ML tasks with an average difference of 10.6 points in BLEU-4 scores. (Bad) More than 80% of the generated code is semantically incorrect. (Bad) Code generation models do not have significance in improving developers’ completion time. (Good) The generated code can help developers write correct code by providing developers with clues for using correct APIs. (Missing) The observation from our user study reveals the missing aspects of code generation for ML tasks, e.g., decomposing code generation for divide-and-conquer into API sequence identification and API usage generation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {51},
numpages = {24},
keywords = {empirical analysis, machine learning tasks, Neural code generation}
}

@article{10.1145/3640333,
author = {Shao, Changjie and Li, Gaolei and Wu, Jun and Zheng, Xi},
title = {Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges facing DNN-based Software Vulnerability Detection},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640333},
doi = {10.1145/3640333},
abstract = {To detect software vulnerabilities with better performance, deep neural networks (DNNs) have received extensive attention recently. However, these vulnerability detection DNN models trained with code representations are vulnerable to specific perturbations on code representations. This motivates us to rethink the bane of software vulnerability detection and find function-agnostic features during code representation which we name as semantic redundant features. This paper first identifies a tight correlation between function-agnostic triggers and semantic redundant feature space (where the redundant features reside) in these DNN models. For correlation identification, we propose a novel Backdoor-based Semantic Redundancy Exploration (BSemRE) framework. In BSemRE, the sensitivity of the trained models to function-agnostic triggers is observed to verify the existence of semantic redundancy in various code representations. Specifically, acting as the typical manifestations of semantic redundancy, naming conventions, ternary operators and identically-true conditions are exploited to generate function-agnostic triggers. Extensive comparative experiments on 1613823 samples of 8 representative vulnerability datasets and state-of-the-art code representation techniques and vulnerability detection models demonstrate that the existence of semantic redundancy determines the upper trustworthiness limit of DNN-based software vulnerability detection. To the best of our knowledge, this is the first work exploring the bane of software vulnerability detection using backdoor triggers.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Software vulnerability detection, deep neural networks, backdoor triggers, semantic redundancy, function-agnostic}
}

@inproceedings{10.1145/3593434.3593505,
author = {Heričko, Tjaša},
title = {Automatic Data-Driven Software Change Identification via Code Representation Learning},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593505},
doi = {10.1145/3593434.3593505},
abstract = {Changes to a software project are inevitable as the software requires continuous adaptations, improvements, and corrections throughout maintenance. Identifying the purpose and impact of changes made to the codebase is critical in software engineering. However, manually identifying and characterizing software changes can be a time-consuming and tedious process that adds to the workload of software engineers. To address this challenge, several attempts have been made to automatically identify and demystify intents of software changes based on software artifacts such as commit change logs, issue reports, change messages, source code files, and software documentation. However, these existing approaches have their limitations. These include a lack of data, limited performance, and an inability to evaluate compound changes. This paper presents a doctoral research proposal that aims to automate the process of identifying commit-level changes in software projects using software repository mining and code representation learning models. The research background, state-of-the-art, research objectives, research agenda, and threats to validity are discussed.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {319–323},
numpages = {5},
keywords = {software maintenance, neural source code embeddings, mining software repositories, machine learning, code commit},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1007/978-3-031-46077-7_36,
author = {Othman, Refat and Russo, Barbara},
title = {VULDAT: Automated Vulnerability Detection from&nbsp;Cyberattack Text},
year = {2023},
isbn = {978-3-031-46076-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-46077-7_36},
doi = {10.1007/978-3-031-46077-7_36},
abstract = {The existing literature on the connection between attacks and system vulnerabilities often relies on manual techniques. We have developed an approach, called VULDAT, to automatically identify software vulnerabilities and weaknesses from the text of an attack by leveraging the information contained in MITRE repositories and datasets that contain descriptions of attacks and information about attack methods, as well as code snippets that describe the related weaknesses of vulnerabilities. Thus, this research focuses on analyzing attack text descriptions and predicting their effects using natural language processing (NLP) and machine learning techniques. This can be helpful to quickly examine new attacks found in the real world and assist security experts in taking the appropriate actions.},
booktitle = {Embedded Computer Systems: Architectures, Modeling, and Simulation: 23rd International Conference, SAMOS 2023, Samos, Greece, July 2–6, 2023, Proceedings},
pages = {494–501},
numpages = {8},
keywords = {TTPs, ATT &amp;CK, Cyberattack text, Vulnerability detection},
location = {Samos, Greece}
}

@article{10.1016/j.jss.2023.111699,
author = {Yuan, Dawei and Wang, Xiaohui and Li, Yao and Zhang, Tao},
title = {Optimizing smart contract vulnerability detection via multi-modality code and entropy embedding},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {202},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111699},
doi = {10.1016/j.jss.2023.111699},
journal = {J. Syst. Softw.},
month = {aug},
numpages = {16},
keywords = {Vulnerability detection, Transfer learning, Bug injection, Smart contract}
}

@inproceedings{10.1145/3609437.3609439,
author = {Huang, Xiangbing and Ma, Yingwei and Zhou, Haifang and Jiang, Zhijie and Zhang, Yuanliang and Wang, Teng and Li, Shanshan},
title = {Towards Better Multilingual Code Search through Cross-Lingual Contrastive Learning},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609439},
doi = {10.1145/3609437.3609439},
abstract = {Recent advances in deep learning have significantly improved the understanding of source code by leveraging large amounts of open-source software data. Thanks to the larger amount of data, code representation models trained with multilingual datasets show superior performance to monolingual models and attract much more attention. However, the entangled source code from various programming languages makes multilingual models hard to differentiate language-specific textual semantics or syntactic structures, which significantly increases the difficulty of model learning from multilingual datasets directly. On the other hand, for a given problem, developers are likely to choose similar identifiers, even if coding in different languages. However, the presence of similar identifiers in multilingual code snippets does not mean that they implement the same functionality, which may misdirect models to overemphasize these unreliable signals and ignore the semantic information of multilingual code. To tackle the above issues, we propose LAMCode, a language-aware multilingual code understanding model. Specifically, we propose a simple yet effective method to perceive linguistic information by injecting language-specific viewer into the language models. Furthermore, we introduce a cross-lingual contrastive learning method by generating more similar training instances but with fewer overlapping features. This method prevents the models from over-relying on similar identifiers across languages. We conduct extensive experiments to evaluate the effectiveness of our approach on a large-scale multilingual dataset. The experimental results show that our approach significantly outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {22–32},
numpages = {11},
keywords = {multilingual, contrastive learning, code representation},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3603273.3635667,
author = {Zhu, Botong and Tan, Huobin},
title = {VuLASTE: Long Sequence Model with Abstract Syntax Tree Embedding for Vulnerability Detection},
year = {2024},
isbn = {9798400708268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603273.3635667},
doi = {10.1145/3603273.3635667},
abstract = {In this paper, we present a model named VuLASTE, treating vulnerability detection as a specialized text classification task. To address the vocabulary explosion problem, VuLASTE utilizes a byte-level BPE algorithm from natural language processing. Within VuLASTE, we introduce a novel AST path embedding to represent source code nesting information. Additionally, we employ a combination of global and dilated window attention from Longformer to extract long sequence semantics from source code. To tackle the issue of data imbalance, a common challenge in vulnerability detection datasets, we employ focal loss as a loss function. This ensures that the model prioritizes poorly classified cases during training. To evaluate our model's performance on real-world source code, we construct a cross-language and multi-repository vulnerability dataset from the Github Security Advisory Database. VuLASTE achieves top 50, top 100, top 200, and top 500 hits of 29, 51, 86, and 228, respectively, surpassing state-of-the-art researches.},
booktitle = {Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications},
pages = {392–396},
numpages = {5},
keywords = {Vulnerability Detection, Open Source Software, Natural Language Processing, Deep Learning},
location = {<conf-loc>, <city>Wuhan</city>, <country>China</country>, </conf-loc>},
series = {AAIA '23}
}

@article{10.1145/3628429,
author = {Kuang, Li and Cheng, Yi and Gao, HongHao},
title = {CCCS: Contrastive Cross-Language Code Search Using Code Graph Information},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3628429},
doi = {10.1145/3628429},
abstract = {Developers often search and reuse existing code snippets to improve software development efficiency during software development. Currently, researchers have proposed many code search methods. However, the search intent of existing methods is basically a natural language query. In order to support code migration and code refactoring, it is necessary to search relevant code snippets of another programming language with code snippets of one programming language. In this paper, we propose a Contrastive Cross-language Code Search method using code graph information, called CCCS. CCCS first converts code snippets into high-dimensional vectors using pre-trained CodeBERT to extract the sequence features of code snippets. Next, the structural features of code snippets are extracted using a graph convolutional neural network. Finally, the model is trained using the contrastive learning method to optimize the vector representation of cross-language code snippets, enabling the model to distinguish code snippets from different programming languages with the same functionality. To evaluate the effectiveness of our method, we conducted comparison experiments and ablation experiments on a small-scale dataset and a large-scale dataset, respectively. The experimental results show that our method far outperforms the state-of-the-art baseline model in terms of MRR metrics.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = {nov},
keywords = {contrastive learning, graph neural network, cross-language, code search}
}

@article{10.1016/j.infsof.2023.107175,
author = {Zain, Zuhaira Muhammad and Sakri, Sapiah and Ismail, Nurul Halimatul Asmak},
title = {Application of Deep Learning in Software Defect Prediction: Systematic Literature Review and Meta-analysis},
year = {2023},
issue_date = {Jun 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {158},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107175},
doi = {10.1016/j.infsof.2023.107175},
journal = {Inf. Softw. Technol.},
month = {jun},
numpages = {28},
keywords = {Meta-Analysis, Systematic Literature Review, Software Defect Prediction, Deep Learning}
}

@inproceedings{10.1145/3633053.3633057,
author = {Petrovska, Olga and Clift, Lee and Moller, Faron and Pearsall, Rebecca},
title = {Incorporating Generative AI into Software Development Education},
year = {2024},
isbn = {9798400709326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633053.3633057},
doi = {10.1145/3633053.3633057},
abstract = {This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way - by developing assessments which encourage learners to critically evaluate AI output - can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools “doing the homework”.},
booktitle = {Proceedings of the 8th Conference on Computing Education Practice},
pages = {37–40},
numpages = {4},
keywords = {software engineering, generative AI, education, assessment, apprenticeship},
location = {<conf-loc>, <city>Durham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CEP '24}
}

@article{10.1145/3633784,
author = {Senarath, Yasas and Mukhopadhyay, Ayan and Purohit, Hemant and Dubey, Abhishek},
title = {Designing a Human-centered AI Tool for Proactive Incident Detection using Crowdsourced Data Sources to Support Emergency Response},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633784},
doi = {10.1145/3633784},
abstract = {Time of incident reporting is a critical aspect of emergency response. However, the conventional approaches to receiving incident reports have time delays. Non-traditional sources such as crowdsourced data present an opportunity to detect incidents proactively. However, detecting incidents from such data streams is challenging due to inherent noise and data uncertainty. Naively maximizing detection accuracy can compromise spatial-temporal localization of inferred incidents, hindering response efforts. This paper presents a novel human-centered AI tool to address the above challenges. We demonstrate how crowdsourced data can aid incident detection while acknowledging associated challenges. We use an existing CROME framework to facilitate training and selection of best incident detection models, based on parameters suited for deployment. The human-centered AI tool provides a visual interface for exploring various measures to analyze the models for the practitioner’s needs, which could help the practitioners select the best model for their situation. Moreover, in this study, we illustrate the tool usage by comparing different models for incident detection. The experiments demonstrate that the CNN-based incident detection method can detect incidents significantly better than various alternative modeling approaches. In summary, this research demonstrates a promising application of human-centered AI tools for incident detection to support emergency response agencies.},
note = {Just Accepted},
journal = {Digit. Gov.: Res. Pract.},
month = {nov},
keywords = {Crowdsourcing, Human-centered AI Tool, Incident Detection, Emergency Response}
}

@article{10.1145/3593800,
author = {Wang, Chong and Peng, Xin and Xing, Zhenchang and Zhang, Yue and Liu, Mingwei and Luo, Rong and Meng, Xiujie},
title = {XCoS: Explainable Code Search Based on Query Scoping and Knowledge Graph},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3593800},
doi = {10.1145/3593800},
abstract = {When searching code, developers may express additional constraints (e.g., functional constraints and nonfunctional constraints) on the implementations of desired functionalities in the queries. Existing code search tools treat the queries as a whole and ignore the different implications of different parts of the queries. Moreover, these tools usually return a ranked list of candidate code snippets without any explanations. Therefore, the developers often find it hard to choose the desired results and build confidence on them. In this article, we conduct a developer survey to better understand and address these issues and induct some insights from the survey results. Based on the insights, we propose XCoS, an explainable code search approach based on query scoping and knowledge graph. XCoS extracts a background knowledge graph from general knowledge bases like Wikidata and Wikipedia. Given a code search query, XCoS identifies different parts (i.e., functionalities, functional constraints, nonfunctional constraints) from it and use the expressions of functionalities and functional constraints to search the codebase. It then links both the query and the candidate code snippets to the concepts in the background knowledge graph and generates explanations based on the association paths between these two parts of concepts together with relevant descriptions. XCoS uses an interactive user interface that allows the user to better understand the associations between candidate code snippets and the query from different aspects and choose the desired results. Our evaluation shows that the quality of the extracted background knowledge and the concept linkings in codebase is generally high. Furthermore, the generated explanations are considered complete, concise, and readable, and the approach can help developers find the desired code snippets more accurately and confidently.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {140},
numpages = {28},
keywords = {concept, knowledge, explainability, Code search}
}

@article{10.1016/j.ins.2023.03.132,
author = {Jie, Wanqing and Chen, Qi and Wang, Jiaqi and Voundi Koe, Arthur Sandor and Li, Jin and Huang, Pengfei and Wu, Yaqi and Wang, Yin},
title = {A novel extended multimodal AI framework towards vulnerability detection in smart contracts},
year = {2023},
issue_date = {Jul 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {636},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2023.03.132},
doi = {10.1016/j.ins.2023.03.132},
journal = {Inf. Sci.},
month = {jul},
numpages = {20},
keywords = {White box, AI approach, Multimodal, Vulnerability detection, Smart contract}
}

@inproceedings{10.1145/3583780.3615157,
author = {Sun, Xiaojie and Bi, Keping and Guo, Jiafeng and Ma, Xinyu and Fan, Yixing and Shan, Hongyu and Zhang, Qishen and Liu, Zhongyi},
title = {Pre-training with Aspect-Content Text Mutual Prediction for Multi-Aspect Dense Retrieval},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615157},
doi = {10.1145/3583780.3615157},
abstract = {Grounded on pre-trained language models (PLMs), dense retrieval has been studied extensively on plain text. In contrast, there has been little research on retrieving data with multiple aspects using dense models. In the scenarios such as product search, the aspect information plays an essential role in relevance matching, e.g., category: Electronics, Computers, and Pet Supplies. A common way of leveraging aspect information for multi-aspect retrieval is to introduce an auxiliary classification objective, i.e., using item contents to predict the annotated value IDs of item aspects. However, by learning the value embeddings from scratch, this approach may not capture the various semantic similarities between the values sufficiently. To address this limitation, we leverage the aspect information as text strings rather than class IDs during pre-training so that their semantic similarities can be naturally captured in the PLMs. To facilitate effective retrieval with the aspect strings, we propose mutual prediction objectives between the text of the item aspect and content. In this way, our model makes more sufficient use of aspect information than conducting undifferentiated masked language modeling (MLM) on the concatenated text of aspects and content. Extensive experiments on two real-world datasets (product and mini-program search) show that our approach can outperform competitive baselines both treating aspect values as classes and conducting the same MLM for aspect and content strings. Code and related dataset will be available at the URL footnotehttps://github.com/sunxiaojie99/ATTEMPT.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4300–4304},
numpages = {5},
keywords = {pre-training, multi-aspect, dense retrieval},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@proceedings{10.5555/3623290,
title = {ICSE-SEIS '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
abstract = {We are delighted to introduce the Software Engineering in Society (SEIS) track program as part of the 45th IEEE/ACM International Conference on Software Engineering, to be held in Melbourne, Australia, on May 14-20, 2023. The aim of the track is to bring together researchers studying various roles that software engineering plays in society.},
location = {Melbourne, Australia}
}

@article{10.1007/s10207-023-00752-5,
author = {Jain, Vikas Kumar and Tripathi, Meenakshi},
title = {An integrated deep learning model for Ethereum smart contract vulnerability detection},
year = {2023},
issue_date = {Feb 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {1},
issn = {1615-5262},
url = {https://doi.org/10.1007/s10207-023-00752-5},
doi = {10.1007/s10207-023-00752-5},
abstract = {Smart contracts are utilized widely in developing safe, secure, and efficient decentralized applications. Smart contracts hold a significant amount of cryptocurrencies, and upgrading or changing them after deployment on the blockchain is difficult. Therefore, it is essential to analyze the integrity of contracts to design secure contracts before deploying them. As a result, the effective detection of various class vulnerabilities in smart contracts is a significant concern. While human specialists are still necessary for vulnerability detection methods that utilize machine learning and deep learning, these approaches often miss numerous vulnerabilities, leading to a significant false-negative rate. This research proposes a two-step hierarchical model using deep learning techniques that significantly improve the feature extraction mechanism for Ethereum smart contracts to circumvent these limitations. The first step is to determine the relationship between opcodes using a transformer for extracting the internal features of contracts to strengthen the contextual information. Then, a Bi-GRU is employed to aggregate forward and backward sequential information for long-term reliance, including vulnerable code. In the second step, the Text-CNN and spatial attention extract the local features to emphasize the significant semantics. Experiments conducted on 49,552 real-world smart contracts have demonstrated that the proposed method is more effective than state-of-the-art methods. Extensive ablation experiments are carried out to additional illustrate the framework design option's efficacy.},
journal = {Int. J. Inf. Secur.},
month = {sep},
pages = {557–575},
numpages = {19},
keywords = {Blockchain, Deep learning, Smart contract, Vulnerability detection}
}

@article{10.1016/j.infsof.2023.107302,
author = {Wang, Wen-Yao and Wu, Chen-Hao and He, Jie},
title = {CLeBPI: Contrastive Learning for Bug Priority Inference},
year = {2023},
issue_date = {Dec 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {164},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107302},
doi = {10.1016/j.infsof.2023.107302},
journal = {Inf. Softw. Technol.},
month = {dec},
numpages = {14},
keywords = {Software maintenance, Bug priority inference, Bug report, Contrastive learning}
}

@article{10.1016/j.jss.2023.111698,
author = {Go, Ken Russel and Soundarapandian, Sruthi and Mitra, Aparupa and Vidoni, Melina and Ferreyra, Nicolás E. Díaz},
title = {Simple stupid insecure practices and GitHub’s code search: A looming threat?},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {202},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111698},
doi = {10.1016/j.jss.2023.111698},
journal = {J. Syst. Softw.},
month = {aug},
numpages = {9},
keywords = {Simple stupid insecure practices, GitHub code search, Python}
}

@article{10.4018/IJDCF.325062,
author = {Jiang, Ying and Yao, Wenjun and Yang, Yang},
title = {The Metric for Automatic Code Generation Based on Dynamic Abstract Syntax Tree},
year = {2023},
issue_date = {Aug 2023},
publisher = {IGI Global},
address = {USA},
volume = {15},
number = {1},
issn = {1941-6210},
url = {https://doi.org/10.4018/IJDCF.325062},
doi = {10.4018/IJDCF.325062},
abstract = {In order to improve the efficiency and quality of software development, automatic code generation technology is the current focus. The quality of the code generated by the automatic code generation technology is also an important issue. However, existing metrics for code automatic generation ignore that the programming process is a continuous dynamic changeable process. So the metric is a dynamic process. This article proposes a metric method based on dynamic abstract syntax tree (DAST). More specifically, the method first builds a DAST through the interaction in behavior information between the automatic code generation tool and programmer. Then the measurement contents are extracted on the DAST. Finally, the metric is completed with contents extracted. The experiment results show that the method can effectively realize the metrics of automatic code generation. Compared with the MAST method, the method in this article can improve the convergence speed by 80% when training the model, and can shorten the time-consuming by an average of 46% when doing the metric prediction.},
journal = {Int. J. Digit. Crime For.},
month = {jun},
pages = {1–20},
numpages = {20},
keywords = {Metric, Extract Algorithm, Dynamic Abstract Syntax Tree, Automatic Code Generation}
}

@inproceedings{10.5555/3618408.3620165,
author = {Zhang, Tianyi and Yu, Tao and Hashimoto, Tatsunori B. and Lewis, Mike and Yih, Wen-tau and Fried, Daniel and Wang, Sida I.},
title = {Coder reviewer reranking for code generation},
year = {2023},
publisher = {JMLR.org},
abstract = {Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1757},
numpages = {15},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@article{10.1016/j.jss.2023.111706,
author = {Wang, Mingke and Tao, Chuanqi and Guo, Hongjing},
title = {LCVD: Loop-oriented code vulnerability detection via graph neural network},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {202},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111706},
doi = {10.1016/j.jss.2023.111706},
journal = {J. Syst. Softw.},
month = {aug},
numpages = {12},
keywords = {Graph neural network, Code representation, Deep learning, Vulnerability detection, Loop-oriented vulnerability}
}

@article{10.1007/s10664-023-10417-5,
author = {Khan, Arif Ali and Khan, Javed Ali and Akbar, Muhammad Azeem and Zhou, Peng and Fahmideh, Mahdi},
title = {Insights into software development approaches: mining Q &amp;A repositories},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10417-5},
doi = {10.1007/s10664-023-10417-5},
journal = {Empirical Softw. Engg.},
month = {nov},
numpages = {38},
keywords = {Software development approaches, Q&amp; A websites, Software process improvement, Software repositories mining}
}

@inproceedings{10.1109/ICSE48619.2023.00169,
author = {McGuire, Sean and Schultz, Erin and Ayoola, Bimpe and Ralph, Paul},
title = {Sustainability is Stratified: Toward a Better Theory of Sustainable Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00169},
doi = {10.1109/ICSE48619.2023.00169},
abstract = {Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or "pillars"---environmental, social, economic, technical and individual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1996–2008},
numpages = {13},
keywords = {meta-synthesis, scoping review, sustainable software engineering, software engineering, sustainable development},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3629479.3629503,
author = {Cardoso, Ana Paula and Santos, Cleicy Priscilla and Collins, Eliane and Lima, Kelen and Quiroga, Pablo and Griego, Marlon},
title = {Evaluation of Automatic Test Case Generation for the Android Operating System using Deep Reinforcement Learning},
year = {2023},
isbn = {9798400707865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629479.3629503},
doi = {10.1145/3629479.3629503},
abstract = {The industry of large-scale software for mobile devices, such as the Android operating system, presents significant challenges regarding software validation and testing. This is due to the need to test on various devices, operating system versions, connections, and different hardware configurations. As a result, the manual creation of test cases can be a time-consuming process, and test cases can become outdated with updates in the Android version. To tackle these challenges, automatic test case generation emerges as an effective solution to streamline test creation and updates. In this context, Artificial Intelligence (AI) techniques, such as Deep Reinforcement Learning (DRL), have been explored to optimize this process and ensure adequate coverage of system requirements. This study evaluated the performance of the DRL state-of-the-art tool for test case generation DRL-MOBTEST [3] in an industry scenario context to generate test cases for Android functional applications (apps). The tool was performed in nine native apps (clock, maps, calculator, wallpaper, calendar, contacts, YouTube, drive, and files) regarding the functionalities coverage. The results showed a coverage range of 74.43%, and we compared it with the random Android SDK tool Monkey in five applications, revealing a trend of 63.52% improvement. The DRL-MOBTEST tool achieved the coverage of basic application paths through the creation of different test input types, such as symbols, numbers, and letters. It enables professionals to focus on complex scenarios and improve software quality across different devices and hardware configurations. However, it’s worth noting that human supervision is still necessary despite the advances offered by automated tools.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Software Quality},
pages = {228–235},
numpages = {8},
keywords = {Test Automation, Reinforcement Learning, Automatic Test Generation, Android, AI},
location = {<conf-loc>, <city>Bras\'{\i}lia</city>, <country>Brazil</country>, </conf-loc>},
series = {SBQS '23}
}

@article{10.1007/s10515-023-00409-6,
author = {Gerosa, Marco and Trinkenreich, Bianca and Steinmacher, Igor and Sarma, Anita},
title = {Can AI serve as a substitute for human subjects in software engineering research?},
year = {2024},
issue_date = {May 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00409-6},
doi = {10.1007/s10515-023-00409-6},
abstract = {Research within sociotechnical domains, such as software engineering, fundamentally requires the human perspective. Nevertheless, traditional qualitative data collection methods suffer from difficulties in participant recruitment, scaling, and labor intensity. This vision paper proposes a novel approach to qualitative data collection in software engineering research by harnessing the capabilities of artificial intelligence (AI), especially large language models (LLMs) like ChatGPT and multimodal foundation models. We explore the potential of AI-generated synthetic text as an alternative source of qualitative data, discussing how LLMs can replicate human responses and behaviors in research settings. We discuss AI applications in emulating humans in interviews, focus groups, surveys, observational studies, and user evaluations. We discuss open problems and research opportunities to implement this vision. In the future, an integrated approach where both AI and human-generated data coexist will likely yield the most effective outcomes.},
journal = {Automated Software Engg.},
month = {jan},
numpages = {12},
keywords = {Qualitative research, Software engineering, Foundation models, Large language models}
}

@inproceedings{10.5555/3615924.3615927,
author = {Nathalia, Nascimento and Paulo, Alencar and Donald, Cowan},
title = {Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {24–33},
numpages = {10},
keywords = {Machine Learning, ChatGPT, Performance Evaluation, AI-based solutions, Software Engineering},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3600061.3603141,
author = {Li, Fu and Huang, Jiaming and Gao, Yi and Dong, Wei},
title = {ChatIoT: Zero-code Generation of Trigger-action Based IoT Programs with ChatGPT},
year = {2023},
isbn = {9798400707827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600061.3603141},
doi = {10.1145/3600061.3603141},
abstract = {Trigger-Action Program (TAP) is a popular and significant form of Internet of Things (IoT) applications, commonly utilized in smart homes. Existing works either just perform actions based on commands or require human intervention to generate TAPs. With the emergence of Large Language Models (LLMs), it becomes possible for users to create IoT TAPs in zero-code manner using natural language. Thus, we propose ChatIoT, which employs LLMs to process natural language in chats and realizes the zero-code generation of TAPs for existing devices.},
booktitle = {Proceedings of the 7th Asia-Pacific Workshop on Networking},
pages = {219–220},
numpages = {2},
location = {<conf-loc>, <city>Hong Kong</city>, <country>China</country>, </conf-loc>},
series = {APNET '23}
}

@article{10.1016/j.future.2023.06.006,
author = {Ye, Guodong and Liu, Xin and Fan, Siqi and Tan, Yuan and Zhou, Qingguo and Zhou, Rui and Zhou, Xiaokang},
title = {Novel supply chain vulnerability detection based on heterogeneous-graph-driven hash similarity in IoT},
year = {2023},
issue_date = {Nov 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {148},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2023.06.006},
doi = {10.1016/j.future.2023.06.006},
journal = {Future Gener. Comput. Syst.},
month = {nov},
pages = {201–210},
numpages = {10},
keywords = {Vulnerability detection, Heterogeneous graph, Supply chain vulnerability, Binary code similarity}
}

@article{10.1016/j.jss.2023.111853,
author = {Chen, Xiang and Xia, Hongling and Pei, Wenlong and Ni, Chao and Liu, Ke},
title = {Boosting multi-objective just-in-time software defect prediction by fusing expert metrics and semantic metrics},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {206},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111853},
doi = {10.1016/j.jss.2023.111853},
journal = {J. Syst. Softw.},
month = {dec},
numpages = {12},
keywords = {Just-in-time defect prediction, Multi-objective optimization, Expert metrics, Semantic metrics, Metric fusion}
}

@article{10.1145/3638247,
author = {Cheng, Yu and Chen, Jieshan and Huang, Qing and Xing, Zhenchang and Xu, Xiwei and Lu, Qinghua},
title = {Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638247},
doi = {10.1145/3638247},
abstract = {The emergence of foundation models, such as large language models (LLMs) GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities across various domains. People can now use natural language (i.e. prompts) to communicate with AI to perform tasks. While people can use foundation models through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the underlying models, is not a production tool for building reusable AI services. APIs like LangChain allow for LLM-based application development but require substantial programming knowledge, thus posing a barrier. To mitigate this, we systematically review, summarise, refine and extend the concept of AI chain by incorporating the best principles and practices that have been accumulated in software engineering for decades into AI chain engineering, to systematize AI chain engineering methodology. We also develop a no-code integrated development environment, Prompt Sapper
, which embodies these AI chain engineering principles and patterns naturally in the process of building AI chains, thereby improving the performance and quality of AI chains. With Prompt Sapper, AI chain engineers can compose prompt-based AI services on top of foundation models through chat-based requirement analysis and visual programming. Our user study evaluated and demonstrated the efficiency and correctness of Prompt Sapper.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {SE for AI, No/Low Code, Large Language Models, Visual Programming, AI Chain Engineering}
}

@article{10.1145/3597207,
author = {Li, Jia and Li, Ge and Li, Zhuo and Jin, Zhi and Hu, Xing and Zhang, Kechi and Fu, Zhiyi},
title = {CodeEditor: Learning to Edit Source Code with Pre-trained Models},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597207},
doi = {10.1145/3597207},
abstract = {Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.In this article, we propose a novel pre-training task specialized in code editing and present an effective pre-trained code editing model named CodeEditor. Compared to previous code infilling tasks, our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the corresponding ground truth, to learn edit patterns. We conduct experiments on four code editing datasets and evaluate the pre-trained CodeEditor in three settings (i.e., fine-tuning, few-shot, and zero-shot). (1) In the fine-tuning setting, we train the pre-trained CodeEditor with four datasets and evaluate it on the test data. CodeEditor outperforms the SOTA baselines by 15%, 25.5%, 9.4%, and 26.6% on four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor with limited data and evaluate it on the test data. CodeEditor substantially performs better than all baselines, even outperforming baselines that are fine-tuned with all data. (3) In the zero-shot setting, we evaluate the pre-trained CodeEditor on the test data without training. CodeEditor correctly edits 1,113 programs, while the SOTA baselines cannot work. The results show that the superiority of our pre-training task and the pre-trained CodeEditor is more effective in automatic code editing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {143},
numpages = {22},
keywords = {deep learning, Pre-training, Source code editing}
}

@article{10.1145/3643674,
author = {Liu, Yue and Le-Cong, Thanh and Widyasari, Ratnadira and Tantithamthavorn, Chakkrit and Li, Li and Le, Xuan-Bach D. and Lo, David},
title = {Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643674},
doi = {10.1145/3643674},
abstract = {Since its introduction in November 2022, ChatGPT has rapidly gained popularity due to its remarkable ability in language understanding and human-like responses. ChatGPT, based on GPT-3.5 architecture, has shown great promise for revolutionizing various research fields, including code generation. However, the reliability and quality of code generated by ChatGPT remain unexplored, raising concerns about potential risks associated with the widespread use of ChatGPT-driven code generation. In this paper, we systematically study the quality of 4,066 ChatGPT-generated code implemented in two popular programming languages, i.e., Java and Python, for 2,033 programming tasks. The goal of this work is three folds. First, we analyze the correctness of ChatGPT on code generation tasks and uncover the factors that influence its effectiveness, including task difficulty, programming language, time that tasks are introduced, and program size. Second, we identify and characterize potential issues with the quality of ChatGPT-generated code. Last, we provide insights into how these issues can be mitigated. Experiments highlight that out of 4,066 programs generated by ChatGPT, 2,756 programs are deemed correct, 1,082 programs provide wrong outputs, and 177 programs contain compilation or runtime errors. Additionally, we further analyze other characteristics of the generated code through static analysis tools, such as code style and maintainability, and find that 1,930 ChatGPT-generated code snippets suffer from maintainability issues. Subsequently, we investigate ChatGPT’s self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step. Experiments suggest that ChatGPT can partially address these challenges, improving code quality by more than 20%, but there are still limitations and opportunities for improvement. Overall, our study provides valuable insights into the current limitations of ChatGPT and offers a roadmap for future research and development efforts to enhance the code generation capabilities of AI models like ChatGPT.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Automated code generation, ChatGPT, code analysis}
}

@inproceedings{10.1007/978-3-031-49342-3_7,
author = {Possato, Tiago and Valentini, João H. and Southier, Luiz F. P. and Teixeira, Marcelo},
title = {Automated Code Generation for&nbsp;DES Controllers Modeled as&nbsp;Finite State Machines},
year = {2023},
isbn = {978-3-031-49341-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49342-3_7},
doi = {10.1007/978-3-031-49342-3_7},
abstract = {Finite State Machines (FSMs) are the foundation to design Discrete Event Systems (DESs). A FSM that designs a DES model can be further processed using Supervisory Control Theory (SCT) to synthesize correct-by-construction software. When applied to industrial-scale DESs, FSMs face limitations in the design, synthesis, and implementation steps. Supremica is a straightforward tool that facilitates design and synthesis but does not reach the implementation phase. This requires additional tools to convert FSM models into code. This paper presents the tool DEScMaker, which receives as input an FSM model outputting from Supremica and converts it into implementable C code. Our approach complements Supremica with code generation and allows taking advantage of its intuitive interface, useful simulator, and safe algorithms while automating a task that, in practice, consists of complex manual programming. An example illustrates the tool and quantifies its advantages.},
booktitle = {Formal Methods: Foundations and Applications: 26th Brazilian Symposium, SBMF 2023,  Manaus, Brazil, December 4–8, 2023,  Proceedings},
pages = {113–130},
numpages = {18},
keywords = {Code generation, Model conversion, Formal modeling},
location = {<conf-loc content-type="InPerson">Manaus, Brazil</conf-loc>}
}

@article{10.1007/s10515-023-00410-z,
author = {Anwar, Zeeshan and Afzal, Hammad},
title = {Mining crowd sourcing repositories for open innovation in software engineering},
year = {2024},
issue_date = {May 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00410-z},
doi = {10.1007/s10515-023-00410-z},
abstract = {Various development tools have been introduced and the choice of suitable development tool depends on the particular context like the type of application to be developed, the development process and application domain, etc. The real challenge is to deliver new features at the right time with a faster development cycle. The selection of suitable development tools will help developers to save time and effort. In this research, we will explore software engineering repositories (like StackOverflow) to collect feedback from developers about development tools. This will explore which features in a development tool are most important, which features are missing, and which features require changes. The answers to these questions can be found by mining the community question-answering sites (CQA). We will use user feedback to innovate the new features in the development tool. Various techniques of Big Data, Data Mining, Deep Learning, and Transformers including Generative Pre-Training Transformer will be used in our research. Some of the major techniques include (i) data collection from CQA sites like StackOverflow, (ii) data preprocessing (iii) categories the data into various topics using topic modeling (iv) sentiment analysis of data to get positive or negative aspects of features (v) ranking of users and their feedback. The output of this research will categorize the users feedback into various ideas, this will help organizations to decide which features are required, which features are not required, which features are difficult or confusing, and which new features should be introduced into a new release.},
journal = {Automated Software Engg.},
month = {jan},
numpages = {6},
keywords = {Open innovation, Community question answering, Topic modeling, Sentiment analysis, Quality assessment, Crowd sourcing}
}

@inproceedings{10.1145/3610969.3611132,
author = {Petrovska, Olga and Clift, Lee and Moller, Faron},
title = {Generative AI in Software Development Education: Insights from a Degree Apprenticeship Programme},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3611132},
doi = {10.1145/3610969.3611132},
abstract = {We describe insights gained from incorporating ChatGPT into assignments for our Software Engineering Degree Apprenticeship programme, including attitudes expressed by the learners and their employers regarding our approach.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {19},
numpages = {1},
keywords = {Software Engineering, Generative AI, Education, Apprenticeships},
location = {<conf-loc>, <city>Swansea</city>, <country>Wales Uk</country>, </conf-loc>},
series = {UKICER '23}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3593434.3594236,
author = {Hussain, Yasir and Huang, Zhiqiu and Zhou, Yu and Khan, Izhar Ahmed and Khan, Nasrullah and Abbas, Muhammad Zahid},
title = {Optimized Tokenization Process for Open-Vocabulary Code Completion: An Empirical Study},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3594236},
doi = {10.1145/3593434.3594236},
abstract = {Studies have substantiated the efficacy of deep learning-based models in various source code modeling tasks. These models are usually trained on large datasets that are divided into smaller units, known as tokens, utilizing either an open or closed vocabulary system. The selection of a tokenization method can have a profound impact on the number of tokens generated, which in turn can significantly influence the performance of the model. This study investigates the effect of different tokenization methods on source code modeling and proposes an optimized tokenizer to enhance the tokenization performance. The proposed tokenizer employs a hybrid approach that initializes with a global vocabulary based on the most frequent unigrams and incrementally builds an open-vocabulary system. The proposed tokenizer is evaluated against popular tokenization methods such as Closed, Unigram, WordPiece, and BPE tokenizers, as well as tokenizers provided by large pre-trained models such as PolyCoder and CodeGen. The results indicate that the choice of tokenization method can significantly impact the number of sub-tokens generated, which can ultimately influence the modeling performance of a model. Furthermore, our empirical evaluation demonstrates that the proposed tokenizer outperforms other baselines, achieving improved tokenization performance both in terms of a reduced number of sub-tokens and time cost. In conclusion, this study highlights the significance of the choice of tokenization method in source code modeling and the potential for improvement through optimized tokenization techniques.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {398–405},
numpages = {8},
keywords = {Source Code Modeling, Open-Vocabulary, Deep Learning, Code Tokenization},
location = {Oulu, Finland},
series = {EASE '23}
}

@article{10.1109/TSE.2023.3267446,
author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
title = {MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation},
year = {2023},
issue_date = {July 2023},
publisher = {IEEE Press},
volume = {49},
number = {7},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2023.3267446},
doi = {10.1109/TSE.2023.3267446},
abstract = {Large language models have demonstrated the ability to generate both natural language and programming language text. Although contemporary code generation models are trained on corpora with several programming languages, they are tested using benchmarks that are typically monolingual. The most widely used code generation benchmarks only target Python, so there is little quantitative evidence of how code generation models perform on other programming languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use MultiPL-E to extend the HumanEval benchmark (Chen et al., 2021) and MBPP benchmark (Austin et al., 2021) to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2022) and InCoder (Fried et al., 2022). We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.},
journal = {IEEE Trans. Softw. Eng.},
month = {jul},
pages = {3675–3691},
numpages = {17}
}

@inproceedings{10.1145/3628797.3628945,
author = {Lê Hồng, Bằng and Lê Đức, Thắng and Đoàn Minh, Trung and Trần Tuấn, Dũng and Phan Thế, Duy and Phạm Văn, Hậu},
title = {Contextual Language Model and Transfer Learning for Reentrancy Vulnerability Detection in Smart Contracts},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628945},
doi = {10.1145/3628797.3628945},
abstract = {The proliferation of smart contracts on blockchain technology has led to several security vulnerabilities, causing significant financial losses and instability in the contract layer. Existing machine learning-based static analysis tools have limited detection accuracy, even for known vulnerabilities. In this study, we propose a novel deep learning-based model combined with attention mechanisms for identifying security vulnerabilities in smart contracts. Our experiments on two large datasets (SmartBugs Wild and Slither Audited Smart Contracts) demonstrate that our approach successfully achieves a 90% detection accuracy in identifying smart contract reentrancy attacks (e.g. performing better than other existing state-of-the-art deep learning-based approaches). In addition, this work also establishes the practical application of deep learning-based technology in smart contract reentrancy vulnerability detection, which can promote future research in this domain.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {739–745},
numpages = {7},
keywords = {Smart contract, Reentrancy vulnerability, Language model, Deep learning, Blockchain},
location = {<conf-loc>, <city>Ho Chi Minh</city>, <country>Vietnam</country>, </conf-loc>},
series = {SOICT '23}
}

@article{10.1016/j.jss.2023.111705,
author = {Chen, Da and Feng, Lin and Fan, Yuqi and Shang, Siyuan and Wei, Zhenchun},
title = {Smart contract vulnerability detection based on semantic graph and residual graph convolutional networks with edge attention},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {202},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111705},
doi = {10.1016/j.jss.2023.111705},
journal = {J. Syst. Softw.},
month = {aug},
numpages = {15},
keywords = {Residual block, Edge attention, Graph convolutional networks, Code graph, Smart contract vulnerability detection}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@article{10.1145/3597202,
author = {Suneja, Sahil and Zhuang, Yufan and Zheng, Yunhui and Laredo, Jim and Morari, Alessandro and Khurana, Udayan},
title = {Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597202},
doi = {10.1145/3597202},
abstract = {AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models’ signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models’ signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model’s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {145},
numpages = {40},
keywords = {explainability, data augmentation, curriculum learning, signal awareness, reliability, neural networks, Machine learning}
}

@inproceedings{10.1145/3628797.3628963,
author = {Thi-Mai-Anh, Bui and Nhat-Hai, Nguyen},
title = {On the Value of Code Embedding and Imbalanced Learning Approaches for Software Defect Prediction},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628963},
doi = {10.1145/3628797.3628963},
abstract = {Automated software defect prediction aims to identify and estimate the likelihood of defects in software source code elements, seeking to enhance software quality while reducing testing costs. Previous research on software defect prediction primarily concentrated on investigating design-related features such as source code complexity and object-oriented design metrics for the purpose of classifying program elements into two categories: (i) defective and (ii) non-detective. Nevertheless, the majority of these studies have relied solely on hand-crafted software metrics, neglecting the valuable asset of source code instruction, which can play a pivotal role in detecting bugs. This study leverages the use of source code embedding techniques to extract essential information from program elements through a convolutional neural network. The likelihood of a source file element (e.g., class or method) being defective is established through the utilization of a fully connected network that incorporates both source code features and design-related attributes. Additionally, we explore specific imbalanced learning strategies to address the skewed defect data distribution issue. To assess the effectiveness of our proposed approach, we conducted experiments on the publicly available dataset, namely PROMISE. The empirical results consistently showcase the superior performance of our method, as it effectively predicts defective source files, outperforming other state-of-the-art models.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {510–516},
numpages = {7},
keywords = {sampling data, cost sensitive learning, convolutional neural network, code embedding},
location = {<conf-loc>, <city>Ho Chi Minh</city>, <country>Vietnam</country>, </conf-loc>},
series = {SOICT '23}
}

@inproceedings{10.1145/3597926.3598135,
author = {Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
title = {How Effective Are Neural Networks for Fixing Security Vulnerabilities},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598135},
doi = {10.1145/3597926.3598135},
abstract = {Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans, to better evaluate LLMs and APR techniques, and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities in VJBench-trans. Our findings include that (1) existing LLMs and APR models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities. Many of the generated patches are uncompilable patches. (2) Fine-tuning with general APR data improves LLMs’ vulnerability-fixing capabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix many Common Weakness Enumeration (CWE) types, such as CWE-325 Missing cryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes 8.7 transformed vulnerabilities, outperforming all the other LLMs and APR models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning LLMs with such data, and applying code simplification transformation to facilitate vulnerability repair.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1282–1294},
numpages = {13},
keywords = {Vulnerability, Language Model, Automated Program Repair, AI and Software Engineering},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@proceedings{10.5555/3623295,
title = {ICSE-SEET '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@inproceedings{10.5555/3618408.3620193,
author = {Zheng, Wenqing and Sharan, S P and Jaiswal, Ajay Kumar and Wang, Kevin and Xi, Yihan and Xu, Dejia and Wang, Zhangyang},
title = {Outline, then details: syntactically guided coarse-to-fine code generation},
year = {2023},
publisher = {JMLR.org},
abstract = {For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to jointly encode the natural language descriptions and syntactically aligned I/O data samples. Extensive evaluations show that ChainCoder outperforms state-of-the-arts, demonstrating that our progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions. Our codes are available at: https://github.com/VITA-Group/ChainCoder.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1785},
numpages = {17},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1007/978-3-031-48639-5_4,
author = {Kaleemunnisa and Scharff, Christelle and Bathula, Krishna Mohan and Chen, Kaiyin},
title = {Analyzing Scrum Team Impediments Using NLP},
year = {2023},
isbn = {978-3-031-48638-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48639-5_4},
doi = {10.1007/978-3-031-48639-5_4},
abstract = {In this research, we focus on the impediments encountered by students in capstone projects following the Scrum methodology. Scrum meeting notes were collected in a dataset to permit Scrum roles and instructors to monitor progress and issues. We identified 9 categories of impediments in this dataset: Android, Coding Skills, Debugging, External Factors, Firebase/Database, Git/GitHub, Teamwork, Time Management, and UI/UX Design. We developed a Large Language Model (LLM) to classify these impediments. Natural Language Processing (NLP) has the potential to support software engineering processes. The novelty of this research is that it attempts to identify impediments faced by students’ Scrum teams with AI and support students and instructors. The relevance of the approach was discussed with subject matter experts (SME) of the industry. The proposed model is useful in both the academic and industry settings, to identify on-the-fly areas that need attention and, if fixed, would increase team productivity.},
booktitle = {Frontiers in Software Engineering Education: Second International Workshop, FISEE 2023, Villebrumier, France, January 23–25, 2023, Invited Papers},
pages = {42–55},
numpages = {14},
keywords = {Software Engineering Education, Scrum, Natural Language Processing (NLP), Large Language Model (LLM), Machine Learning, Impediments, Artificial Intelligence (AI), Agile},
location = {<conf-loc content-type="InPerson">Villebrumier, France</conf-loc>}
}

@inproceedings{10.1007/978-3-031-49252-5_8,
author = {Taromirad, Masoumeh and Runeson, Per},
title = {A Literature Survey of&nbsp;Assertions in&nbsp;Software Testing},
year = {2023},
isbn = {978-3-031-49251-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49252-5_8},
doi = {10.1007/978-3-031-49252-5_8},
abstract = {Assertions are one of the most useful automated techniques for checking program’s behaviour and hence have been used for different verification and validation tasks. We provide an overview of the last two decades of research involving ‘assertions’ in software testing. Based on a term–based search, we filtered the inclusion of relevant papers and synthesised them w.r.t. the problem addressed, the solution designed, and the evaluation conducted. The survey rendered 119 papers on assertions in software testing. After test oracle, the dominant problem focus is test generation, followed by engineering aspects of assertions. Solutions are typically embedded in tool prototypes and evaluated throughout limited number of cases while using large–scale industrial settings is still a noticeable method. We conclude that assertions would be worth more attention in future research, particularly regarding the new and emerging demands (e.g.,&nbsp;verification of programs with uncertainty), for effective, applicable, and domain-specific solutions.},
booktitle = {Engineering of Computer-Based Systems: 8th International Conference, ECBS 2023, Västerås, Sweden, October 16–18, 2023, Proceedings},
pages = {75–96},
numpages = {22},
keywords = {literature survey, testing, assertions},
location = {<conf-loc content-type="InPerson">Västerås, Sweden</conf-loc>}
}

@inproceedings{10.1109/ICSE48619.2023.00181,
author = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
title = {On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00181},
doi = {10.1109/ICSE48619.2023.00181},
abstract = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in ~46% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code (±28%).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2149–2160},
numpages = {12},
keywords = {recommender systems, empirical study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1007/s11704-022-2440-7,
author = {Li, Mingzhen and Liu, Changxi and Liao, Jianjin and Zheng, Xuegui and Yang, Hailong and Sun, Rujun and Xu, Jun and Gan, Lin and Yang, Guangwen and Luan, Zhongzhi and Qian, Depei},
title = {Towards optimized tensor code generation for deep learning on sunway many-core processor},
year = {2023},
issue_date = {Apr 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-022-2440-7},
doi = {10.1007/s11704-022-2440-7},
abstract = {The flourish of deep learning frameworks and hardware platforms has been demanding an efficient compiler that can shield the diversity in both software and hardware in order to provide application portability. Among the existing deep learning compilers, TVM is well known for its efficiency in code generation and optimization across diverse hardware devices. In the meanwhile, the Sunway many-core processor renders itself as a competitive candidate for its attractive computational power in both scientific computing and deep learning workloads. This paper combines the trends in these two directions. Specifically, we propose swTVM} that extends the original TVM to support ahead-of-time compilation for architecture requiring cross-compilation such as Sunway. In addition, we leverage the architecture features during the compilation such as core group for massive parallelism, DMA for high bandwidth memory transfer and local device memory for data locality, in order to generate efficient codes for deep learning workloads on Sunway. The experiment results show that the codes generated by swTVM} achieve 1.79× improvement of inference latency on average compared to the state-of-the-art deep learning framework on Sunway, across eight representative benchmarks. This work is the first attempt from the compiler perspective to bridge the gap of deep learning and Sunway processor particularly with productivity and efficiency in mind. We believe this work will encourage more people to embrace the power of deep learning and Sunway many-core processor.},
journal = {Front. Comput. Sci.},
month = {sep},
numpages = {15},
keywords = {performance optimization, code generation, deep learning compiler, sunway processor}
}

@article{10.1145/3617946.3617957,
author = {Panichella, Sebastiano and Di Sorbo, Andrea},
title = {Summary of the 2nd Natural Language-based Software Engineering Workshop (NLBSE 2023)},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617957},
doi = {10.1145/3617946.3617957},
abstract = {Natural language processing (NLP) involves the automated anal- ysis and manipulation of human language. This includes algo- rithms that can analyze text created by humans and algorithms that can generate text that appears natural. Nowadays, NLP methods are becoming increasingly prevalent to enhance various aspects of software development. Indeed, throughout the software development lifecycle, numerous natural language artifacts are produced. Therefore, the existence of NLP-based approaches and tools has opened up possibilities for improving the e ectiveness and productivity of software engineers, processes, and products. The second edition of the Natural Language-Based Software Engi- neering Workshop (NLBSE) took place in 2023 alongside the 45th International Conference on Software Engineering (ICSE 2023), where the research community engaged in discussions about these approaches. This event brought together researchers and practi- tioners from the elds of NLP and software engineering to ex- change experiences, establish future research directions, and pro- mote the adoption of NLP techniques and tools in tackling chal- lenges speci c to software engineering. In this paper, we present a summary of the 2nd edition of the workshop, which comprised three full papers, four short/position papers, ve tool competi- tion/demonstration papers, two keynote talks (Automated Bug Management: Re ections &amp; the Road Ahead" by David Lo and Trends and Opportunities in the Application of Large Language Models: the Quest for Maximum E ect" by Albert Ziegler), fol- lowed by extensive discussion among NLBSE participants. More details can be found at https://nlbse2023.github.io/index. html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {60–63},
numpages = {4}
}

@article{10.5555/3636517.3636522,
author = {Crandall, Aaron S. and Sprint, Gina and Fischer, Bryan},
title = {Generative Pre-Trained Transformer (GPT) Models as a Code Review Feedback Tool in Computer Science Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {1},
issn = {1937-4771},
abstract = {Undergraduate computer science and software engineering students benefit significantly from in-depth reviews of their code early and often in their courses. Performing these reviews is time-consuming for teaching assistants and professors to complete, consequently impacting the timeliness and consistency of the provided feedback. When code feedback is not delivered close to the time of authorship, the utility of the review for students is diminished. Prior work with Automatic Static Analysis Tools has shown promise at using artificial intelligence to automate code reviews, with some success integrating them into classroom environments. To leverage new advances in Generative Pre-Trained Transformer (GPT) models, this work reports on an Automatic Review Tool (ART) to provide timely, automatically generated code reviews. ART was evaluated in a second-semester computer science course by integrating ART into the course's Github-based assignment submission system. A cohort of student volunteers (N = 74) read the ART reviews and provided feedback using a survey spanning two of their course assignments. The results of this pilot study show that students perceived ART was successful at detecting defects and offering style-based suggestions, and students were receptive to receiving future automated reviews of their work.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {38–47},
numpages = {10}
}

@article{10.1145/3617946.3617959,
author = {Krusche, Stephan and Bell, Jonathan and Tenbergen, Bastian},
title = {Software Engineering Education for the Next Generation:SEENG 2023 Workshop Report},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617959},
doi = {10.1145/3617946.3617959},
abstract = {The 5th International Workshop on Software Engineering Education for the Next Generation was held on May 16, 2023 in Melbourne, Australia. The workshop was part of the 45th International Conference on Software Engineering. It specifically supported the general theme of "Educating the Next Generation of Software Engineers". Building on its predecessors, the workshop used a highly interactive format, structured around eight short paper presentations to generate discussion topics, an activity to select the most interesting topics, and structured breakout sessions. This enabled the participants to discuss the most interesting topics in detail. Participants presented the results of the breakout sessions using mind maps.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {66–69},
numpages = {4}
}

@inproceedings{10.1109/ICSE48619.2023.00180,
author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Chen, Dongxiao and Ge, Jidong and Luo, Bin},
title = {An Empirical Comparison of Pre-Trained Models of Source Code},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00180},
doi = {10.1109/ICSE48619.2023.00180},
abstract = {While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently-developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2136–2148},
numpages = {13},
keywords = {AI for SE, pre-training of source code},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>}
}

@inproceedings{10.1109/ICSE48619.2023.00205,
author = {Nashid, Noor and Sintaha, Mifta and Mesbah, Ali},
title = {Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00205},
doi = {10.1109/ICSE48619.2023.00205},
abstract = {Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, CEDAR, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare CEDAR with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, CEDAR outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, CEDAR yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as CEDAR could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2450–2462},
numpages = {13},
keywords = {test assertion generation, program repair, few-shot learning, transformers, large language models},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-43240-8_10,
author = {Enoiu, Eduard Paul and Gay, Gregory and Esber, Jameel and Feldt, Robert},
title = {Understanding Problem Solving in&nbsp;Software Testing: An Exploration of&nbsp;Tester Routines and&nbsp;Behavior},
year = {2023},
isbn = {978-3-031-43239-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43240-8_10},
doi = {10.1007/978-3-031-43240-8_10},
abstract = {Software testing is a difficult, intellectual activity performed in a social environment. Naturally, testers use and allocate multiple cognitive resources towards this task. The goal of this study is to understand better the routine and behaviour of human testers and their mental models when performing testing. We investigate this topic by surveying 38 software testers and developers in Sweden. The survey explores testers’ cognitive processes when performing testing by investigating the knowledge they bring, the activities they select and perform, and the challenges they face in their routine. By analyzing the survey results, we provide a characterization of tester practices and identify insights regarding the problem-solving process. We use these descriptions to further enhance a cognitive model of software testing.},
booktitle = {Testing Software and Systems: 35th IFIP WG 6.1 International Conference, ICTSS 2023, Bergamo, Italy, September 18–20, 2023, Proceedings},
pages = {143–159},
numpages = {17},
keywords = {Software Testing, Problem Solving, Test Design},
location = {Bergamo, Italy}
}

@proceedings{10.1145/3617555,
title = {PROMISE 2023: Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 19th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2023), to be held in presence on December 8th, 2023, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2023).},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@inproceedings{10.5555/3615924.3615928,
author = {Garima, Malik and Mucahit, Cevik and Ayşe, Başar},
title = {Data Augmentation for Conflict and Duplicate Detection in Software Engineering Sentence Pairs},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper explores the use of text data augmentation techniques to enhance conflict and duplicate detection in software engineer-ing tasks through sentence pair classification. T he s tudy adapts generic augmentation techniques such as shuffling, back translation, and paraphrasing and proposes new data augmentation techniques such as Noun-Verb Substitution, target-lemma replacement and Actor-Action Substitution for software requirement texts. A com-prehensive empirical analysis is conducted on six software text datasets to identify conflicts and duplicates among sentence pairs. The results demonstrate that data augmentation techniques have a significant i mpact o n t he p erformance o f a ll s oftware p air text datasets. On the other hand, in cases where the datasets are rela-tively balanced, the use of augmentation techniques may result in a negative effect on the classification performance.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {34–43},
numpages = {10},
keywords = {Sentence Pair Classification, Software Engineering, Duplicate Detection, Conflict Detection, Text Data Augmentation},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3624032.3624035,
author = {Guilherme, Vitor and Vincenzi, Auri},
title = {An initial investigation of ChatGPT unit test generation capability},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624035},
doi = {10.1145/3624032.3624035},
abstract = {Context: Software testing ensures software quality, but developers often disregard it. The use of automated testing generation is pursued to reduce the consequences of overlooked test cases in a software project. Problem: In the context of Java programs, several tools can completely automate generating unit test sets. Additionally, studies are conducted to offer evidence regarding the quality of the generated test sets. However, it is worth noting that these tools rely on machine learning and other AI algorithms rather than incorporating the latest advancements in Large Language Models (LLMs). Solution: This work aims to evaluate the quality of Java unit tests generated by an OpenAI LLM algorithm, using metrics like code coverage and mutation test score. Method: For this study, 33 programs used by other researchers in the field of automated test generation were selected. This approach was employed to establish a baseline for comparison purposes. For each program, 33 unit test sets were generated automatically, without human interference, by changing Open AI API parameters. After executing each test set, metrics such as code line coverage, mutation score, and success rate of test execution were collected to evaluate the efficiency and effectiveness of each set. Summary of Results: Our findings revealed that the OpenAI LLM test set demonstrated similar performance across all evaluated aspects compared to traditional automated Java test generation tools used in the previous research. These results are particularly remarkable considering the simplicity of the experiment and the fact that the generated test code did not undergo human analysis.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {15–24},
numpages = {10},
keywords = {testing tools, software testing, mutation testing, experimental software engineering, coverage testing, automated test generation},
location = {<conf-loc>, <city>Campo Grande, MS</city>, <country>Brazil</country>, </conf-loc>},
series = {SAST '23}
}

@inproceedings{10.1007/978-3-031-48796-5_14,
author = {Dakhama, Aidan and Even-Mendoza, Karine and Langdon, W.B. and Menendez, Hector and Petke, Justyna},
title = {SearchGEM5: Towards Reliable Gem5 with&nbsp;Search Based Software Testing and&nbsp;Large Language Models},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48796-5_14},
doi = {10.1007/978-3-031-48796-5_14},
abstract = {We introduce a novel automated testing technique that combines LLM and search-based fuzzing. We use ChatGPT to parameterise C programs. We compile the resultant code snippets, and feed compilable ones to SearchGEM5, our extension to AFL++ fuzzer with customised new mutation operators. We run thus created 4005 binaries through our system under test, gem5, increasing its existing test coverage by more than 1000 lines. We discover 244 instances where gem5 simulation of the binary differs from the binary’s expected behaviour.},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {160–166},
numpages = {7},
keywords = {genetic improvement of tests, SBFT, SBSE, LLM, AI},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@inproceedings{10.1145/3617555.3634736,
author = {Khomh, Foutse},
title = {Harnessing Predictive Modeling and Software Analytics in the Age of LLM-Powered Software Development (Invited Talk)},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3634736},
doi = {10.1145/3617555.3634736},
abstract = {In the rapidly evolving landscape of software development, Large Language Models (LLM) have emerged as powerful tools that can significantly impact the way software code is written, reviewed, and optimized, making them invaluable resources for programmers. They offer developers the ability to leverage pre-trained knowledge and tap into vast code repositories, enabling faster development cycles and reducing the time spent on repetitive or mundane coding tasks. However, while these models offer substantial benefits, their adoption also presents multiple challenges. For example, they might generate code snippets that are syntactically correct but functionally flawed, requiring human review and validation. Moreover, the ethical considerations surrounding these models, such as biases in the training data, should be carefully addressed to ensure fair and inclusive software development practices. This talk will provide an overview and reflection on some of these challenges, present some preliminary solutions, and discuss opportunities for predictive models and data analytics.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {1},
numpages = {1},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {PROMISE 2023}
}

@article{10.1145/3617367,
author = {Prather, James and Reeves, Brent N. and Denny, Paul and Becker, Brett A. and Leinonen, Juho and Luxton-Reilly, Andrew and Powell, Garrett and Finnie-Ansley, James and Santos, Eddie Antonio},
title = {“It’s Weird That it Knows What I Want”: Usability and Interactions with Copilot for Novice Programmers},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3617367},
doi = {10.1145/3617367},
abstract = {Recent developments in deep learning have resulted in code-generation models that produce source code from natural language and code-based prompts with high accuracy. This is likely to have profound effects in the classroom, where novices learning to code can now use free tools to automatically suggest solutions to programming exercises and assignments. However, little is currently known about how novices interact with these tools in practice. We present the first study that observes students at the introductory level using one such code auto-generating tool, Github Copilot, on a typical introductory programming (CS1) assignment. Through observations and interviews we explore student perceptions of the benefits and pitfalls of this technology for learning, present new observed interaction patterns, and discuss cognitive and metacognitive difficulties faced by students. We consider design implications of these findings, specifically in terms of how tools like Copilot can better support and scaffold the novice programming experience.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {nov},
articleno = {4},
numpages = {31},
keywords = {OpenAI, novice programming, LLM, large language models, introductory programming, HCI, GPT-3, GitHub, CS1, Copilot, Codex, automatic code generation, Artificial Intelligence, AI}
}

@proceedings{10.1145/3617570,
title = {QP4SE 2023: Proceedings of the 2nd International Workshop on Quantum Programming for Software Engineering},
year = {2023},
isbn = {9798400703768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the second edition of the workshop on Quantum Programming for Software Engineering (QP4SE) to be held virtually, December 4th, 2023, co-located with ESEC/FSE 2023, San Francisco.},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@article{10.1016/j.cose.2023.103508,
author = {Gu, Yeming and Shu, Hui and Kang, Fei},
title = {BinAIV: Semantic-enhanced vulnerability detection for Linux x86 binaries},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {135},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103508},
doi = {10.1016/j.cose.2023.103508},
journal = {Comput. Secur.},
month = {dec},
numpages = {12},
keywords = {Deep learning, Binary code, Code similarity, Vulnerability detection, Function semantic}
}

@article{10.1016/j.engappai.2023.106304,
author = {Alizadehsani, Zakieh and Ghaemi, Hadi and Shahraki, Amin and Gonzalez-Briones, Alfonso and Corchado, Juan M.},
title = {DCServCG: A data-centric service code generation using deep learning},
year = {2023},
issue_date = {Aug 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {123},
number = {PB},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106304},
doi = {10.1016/j.engappai.2023.106304},
journal = {Eng. Appl. Artif. Intell.},
month = {aug},
numpages = {17},
keywords = {Code auto-completion, Transformers, Language modeling, SOA, Web service, Code mining}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00029,
author = {Arony, Nowshin Nawar and Devathasan, Kezia and Li, Ze Shi and Damian, Daniela},
title = {Leveraging Diversity in Software Engineering Education through Community Engaged Learning and a Supportive Network},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00029},
doi = {10.1109/ICSE-SEET58685.2023.00029},
abstract = {While a lack of diversity is a longstanding problem in computer science and engineering, universities and organizations continue to look for solutions to this issue. Among the first of its kind, we launched INSPIRE: STEM for Social Impact, a program at the University of Victoria, Canada, aimed to motivate and empower students from underrepresented groups in computer science and engineering to develop digital solutions for society impactful projects by engaging in experiential learning projects with identified community-partners. The twenty-four students in the program came from diverse backgrounds in terms of academic areas of study, genders, ethnicities, and levels of technical and educational experience. Working with six community partners, these students spent four months learning and developing solutions for a societal and/or environmental problem with potential for local and global impacts. Our experiences indicate that working in a diverse team with real clients on solving pressing issues produces a sense of competence, relatedness, and autonomy which are the basis of self-determination theory. Due to the unique structure of this program, the three principles of self-determination theory emerged through different experiences, ultimately motivating the students to build a network of like-minded people. The importance of such a network is profound in empowering students to succeed and, in retrospect, remain in software engineering fields. We address the diversity problem by providing diverse, underrepresented students with a safe and like-minded environment where they can learn and realize their full potential. Hence, in this paper, we describe the program design, experiences, and lessons learned from this approach. We also provide recommendations for universities and organizations that may want to adapt our approach.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {247–258},
numpages = {12},
keywords = {design thinking, experiential learning, software engineering education diversity and inclusion},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@proceedings{10.5555/3623288,
title = {ICSE-NIER '23: Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Australia}
}

@article{10.1145/3635439.3635446,
author = {Jabbarvand, Reyhaneh and Tizpaz-Niari, Saeid and Barr, Earl T. and Chandra, Satish},
title = {Summary of the 1st Interpretability and Robustness in Neural Software Engineering (InteNSE 2023)},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3635439.3635446},
doi = {10.1145/3635439.3635446},
abstract = {InteNSE is an interdisciplinary workshop for research at the intersection of Machine Learning (ML) and Software Engineering (SE) and would be a pioneer in emphasizing the implicit properties of neural software engineering and analysis. Due to recent computational advancements, ML has become an inseparable part of the SE research community. ML can indeed improve and revolutionize many SE tasks. However, most research in the AI and SE communities consider ML as a closed box, i.e., only considering the final performance of the developed models as an evaluation metric. Ignoring the implicit properties of neural models, such as interpretability and robustness, one cannot validate the model's actual performance, generalizability, and whether it is learning what it is supposed to do. Specifically, in the domain of SE, where the result of ML4SE tools is code synthesis, bug finding, or repair, interpretability and robustness are crucial to ensure the reliability of the products.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {30–33},
numpages = {4}
}

@article{10.1007/s10664-023-10378-9,
author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Zhang, Hongyu and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
title = {CoCoAST: Representing Source Code via Hierarchical Splitting and Reconstruction of Abstract Syntax Trees},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10378-9},
doi = {10.1007/s10664-023-10378-9},
abstract = {Recently, machine learning techniques especially deep learning techniques have made substantial progress on some code intelligence tasks such as code summarization, code search, clone detection, etc. How to represent source code to effectively capture the syntactic, structural, and semantic information is a key challenge. Recent studies show that the information extracted from abstract syntax trees (ASTs) is conducive to code representation learning. However, existing approaches fail to fully capture the rich information in ASTs due to the large size/depth of ASTs. In this paper, we propose a novel model CoCoAST that hierarchically splits and reconstructs ASTs to comprehensively capture the syntactic and semantic information of code without the loss of AST structural information. First, we hierarchically split a large AST into a set of subtrees and utilize a recursive neural network to encode the subtrees. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, we combine AST representation carrying the syntactic and structural information and source code embedding representing the lexical information to obtain the final neural code representation. We have applied our source code representation to two common program comprehension tasks, code summarization and code search. Extensive experiments have demonstrated the superiority of CoCoAST. To facilitate reproducibility, our data and code are available .},
journal = {Empirical Softw. Engg.},
month = {oct},
numpages = {41},
keywords = {68T50, 68T07, 68-04, Neural networks, Abstract syntax trees, Code representation learning, Program comprehension}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>}
}

@inproceedings{10.1145/3593434.3593463,
author = {Savarimuthu, Bastin Tony Roy and Zareen, Zoofishan and Cheriyan, Jithin and Yasir, Muhammad and Galster, Matthias},
title = {Barriers for Social Inclusion in Online Software Engineering Communities - A Study of Offensive Language Use in Gitter Projects},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593463},
doi = {10.1145/3593434.3593463},
abstract = {Social inclusion is a fundamental feature of thriving societies. This paper first investigates barriers for social inclusion in online Software Engineering (SE) communities, by identifying a set of 11 attributes and organising them as a taxonomy. Second, by applying the taxonomy and analysing language used in the comments posted by members in 189 Gitter projects (with &gt; 3 million comments), it presents the evidence for the social exclusion problem. It employs a keyword-based search approach for this purpose. Third, it presents a framework for improving social inclusion in SE communities.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {217–222},
numpages = {6},
keywords = {software engineering communities, exclusion, Social inclusion},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00013,
author = {Huang, Qing and Zhu, Jiahui and Li, Zhilong and Xing, Zhenchang and Wang, Changjing and Xu, Xiwei},
title = {PCR-Chain: Partial Code Reuse Assisted by Hierarchical Chaining of Prompts on Frozen Copilot},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00013},
doi = {10.1109/ICSE-Companion58688.2023.00013},
abstract = {API documentation, technical blogs and programming Q&amp;A sites contain a large amount of partial code that can be reused in programming tasks. However, due to unresolved simple names and last-mile syntax errors, such partial code is frequently not compilable. To facilitate partial code reuse, we develop PCR-Chain for resolving FQNs and fixing last-mile syntax errors in partial code based on a giant pre-trained code model (e.g., Copilot). Methodologically, PCR-Chain is backed up by the underlying global-level prompt architecture (which combines three design ideas: hierarchical task breakdown, prompt composition including sequential and conditional structures, and a mix of prompt-based AI and non-AI units) and the local-level prompt design. Technically, we propose PCR-Chain, which employs in-context learning rather than supervised fine-tuning with gradient updates on downstream task data. This approach enables the frozen, giant pre-trained code model to learn the desired behavior for a specific task through behavior-describing prompts and imitate it to complete the task. Experimental results show that PCR-Chain automatically resolves the FQNs and fixes last-mile syntax errors in 50 partial code samples collected from Stack Overflow with high success rates, without requiring any program analysis. The correct execution of the unit, module, and PCR-Chain demonstrates the effectiveness of the prompt design, prompt composition, and prompt architecture.Website:https://github.com/SE-qinghuang/PCR-ChainDemo Video: https://youtu.be/6HGRNdc2_JE},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {1–5},
numpages = {5},
keywords = {hierarchical prompts, AI chain, frozen copilot, pre-trained language model, in-context learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-48796-5_12,
author = {Sobania, Dominik and Geiger, Alina and Callan, James and Brownlee, Alexander and Hanna, Carol and Moussa, Rebecca and López, Mar Zamorano and Petke, Justyna and Sarro, Federica},
title = {Evaluating Explanations for&nbsp;Software Patches Generated by&nbsp;Large Language Models},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48796-5_12},
doi = {10.1007/978-3-031-48796-5_12},
abstract = {Large language models (LLMs) have recently been integrated in a variety of applications including software engineering tasks. In this work, we study the use of LLMs to enhance the explainability of software patches. In particular, we evaluate the performance of GPT 3.5 in explaining patches generated by the search-based automated program repair system ARJA-e for 30 bugs from the popular Defects4J benchmark. We also investigate the performance achieved when explaining the corresponding patches written by software developers. We find that on average 84% of the LLM explanations for machine-generated patches were correct and 54% were complete for the studied categories in at least 1 out of 3 runs. Furthermore, we find that the LLM generates more accurate explanations for machine-generated patches than for human-written ones.},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {147–152},
numpages = {6},
keywords = {Genetic Improvement, Program Repair, AI Explainability, Software Patches, Large Language Models},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@proceedings{10.1145/3617553,
title = {Gamify 2023: Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the Program Committee, we are pleased to present the proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation (Gamify 2023). The workshop is virtually co-located with the 2023 edition of the ESEC/FSE conference, held in San Francisco (CA, USA). The workshop will be held online only the 4th of December 2023.},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@article{10.1016/j.future.2023.05.016,
author = {Wang, Jin and Xiao, Hui and Zhong, Shuwen and Xiao, Yinhao},
title = {DeepVulSeeker: A novel vulnerability identification framework via code graph structure and pre-training mechanism},
year = {2023},
issue_date = {Nov 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {148},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2023.05.016},
doi = {10.1016/j.future.2023.05.016},
journal = {Future Gener. Comput. Syst.},
month = {nov},
pages = {15–26},
numpages = {12},
keywords = {Code feature, Vulnerability pattern, Pre-training, Neural network, Software security, Vulnerability identification}
}

@inproceedings{10.1145/3613372.3614199,
author = {Fontão, Awdren and Matsubara, Edson and Mongelli, Henrique and Medeiros, Marcio and Lourenço, Carlos and Martins, Henrique and Cortez, Igor and Borges, Maria},
title = {Hyacinth macaw: a project-based learning program to develop talents in Software Engineering for Artificial Intelligence},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3614199},
doi = {10.1145/3613372.3614199},
abstract = {Software Engineering for Artificial Intelligence (SE4A) uses SE principles to design and maintain AI systems, requiring analytical thinking for software complexity, while AI demands mathematical knowledge and algorithm adjustment. The IEEE Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering states that extracurricular elements impact students’ preparation. This study focuses on the first module of a project-based learning talent development program involving undergraduate students, two expert professors (in AI and SE), and mentors from sponsoring companies. An exploratory case study with 39 students from four courses was conducted, challenging them to deliver an MVP in machine learning within 1.5 months. Results showed high agreement (87.5%) in applying learned skills to future projects, recognizing SE’s benefits (96.9%) in AI, and acknowledging the connection between SE and AI (78.1%). Participants applied relevant knowledge in ML performance, data analysis, and software architecture for AI. We share strategies used by students to enhance developer experience.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {312–321},
numpages = {10},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@proceedings{10.1145/3593663,
title = {ECSEE '23: Proceedings of the 5th European Conference on Software Engineering Education},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seeon/Bavaria, Germany}
}

@article{10.5555/3636988.3637014,
author = {Anewalt, Karen and Polack, Jennifer},
title = {Industry Trends in Software Engineering: Alumni Perspectives},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {3},
issn = {1937-4771},
abstract = {It is important for computer science curricula to prepare graduates for their future careers. Alignment efforts between academia and industry benefit both communities. Having data about current industry trends, including tools and critical experiences, allows academia to adjust course assignments and curricula to provide relevant and needed material in today's computer science job market. We present survey responses from industry professionals related to tool, project, communication, and collaboration experiences essential for new employees. The collected data can be used to update and enhance current assignments across curricula. Responding to industry trends and demands can give future computer science professionals valuable experience as they begin their careers.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {159–170},
numpages = {12}
}

@proceedings{10.1145/3624032,
title = {SAST '23: Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Campo Grande, MS</city>, <country>Brazil</country>, </conf-loc>}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00077,
author = {Ding, Zishuo},
title = {Towards Utilizing Natural Language Processing Techniques to Assist in Software Engineering Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00077},
doi = {10.1109/ICSE-Companion58688.2023.00077},
abstract = {Machine learning-based approaches have been widely used to address natural language processing (NLP) problems. Considering the similarity between natural language text and source code, researchers have been working on applying techniques from NLP to deal with code. On the other hand, source code and natural language are by nature different. For example, code is highly structured and executable. Thus, directly applying the NLP techniques may not be optimal, and how to effectively optimize these NLP techniques to adapt to software engineering (SE) tasks remains a challenge. Therefore, to tackle the challenge, in this dissertation, we focus on two research directions: 1) distributed code representations, and 2) logging statements, which are two important intersections between the natural language and source code. For distributed code representations, we first discuss the limitations of existing code embedding techniques, and then, we propose a novel approach to learn more generalizable code embeddings in a task-agnostic manner. For logging statements, we first propose an automated deep learning-based approach to automatically generate accurate logging texts by translating the related source code into short textual descriptions. Then, we make the first attempt to comprehensively study the temporal relations between logging and its corresponding source code, which is later used to detect issues in logging statements. We anticipate that our study can provide useful suggestions and support to developers in utilizing NLP techniques to assist in SE tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {286–290},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3617946.3617955,
author = {Bucchiarone, Antonio and Cooper, Kendra M. L. and Lin, Dayi and Smith, Adam and Wanick, Vanissa},
title = {Fostering Collaboration and Advancing Research in Software Engineering and Game Development for Serious Contexts},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617955},
doi = {10.1145/3617946.3617955},
abstract = {The potential benefits of using the engaging and interactive nature of games to achieve specific objectives have been recognized by researchers and professionals from numerous domains. Serious games have been developed to impart knowledge, skills, and awareness in areas such as education, healthcare and the environment, while gamification has been applied to enhance the engagement, motivation, and participation of users in non-game activities such as sustainability and learning. As a result, the fields of game engineering, software engineering, and user experience are increasingly converging to create innovative solutions that blend the strengths of games with real-world applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {46–50},
numpages = {5}
}

@proceedings{10.1145/3631991,
title = {WSSE '23: Proceedings of the 2023 5th World Symposium on Software Engineering},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>}
}

@article{10.1016/j.ipm.2023.103415,
author = {Guo, Juncai and Liu, Jin and Liu, Xiao and Wan, Yao and Li, Li},
title = {Summarizing source code with Heterogeneous Syntax Graph and dual position},
year = {2023},
issue_date = {Sep 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {60},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2023.103415},
doi = {10.1016/j.ipm.2023.103415},
journal = {Inf. Process. Manage.},
month = {sep},
numpages = {21},
keywords = {Heterogeneity, Transformer, Graph neural network, Programming language understanding, Code summarization}
}

@inproceedings{10.1145/3583131.3590379,
author = {Applis, Leonhard and Panichella, Annibale and Marang, Ruben},
title = {Searching for Quality: Genetic Algorithms and Metamorphic Testing for Software Engineering ML},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590379},
doi = {10.1145/3583131.3590379},
abstract = {More machine learning (ML) models are introduced to the field of Software Engineering (SE) and reached a stage of maturity to be considered for real-world use; But the real world is complex, and testing these models lacks often in explainability, feasibility and computational capacities. Existing research introduced meta-morphic testing to gain additional insights and certainty about the model, by applying semantic-preserving changes to input-data while observing model-output. As this is currently done at random places, it can lead to potentially unrealistic datapoints and high computational costs. With this work, we introduce genetic search as an aid for metamorphic testing in SE ML. Exploiting the delta in output as a fitness function, the evolutionary intelligence optimizes the transformations to produce higher deltas with less changes. We perform a case study minimizing F1 and MRR for Code2Vec on a representative sample from java-small with both genetic and random search. Our results show that within the same amount of time, genetic search was able to achieve a decrease of 10% in F1 while random search produced 3% drop.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1490–1498},
numpages = {9},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@article{10.1007/s10664-023-10342-7,
author = {Lan, Jinpeng and Gong, Lina and Zhang, Jingxuan and Zhang, Haoxiang},
title = {BTLink : automatic link recovery between issues and commits based on pre-trained BERT model},
year = {2023},
issue_date = {Jul 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10342-7},
doi = {10.1007/s10664-023-10342-7},
abstract = {Data traceability in software development can connect different software artifacts to enhance the observability of developer practices. In particular, traceability links between issues and commits (i.e., issue-commit links) play a key role in software maintenance tasks (e.g., bug localization and bug prediction). In practice, developers typically manually make the issue-commit links by adding the issue identifier into the message of the corresponding commits, which results in missing issue commit links being prevalent in software projects. To recover the missing issue commit links, previous studies have proposed some automatic approaches. However, due to the difference between heuristic rules and real-world behavior, as well as insufficient semantic understanding, these approaches cannot achieve the expected performance. Since the text contained in issues and commits contains highly related information, thorough text understanding can improve traceability links. Meanwhile, pre-trained models (i.e., PTMs) have been successfully used to explore the semantic information of text in various software engineering tasks (e.g., software code generation). Therefore, our study proposes a novel BERT -based method (i.e., BTLink) that employs the pre-trained models to automatically recover the issue-commits links. Our proposed BTlink method includes a BERT embedding layer, a fusion layer, and a classifier layer. First, we build two pre-trained BERT encoders to respectively explore the feature representation of the issue text in combination with commit code and commit text. Then we build the fusion layer to examine the joint feature vector. Finally, we build the classifier layer to identify the links between issue and commit. In addition, to further our investigation and verify the effectiveness of BTLink, we conduct an extensive case study on 12 issue-commit links datasets from open source software projects, and observe that: (i) compared to state-of-the-art approaches, our proposed BTLink improves the performance of automatic issue-commit links recovery on all studied measures; (ii) both text and code information in the issues and commits are effective to recover more accurate issue-commit links; (iii) our proposed BTLink is more applicable to the cross-project context compared to state-of-the-art approaches.},
journal = {Empirical Softw. Engg.},
month = {jul},
numpages = {55},
keywords = {Mining software repositories, pre-trained BERT model, Commit, Issue report, Issue-commit links recovery}
}

@inproceedings{10.1145/3611643.3616253,
author = {Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish},
title = {Grace: Language Models Meet Code Edits},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616253},
doi = {10.1145/3611643.3616253},
abstract = {Developers spend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models (LLMs) with the knowledge of relevant prior associated edits, which we call the Grace (Generation conditioned on Associated Code Edits) method. The generative capability of the LLMs helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known LLMs, codex and CodeT5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, Grace boosts the performance of the LLMs significantly, enabling them to generate 29% and 54% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1483–1495},
numpages = {13},
keywords = {Associated edits, Code editing, Large language models, Pre-trained model, Programming language processing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-981-99-8385-8_14,
author = {Farah, Juan Carlos and Ingram, Sandy and Spaenlehauer, Basile and Lasne, Fanny Kim-Lan and Gillet, Denis},
title = {Prompting Large Language Models to&nbsp;Power Educational Chatbots},
year = {2023},
isbn = {978-981-99-8384-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8385-8_14},
doi = {10.1007/978-981-99-8385-8_14},
abstract = {The recent rise in both popularity and performance of large language models has garnered considerable interest regarding their applicability to education. Technologies like ChatGPT, which can engage in human-like dialog, have already disrupted educational practices given their ability to answer a wide array of questions. Nevertheless, integrating these technologies into learning contexts faces both technological and pedagogical challenges, such as providing appropriate user interfaces and configuring interactions to ensure that conversations stay on topic. To better understand the potential large language models have to power educational chatbots, we propose an architecture to support educational chatbots that can be powered by these models. Using this architecture, we created a chatbot interface that was integrated into a web application aimed at teaching software engineering best practices. The application was then used to conduct a case study comprising a controlled experiment with 26 university software engineering students. Half of the students interacted with a version of the application equipped with the chatbot, while the other half completed the same lesson without the chatbot. While the results of our quantitative analysis did not identify significant differences between conditions, qualitative insights suggest that learners appreciated the chatbot. These results could serve as a starting point to optimize strategies for integrating large language models into pedagogical scenarios.},
booktitle = {Advances in Web-Based Learning – ICWL 2023: 22nd International Conference, ICWL 2023, Sydney, NSW, Australia, November 26–28, 2023, Proceedings},
pages = {169–188},
numpages = {20},
keywords = {Digital Education, Software Engineering Education, Large Language Models, GPT-3, Prompting, Educational Chatbots},
location = {<conf-loc content-type="InPerson">Sydney, NSW, Australia</conf-loc>}
}

@inproceedings{10.1145/3583131.3590481,
author = {Liventsev, Vadim and Grishina, Anastasiia and Härmä, Aki and Moonen, Leon},
title = {Fully Autonomous Programming with Large Language Models},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590481},
doi = {10.1145/3583131.3590481},
abstract = {Current approaches to program synthesis with Large Language Models (LLMs) exhibit a "near miss syndrome": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug (SED), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven LLMs, one needs to determine which prompts perform best as instructions for LLMs, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generation techniques. We use OpenAI Codex as the LLM and Program Synthesis Benchmark 2 as a database of problem descriptions and tests for evaluation. The resulting framework outperforms both conventional usage of Codex without the repair phase and traditional genetic programming approaches.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1146–1155},
numpages = {10},
keywords = {program repair, large language models, automatic programming},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1109/ICSE48619.2023.00194,
author = {Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
title = {Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00194},
doi = {10.1109/ICSE48619.2023.00194},
abstract = {Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2312–2323},
numpages = {12},
keywords = {test generation, natural language processing, software engineering},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.jss.2023.111741,
author = {Evtikhiev, Mikhail and Bogomolov, Egor and Sokolov, Yaroslav and Bryksin, Timofey},
title = {Out of the BLEU: How should we assess quality of the Code Generation models?},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {203},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111741},
doi = {10.1016/j.jss.2023.111741},
journal = {J. Syst. Softw.},
month = {sep},
numpages = {17},
keywords = {Code similarity, Neural networks, Metrics, Code generation}
}

@inproceedings{10.1007/978-3-031-50040-4_2,
author = {Mourão, Erica and Trevisan, Daniela and Viterbo, José and Pantoja, Carlos Eduardo},
title = {Investigating Developers’ Perception on&nbsp;Success Factors for Research Software Development},
year = {2023},
isbn = {978-3-031-50039-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-50040-4_2},
doi = {10.1007/978-3-031-50040-4_2},
abstract = {Modern research depends on software libraries, tools, and applications, specially in Artificial Intelligence (AI) to support science, engineering, and business. Research software is often developed in academia by academic researchers in Information and Communication Technology (ICT). However, this software rarely achieves effective success: it is often developed and maintained in an inefficient and unsustainable way, resulting in failure, non-adoption, and abandonment. The goal of this work is to better understand how academic research software developers evaluate success factors in different categories: technical, organizational, and people. A survey with thirty Brazilian academic research software developers was conducted to gather information about the level of importance of the factors. An analysis of the collected data was conducted to identify the importance of these success factors. The results show the ranking of success factors into categories among the research developers. Our study indicates that academic developers can conduct more software development by being aware of the success factors that can provide software sustainability and successfully. The comprehension of the level of importance of each factor will help to improve and guide the support for research software developers on development.},
booktitle = {New Sustainable Horizons in Artificial Intelligence and Digital Solutions: 22nd IFIP WG 6.11 Conference on e-Business, e-Services and e-Society, I3E 2023, Curitiba, Brazil, November 9–11, 2023, Proceedings},
pages = {14–26},
numpages = {13},
keywords = {Brazilian Academic Developers, University, Academia, Success Factors, ICT, Research Software, Empirical Research},
location = {<conf-loc content-type="InPerson">Curitiba, Brazil</conf-loc>}
}

@article{10.1016/j.jss.2023.111786,
author = {Nguyen, Son and Manh, Cuong Tran and Tran, Kien T. and Nguyen, Tan M. and Nguyen, Thu-Trang and Ngo, Kien-Tuan and Vo, Hieu Dinh},
title = {         ARist: An effective API argument recommendation approach},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {204},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111786},
doi = {10.1016/j.jss.2023.111786},
journal = {J. Syst. Softw.},
month = {oct},
numpages = {17},
keywords = {Statistical language model, Program analysis, Code completion, Effective argument recommendation}
}

@inproceedings{10.1145/3623762.3633499,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {The Robots Are Here: Navigating the Generative AI Revolution in Computing Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633499},
doi = {10.1145/3623762.3633499},
abstract = {Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {108–159},
numpages = {52},
keywords = {ai, artificial intelligence, chatgpt, code generation, codex, computer programming, copilot, cs1, curriculum, generative ai, github, gpt, gpt-3, gpt-4, large language models, llm, llms, novice programming, openai, pedagogical practices, programming},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE-WGR '23}
}

@article{10.1016/j.knosys.2023.110830,
author = {Wijekoon, Anjana and Wiratunga, Nirmalie},
title = {A user-centred evaluation of DisCERN: Discovering counterfactuals for code vulnerability detection and correction},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {278},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2023.110830},
doi = {10.1016/j.knosys.2023.110830},
journal = {Know.-Based Syst.},
month = {oct},
numpages = {18},
keywords = {Explainable AI, Vulnerability detection, Counterfactual explanations}
}

@inproceedings{10.1145/3597926.3598067,
author = {Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming},
title = {Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598067},
doi = {10.1145/3597926.3598067},
abstract = {Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  
To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  
This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {423–435},
numpages = {13},
keywords = {Fuzz Testing, Large Language Model, Test Generation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.5555/3618408.3619164,
author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
title = {DS-1000: a natural and reliable benchmark for data science code generation},
year = {2023},
publisher = {JMLR.org},
abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS- 1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from Stack-Overflow. Second, our automatic evaluation is highly specific (reliable) - across all Codex-002- predicted solutions that our evaluation accepts, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {756},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.5555/3620237.3620357,
author = {Niu, Liang and Mirza, Shujaat and Maradni, Zayd and Pöpper, Christina},
title = {CodexLeaks: privacy leaks from code generation language models in github copilot},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Code generation language models are trained on billions of lines of source code to provide code generation and autocompletion features, like those offered by code assistant GitHub Copilot with more than a million users. These datasets may contain sensitive personal information--personally identifiable, private, or secret--that these models may regurgitate.This paper introduces and evaluates a semi-automated pipeline for extracting sensitive personal information from the Codex model used in GitHub Copilot. We employ carefully-designed templates to construct prompts that are more likely to result in privacy leaks. To overcome the non-public training data, we propose a semi-automated filtering method using a blind membership inference attack. We validate the effectiveness of our membership inference approach on different code generation models. We utilize hit rate through the GitHub Search API as a distinguishing heuristic followed by human-in-the-loop evaluation, uncovering that approximately 8% (43) of the prompts yield privacy leaks. Notably, we observe that the model tends to produce indirect leaks, compromising privacy as contextual integrity by generating information from individuals closely related to the queried subject in the training corpus.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {120},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{10.1145/3625291,
author = {Xiao, Ya and Song, Wenjia and Ahmed, Salman and Ge, Xinyang and Viswanath, Bimal and Meng, Na and Yao, Danfeng (Daphne)},
title = {Measurement of Embedding Choices on Cryptographic API Completion Tasks},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3625291},
doi = {10.1145/3625291},
abstract = {In this paper, we conduct a measurement study to comprehensively compare the accuracy impacts of multiple embedding options in cryptographic API completion tasks. Embedding is the process of automatically learning vector representations of program elements. Our measurement focuses on design choices of three important aspects, program analysis preprocessing, token-level embedding, and sequence-level embedding. Our findings show that program analysis is necessary even under advanced embedding. The results show 36.20% accuracy improvement on average when program analysis preprocessing is applied to transfer bytecode sequences into API dependence paths. With program analysis and the token-level embedding training, the embedding dep2vec improves the task accuracy from 55.80% to 92.04%. Moreover, only a slight accuracy advantage (0.55% on average) is observed by training the expensive sequence-level embedding compared with the token-level embedding. Our experiments also suggest the differences made by the data. In the cross-app learning setup and a data scarcity scenario, sequence-level embedding is more necessary and results in a more obvious accuracy improvement (5.10%).},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
keywords = {neural code completion, embedding, deep learning, neural networks, program analysis, API dependency, cryptography, secure coding, Java}
}

@proceedings{10.1145/3584871,
title = {ICSIM '23: Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {<conf-loc>, <city>Palmerston North</city>, <country>New Zealand</country>, </conf-loc>}
}

@proceedings{10.1145/3593434,
title = {EASE '23: Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Oulu, Finland}
}

@inproceedings{10.5555/3618408.3619494,
author = {Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Ves and Yih, Wen-tau and Wang, Sida I. and Lin, Xi Victoria},
title = {LEVER: learning to verify language-to-code generation with execution},
year = {2023},
publisher = {JMLR.org},
abstract = {The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1086},
numpages = {23},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1109/ICSE48619.2023.00015,
author = {Gao, Shuzheng and Zhang, Hongyu and Gao, Cuiyun and Wang, Chaozheng},
title = {Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00015},
doi = {10.1109/ICSE48619.2023.00015},
abstract = {Previous research on code intelligence usually trains a deep learning model on a fixed dataset in an offline manner. However, in real-world scenarios, new code repositories emerge incessantly, and the carried new knowledge is beneficial for providing up-to-date code intelligence services to developers. In this paper, we aim at the following problem: How to enable code intelligence models to continually learn from ever-increasing data? One major challenge here is catastrophic forgetting, meaning that the model can easily forget knowledge learned from previous datasets when learning from the new dataset. To tackle this challenge, we propose REPEAT, a novel method for continual learning of code intelligence models. Specifically, REPEAT addresses the catastrophic forgetting problem with representative exemplars replay and adaptive parameter regularization. The representative exemplars replay component selects informative and diverse exemplars in each dataset and uses them to retrain model periodically. The adaptive parameter regularization component recognizes important parameters in the model and adaptively penalizes their changes to preserve the knowledge learned before. We evaluate the proposed approach on three code intelligence tasks including code summarization, software vulnerability detection, and code clone detection. Extensive experiments demonstrate that REPEAT consistently outperforms baseline methods on all tasks. For example, REPEAT improves the conventional fine-tuning method by 1.22, 5.61, and 1.72 on code summarization, vulnerability detection and clone detection, respectively.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {30–42},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3593434.3593468,
author = {Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fahmideh, Mahdi and Aktar, Mst Shamima and Mikkonen, Tommi},
title = {Towards Human-Bot Collaborative Software Architecting with ChatGPT},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593468},
doi = {10.1145/3593434.3593468},
abstract = {Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders’ perspectives, designers’ intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects’ knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects’ productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {279–285},
numpages = {7},
keywords = {ChatGPT, DevBots, Large Language Models, Software Architecture},
location = {Oulu, Finland},
series = {EASE '23}
}

@proceedings{10.1145/3617573,
title = {SEA4DQ 2023: Proceedings of the 3rd International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things},
year = {2023},
isbn = {9798400703782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 3rd International Workshop on Software Engineering and AI for Data Quality (SEA4DQ 2023) taking place on December 4, 2023, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) in San Francisco, CA, USA.},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@article{10.1145/3618338,
author = {Su, Zejia and Fan, Qingnan and Chen, Xuelin and Van Kaick, Oliver and Huang, Hui and Hu, Ruizhen},
title = {Scene-Aware Activity Program Generation with Language Guidance},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3618338},
doi = {10.1145/3618338},
abstract = {We address the problem of scene-aware activity program generation, which requires decomposing a given activity task into instructions that can be sequentially performed within a target scene to complete the activity. While existing methods have shown the ability to generate rational or executable programs, generating programs with both high rationality and executability still remains a challenge. Hence, we propose a novel method where the key idea is to explicitly combine the language rationality of a powerful language model with dynamic perception of the target scene where instructions are executed, to generate programs with high rationality and executability. Our method iteratively generates instructions for the activity program. Specifically, a two-branch feature encoder operates on a language-based and graph-based representation of the current generation progress to extract language features and scene graph features, respectively. These features are then used by a predictor to generate the next instruction in the program. Subsequently, another module performs the predicted action and updates the scene for perception in the next iteration. Extensive evaluations are conducted on the VirtualHome-Env dataset, showing the advantages of our method over previous work. Key algorithmic designs are validated through ablation studies, and results on other types of inputs are also presented to show the generalizability of our method.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {252},
numpages = {16},
keywords = {activity program generation, dynamic scene graph, language guidance}
}

@article{10.1016/j.cose.2023.103469,
author = {Wu, Bolun and Zou, Futai and Yi, Ping and Wu, Yue and Zhang, Liang},
title = {SlicedLocator: Code vulnerability locator based on sliced dependence graph},
year = {2023},
issue_date = {Nov 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {134},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103469},
doi = {10.1016/j.cose.2023.103469},
journal = {Comput. Secur.},
month = {nov},
numpages = {13},
keywords = {Vulnerability detection, Localization, Program analysis, Program representation, Deep learning}
}

@inproceedings{10.1145/3611643.3613869,
author = {Xu, Zhiwei and Zhou, Min and Zhao, Xibin and Chen, Yang and Cheng, Xi and Zhang, Hongyu},
title = {xASTNN: Improved Code Representations for Industrial Practice},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613869},
doi = {10.1145/3611643.3613869},
abstract = {The application of deep learning techniques in software engineering becomes increasingly popular. One key problem is developing high-quality and easy-to-use source code representations for code-related tasks. The research community has acquired impressive results in recent years. However, due to the deployment difficulties and performance bottlenecks, seldom these approaches are applied to the industry. In this paper, we present xASTNN, an eXtreme Abstract Syntax Tree (AST)-based Neural Network for source code representation, aiming to push this technique to industrial practice. The proposed xASTNN has three advantages. First, xASTNN is completely based on widely-used ASTs and does not require complicated data pre-processing, making it applicable to various programming languages and practical scenarios. Second, three closely-related designs are proposed to guarantee the effectiveness of xASTNN, including statement subtree sequence for code naturalness, gated recursive unit for syntactical information, and gated recurrent unit for sequential information. Third, a dynamic batching algorithm is introduced to significantly reduce the time complexity of xASTNN. Two code comprehension downstream tasks, code classification and code clone detection, are adopted for evaluation. The results demonstrate that our xASTNN can improve the state-of-the-art while being faster than the baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1727–1738},
numpages = {12},
keywords = {neural code representation, code feature learning, big code},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3593434.3593489,
author = {Agbese, Mamia and Mohanani, Rahul and Khan, Arif Ali and Abrahamsson, Pekka},
title = {Ethical Requirements Stack: A framework for implementing ethical requirements of AI in software engineering practices},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593489},
doi = {10.1145/3593434.3593489},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {326–328},
numpages = {3},
keywords = {AI, AI ethics, AI ethics principles, Agile portfolio management, Ethical requirements, Ethical requirements stack},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1007/978-3-031-36272-9_79,
author = {Koutcheme, Charles},
title = {Training Language Models for&nbsp;Programming Feedback Using Automated Repair Tools},
year = {2023},
isbn = {978-3-031-36271-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36272-9_79},
doi = {10.1007/978-3-031-36272-9_79},
abstract = {In introductory programming courses, automated repair tools (ARTs) are used to provide feedback to students struggling with debugging. Most successful ARTs take advantage of context-specific educational data to construct repairs to students’ buggy codes. Recent work in student program repair using large language models (LLMs) has also started to utilize such data. An underexplored area in this field is the use of ARTs in combination with LLMs. In this paper, we propose to transfer the repairing capabilities of existing ARTs to open large language models by finetuning LLMs on ART corrections to buggy codes. We experiment with this approach using three large datasets of Python programs written by novices. Our results suggest that a finetuned LLM provides more reliable and higher-quality repairs than the repair tool used for finetuning the model. This opens venues for further deploying and using educational LLM-based repair techniques.},
booktitle = {Artificial Intelligence in Education: 24th International Conference, AIED 2023, Tokyo, Japan, July 3–7, 2023, Proceedings},
pages = {830–835},
numpages = {6},
keywords = {Program Repair, Large Language Models, Computing Education},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3597926.3598141,
author = {Kaboré, Abdoul Kader and Barr, Earl T. and Klein, Jacques and Bissyandé, Tegawendé F.},
title = {CodeGrid: A Grid Representation of Code},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598141},
doi = {10.1145/3597926.3598141},
abstract = {Code representation is a key step in the application of AI in software engineering. Generic NLP representations are effective but do not exploit all the rich structure inherent to code. Recent work has focused on extracting abstract syntax trees (AST) and integrating their structural information into code representations.These AST-enhanced representations advanced the state of the art and accelerated new applications of AI to software engineering. ASTs, however, neglect important aspects of code structure, notably control and data flow, leaving some potentially relevant code signal unexploited. For example, purely image-based representations perform nearly as well as AST-based representations, despite the fact that they must learn to even recognize tokens, let alone their semantics. This result, from prior work, is strong evidence that these new code representations can still be improved; it also raises the question of just what signal image-based approaches are exploiting. We answer this question. We show that code is spatial and exploit this fact to propose , a new representation that embeds tokens into a grid that preserves code layout. Unlike some of the existing state of the art, is agnostic to the downstream task: whether that task is generation or classification, can complement the learning algorithm with spatial signal. For example, we show that CNNs, which are inherently spatially-aware models, can exploit outputs to effectively tackle fundamental software engineering tasks, such as code classification, code clone detection and vulnerability detection. PixelCNN leverages ’s grid representations to achieve code completion. Through extensive experiments, we validate our spatial code hypothesis, quantifying model performance as we vary the degree to which the representation preserves the grid. To demonstrate its generality, we show that augments models, improving their performance on a range of tasks, On clone detection, improves ASTNN’s performance by 3.3% F1 score.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1357–1369},
numpages = {13},
keywords = {Code TypeSetting, Spatial-Aware Neural Network},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3568813.3600138,
author = {Lau, Sam and Guo, Philip},
title = {From "Ban It Till We Understand It" to "Resistance is Futile": How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600138},
doi = {10.1145/3568813.3600138},
abstract = {Over the past year (2022–2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {106–121},
numpages = {16},
keywords = {AI coding tools, ChatGPT, Copilot, LLM, instructor perspectives},
location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
series = {ICER '23}
}

@article{10.1145/3583562,
author = {Sarker, Jaydeb and Turzo, Asif Kamal and Dong, Ming and Bosu, Amiangshu},
title = {Automated Identification of Toxic Code Reviews Using ToxiCR},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583562},
doi = {10.1145/3583562},
abstract = {Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {118},
numpages = {32},
keywords = {Toxicity, code review, sentiment analysis, Natural Language Processing, tool development}
}

@article{10.1007/s10514-023-10135-3,
author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
title = {ProgPrompt: program generation for situated robot task planning using large language models},
year = {2023},
issue_date = {Dec 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {8},
issn = {0929-5593},
url = {https://doi.org/10.1007/s10514-023-10135-3},
doi = {10.1007/s10514-023-10135-3},
abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website and code at},
journal = {Auton. Robots},
month = {aug},
pages = {999–1012},
numpages = {14},
keywords = {Robot task planning, LLM code generation, Planning domain generalization, Symbolic planning}
}

@inproceedings{10.1145/3611643.3616331,
author = {Yang, Jun and Wang, Yuehan and Lou, Yiling and Wen, Ming and Zhang, Lingming},
title = {A Large-Scale Empirical Review of Patch Correctness Checking Approaches},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616331},
doi = {10.1145/3611643.3616331},
abstract = {Automated Program Repair (APR) techniques have drawn wide attention from both academia and industry. Meanwhile, one main limitation with the current state-of-the-art APR tools is that patches passing all the original tests are not necessarily the correct ones wanted by developers, i.e., the plausible patch problem. To date, various Patch-Correctness Checking (PCC) techniques have been proposed to address this important issue. However, they are only evaluated on very limited datasets as the APR tools used for generating such patches can only explore a small subset of the search space of possible patches, posing serious threats to external validity to existing PCC studies. In this paper, we construct an extensive PCC dataset, PraPatch (the largest manually labeled PCC dataset to our knowledge), to revisit all nine state-of-the-art PCC techniques. More specifically, our PCC dataset PraPatch includes 1,988 patches generated from the recent PraPR APR tool, which leverages highly-optimized bytecode-level patch executions and can exhaustively explore all possible plausible patches within its large predefined search space (including well-known fixing patterns from various prior APR tools). Our extensive study of representative PCC techniques on PraPatch has revealed various findings, including: 1) the assumption made by existing static PCC techniques that correct patches are more similar to buggy code than incorrect plausible patches no longer holds, 2) state-of-the-art learning-based techniques tend to suffer from the dataset overfitting problem, 3) while dynamic techniques overall retain their effectiveness on our new dataset, their performance drops substantially on patches with more complicated changes and 4) the very recent naturalness-based techniques can substantially outperform traditional static techniques and could be a promising direction for PCC. Based on our findings, we also provide various guidelines/suggestions for advancing PCC in the near future.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1203–1215},
numpages = {13},
keywords = {Empirical assessment, Patch correctness, Program repair},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-3-031-44836-2_2,
author = {Syu, Yang and Wang, Chien-Min},
title = {A Gap Between Automated Service Composition Research and Software Engineering Development Practice: Service Descriptions},
year = {2023},
isbn = {978-3-031-44835-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44836-2_2},
doi = {10.1007/978-3-031-44836-2_2},
abstract = {In research, automatic service composition (ASC) has been a widely studied academic subject for many years. However, this field still contains topics and issues that remain unidentified or uninvestigated. In this paper, we focus on one such unsolved problem of the ASC, elaborating on a current effort and future plan.This recognized problem is caused by the difference between the formation of service descriptions used by human composers and that used by ASC approaches. In practice, engineers are used to write various development-related documents in natural language, such as their requirement specifications and software component descriptions. However, an exhaustive survey found that most existing ASC studies are assumed to take their required service descriptions in a tuple-based format. Although this difference and problem in theory can be sufficiently addressed using manual processing techniques (e.g., human transformation or extraction), we consider such human intervention to be inefficient, costly, and, most importantly, harmful to the level of automation of ASC. Thus, this study develops an automated solution to this problem.We first introduce some necessary background knowledge and foundation so that the targeted problem can be fully understood and motivated. Then, the specific problem to be studied is clearly defined and exemplified along with a detailed explanation. Finally, the three key components to explore and address this research problem (dataset, approach, and evaluation) are discussed in detail, including the current work of the authors and proposals for future research.},
booktitle = {Web Services – ICWS 2023: 30th International Conference, Held as Part of the Services Conference Federation, SCF 2023, Honolulu, HI, USA, September 23–26, 2023, Proceedings},
pages = {18–31},
numpages = {14},
keywords = {automatic service composition, service description, natural language processing, rule-based extraction, conditional random field, deep learning},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/3587103.3594206,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Caspersen, Michael E. and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {Transformed by Transformers: Navigating the AI Coding Revolution for Computing Education: An ITiCSE Working Group Conducted by Humans},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594206},
doi = {10.1145/3587103.3594206},
abstract = {The recent advent of highly accurate and scalable large language models (LLMs) has taken the world by storm. From art to essays to computer code, LLMs are producing novel content that until recently was thought only humans could produce. Recent work in computing education has sought to understand the capabilities of LLMs for solving tasks such as writing code, explaining code, creating novel coding assignments, interpreting programming error messages, and more. However, these technologies continue to evolve at an astonishing rate leaving educators little time to adapt. This working group seeks to document the state-of-the-art for code generation LLMs, detail current opportunities and challenges related to their use, and present actionable approaches to integrating them into computing curricula.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {561–562},
numpages = {2},
keywords = {AI, CS1, GPT, GitHub, LLM, artificial intelligence, code generation, codex, computer programming, copilot, large language models, novice programming, openAI, pedagogical practices},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3611643.3616310,
author = {Wan, Yuxuan and Wang, Wenxuan and He, Pinjia and Gu, Jiazhen and Bai, Haonan and Lyu, Michael R.},
title = {BiasAsker: Measuring the Bias in Conversational AI System},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616310},
doi = {10.1145/3611643.3616310},
abstract = {Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT, and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to modern AI techniques’ data-driven, black-box nature, comprehensively identifying and measuring biases in conversational systems remains challenging. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups and biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods based solely on sentiment and toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset containing a total of 841 groups and 5,021 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on eight commercial systems and two famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {515–527},
numpages = {13},
keywords = {Software testing, conversational models, social bias},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3607188,
author = {Huang, Qing and Sun, Yanbang and Xing, Zhenchang and Yu, Min and Xu, Xiwei and Lu, Qinghua},
title = {API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607188},
doi = {10.1145/3607188},
abstract = {Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {6},
numpages = {25},
keywords = {API entity, API relation, joint extraction, dynamic prompt}
}

@article{10.1016/j.infsof.2023.107217,
author = {Gomes, Luiz and da Silva Torres, Ricardo and Côrtes, Mario Lúcio},
title = {BERT- and TF-IDF-based feature extraction for long-lived bug prediction in FLOSS: A comparative study},
year = {2023},
issue_date = {Aug 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {160},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107217},
doi = {10.1016/j.infsof.2023.107217},
journal = {Inf. Softw. Technol.},
month = {aug},
numpages = {12},
keywords = {BERT, Natural Language Processing, Text mining, Machine learning, Long-lived bugs, Bug Tracking System, Software maintenance}
}

@inproceedings{10.1109/ICSE48619.2023.00055,
author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Luo, Bin},
title = {CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00055},
doi = {10.1109/ICSE48619.2023.00055},
abstract = {Despite the recent advances showing that a model pre-trained on large-scale source code data is able to gain appreciable generalization capability, it still requires a sizeable amount of data on the target task for fine-tuning. And the effectiveness of the model generalization is largely affected by the size and quality of the fine-tuning data, which is detrimental for target tasks with limited or unavailable resources. Therefore, cross-task generalization, with the goal of improving the generalization of the model to unseen tasks that have not been seen before, is of strong research and application value.In this paper, we propose a large-scale benchmark that includes 216 existing code-related tasks. Then, we annotate each task with the corresponding meta information such as task description and instruction, which contains detailed information about the task and a solution guide. This also helps us to easily create a wide variety of "training/evaluation" task splits to evaluate the various cross-task generalization capabilities of the model. Then we perform some preliminary experiments to demonstrate that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross-task learning research on our benchmark. We hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross-task generalization.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {537–549},
numpages = {13},
keywords = {pre-training of source code, cross-task transfer learning, few-shot learning, AI for SE},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3626111.3628183,
author = {Mani, Sathiya Kumaran and Zhou, Yajie and Hsieh, Kevin and Segarra, Santiago and Eberl, Trevor and Azulai, Eliran and Frizler, Ido and Chandra, Ranveer and Kandula, Srikanth},
title = {Enhancing Network Management Using Code Generated by Large Language Models},
year = {2023},
isbn = {9798400704154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626111.3628183},
doi = {10.1145/3626111.3628183},
abstract = {Analyzing network topologies and communication graphs is essential in modern network management. However, the lack of a cohesive approach results in a steep learning curve, increased errors, and inefficiencies. In this paper, we present a novel approach that enables natural-language-based network management experiences, leveraging large language models (LLMs) to generate task-specific code from natural language queries. This method addresses the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, removing the need to share network data with LLMs, and focusing on application-specific requests combined with program synthesis techniques. We develop and evaluate a prototype system using benchmark applications, demonstrating high accuracy, cost-effectiveness, and potential for further improvements using complementary program synthesis techniques.},
booktitle = {Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
pages = {196–204},
numpages = {9},
keywords = {Communication graphs, Graph manipulation, Large language model, Natural language processing, Network lifecycle management, Network management, Program synthesis},
location = {<conf-loc>, <city>Cambridge</city>, <state>MA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '23}
}

@inproceedings{10.1007/978-3-031-49252-5_14,
author = {Busch, Daniel and Bainczyk, Alexander and Steffen, Bernhard},
title = {Towards LLM-Based System Migration in&nbsp;Language-Driven Engineering},
year = {2023},
isbn = {978-3-031-49251-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49252-5_14},
doi = {10.1007/978-3-031-49252-5_14},
abstract = {In this paper we show how our approach of extending Language Driven Engineering (LDE) with natural language-based code generation supports system migration: The characteristic decomposition of LDE into tasks that are solved with dedicated domain-specific languages divides the migration tasks into portions adequate to apply LLM-based code generation. We illustrate this effect by migrating a low-code/no-code generator for point-and-click adventures from JavaScript to TypeScript in a way that maintains an important property: generated web applications can automatically be validated via automata learning and model analysis by design. In particular, this allows to easily test the correctness of migration by learning the difference automaton for the generated products of the source and the target system of the migration.},
booktitle = {Engineering of Computer-Based Systems: 8th International Conference, ECBS 2023, Västerås, Sweden, October 16–18, 2023, Proceedings},
pages = {191–200},
numpages = {10},
keywords = {Software Engineering, Low-Code/No-Code, Language-driven Engineering, Large Language Models, Migration, Transformation, Automata Learning, Verification, Web Application},
location = {<conf-loc content-type="InPerson">Västerås, Sweden</conf-loc>}
}

@article{10.1007/s10664-023-10380-1,
author = {Asare, Owura and Nagappan, Meiyappan and Asokan, N.},
title = {Is GitHub’s Copilot as bad as humans at introducing vulnerabilities in code?},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10380-1},
doi = {10.1007/s10664-023-10380-1},
abstract = {Several advances in deep learning have been successfully applied to the software development process. Of recent interest is the use of neural language models to build tools, such as Copilot, that assist in writing code. In this paper we perform a comparative empirical analysis of Copilot-generated code from a security perspective. The aim of this study is to determine if Copilot is as bad as human developers. We investigate whether Copilot is just as likely to introduce the same software vulnerabilities as human developers. Using a dataset of C/C++ vulnerabilities, we prompt Copilot to generate suggestions in scenarios that led to the introduction of vulnerabilities by human developers. The suggestions are inspected and categorized in a 2-stage process based on whether the original vulnerability or fix is reintroduced. We find that Copilot replicates the original vulnerable code about 33% of the time while replicating the fixed code at a 25% rate. However this behaviour is not consistent: Copilot is more likely to introduce some types of vulnerabilities than others and is also more likely to generate vulnerable code in response to prompts that correspond to older vulnerabilities. Overall, given that in a significant number of cases it did not replicate the vulnerabilities previously introduced by human developers, we conclude that Copilot, despite performing differently across various vulnerability types, is not as bad as human developers at introducing vulnerabilities in code.},
journal = {Empirical Softw. Engg.},
month = {sep},
numpages = {24},
keywords = {copilot, code security, software engineering, language models}
}

@inproceedings{10.1007/978-3-031-41734-4_31,
author = {Shukla, Shreya and Gatti, Prajwal and Kumar, Yogesh and Yadav, Vikash and Mishra, Anand},
title = {Towards Making Flowchart Images Machine Interpretable},
year = {2023},
isbn = {978-3-031-41733-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-41734-4_31},
doi = {10.1007/978-3-031-41734-4_31},
abstract = {Computer programming textbooks and software documentations often contain flowcharts to illustrate the flow of an algorithm or procedure. Modern OCR engines often tag these flowcharts as graphics and ignore them in further processing. In this paper, we work towards making flowchart images machine-interpretable by converting them to executable Python codes. To this end, inspired by the recent success in natural language to code generation literature, we present a novel transformer-based framework, namely FloCo-T5. Our model is well-suited for this task, as it can effectively learn semantics, structure, and patterns of programming languages, which it leverages to generate syntactically correct code. We also used a task-specific pre-training objective to pre-train FloCo-T5 using a large number of logic-preserving augmented code samples. Further, to perform a rigorous study of this problem, we introduce the FloCo dataset that contains 11,884 flowchart images and their corresponding Python codes. Our experiments show promising results, and FloCo-T5 clearly outperforms related competitive baselines on code generation metrics. We make our dataset and implementation publicly available ().},
booktitle = {Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San José, CA, USA, August 21–26, 2023, Proceedings, Part V},
pages = {505–521},
numpages = {17},
keywords = {Flowchart Understanding, Code Generation, Large Language Models},
location = {San José, CA, USA}
}

@article{10.1145/3591868,
author = {Wei, Hongwei and Su, Xiaohong and Gao, Cuiyun and Zheng, Weining and Tao, Wenxin},
title = {A Hypothesis Testing-based Framework for Software Cross-modal Retrieval in Heterogeneous Semantic Spaces},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3591868},
doi = {10.1145/3591868},
abstract = {Software cross-modal retrieval is a popular yet challenging direction, such as bug localization and code search. Previous studies generally map natural language texts and codes into a homogeneous semantic space for similarity measurement. However, it is not easy to accurately capture their similar semantics in a homogeneous semantic space due to the semantic gap. Therefore, we propose to map the multi-modal data into heterogeneous semantic spaces to capture their unique semantics. Specifically, we propose a novel software cross-modal retrieval framework named Deep Hypothesis Testing (DeepHT). In DeepHT, to capture the unique semantics of the code’s control flow structure, all control flow paths (CFPs) in the control flow graph are mapped to a CFP sample set in the sample space. Meanwhile, the text is mapped to a CFP correlation distribution in the distribution space to model its correlation with different CFPs. The matching score is calculated according to how well the sample set obeys the distribution using hypothesis testing. The experimental results on two text-to-code retrieval tasks (i.e., bug localization and code search) and two code-to-text retrieval tasks (i.e., vulnerability knowledge retrieval and historical patch retrieval) show that DeepHT outperforms the baseline methods.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {123},
numpages = {28},
keywords = {Software cross-modal retrieval, hypothesis testing, deep learning}
}

@inproceedings{10.1145/3613372.3613413,
author = {Dantas, Carlos and Rocha, Adriano and Maia, Marcelo},
title = {Assessing the Readability of ChatGPT Code Snippet Recommendations: A Comparative Study},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613413},
doi = {10.1145/3613372.3613413},
abstract = {Developers often rely on code search engines to find high-quality and reusable code snippets online, such as those available on Stack Overflow. Recently, ChatGPT, a language model trained for dialog tasks, has been gaining attention as a promising approach for code snippet generation. However, there is still a need for in-depth analysis of the quality of its recommendations. In this work, we propose the evaluation of the readability of code snippets generated by ChatGPT, comparing them with those recommended by CROKAGE, a state-of-the-art code search engine for Stack Overflow. We compare the recommended snippets of both approaches using readability issues raised by the automated static analysis tool (ASAT) SonarQube. Our results show that ChatGPT can generate cleaner code snippets and more consistent naming and code conventions than those written by humans and recommended by CROKAGE. However, in some cases, ChatGPT generates code that lacks recent features from Java API such as try-with-resources, lambdas, and others. Overall, our findings suggest that ChatGPT can provide valuable assistance to developers searching for didactic and high-quality code snippets online. However, it is still important for developers to review the generated code, either manually or assisted by an ASAT, to prevent potential readability issues, as the correctness of the generated code snippets.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {283–292},
numpages = {10},
keywords = {readability, code snippets, Stack Overflow, SonarQube, ChatGPT},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@inproceedings{10.1145/3605770.3625215,
author = {Bukhari, Sufiyan and Tan, Benjamin and De Carli, Lorenzo},
title = {Distinguishing AI- and Human-Generated Code: A Case Study},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605770.3625215},
doi = {10.1145/3605770.3625215},
abstract = {While the use of AI assistants for code generation has the potential to revolutionize the way software is produced, assistants may generate insecure code, either by accident or as a result of poisoning attacks. They may also inadvertently violate copyright laws by mimicking code protected by restrictive licenses. We argue for the importance of tracking the provenance of AI-generated code in the software supply chain, so that adequate controls can be put in place to mitigate risks. For that, it is necessary to have techniques that can distinguish between human- and AI-generate code, and we conduct a case study in regards to whether such techniques can reliably work. We evaluate the effectiveness of lexical and syntactic features for distinguishing AI- and human-generated code on a standardized task. Results show accuracy up to 92%, suggesting that the problem deserves further investigation.},
booktitle = {Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {17–25},
numpages = {9},
keywords = {ai code generation, program analysis, supply chain security},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {SCORED '23}
}

@inproceedings{10.1145/3611643.3616244,
author = {Weyssow, Martin and Zhou, Xin and Kim, Kisub and Lo, David and Sahraoui, Houari},
title = {On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616244},
doi = {10.1145/3611643.3616244},
abstract = {Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen APIs over time. We study two widely used PLM architectures, i.e., a GPT2 decoder and a RoBERTa encoder, on two downstream tasks, API call and API usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of APIs, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in PLMs across both downstream tasks while achieving comparable or superior performance.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1470–1482},
numpages = {13},
keywords = {continual learning, deep learning for code, out-of-distribution generalization, pre-trained language models},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-3-031-48796-5_13,
author = {Brownlee, Alexander E. I. and Callan, James and Even-Mendoza, Karine and Geiger, Alina and Hanna, Carol and Petke, Justyna and Sarro, Federica and Sobania, Dominik},
title = {Enhancing Genetic Improvement Mutations Using Large Language Models},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48796-5_13},
doi = {10.1007/978-3-031-48796-5_13},
abstract = {Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI’s API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {153–159},
numpages = {7},
keywords = {Large language models, Genetic Improvement},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@inproceedings{10.1145/3597926.3598142,
author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong and Hu, Chunming and Liu, Yang},
title = {Detecting Condition-Related Bugs with Control Flow Graph Neural Network},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598142},
doi = {10.1145/3597926.3598142},
abstract = {Automated bug detection is essential for high-quality software development and has attracted much attention over the years. Among the various bugs, previous studies show that the condition expressions are quite error-prone and the condition-related bugs are commonly found in practice. Traditional approaches to automated bug detection are usually limited to compilable code and require tedious manual effort. Recent deep learning-based work tends to learn general syntactic features based on Abstract Syntax Tree (AST) or apply the existing Graph Neural Networks over program graphs. However, AST-based neural models may miss important control flow information of source code, and existing Graph Neural Networks for bug detection tend to learn local neighbourhood structure information. Generally, the condition-related bugs are highly influenced by control flow knowledge, therefore we propose a novel CFG-based Graph Neural Network (CFGNN) to automatically detect condition-related bugs, which includes a graph-structured LSTM unit to efficiently learn the control flow knowledge and long-distance context information.  
We also adopt the API-usage attention mechanism to leverage the API knowledge. To evaluate the proposed approach, we collect real-world bugs in popular GitHub repositories and build a large-scale condition-related bug dataset. The experimental results show that our proposed approach significantly outperforms the state-of-the-art methods for detecting condition-related bugs.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1370–1382},
numpages = {13},
keywords = {Bug detection, deep learning, graph neural network},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00149,
author = {Ahmed, Toufique and Ghosh, Supriyo and Bansal, Chetan and Zimmermann, Thomas and Zhang, Xuchao and Rajmohan, Saravan},
title = {Recommending Root-Cause and Mitigation Steps for Cloud Incidents Using Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00149},
doi = {10.1109/ICSE48619.2023.00149},
abstract = {Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1737–1749},
numpages = {13},
keywords = {incident management, service quality, GPT-3.x, large language models},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3616358,
author = {Ni, Chao and Yin, Xin and Yang, Kaiwen and Zhao, Dehai and Xing, Zhenchang and Xia, Xin},
title = {Distinguishing Look-Alike Innocent and Vulnerable Code by Subtle Semantic Representation Learning and Explanation},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616358},
doi = {10.1145/3611643.3616358},
abstract = {Though many deep learning (DL)-based vulnerability detection approaches have been proposed and indeed achieved remarkable performance, they still have limitations in the generalization as well as the practical usage. More precisely, existing DL-based approaches (1) perform negatively on prediction tasks among functions that are lexically similar but have contrary semantics; (2) provide no  
intuitive developer-oriented explanations to the detected results.  

In this paper, we propose a novel approach named SVulD, a function-level S ubtle semantic embedding for Vulnerability Detection along with intuitive explanations, to alleviate the above limitations.  
Specifically, SVulD firstly trains a model to learn distinguishing semantic representations of functions regardless of their lexical similarity. Then, for the detected vulnerable functions, SVulD provides natural language explanations (e.g., root cause) of results to help developers intuitively understand the vulnerabilities. To evaluate the effectiveness of SVulD, we conduct large-scale experiments on a widely used practical vulnerability dataset and compare it with four state-of-the-art (SOTA) approaches by considering five performance measures. The experimental results indicate that SVulD outperforms all SOTAs with a substantial improvement (i.e., 23.5%-68.0% in terms of F1-score, 15.9%-134.8% in terms of PR-AUC and 7.4%-64.4% in terms of Accuracy). Besides, we conduct a user-case study to evaluate the usefulness of SVulD for developers on understanding the vulnerable code and the participants’ feedback demonstrates that SVulD is helpful for development practice.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1611–1622},
numpages = {12},
keywords = {Contrastive Learning, Developer-oriented Explanation, Subtle Semantic Difference, Vulnerability Detection},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3613891,
author = {Jin, Pengxiang and Zhang, Shenglin and Ma, Minghua and Li, Haozhe and Kang, Yu and Li, Liqun and Liu, Yudong and Qiao, Bo and Zhang, Chaoyun and Zhao, Pu and He, Shilin and Sarro, Federica and Dang, Yingnong and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Assess and Summarize: Improve Outage Understanding with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613891},
doi = {10.1145/3611643.3613891},
abstract = {Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results obtained show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1657–1668},
numpages = {12},
keywords = {Outage Understanding, Large Language Model, Cloud Systems},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3611664,
author = {Guo, Hanyang and Chen, Xiangping and Huang, Yuan and Wang, Yanlin and Ding, Xi and Zheng, Zibin and Zhou, Xiaocong and Dai, Hong-Ning},
title = {Snippet Comment Generation Based on Code Context Expansion},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611664},
doi = {10.1145/3611664},
abstract = {Code commenting plays an important role in program comprehension. Automatic comment generation helps improve software maintenance efficiency. The code comments to annotate a method mainly include header comments and snippet comments. The header comment aims to describe the functionality of the entire method, thereby providing a general comment at the beginning of the method. The snippet comment appears at multiple code segments in the body of a method, where a code segment is called a code snippet. Both of them help developers quickly understand code semantics, thereby improving code readability and code maintainability. However, existing automatic comment generation models mainly focus more on header comments, because there are public datasets to validate the performance. By contrast, it is challenging to collect datasets for snippet comments, because it is difficult to determine their scope. Even worse, code snippets are often too short to capture complete syntax and semantic information. To address this challenge, we propose a novel Snippet Comment Generation approach called SCGen. First, we utilize the context of the code snippet to expand the syntax and semantic information. Specifically, 600,243 snippet code-comment pairs are collected from 959 Java projects. Then, we capture variables from code snippets and extract variable-related statements from the context. After that, we devise an algorithm to parse and traverse abstract syntax tree (AST) information of code snippets and corresponding context. Finally, SCGen generates snippet comments after inputting the source code snippet and corresponding AST information into a sequence-to-sequence-based model. We conducted extensive experiments on the dataset we collected to evaluate our SCGen. Our approach obtains 18.23 in BLEU-4 metrics, 18.83 in METEOR, and 23.65 in ROUGE-L, which outperforms state-of-the-art comment generation models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {24},
numpages = {30},
keywords = {Snippet comment generation, code summarization, neural machine translation, contextual information}
}

@inproceedings{10.1145/3609437.3609463,
author = {Li, Keqi and Tang, Xingli and Li, Fenghang and Zhou, Hui and Ye, Chunyang and Zhang, Wenyu},
title = {PyBartRec: Python API Recommendation with Semantic Information},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609463},
doi = {10.1145/3609437.3609463},
abstract = {API recommendation has been widely used to enhance developers’ efficiency in software development. However, existing API recommendation methods for dynamic languages such as Python usually suffer from the limitations of incorrect type inference and lack of rich contextual semantics. To address these issues, we propose in this paper a novel approach, PyBartRec, to recommend APIs for Python programs concerning their rich semantics. Instead of analyzing the data flow information only, our approach utilizes a Transformer-based pre-trained model to extract the semantic features of Python code snippets. Such contextual information allows our approach to recommend correct APIs even when the APIs are not included in the local data flow information. We also use such information to perform a post-processing type inference. By narrowing the range of candidate types, our approach can recommend APIs accurately even in the failure scenarios of type inference. We evaluated PyBartRec in eight popular Python projects and the experimental results show that our approach significantly outperforms the state-of-the-art solutions. In particular, the average top-1 accuracy and average top-10 accuracy of PyBartRec within the same project are over 40%, and 60%, respectively. In cross-project recommendation, the MRR of PyBartRec is also over 40%.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {33–43},
numpages = {11},
keywords = {type inference, context analysis, code representation, Python, API Recommendation},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3539618.3591680,
author = {Han, Siqi and Wang, Yu and Lu, Xuesong},
title = {ErrorCLR: Semantic Error Classification, Localization and Repair for Introductory Programming Assignments},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591680},
doi = {10.1145/3539618.3591680},
abstract = {Programming education at scale increasingly relies on automated feedback to help students learn to program. An important form of feedback is to point out semantic errors in student programs and provide hints for program repair. Such automated feedback depends essentially on solving the tasks of classification, localization and repair of semantic errors. Although there are datasets for the tasks, we observe that they do not have the annotations supporting all three tasks. As such, existing approaches for semantic error feedback treat error classification, localization and repair as independent tasks, resulting in sub-optimal performance on each task. Moreover, existing datasets either contain few programming assignments or have few programs for each assignment. Therefore, existing approaches often leverage rule-based methods and evaluate them with a small number of programming assignments. To tackle the problems, we first describe the creation of a new dataset COJ2022 that contains 5,914 C programs with semantic errors submitted to 498 different assignments in an introductory programming course, where each program is annotated with the error types and locations and is coupled with the repaired program submitted by the same student. We show the advantages of COJ2022 over existing datasets on various aspects. Second, we treat semantic error classification, localization and repair as dependent tasks, and propose a novel two-stage method ErrorCLR to solve them. Specifically, in the first stage we train a model based on graph matching networks to jointly classify and localize potential semantic errors in student programs, and in the second stage we mask error spans in buggy programs using information of error types and locations and train a CodeT5 model to predict correct spans. The predicted spans replace the error spans to form repaired programs. Experimental results show that ErrorCLR remarkably outperforms the comparative methods for all three tasks on COJ2022 and other public datasets. We also conduct a case study to visualize and interpret what is learned by the graph matching network in ErrorCLR. We have released the source code and COJ2022 at https://github.com/DaSESmartEdu/ErrorCLR.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1345–1354},
numpages = {10},
keywords = {automated feedback, deep learning models, program repair, programming education, semantic program errors},
location = {<conf-loc>, <city>Taipei</city>, <country>Taiwan</country>, </conf-loc>},
series = {SIGIR '23}
}

@inproceedings{10.1145/3611643.3613895,
author = {Liu, Xiaoyu and Jang, Jinu and Sundaresan, Neel and Allamanis, Miltiadis and Svyatkovskiy, Alexey},
title = {AdaptivePaste: Intelligent Copy-Paste in IDE},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613895},
doi = {10.1145/3611643.3613895},
abstract = {In software development, it is common for programmers to copy-paste or port code snippets and then adapt them to their use case. This scenario motivates the code adaptation task – a variant of program repair which aims to adapt variable identifiers in a pasted snippet of code to the surrounding, preexisting context. However, no existing approach has been shown to effectively address this  
task. In this paper, we introduce AdaptivePaste, a learning-based approach to source code adaptation, based on transformers and a  
dedicated dataflow-aware deobfuscation pre-training task to learn meaningful representations of variable usage patterns. We demonstrate that AdaptivePaste can learn to adapt Python source code snippets with 67.8% exact match accuracy. We study the impact of confidence thresholds on the model predictions, showing the model precision can be further improved to 85.9% with our parallel-decoder transformer model in a selective code adaptation setting. To assess the practical use of AdaptivePaste we perform a user study among Python software developers on real-world copy-paste instances. The results show that AdaptivePaste reduces dwell time to nearly half the time it takes to port code manually, and helps to avoid bugs. In addition, we utilize the participant feedback to identify potential avenues for improvement.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1844–1854},
numpages = {11},
keywords = {Code adaptation, Machine learning},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-3-031-46002-9_24,
author = {Busch, Daniel and Nolte, Gerrit and Bainczyk, Alexander and Steffen, Bernhard},
title = {ChatGPT in&nbsp;the&nbsp;Loop: A Natural Language Extension for&nbsp;Domain-Specific Modeling Languages},
year = {2023},
isbn = {978-3-031-46001-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-46002-9_24},
doi = {10.1007/978-3-031-46002-9_24},
abstract = {This paper presents an approach to no-code development based on the interplay of formally defined (graphical) Domain-Specific Languages and informal, intuitive Natural Language which is enriched with contextual information to enable referencing of formally defined entities. The paper focuses on the use and automated integration of these enriched intuitive languages via ChatGPT-based code generation to exploit the best of both language paradigms for domain-specific application development. To compensate for the lack of control over the intuitive languages we apply automated system-level validation via automata learning and subsequent model checking. All this is illustrated using the development of point-and-click adventures as a minimal viable example.},
booktitle = {Bridging the Gap Between AI and Reality: First International Conference, AISoLA 2023, Crete, Greece, October 23–28, 2023, Proceedings},
pages = {375–390},
numpages = {16},
keywords = {Software Engineering, Low-Code/No-Code, Language-driven Engineering, Large Language Models, Automata Learning, Verification, Prompt Engineering, Web Application},
location = {<conf-loc content-type="InPerson">Crete, Greece</conf-loc>}
}

@inproceedings{10.1145/3613372.3613396,
author = {Carneiro, Guilherme and Ferreira, José and Ramalho, Franklin and Massoni, Tiago},
title = {Similar Bug Reports Recommendation System using BERT},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613396},
doi = {10.1145/3613372.3613396},
abstract = {In order to document software issues so that they can be later analyzed and corrected, Bug Reports (BR) are used. According to Mozilla’s Bugzilla, as an example, over 8,000 new bugs were reported for Firefox in 2020. Thus, a recommendation system can be a valuable tool to improve productivity in software development, especially when dealing with a high volume of BRs to be reviewed and possibly fixed by the maintainers. This study proposes and evaluates a BR recommendation system based on textual similarity, with the differential use of the state-of-the-art text comprehension model BERT as one of the factors in the similarity calculation. We use a dataset with 106k Mozilla BRs extracted from Bugzilla, an open-source platform. The main objective is to improve suggestions for BRs with a context close to that provided by the maintainer. In the study, we experimented with the BERT model adopting the similarity calculation as individually as together with the well-known TF-IDF vectorization technique. The results attest that there were gains of approximately 14% in the frequency of relevant BRs for the first 20 recommendations compared to a baseline technique that adopts only the TF-IDF vectorization approach. The BERT model added improvements to the evaluated metrics (precision, feedback, and likelihood) when complementary to TF-IDF, but did not perform positively in an isolated manner. Overall, the findings could have implications for software development teams handling a high volume of BRs and potentially increase their productivity in resolving BRs.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {378–387},
numpages = {10},
keywords = {BERT, Bug Reports, Bugzilla, Recommendation System},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@inproceedings{10.1109/ICSE48619.2023.00060,
author = {Fang, Sen and Zhang, Tao and Tan, Youshuai and Jiang, He and Xia, Xin and Sun, Xiaobing},
title = {RepresentThemAll: A Universal Learning Representation of Bug Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00060},
doi = {10.1109/ICSE48619.2023.00060},
abstract = {Deep learning techniques have shown promising performance in automated software maintenance tasks associated with bug reports. Currently, all existing studies learn the customized representation of bug reports for a specific downstream task. Despite early success, training multiple models for multiple downstream tasks faces three issues: complexity, cost, and compatibility, due to the customization, disparity, and uniqueness of these automated approaches. To resolve the above challenges, we propose RepresentThemAll, a pre-trained approach that can learn the universal representation of bug reports and handle multiple downstream tasks. Specifically, RepresentThemAll is a universal bug report framework that is pre-trained with two carefully designed learning objectives: one is the dynamic masked language model and another one is a contrastive learning objective, "find yourself". We evaluate the performance of RepresentThemAll on four downstream tasks, including duplicate bug report detection, bug report summarization, bug priority prediction, and bug severity prediction. Our experimental results show that RepresentThemAll outperforms all baseline approaches on all considered downstream tasks after well-designed fine-tuning.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {602–614},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-48796-5_9,
author = {Tawosi, Vali and Alamir, Salwa and Liu, Xiaomo},
title = {Search-Based Optimisation of&nbsp;LLM Learning Shots for&nbsp;Story Point Estimation},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48796-5_9},
doi = {10.1007/978-3-031-48796-5_9},
abstract = {One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a meta-learning process known as few-shot learning. In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM’s estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting.},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {123–129},
numpages = {7},
keywords = {Search-Based Software Effort Estimation, Large Language Model, Few-shot Learning, Multi-objective Optimisation},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@inproceedings{10.1145/3611643.3616327,
author = {Davis, Matthew and Choi, Sangheon and Estep, Sam and Myers, Brad and Sunshine, Joshua},
title = {NaNofuzz: A Usable Tool for Automatic Test Generation},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616327},
doi = {10.1145/3611643.3616327},
abstract = {In the United States alone, software testing labor is estimated to cost $48 billion USD per year. Despite widespread test execution automation and automation in other areas of software engineering, test suites continue to be created manually by software engineers. We have built a test generation tool, called NaNofuzz, that helps users find bugs in their code by suggesting tests where the output is likely indicative of a bug, e.g., that return NaN (not-a-number) values. NaNofuzz is an interactive tool embedded in a development environment to fit into the programmer's workflow. NaNofuzz tests a function with as little as one button press, analyses the program to determine inputs it should evaluate, executes the program on those inputs, and categorizes outputs to prioritize likely bugs. We conducted a randomized controlled trial with 28 professional software engineers using NaNofuzz as the intervention treatment and the popular manual testing tool, Jest, as the control treatment. Participants using NaNofuzz on average identified bugs more accurately (p &lt; .05, by 30%), were more confident in their tests (p &lt; .03, by 20%), and finished their tasks more quickly (p &lt; .007, by 30%).},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1114–1126},
numpages = {13},
keywords = {Empirical software engineering, automatic test generation, experiments, human subjects, software testing, usable testing, user study},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616369,
author = {Gotmare, Akhilesh Deepak and Li, Junnan and Joty, Shafiq and Hoi, Steven C.H.},
title = {Efficient Text-to-Code Retrieval with Cascaded Fast and Slow Transformer Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616369},
doi = {10.1145/3611643.3616369},
abstract = {The goal of semantic code search or text-to-code search is to retrieve a semantically relevant code snippet from an existing code database using a natural language query. When constructing a practical semantic code search system, existing approaches fail to provide an optimal balance between retrieval speed and the relevance of the retrieved results. We propose an efficient and effective text-to-code search framework with cascaded fast and slow models, in which a fast transformer encoder model is learned to optimize a scalable index for fast retrieval followed by learning a slow classification-based re-ranking model to improve the accuracy of the top K results from the fast retrieval. To further reduce the high memory cost of deploying two separate models in practice, we propose to jointly train the fast and slow model based on a single transformer encoder with shared parameters. Empirically our cascaded method is not only efficient and scalable, but also achieves state-of-the-art results with an average mean reciprocal ranking (MRR) score of 0.7795 (across 6 programming languages) on the CodeSearchNet benchmark as opposed to the prior state-of-the-art result of 0.744 MRR. Our codebase can be found at this link.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {388–400},
numpages = {13},
keywords = {Cascaded retrieval schemes, Code retrieval, Developer Productivity, Text to Code Search, Transformer models, top K retrieval},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1007/s10664-023-10346-3,
author = {Fu, Michael and Tantithamthavorn, Chakkrit and Le, Trung and Kume, Yuki and Nguyen, Van and Phung, Dinh and Grundy, John},
title = {AIBugHunter: A Practical tool for predicting, classifying and repairing software vulnerabilities},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10346-3},
doi = {10.1007/s10664-023-10346-3},
abstract = {Many Machine Learning(ML)-based approaches have been proposed to automatically detect, localize, and repair software vulnerabilities. While ML-based methods are more effective than program analysis-based vulnerability analysis tools, few have been integrated into modern Integrated Development Environments (IDEs), hindering practical adoption. To bridge this critical gap, we propose in this article AIBugHunter, a novel Machine Learning-based software vulnerability analysis tool for C/C++ languages that is integrated into the Visual Studio Code (VS Code) IDE. AIBugHunter&nbsp;helps software developers to achieve real-time vulnerability detection, explanation, and repairs during programming. In particular, AIBugHunter&nbsp;scans through developers’ source code to (1) locate vulnerabilities, (2) identify vulnerability types, (3) estimate vulnerability severity, and (4) suggest vulnerability repairs. We integrate our previous works (i.e., LineVul and VulRepair) to achieve vulnerability localization and repairs. In this article, we propose a novel multi-objective optimization (MOO)-based vulnerability classification approach and a transformer-based estimation approach to help AIBugHunter&nbsp;accurately identify vulnerability types and estimate severity. Our empirical experiments on a large dataset consisting of 188K+ C/C++ functions confirm that our proposed approaches are more accurate than other state-of-the-art baseline methods for vulnerability classification and estimation. Furthermore, we conduct qualitative evaluations including a survey study and a user study to obtain software practitioners’ perceptions of our AIBugHunter&nbsp;tool and assess the impact that AIBugHunter&nbsp;may have on developers’ productivity in security aspects. Our survey study shows that our AIBugHunter&nbsp;is perceived as useful where 90% of the participants consider adopting our AIBugHunter&nbsp;during their software development. Last but not least, our user study shows that our AIBugHunter&nbsp;can enhance developers’ productivity in combating cybersecurity issues during software development. AIBugHunter&nbsp;is now publicly available in the Visual Studio Code marketplace.},
journal = {Empirical Softw. Engg.},
month = {nov},
numpages = {33},
keywords = {Vulnerability prediction, Vulnerability localization, Vulnerability classification, Vulnerability repair}
}

@inproceedings{10.1109/ICSE48619.2023.00072,
author = {Liu, Fang and Li, Jia and Zhang, Li},
title = {Syntax and Domain Aware Model for Unsupervised Program Translation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00072},
doi = {10.1109/ICSE48619.2023.00072},
abstract = {There is growing interest in software migration as the development of software and society. Manually migrating projects between languages is error-prone and expensive. In recent years, researchers have begun to explore automatic program translation using supervised deep learning techniques by learning from large-scale parallel code corpus. However, parallel resources are scarce in the programming language domain, and it is costly to collect bilingual data manually. To address this issue, several unsupervised programming translation systems are proposed. However, these systems still rely on huge monolingual source code to train, which is very expensive. Besides, these models cannot perform well for translating the languages that are not seen during the pre-training procedure. In this paper, we propose SDA-Trans, a syntax and domain-aware model for program translation, which leverages the syntax structure and domain knowledge to enhance the cross-lingual transfer ability. SDA-Trans adopts unsupervised training on a smaller-scale corpus, including Python and Java monolingual programs. The experimental results on function translation tasks between Python, Java, and C++ show that SDA-Trans outperforms many large-scale pre-trained models, especially for unseen language translation.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {755–767},
numpages = {13},
keywords = {program translation, neural networks, syntax structure, unsupervised learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00178,
author = {Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
title = {Learning Deep Semantics for Test Completion},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00178},
doi = {10.1109/ICSE48619.2023.00178},
abstract = {Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TECO---a deep learning model using code semantics for test completion. The key insight underlying TECO is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. TECO extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TECO, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that TECO achieves an exact-match accuracy of 18, which is 29% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, TECO can generate runnable code in 29% of the cases compared to 18% obtained by the best baseline. Moreover, TECO is significantly better than prior work on test oracle generation.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2111–2123},
numpages = {13},
keywords = {test completion, deep neural networks, programming language semantics},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1109/TSE.2023.3255177,
author = {Le-Cong, Thanh and Luong, Duc-Minh and Le, Xuan Bach D. and Lo, David and Tran, Nhat-Hoa and Quang-Huy, Bui and Huynh, Quyet-Thang},
title = {Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning},
year = {2023},
issue_date = {June 2023},
publisher = {IEEE Press},
volume = {49},
number = {6},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2023.3255177},
doi = {10.1109/TSE.2023.3255177},
abstract = {Automated program repair (APR) faces the challenge of test overfitting, where generated patches pass validation tests but fail to generalize. Existing methods for patch assessment involve generating new tests or manual inspection, which can be time-consuming or biased. In this paper, we propose a novel technique, &lt;sc&gt;Invalidator&lt;/sc&gt;, to automatically assess the correctness of APR-generated patches via semantic and syntactic reasoning. &lt;sc&gt;Invalidator&lt;/sc&gt; leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model. Given a buggy program and the developer-patched program, &lt;sc&gt;Invalidator&lt;/sc&gt; infers likely invariants on both programs. Then, &lt;sc&gt;Invalidator&lt;/sc&gt; determines that an APR-generated patch overfits if: (1) it violates correct specifications or (2) maintains erroneous behaviors from the original buggy program. In case our approach fails to determine an overfitting patch based on invariants, &lt;sc&gt;Invalidator&lt;/sc&gt; utilizes a trained model from labeled patches to assess patch correctness based on program syntax. The benefit of &lt;sc&gt;Invalidator&lt;/sc&gt; is threefold. First, &lt;sc&gt;Invalidator&lt;/sc&gt; leverages both semantic and syntactic reasoning to enhance its discriminative capability. Second, &lt;sc&gt;Invalidator&lt;/sc&gt; does not require new test cases to be generated, but instead only relies on the current test suite and uses invariant inference to generalize program behaviors. Third, &lt;sc&gt;Invalidator&lt;/sc&gt; is fully automated. Experimental results demonstrate that &lt;sc&gt;Invalidator&lt;/sc&gt; outperforms existing methods in terms of Accuracy and F-measure, correctly identifying 79&amp;#x0025; of overfitting patches and detecting 23&amp;#x0025; more overfitting patches than the best baseline.},
journal = {IEEE Trans. Softw. Eng.},
month = {jun},
pages = {3411–3429},
numpages = {19}
}

@article{10.1007/s10664-023-10402-y,
author = {Ferrara, Carmine and Sellitto, Giulia and Ferrucci, Filomena and Palomba, Fabio and De Lucia, Andrea},
title = {Fairness-aware machine learning engineering: how far are we?},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10402-y},
doi = {10.1007/s10664-023-10402-y},
abstract = {Machine learning is part of the daily life of people and companies worldwide. Unfortunately, bias in machine learning algorithms risks unfairly influencing the decision-making process and reiterating possible discrimination. While the interest of the software engineering community in software fairness is rapidly increasing, there is still a lack of understanding of various aspects connected to fair machine learning engineering, i.e., the software engineering process involved in developing fairness-critical machine learning systems. Questions connected to the practitioners’ awareness and maturity about fairness, the skills required to deal with the matter, and the best development phase(s) where fairness should be faced more are just some examples of the knowledge gaps currently open. In this paper, we provide insights into how fairness is perceived and managed in practice, to shed light on the instruments and approaches that practitioners might employ to properly handle fairness. We conducted a survey with 117 professionals who shared their knowledge and experience highlighting the relevance of fairness in practice, and the skills and tools required to handle it. The key results of our study show that fairness is still considered a second-class quality aspect in the development of artificial intelligence systems. The building of specific methods and development environments, other than automated validation tools, might help developers to treat fairness throughout the software lifecycle and revert this trend.},
journal = {Empirical Softw. Engg.},
month = {nov},
numpages = {46},
keywords = {Software fairness, Machine learning, Empirical software engineering, Survey study, Practitioners’ perspective}
}

@inproceedings{10.1109/ICSE48619.2023.00085,
author = {Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
title = {CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00085},
doi = {10.1109/ICSE48619.2023.00085},
abstract = {Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {919–931},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00063,
author = {Mahbub, Parvez and Shuvo, Ohiduzzaman and Rahman, Mohammad Masudur},
title = {Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00063},
doi = {10.1109/ICSE48619.2023.00063},
abstract = {Software bugs claim ≈ 50% of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {640–652},
numpages = {13},
keywords = {software bug, bug explanation, software engineering, software maintenance, natural language processing, deep learning, transformers},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00119,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Che, Xing and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00119},
doi = {10.1109/ICSE48619.2023.00119},
abstract = {Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1355–1367},
numpages = {13},
keywords = {text input generation, GUI testing, android app, large language model, prompt-tuning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00207,
author = {Liu, Shangqing and Wu, Bozhi and Xie, Xiaofei and Meng, Guozhu and Liu, Yang},
title = {ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00207},
doi = {10.1109/ICSE48619.2023.00207},
abstract = {Large-scale pre-trained models such as CodeBERT, GraphCodeBERT have earned widespread attention from both academia and industry. Attributed to the superior ability in code representation, they have been further applied in multiple downstream tasks such as clone detection, code search and code translation. However, it is also observed that these state-of-the-art pre-trained models are susceptible to adversarial attacks. The performance of these pre-trained models drops significantly with simple perturbations such as renaming variable names. This weakness may be inherited by their downstream models and thereby amplified at an unprecedented scale. To this end, we propose an approach namely ContraBERT that aims to improve the robustness of pre-trained models via contrastive learning. Specifically, we design nine kinds of simple and complex data augmentation operators on the programming language (PL) and natural language (NL) data to construct different variants. Furthermore, we continue to train the existing pre-trained models by masked language modeling (MLM) and contrastive pre-training task on the original samples with their augmented variants to enhance the robustness of the model. The extensive experiments demonstrate that ContraBERT can effectively improve the robustness of the existing pre-trained models. Further study also confirms that these robustness-enhanced models provide improvements as compared to original models over four popular downstream tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2476–2487},
numpages = {12},
keywords = {code pre-trained models, contrastive learning, model robustness},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3609437.3609465,
author = {Zhao, Yunfei and Dong, Yihong and Li, Ge},
title = {Seq2Seq or Seq2Tree: Generating Code Using Both Paradigms via Mutual Learning},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609465},
doi = {10.1145/3609437.3609465},
abstract = {Code generation aims to automatically generate the source code based on given natural language (NL) descriptions, which is of great significance for automated software development. Some code generation models follow a language model-based paradigm (LMBP) to generate source code tokens sequentially. Some others focus on deriving the grammatical structure by generating the program’s abstract syntax tree (AST), i.e., using the grammatical structure-based paradigm (GSBP). Existing studies are trying to generate code through one of the above two models. However, human developers often consider both paradigms: building the grammatical structure of the code and writing source code sentences according to the language model. Therefore, we argue that code generation should consider both GSBP and LMBP. In this paper, we use mutual learning to combine two classes of models to make the two different paradigms train together. To implement the mutual learning framework, we design alignment methods between code and AST. Under this framework, models can be enhanced through shared encoders and knowledge interaction in aligned training steps. We experiment on three Python-based code generation datasets. Experimental results and ablation analysis confirm the effectiveness of our approach. Our results demonstrate that considering both GSBP and LMBP is helpful in improving the performance of code generation.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {238–248},
numpages = {11},
keywords = {abstract syntax tree, code generation, mutual learning, neural networks},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3609437.3609438,
author = {Shi, Chaoxuan and Zhu, Tingwei and Zhang, Tian and Pang, Jun and Pan, Minxue},
title = {Structural-semantics Guided Program Simplification for Understanding Neural Code Intelligence Models},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609438},
doi = {10.1145/3609437.3609438},
abstract = {Neural code intelligence models are cutting-edge automated code understanding technologies that have achieved remarkable performance in various software engineering tasks. However, the lack of deep learning models’ interpretability hinders the application of deep learning based code intelligence models in real-world scenarios, particularly in security-critical domains. Previous studies use program simplification to understand neural code intelligence models, but they have overlooked the fact that the most significant difference between source code and natural language is the code’s structural semantics. In this paper, we first conduct an empirical study to identify the critical code structural semantic features valued by neural code intelligence models, and then we propose a novel program simplification method called SSGPS (Structural-Semantics Guided Program Simplification). Results on three code summarization models show that SSGPS can reduce training and testing time by 20-40% while controlling the decrease in model performance by less than 4%, demonstrating that our method can retain the critical code structural semantics for understanding neural code intelligence models.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {1–11},
numpages = {11},
keywords = {Code Structural Semantics, Interpretable AI, Neural Code Intelligence Model, Program Simplification},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1109/ICSE48619.2023.00168,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Su, Yuhui and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Ex Pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00168},
doi = {10.1109/ICSE48619.2023.00168},
abstract = {Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1983–1995},
numpages = {13},
keywords = {GUI testing, deep learning, program analysis, empirical study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3635711,
author = {He, Junda and Zhou, Xin and Xu, Bowen and Zhang, Ting and Kim, Kisub and Yang, Zhou and Thung, Ferdian and Irsan, Ivana Clairine and Lo, David},
title = {Representation Learning for Stack Overflow Posts: How Far are We?},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635711},
doi = {10.1145/3635711},
abstract = {The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers’ interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network (CNN) and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks, i.e., tag recommendation, relatedness prediction, and API recommendation. The results show that Post2Vec cannot further improve the state-of-the-art techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, GPT2) and (2) language models built with software engineering-related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the “No Silver Bullet” concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the state-of-the-art performance significantly for all the downstream tasks.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {Stack Overflow, Transformers, Pre-trained Models}
}

@inproceedings{10.1145/3611643.3617854,
author = {Bhuiyan, Masudul Hasan Masud},
title = {The Call Graph Chronicles: Unleashing the Power Within},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3617854},
doi = {10.1145/3611643.3617854},
abstract = {Call graph generation is critical for program understanding and  
analysis, but achieving both accuracy and precision is challenging.  
Existing methods trade off one for the other, particularly in dy-  
namic languages like JavaScript. This paper introduces "Graphia,"  
an approach that combines structural and semantic information  
using a Graph Neural Network (GNN) to enhance call graph accu-  
racy. Graphia’s two-step process employs an initial call graph as  
training data for the GNN, which then uncovers true call edges in  
new programs. Experimental results show Graphia significantly  
improves true positive rates in vulnerability detection, achieving up  
to 95%. This approach advances call graph accuracy by effectively  
incorporating code structure and context, particularly in complex  
dynamic language scenarios.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2210–2212},
numpages = {3},
keywords = {Call Graphs, Graph Neural Network, Software Engineering},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3635439.3635444,
author = {Arbab, Farhad and Autili, Marco and Ciccozzi, Federico and Poizat, Pascal and Tivoli, Massimo},
title = {Summary of the 5th International Workshop on Automated andverifiable Software sYstem DEvelopment (ASYDE)co-located with the 38th IEEE/ACM ASE 2023},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3635439.3635444},
doi = {10.1145/3635439.3635444},
abstract = {Over the past three decades, automation in software development has gone mainstream. Software development teams strive to automate as much of the software development activities as possible, spanning requirements specification, system modeling, code generation, testing, deployment, verification, as well as release phases, project status reporting and system maintenance. Automation helps to reduce development time and cost, as well as to concentrate knowledge by bringing quality into every step of the development process. The Workshop on Automated and verifiable Software sYstem DEvelopment (ASYDE) provided a forum to share and discuss innovative contributions to research and practice related to novel software engineering approaches to automated and verifiable development of software systems. The 5th edition took place on September 11th, 2023, in Kirchberg, Luxembourg. Notably, this marked the inaugural co-location of ASYDE with the IEEE/ACM International Conference on Automated Software Engineering (ASE).},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {24–26},
numpages = {3}
}

@inproceedings{10.1145/3611643.3616356,
author = {Du, Xiaohu and Wen, Ming and Wei, Zichao and Wang, Shangwen and Jin, Hai},
title = {An Extensive Study on Adversarial Attack against Pre-trained Models of Code},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616356},
doi = {10.1145/3611643.3616356},
abstract = {Transformer-based pre-trained models of code (PTMC) have been widely utilized and have achieved state-of-the-art performance in many mission-critical applications. However, they can be vulnerable to adversarial attacks through identifier substitution or coding style transformation, which can significantly degrade accuracy and may further incur security concerns. Although several approaches have been proposed to generate adversarial examples for PTMC, the effectiveness and efficiency of such approaches, especially on different code intelligence tasks, has not been well understood. To bridge this gap, this study systematically analyzes five state-of-the-art adversarial attack approaches from three perspectives: effectiveness, efficiency, and the quality of generated examples. The results show that none of the five approaches balances all these perspectives. Particularly, approaches with a high attack success rate tend to be time-consuming; the adversarial code they generate often lack naturalness, and vice versa. To address this limitation, we explore the impact of perturbing identifiers under different contexts and find that identifier substitution within for and if statements is the most effective. Based on these findings, we propose a new approach that prioritizes different types of statements for various tasks and further utilizes beam search to generate adversarial examples. Evaluation results show that it outperforms the state-of-the-art ALERT in terms of both effectiveness and efficiency while preserving the naturalness of the generated adversarial examples.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {489–501},
numpages = {13},
keywords = {Adversarial Attack, Deep Learning, Pre-Trained Model},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3613093,
author = {Cabra-Acela, Laura and Mojica-Hanke, Anamaria and Linares-Vásquez, Mario and Herbold, Steffen},
title = {On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613093},
doi = {10.1145/3611643.3613093},
abstract = {Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support users’ daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&amp;A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user’s context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at  GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2142–2146},
numpages = {5},
keywords = {Good practices, Information retrieval, Large language models, Machine learning},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616350,
author = {Zhang, Jiyang and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos},
title = {Multilingual Code Co-evolution using Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616350},
doi = {10.1145/3611643.3616350},
abstract = {Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {695–707},
numpages = {13},
keywords = {Language model, code translation, software evolution},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3628162,
author = {Shoufan, Abdulhadi},
title = {Can Students without Prior Knowledge Use ChatGPT to Answer Test Questions? An Empirical Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
url = {https://doi.org/10.1145/3628162},
doi = {10.1145/3628162},
abstract = {With the immense interest in ChatGPT worldwide, education has seen a mix of both excitement and skepticism. To properly evaluate its impact on education, it is crucial to understand how far it can help students without prior knowledge answer assessment questions. This study aims to address this question as well as the impact of the question type. We conducted multiple experiments with computer engineering students (experiment group: n=41 to 56), who were asked to use ChatGPT to answer previous test questions before learning about the related topics. Their scores were then compared with the scores of previous-term students who answered the same questions in a quiz or exam setting (control group: n=24 to 61). The results showed a wide range of effect sizes, from -2.55 to 1.23, depending on the question type and content. The experiment group performed best answering code analysis and conceptual questions but struggled with code completion and questions that involved images. However, the performance in code generation tasks was inconsistent. Overall, the ChatGPT group’s answers lagged slightly behind the control group’s answers with an effect size of -0.16. We conclude that ChatGPT, at least in the field of this study, is not yet ready to rely on by students who do not have sufficient background to evaluate generated answers. We suggest that educators try using ChatGPT and educate students on effective questioning techniques and how to assess the generated responses. This study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.},
journal = {ACM Trans. Comput. Educ.},
month = {dec},
articleno = {45},
numpages = {29},
keywords = {ChatGPT, large language models}
}

@article{10.1145/3617946.3617953,
author = {Biagiola, Matteo and Cardozo, Nicolás and Shin, Donghwan and Khomh, Foutse and Stocco, Andrea and Riccio, Vincenzo},
title = {Summary of the Fourth International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest 2023)},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617953},
doi = {10.1145/3617946.3617953},
abstract = {Deep Learning (DL) techniques help software developers thanks to their ability to learn from historical information which is useful in several program analysis and testing tasks (e.g., malware detection, fuzz testing, bug-finding, and type-checking). DL-based software systems are also increasingly adopted in safety-critical domains, such as autonomous driving, medical diagnosis, and aircraft collision avoidance systems. In particular, testing the correctness and reliability of DL-based systems is paramount, since a failure of such systems would cause a significant safety risk for the involved people and/or environment. The 4th International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest 2023) was co-located with the 45th International Conference on Software Engineering (ICSE), with the goal of targeting research at the intersection of software engineering and deep learning and devise novel approaches and tools to ensure the interpretability and dependability of software systems that depends on DL components.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {39–40},
numpages = {2}
}

@inproceedings{10.1145/3597926.3598096,
author = {He, Yichen and Wang, Liran and Wang, Kaiyi and Zhang, Yupeng and Zhang, Hang and Li, Zhoujun},
title = {COME: Commit Message Generation with Modification Embedding},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598096},
doi = {10.1145/3597926.3598096},
abstract = {Commit messages concisely describe code changes in natural language and are important for program comprehension and maintenance. Previous studies proposed some approaches for automatic commit message generation, but their performance is limited due to inappropriate representation of code changes and improper combination of translation-based and retrieval-based approaches. To address these problems, this paper introduces a novel framework named COME, in which modification embeddings are used to represent code changes in a fine-grained way, a self-supervised generative task is designed to learn contextualized code change representation, and retrieval-based and translation-based methods are combined through a decision algorithm. The average improvement of COME over the state-of-the-art approaches is 9.2% on automatic evaluation metrics and 8.0% on human evaluation metrics. We also analyse the effectiveness of COME's three main components and each of them results in an improvement of 8.6%, 8.7% and 5.2%.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {792–803},
numpages = {12},
keywords = {Automatic Commit Message Generation, Contextualized Code Change Representation Learning, Self-supervised Learning},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1007/978-3-031-35894-4_2,
author = {Amaro, Ilaria and Della Greca, Attilio and Francese, Rita and Tortora, Genoveffa and Tucci, Cesare},
title = {AI Unreliable Answers: A Case Study on&nbsp;ChatGPT},
year = {2023},
isbn = {978-3-031-35893-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35894-4_2},
doi = {10.1007/978-3-031-35894-4_2},
abstract = {ChatGPT is a general domain chatbot which is object of great attention stimulating all the world discussions on the power and the consequences of the Artificial Intelligence diffusion in all the field, ranging from education, research, music to software development, health care, cultural heritage, and entertainment.In this paper, we try to investigate whether and when the answers provided by ChatGPT are unreliable and how this is perceived by expert users, such as Computer Science students. To this aim, we first analyze the reliability of the answers provided by ChatGPT by experimenting its narrative, problem solving, searching, and logic capabilities and report examples of answers. Then, we conducted a user study in which 15 participants that already knew the chatbot proposed a set of predetermined queries generating both correct and incorrect answers and then we collected their satisfaction. Results revealed that even if the present version of ChatGPT sometimes is unreliable, people still plan to use it. Thus, it is recommended to use the present version of ChatGPT always with the support of human verification and interpretation.},
booktitle = {Artificial Intelligence in HCI: 4th International Conference, AI-HCI 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part II},
pages = {23–40},
numpages = {18},
keywords = {ChatGPT, Satisfaction, Case Study},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1145/3597926.3598060,
author = {Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun},
title = {Who Judges the Judge: An Empirical Study on Online Judge Tests},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598060},
doi = {10.1145/3597926.3598060},
abstract = {Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2% of false positives have perfect (100%) line coverage, 78.9% have perfect branch coverage, and 32.5% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {334–346},
numpages = {13},
keywords = {Online judge platform, software testing, test assessment},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00199,
author = {Griebl, Elisabeth and Fein, Benedikt and Obermüller, Florian and Fraser, Gordon and Just, René},
title = {On the Applicability of Language Models to Block-Based Programs},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00199},
doi = {10.1109/ICSE48619.2023.00199},
abstract = {Block-based programming languages like SCRATCH are increasingly popular for programming education and end-user programming. Recent program analyses build on the insight that source code can be modelled using techniques from natural language processing. Many of the regularities of source code that support this approach are due to the syntactic overhead imposed by textual programming languages. This syntactic overhead, however, is precisely what block-based languages remove in order to simplify programming. Consequently, it is unclear how well this modelling approach performs on block-based programming languages. In this paper, we investigate the applicability of language models for the popular block-based programming language SCRATCH. We model SCRATCH programs using n-gram models, the most essential type of language model, and transformers, a popular deep learning model. Evaluation on the example tasks of code completion and bug finding confirm that blocks inhibit predictability, but the use of language models is nevertheless feasible. Our findings serve as foundation for improving tooling and analyses for block-based languages.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2374–2386},
numpages = {13},
keywords = {block-based programs, scratch, natural language model, code completion, bugram},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3624032.3624034,
author = {Guerino, Lucca and Vincenzi, Auri},
title = {An Experimental Study Evaluating Cost, Adequacy, and Effectiveness of Pynguin's Test Sets},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624034},
doi = {10.1145/3624032.3624034},
abstract = {Context: Software testing is a very relevant step in quality assurance, but developers frequently overlook it. We pursued testing automation to minimize the impact of missing test cases in a software project. Problem: However, for Python programs, there are not many tools able to fully automate the generation of unit test sets, and the one available demands studies to provide evidence of the quality of the generated test set. Solution: This work aims to evaluate the quality of different unit test generation algorithms for Python, implemented in a tool named Pynguin. Method: In the analysis of the selected programs, the Pynguin test generation tool is executed with each of its algorithms, including random, as a way to generate complete unit test sets. Then, we evaluate each generated test set’s efficacy, efficiency, and cost. We use four different fault models, implemented by four mutation testing tools, to measure efficacy. We use line and branch coverage to measure efficiency, the number of test cases, and test set execution time to measure cost. Summary of Results: We identified that RANDOM test set performed worst concerning all evaluated aspects, DYNAMOSA and MOSA, the two algorithms that generate the best test sets regarding efficacy, efficiency, and cost. By combining all Pynguin smart algorithms (DYNAMOSA, MIO, MOSA, WHOLE-SUITE), the resultant test set overcomes the individual test sets efficiency by around 1%, for coverage and efficacy by 4.5% on average, concerning previous mutation score, at a reasonable cost, without a test set minimization.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {5–14},
numpages = {10},
keywords = {automated test generation, coverage testing, experimental software engineering, mutation testing, software testing, testing tools},
location = {<conf-loc>, <city>Campo Grande, MS</city>, <country>Brazil</country>, </conf-loc>},
series = {SAST '23}
}

@inproceedings{10.1145/3597926.3598075,
author = {Gao, Tianchang and Chen, Junjie and Zhao, Yingquan and Zhang, Yuqun and Zhang, Lingming},
title = {Vectorizing Program Ingredients for Better JVM Testing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598075},
doi = {10.1145/3597926.3598075},
abstract = {JVM testing is one of the most widely-used methodologies for guaranteeing the quality of JVMs. Among various JVM testing techniques, synthesis-based JVM testing, which constructs a test program by synthesizing various code snippets (also called program ingredients), has been demonstrated state-of-the-art. The existing synthesis-based JVM testing work puts more efforts in ensuring the validity of synthesized test programs, but ignores the influence of huge ingredient space, which largely limits the ingredient exploration efficiency as well as JVM testing performance. In this work, we propose Vectorized JVM Testing (called VECT) to further promote the performance of synthesis-based JVM testing. Its key insight is to reduce the huge ingredient space by clustering semantically similar ingredients via vectorizing ingredients using state-of-the-art code representation. To make VECT complete and more effective, based on vectorized ingredients, VECT further designs a feedback-driven ingredient selection strategy and an enhanced test oracle. We conducted an extensive study to evaluate VECT on three popular JVMs (i.e., HotSpot, OpenJ9, and Bisheng JDK) involving five OpenJDK versions. The results demonstrate VECT detects 115.03% ~ 776.92% more unique inconsistencies than the state-of-the-art JVM testing technique during the same testing time. In particular, VECT detects 26 previously unknown bugs for them, 15 of which have already been confirmed/fixed by developers.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {526–537},
numpages = {12},
keywords = {JVM Testing, Java Virtual Machine, Program Synthesis, Test Oracle},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3605764.3623985,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79–90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {AISec '23}
}

@inproceedings{10.1145/3631991.3632024,
author = {Chen, Shunxing and Xu, Xiaoshu and Zhang, Huanhuan and Zhang, Yunfeng},
title = {Roles of ChatGPT in virtual teaching assistant and intelligent tutoring system: opportunities and challenges},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3632024},
doi = {10.1145/3631991.3632024},
abstract = {Artificial Intelligence (AI), specifically the Generative Pre-trained Transformer 4 (GPT-4), or ChatGPT, promises to revolutionize Virtual Teaching Assistants (VTAs) and Intelligent Tutoring Systems (ITS). This advanced language model fosters enhanced student engagement and personalized, adaptive learning experiences. However, amidst the substantial benefits, several critical challenges encompassing response reliability, data privacy, algorithmic biases, and interpretability necessitate deliberate scrutiny. The proposed study aims to examine the opportunities and hurdles inherent to the deployment of ChatGPT in the educational landscape. With a focus on high-quality, Google Scholar, Scopus, and Web of Science-indexed literature, the review encompasses a comprehensive exploration of empirical studies, theoretical perspectives, and practical implications related to ChatGPT. Through this literature review, we will shed light on the dynamic intersection of AI and education. The elucidation of nuanced implications will empower educators, policymakers, and AI developers to make informed decisions and devise effective strategies, thereby facilitating an optimized integration of ChatGPT into the educational ecosystem.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {201–206},
numpages = {6},
keywords = {ChatGPT, challenges, education, intelligent tutoring systems, opportunities, virtual teaching assistants},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00128,
author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
title = {Automated Repair of Programs from Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00128},
doi = {10.1109/ICSE48619.2023.00128},
abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1469–1481},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3617555.3617873,
author = {Chand, Sivajeet and Pandey, Sushant Kumar and Horkoff, Jennifer and Staron, Miroslaw and Ochodek, Miroslaw and Durisic, Darko},
title = {Comparing Word-Based and AST-Based Models for Design Pattern Recognition},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617873},
doi = {10.1145/3617555.3617873},
abstract = {Design patterns (DPs) provide reusable and general solutions for frequently encountered problems. Patterns are important to maintain the structure and quality of software products, in particular  
in large and distributed systems like automotive software. Modern language models (like Code2Vec or Word2Vec) indicate a deep understanding of programs, which has been shown to help in such  
tasks as program repair or program comprehension, and therefore show promise for DPR in industrial contexts. The models are trained in a self-supervised manner, using a large unlabelled code  
base, which allows them to quantify such abstract concepts as programming styles, coding guidelines, and, to some extent, the semantics of programs. This study demonstrates how two language  
models—Code2Vec and Word2Vec, trained on two public automotive repositories, can show the separation of programs containing specific DPs. The results show that the Code2Vec and Word2Vec  
produce average F1-scores of 0.781 and 0.690 on open-source Java  
programs, showing promise for DPR in practice.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {44–48},
numpages = {5},
keywords = {Design Patterns, NLP, Programming Language Models},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {PROMISE 2023}
}

@inproceedings{10.1145/3611643.3616266,
author = {Wang, Longtian and Xie, Xiaofei and Du, Xiaoning and Tian, Meng and Guo, Qing and Yang, Zheng and Shen, Chao},
title = {DistXplore: Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616266},
doi = {10.1145/3611643.3616266},
abstract = {Deep learning (DL) models are trained on sampled data, where the distribution of training data differs from that of real-world data (i.e., the distribution shift), which reduces the model's robustness. Various testing techniques have been proposed, including distribution-unaware and distribution-aware methods. However, distribution-unaware testing lacks effectiveness by not explicitly considering the distribution of test cases and may generate redundant errors (within same distribution). Distribution-aware testing techniques primarily focus on generating test cases that follow the training distribution, missing out-of-distribution data that may also be valid and should be considered in the testing process.  
In this paper, we propose a novel distribution-guided approach for generating valid test cases with diverse distributions, which can better evaluate the model's robustness (i.e., generating hard-to-detect errors) and enhance the model's robustness (i.e., enriching training data). Unlike existing testing techniques that optimize individual test cases, DistXplore optimizes test suites that represent specific distributions. To evaluate and enhance the model's robustness, we design two metrics: distribution difference, which maximizes the similarity in distribution between two different classes of data to generate hard-to-detect errors, and distribution diversity, which increase the distribution diversity of generated test cases for enhancing the model's robustness. To evaluate the effectiveness of DistXplore in model evaluation and enhancement, we compare DistXplore with 14 state-of-the-art baselines on 10 models across 4 datasets. The evaluation results show that DisXplore not only detects a larger number of errors (e.g., 2×+ on average). Furthermore, DistXplore achieves a higher improvement in empirical robustness (e.g., 5.2% more accuracy improvement than the baselines on average).},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {68–80},
numpages = {13},
keywords = {Deep learning, distribution diversity, model enhancement, software testing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1007/s00521-023-08833-1,
author = {Kaur, Kamaljit and Kaur, Parminder},
title = {MNoR-BERT: multi-label classification of non-functional requirements using BERT},
year = {2023},
issue_date = {Oct 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {30},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-023-08833-1},
doi = {10.1007/s00521-023-08833-1},
abstract = {In the era of Internet access, software is easily available on digital distribution platforms such as app stores. The distribution of software on these platforms makes user feedback more accessible and can be used from requirements engineering to software maintenance context. However, such user reviews might contain technical information about the app that can be valuable for developers and software companies. Due to pervasive use of mobile apps, a large amount of data is created by users on daily basis. Manual identification and classification of such reviews are time-consuming and laborious tasks. Hence, automating this process is essential for assisting developers in managing these reviews efficiently. Prior studies have focused on classification of these reviews into bug reports, user experience, and feature requests. Nevertheless to date, a very few research papers have extracted Non-Functional Requirements (NFRs) present in these reviews. NFRs are considered as the set of quality attributes such as reliability, performance, security and usability of the software. Previous studies have utilized machine learning techniques to classify these reviews into their respective classes. However, it was observed that existing studies treat review classification problems as single-label classification problem, and also underestimate the contextual relationship between the words of review statements. To alleviate this limitation, the proposed research work used a transfer learning model to classify multi-label app reviews into four NFRs: Dependability, Performance, Supportability, and Usability. The proposed approach evaluates the performance of the pre-trained language model for multi-label review classification. In this paper, a set of experiments are conducted to compare the performance of the proposed model against the baseline machine learning with binary relevance and keyword based approach. We evaluated our approach over a dataset of 6000 user reviews of 24 iOS apps. Experimental results show that the proposed model outperforms state-of-the-art baseline techniques with respect to precision, recall, and F1-measure.},
journal = {Neural Comput. Appl.},
month = {aug},
pages = {22487–22509},
numpages = {23},
keywords = {Requirements engineering, Non-functional requirements, Transfer learning, BERT}
}

@article{10.1145/3599975.3599981,
author = {Soldani, Jacopo},
title = {An Interview with Chunyang Chen - 2023 SIGSOFT Awardee},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3599975.3599981},
doi = {10.1145/3599975.3599981},
abstract = {Chunyang Chen received the 2023 SIGSOFT Early Career Researcher Award for outstanding contributions to the study of intelligent software development automation including automated mobile application development, software testing, migration, and accessibility. He received a Ph.D. in computer science from School of Computer Science and Engineering, Nanyang Technological University (Singapore). He is now a senior lecturer in the Faculty of Information Technology of the Monash University (Australia).},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jun},
pages = {16–17},
numpages = {2}
}

@article{10.1145/3604608,
author = {Yang, Shouguo and Xu, Zhengzi and Xiao, Yang and Lang, Zhe and Tang, Wei and Liu, Yang and Shi, Zhiqiang and Li, Hong and Sun, Limin},
title = {Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3604608},
doi = {10.1145/3604608},
abstract = {Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. To this end, we propose an approach, named Robin, to confirm recurring vulnerabilities by filtering out patched functions. Robin is powered by a lightweight symbolic execution to solve the set of function inputs that can lead to the vulnerability-related code. It then executes the target functions with the same inputs to capture the vulnerable or patched behaviors for patched function filtration. Experimental results show that Robin achieves high accuracy for patch detection across different compilers and compiler optimization levels respectively on 287 real-world vulnerabilities of 10 different software. Based on accurate patch detection, Robin significantly reduces the false-positive rate of state-of-the-art vulnerability detection tools (by 94.3% on average), making them more practical. Robin additionally detects 12 new potentially vulnerable functions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {158},
numpages = {29},
keywords = {Patch detection, vulnerability detection, under constrained symbolic execution, malicious function input}
}

@article{10.1016/j.infsof.2023.107219,
author = {Song, Zihua and Wang, Junfeng and Yang, Kaiyuan and Wang, Jigang},
title = {HGIVul: Detecting inter-procedural vulnerabilities based on hypergraph convolution},
year = {2023},
issue_date = {Aug 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {160},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107219},
doi = {10.1016/j.infsof.2023.107219},
journal = {Inf. Softw. Technol.},
month = {aug},
numpages = {12},
keywords = {Vulnerability detection, Inter-procedural vulnerability, Hypergraph neural network, Software security engineering, Static analysis}
}

@inproceedings{10.1145/3587102.3588792,
author = {Savelka, Jaromir and Agarwal, Arav and Bogart, Christopher and Song, Yifan and Sakr, Majd},
title = {Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588792},
doi = {10.1145/3587102.3588792},
abstract = {We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (&lt;70% on even entry-level modules). Yet, it is clear that a straightforward application of these easily accessible models could enable a learner to obtain a non-trivial portion of the overall available score (&gt;55%) in introductory and intermediate courses alike. While the models exhibit remarkable capabilities, including correcting solutions based on auto-grader's feedback, some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps). These findings can be leveraged by instructors wishing to adapt their assessments so that GPT becomes a valuable assistant for a learner as opposed to an end-to-end solution.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {117–123},
numpages = {7},
keywords = {AI code generation, GPT, GitHub copilot, alphacode, codex, generative pre-trained transformers, introductory and intermediate programming, programming knowledge assessment, python course},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@article{10.1016/j.jksuci.2023.101675,
author = {Sohail, Shahab Saquib and Farhat, Faiza and Himeur, Yassine and Nadeem, Mohammad and Madsen, Dag Øivind and Singh, Yashbir and Atalla, Shadi and Mansoor, Wathiq},
title = {Decoding ChatGPT: A taxonomy of existing research, current challenges, and possible future directions},
year = {2024},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {35},
number = {8},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2023.101675},
doi = {10.1016/j.jksuci.2023.101675},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = {jan},
numpages = {23},
keywords = {ChatGPT, Large language models (LLMs), Generative Pre-trained Transformer (GPT), AI Generated Content (AIGC), Systematic review, Trustworthy AI}
}

@article{10.1145/3587155,
author = {Chen, Junjie and Liang, Yihua and Shen, Qingchao and Jiang, Jiajun and Li, Shuochuan},
title = {Toward Understanding Deep Learning Framework Bugs},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3587155},
doi = {10.1145/3587155},
abstract = {DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such a wide effect demonstrates the necessity and importance of guaranteeing DL frameworks’ quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating designing effective bug detection and debugging approaches. Hence, in this work, we conduct the most large-scale study on 1,000 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with five components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques, we obtain 12 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging. Finally, based on the guidelines, we design and implement a prototype DL-framework testing tool, called TenFuzz, which is evaluated to be effective and finds three unknown bugs on the latest TensorFlow framework in a preliminary study, indicating the significance of our guidelines.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {135},
numpages = {31},
keywords = {Deep learning frameworks, bug analysis, empirical study, deep learning testing}
}

@inproceedings{10.1145/3580305.3599827,
author = {Drori, Iddo and Zhang, Sarah J. and Shuttleworth, Reece and Zhang, Sarah and Tyser, Keith and Chin, Zad and Lantigua, Pedro and Surbehera, Saisamrit and Hunter, Gregory and Austin, Derek and Tang, Leonard and Hicke, Yann and Simhon, Sage and Karnik, Sathwik and Granberry, Darnell and Udell, Madeleine},
title = {From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599827},
doi = {10.1145/3580305.3599827},
abstract = {A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3947–3955},
numpages = {9},
keywords = {few-shot learning, large language models, machine learning, program synthesis, quantitative reasoning},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@inproceedings{10.1145/3597926.3605233,
author = {Shrestha, Sohil Lal},
title = {Harnessing Large Language Models for Simulink Toolchain Testing and Developing Diverse Open-Source Corpora of Simulink Models for Metric and Evolution Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3605233},
doi = {10.1145/3597926.3605233},
abstract = {MATLAB/Simulink is a de-facto standard tool in several safety-critical industries such as automotive, aerospace, healthcare, and industrial automation for system modeling and analysis, compiling models to code, and deploying code to embedded hardware. On one hand, testing cyber-physical system (CPS) development tools such as MathWorks’ Simulink is important as a bug in the toolchain may propagate to the artifacts they produce. On the other hand, it is equally important to understand modeling practices and model evolution to support engineers and scientists as they are widely used in design, simulation, and verification of CPS models. Existing work in this area is limited by two main factors, i.e., (1) inefficiencies of state-of-the-art testing schemes in finding critical tool-chain bugs and (2) the lack of a reusable corpus of public Simulink models. In my thesis, I propose to (1) curate a large reusable corpus of Simulink models to help understand modeling practices and model evolution and (2) leverage such a corpus with deep-learning based language models to test the toolchain.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1541–1545},
numpages = {5},
keywords = {Cyber-physical system development, GPT-2, Simulink, deep learning, mining software repositories, model evolution, open-source, programming language modeling, tool chain bugs},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.5555/3618408.3619552,
author = {Pei, Kexin and Bieber, David and Shi, Kensen and Sutton, Charles and Yin, Pengcheng},
title = {Can large language models reason about program invariants?},
year = {2023},
publisher = {JMLR.org},
abstract = {Identifying invariants is an important program analysis task with applications towards program understanding, bug finding, vulnerability analysis, and formal verification. Existing tools for identifying program invariants rely on dynamic analysis, requiring traces collected from multiple executions in order to produce reliable invariants. We study the application of large language models to invariant prediction, finding that models trained on source code and fine-tuned for invariant generation can perform invariant prediction as static rather than dynamic analysis. Using a scratch-pad approach where invariants are predicted sequentially through a program gives the best performance, finding invariants statically of quality comparable to those obtained by a dynamic analysis tool with access to five program traces.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1144},
numpages = {25},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1145/3624062.3624064,
author = {Chen, Brian and Mustakin, Nafis and Hoang, Alvin and Fuad, Sakib and Wong, Daniel},
title = {VSCuda: LLM based CUDA extension for Visual Studio Code},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624064},
doi = {10.1145/3624062.3624064},
abstract = {The CUDA programming language has developed to constantly accommodate new functionalities since its introduction. For beginners in particular, it is already difficult to remember the expected function parameters of commonly used CUDA features, let alone optimize their code by exploiting newly introduced functionalities. To reduce the burden for CUDA programmers, we propose VSCuda, a Visual Studio Code extension for CUDA C/C++ that includes functionalities including but not limited to CUDA syntax highlighting, code help for all CUDA APIs, code completion for all CUDA functions, and integrated code improvement suggestions from state-of-the-art large language models.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {11–17},
numpages = {7},
keywords = {CUDA, IDE extension, Large Language Models, Visual Studio Code},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC-W '23}
}

@inproceedings{10.1145/3597926.3598079,
author = {Wang, Wenxuan and Huang, Jingyuan and Chen, Chang and Gu, Jiazhen and Zhang, Jianping and Wu, Weibin and He, Pinjia and Lyu, Michael},
title = {Validating Multimedia Content Moderation Software via Semantic Fusion},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598079},
doi = {10.1145/3597926.3598079},
abstract = {The exponential growth of social media platforms, such as Facebook, Instagram, Youtube, and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography. To this end, content moderation software has been widely deployed on these platforms to detect and blocks toxic content. However, due to the complexity of content moderation models and the difficulty of understanding information across multiple modalities, existing content moderation software can fail to detect toxic content, which often leads to extremely negative impacts (e.g., harmful effects on teen mental health).  
We introduce Semantic Fusion, a general, effective methodology for validating multimedia content moderation software. Our key idea is to fuse two or more existing single-modal inputs (e.g., a textual sentence and an image) into a new input that combines the semantics of its ancestors in a novel manner and has toxic nature by construction. This fused input is then used for validating multimedia content moderation software. We realized Semantic Fusion as DUO, a practical content moderation software testing tool. In our evaluation, we employ DUO to test five commercial content moderation software and two state-of-the-art models against three kinds of toxic contents. The results show that DUO achieves up to 100% error finding rate (EFR) when testing moderation software and it obtains up to 94.1% EFR when testing the state-of-the-art models. In addition, we leverage the test cases generated by DUO to retrain the two models we explored, which largely improves model robustness (2.5%∼5.7% EFR) while maintaining the accuracy on the original test set.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {576–588},
numpages = {13},
keywords = {Software testing, metamorphic testing, multimedia content moderation, semantic fusion},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3600006.3613157,
author = {Guo, Zhiyuan and He, Zijian and Zhang, Yiying},
title = {Mira: A Program-Behavior-Guided Far Memory System},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613157},
doi = {10.1145/3600006.3613157},
abstract = {Far memory, where memory accesses are non-local, has become more popular in recent years as a solution to expand memory size and avoid memory stranding. Prior far memory systems have taken two approaches: transparently swap memory pages between local and far memory, and utilizing new programming models to explicitly move fine-grained data between local and far memory. The former requires no program changes but comes with performance penalty. The latter has potentially better performance but requires significant program changes.We propose a new far-memory approach by automatically inferring program behavior and efficiently utilizing it to improve application performance. With this idea, we build Mira. Mira utilizes program analysis results, profiled execution information, and system environments together to guide code compilation and system configurations for far memory. Our evaluation shows that Mira outperforms prior swap-based and programming-model-based systems by up to 18 times.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {692–708},
numpages = {17},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@article{10.1145/3638243,
author = {Oakes, Bentley James and Famelis, Michalis and Sahraoui, Houari},
title = {Building Domain-Specific Machine Learning Workflows: A Conceptual Framework for the State-of-the-Practice},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638243},
doi = {10.1145/3638243},
abstract = {Domain experts are increasingly employing machine learning to solve their domain-specific problems. This article presents to software engineering researchers the six key challenges that a domain expert faces in addressing their problem with a computational workflow, and the underlying executable implementation. These challenges arise out of our conceptual framework which presents the “route” of transformations that a domain expert may choose to take while developing their solution. To ground our conceptual framework in the state-of-the-practice, this article discusses a selection of available textual and graphical workflow systems and their support for the transformations described in our framework. Example studies from the literature in various domains are also examined to highlight the tools used by the domain experts as well as a classification of the domain-specificity and machine learning usage of their problem, workflow, and implementation. The state-of-the-practice informs our discussion of the six key challenges, where we identify which challenges and transformations are not sufficiently addressed by available tools. We also suggest possible research directions for software engineering researchers to increase the automation of these tools and disseminate best-practice techniques between software engineering and various scientific domains.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {computational workflow, workflow composition, domain experts, machine learning, machine learning pipelines, software engineering framework}
}

@article{10.1145/3630008,
author = {Li, Jia and Li, Zhuo and Zhang, HuangZhao and Li, Ge and Jin, Zhi and Hu, Xing and Xia, Xin},
title = {Poison Attack and Poison Detection on Deep Source Code Processing Models},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3630008},
doi = {10.1145/3630008},
abstract = {In the software engineering (SE) community, deep learning (DL) has recently been applied to many source code processing tasks, achieving state-of-the-art results. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat to DL models, namely poison attacks. The attackers aim to inject insidious backdoors into DL models by poisoning the training data with poison samples. The backdoors mean that poisoned models work normally with clean inputs but produce targeted erroneous results with inputs embedded with specific triggers. By using triggers to activate backdoors, attackers can manipulate poisoned models in security-related scenarios (e.g., defect detection) and lead to severe consequences. To verify the vulnerability of deep source code processing models to poison attacks, we present a poison attack approach for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable and functionality-preserving poison samples and effectively attack deep source code processing models by poisoning the training data with poison samples. To defend against poison attacks, we further propose an effective poison detection approach named CodeDetector. CodeDetector can automatically identify poison samples in the training data. We apply CodePoisoner and CodeDetector to six deep source code processing models, including defect detection, clone detection, and code repair models. The results show that ① CodePoisoner
conducts successful poison attacks with a high attack success rate (avg: 98.3%, max: 100%). It validates that existing deep source code processing models have a strong vulnerability to poison attacks. ② CodeDetector effectively defends against multiple poison attack approaches by detecting (max: 100%) poison samples in the training data. We hope this work can help SE researchers and practitioners notice poison attacks and inspire the design of more advanced defense techniques.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {Poison Attack, Poison Detection, Source Code Processing, Deep Learning}
}

@article{10.1016/j.cose.2023.103476,
author = {Okey, Ogobuchi Daniel and Udo, Ekikere Umoren and Rosa, Renata Lopes and Rodríguez, Demostenes Zegarra and Kleinschmidt, João Henrique},
title = {Investigating ChatGPT and cybersecurity: A perspective on topic modeling and sentiment analysis},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {135},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103476},
doi = {10.1016/j.cose.2023.103476},
journal = {Comput. Secur.},
month = {dec},
numpages = {10},
keywords = {ChatGPT, Cybersecurity, Sentiment analysis, Generative pre-trained transformers, Artificial intelligence, Data security}
}

@article{10.1016/j.jss.2023.111734,
author = {Moradi Dakhel, Arghavan and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C. and Jiang, Zhen Ming (Jack)},
title = {GitHub Copilot AI pair programmer: Asset or Liability?},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {203},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111734},
doi = {10.1016/j.jss.2023.111734},
journal = {J. Syst. Softw.},
month = {sep},
numpages = {23},
keywords = {Code completion, Language model, GitHub copilot, Testing}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00035,
author = {Heo, Jinseok and Jeong, Hohyeon and Choi, Dongwook and Lee, Eunseok},
title = {REFERENT: Transformer-Based Feedback Generation Using Assignment Information for Programming Course},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00035},
doi = {10.1109/ICSE-SEET58685.2023.00035},
abstract = {Students require feedback on programming assignments to improve their programming skills. An Automated feedback generation (AFG) technique proposes to provide feedback-corrected submissions for incorrect student programming submissions in programming courses. However, these techniques are limited as they rely on the availability of correct submissions as a reference to generate feedback. In situations where correct submissions are not available, they resort to using mutation operators, which can lead to a search space explosion problem. In this work, we propose REFERENT, Transformer-based feedback generation using assignment information. REFERENT uses transfer learning on a pre-trained model with data from students' submission history from the past assignment. To generate assignment-related feedback, we use a title, tag, assignment description, and test case as assignment information. REFERENT can generate feedback without a reference program in limited resources. We conducted a preliminary study to confirm the effectiveness of REFERENT and the feasibility of using assignment information. REFERENT generated feedback for 32.7% of incorrect submissions without reference programs and that its performance increased up to 50.7% when reference programs were used. We also check whether the submission history, assignment information, and repair knowledge of open-source software help generate feedback.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {308–313},
numpages = {6},
keywords = {programming assignment, automated feedback generation, transformer, transfer learning, assignment information},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@inproceedings{10.1145/3593434.3593458,
author = {Valový, Marcel},
title = {Psychological Aspects of Pair Programming: A Mixed-methods Experimental Study},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593458},
doi = {10.1145/3593434.3593458},
abstract = {With the recent advent of artificially intelligent pairing partners in software engineering, it is interesting to renew the study of the psychology of pairing. Pair programming provides an attractive way of teaching software engineering to university students. Its study can also lead to a better understanding of the needs of professional software engineers in various programming roles and for the improvement of the concurrent pairing software. [Objective] This preliminary study aimed to gain quantitative and qualitative insights into pair programming, especially students’ attitudes towards its specific roles and what they require from the pairing partners. The research's goal is to use the findings to design further studies on pairing with artificial intelligence. [Method] Using a mixed-methods and experimental approach, we distinguished the effects of the pilot, navigator, and solo roles on (N = 35) students’ intrinsic motivation. Four experimental sessions produced a rich data corpus in two software engineering university classrooms. It was quantitatively investigated using the Shapiro-Wilk normality test and one-way analysis of variance (ANOVA) to confirm the relations and significance of variations in mean intrinsic motivation in different roles. Consequently, seven semi-structured interviews were conducted with the experiment's participants. The qualitative data excerpts were subjected to the thematic analysis method in an essentialist way. [Results] The systematic coding interview transcripts elucidated the research topic by producing seven themes for understanding the psychological aspects of pair programming and for its improvement in university classrooms. Statistical analysis of 612 self-reported intrinsic motivation inventories confirmed that students find programming in pilot-navigator roles more interesting and enjoyable than programming simultaneously. [Conclusion] The executed experimental settings are viable for inspecting the associations between students’ attitudes and the distributed cognition practice. The preliminary results illuminate the psychological aspects of the pilot-navigator roles and reveal many areas for improvement. The results also provide a strong basis for conducting further studies with the same design involving the big five personality and intrinsic motivation on using artificial intelligence in pairing and to allow comparison of those results with results of pairing with human partners.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {210–216},
numpages = {7},
keywords = {Thematic analysis, Software engineering, Pair programming, Intrinsic motivation, Agile development},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1007/978-3-031-34671-2_23,
author = {Nair, Madhav and Sadhukhan, Rajat and Mukhopadhyay, Debdeep},
title = {How Hardened is Your Hardware? Guiding ChatGPT to&nbsp;Generate Secure Hardware Resistant to&nbsp;CWEs},
year = {2023},
isbn = {978-3-031-34670-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-34671-2_23},
doi = {10.1007/978-3-031-34671-2_23},
abstract = {The development of Artificial Intelligence (AI) based systems to automatically generate hardware systems has gained an impulse that aims to accelerate the hardware design cycle with no human intervention. Recently, the striking AI-based system ChatGPT from OpenAI has achieved a momentous headline and has gone viral within a short span of time since its launch. This chatbot has the capability to interactively communicate with the designers through a prompt to generate software and hardware code, write logic designs, and synthesize designs for implementation on Field Programmable Gate Array (FPGA) or Application Specific Integrated Circuits (ASIC). However, an unvetted ChatGPT prompt by a designer with an aim to generate hardware code may lead to security vulnerabilities in the generated code. In this work, we systematically investigate the necessary strategies to be adopted by a designer to enable ChatGPT to recommend secure hardware code generation. To perform this analysis, we prompt ChatGPT to generate code scenarios listed in Common Vulnerability Enumerations (CWEs) under the hardware design (CWE-1194) view from MITRE. We first demonstrate how a ChatGPT generates insecure code given the diversity of prompts. Finally, we propose techniques to be adopted by a designer to generate secure hardware code. In total, we create secure hardware code for 10 noteworthy CWEs under hardware design view listed on MITRE site.},
booktitle = {Cyber Security, Cryptology, and Machine Learning: 7th International Symposium, CSCML 2023, Be'er Sheva, Israel, June 29–30, 2023, Proceedings},
pages = {320–336},
numpages = {17},
keywords = {ChatGPT, Common Vulnerability Enumeration, Hardware Design},
location = {Be'er Sheva, Israel}
}

@article{10.1007/s10462-023-10580-7,
author = {Tang, Mingjing and Zhang, Shu and Zheng, Ming and Ma, Zifei and Gao, Wei},
title = {SCL-SKG: software knowledge extraction with span-level contrastive learning},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {56},
number = {Suppl 2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-023-10580-7},
doi = {10.1007/s10462-023-10580-7},
abstract = {The text of software knowledge community contains abundant knowledge of software engineering field. The software knowledge entity and relation can be extracted automatically and efficiently to form the software knowledge graph, which is helpful for software knowledge-centric intelligent applications, such as intelligent question answering, automatic document generation and software expert recommendation. Most existing methods are confronted with problems of task dependence and entity overlap. In this paper, we propose a software knowledge extraction method based on span-level contrastive learning. From the level of sentence sequence modelling, we model the sentence sequence with span as a unit, and generate abundant positive and negative samples of entity span through the span representation layer to avoid the problem that the token-level method cannot select overlapping entities. From the level of feature learning, we propose supervised entity contrastive learning and relation contrastive learning, which obtain enhanced feature representation of entity span and entity pair through positive and negative sample enhancement and contrastive loss function construction. Experiments are conducted on the dataset which is constructed based on texts of the StackOverflow, and show that our approach achieves a better performance than baseline models.},
journal = {Artif. Intell. Rev.},
month = {aug},
pages = {2383–2406},
numpages = {24},
keywords = {Software knowledge graph, Entity extraction, Relation extraction, Contrastive learning, StackOverflow}
}

@article{10.1016/j.cose.2023.103322,
author = {Sun, Xuekai and Liu, Chunling and Dong, Weiyu and Liu, Tieming},
title = {Improvements to code2vec: Generating path vectors using RNN},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {132},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103322},
doi = {10.1016/j.cose.2023.103322},
journal = {Comput. Secur.},
month = {sep},
numpages = {10},
keywords = {Source code analysis, Machine learning, Vector representation, Code embedding, Code classification}
}

@inproceedings{10.1109/ICSE48619.2023.00211,
author = {Nong, Yu and Ou, Yuzhe and Pradel, Michael and Chen, Feng and Cai, Haipeng},
title = {VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00211},
doi = {10.1109/ICSE48619.2023.00211},
abstract = {Building new, powerful data-driven defenses against prevalent software vulnerabilities needs sizable, quality vulnerability datasets, so does large-scale benchmarking of existing defense solutions. Automatic data generation would promisingly meet the need, yet there is little work aimed to generate much-needed quality vulnerable samples. Meanwhile, existing similar and adaptable techniques suffer critical limitations for that purpose. In this paper, we present VULGEN, the first injection-based vulnerability-generation technique that is not limited to a particular class of vulnerabilities. VULGEN combines the strengths of deterministic (pattern-based) and probabilistic (deep-learning/DL-based) program transformation approaches while mutually overcoming respective weaknesses. This is achieved through close collaborations between pattern mining/application and DL-based injection localization, which separates the concerns with how and where to inject. By leveraging large, pretrained programming language modeling and only learning locations, VULGEN mitigates its own needs for quality vulnerability data (for training the localization model). Extensive evaluations show that VULGEN significantly outperforms a state-of-the-art (SOTA) pattern-based peer technique as well as both Transformer- and GNN-based approaches in terms of the percentages of generated samples that are vulnerable and those also exactly matching the ground truth (by 38.0--430.1% and 16.3--158.2%, respectively). The VULGEN-generated samples led to substantial performance improvements for two SOTA DL-based vulnerability detectors (by up to 31.8% higher in F1), close to those brought by the ground-truth real-world samples and much higher than those by the same numbers of existing synthetic samples.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2527–2539},
numpages = {13},
keywords = {vulnerability detection, deep learning, pattern mining, bug injection, data generation, software vulnerability},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3576915.3623175,
author = {He, Jingxuan and Vechev, Martin},
title = {Large Language Models for Code: Security Hardening and Adversarial Testing},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623175},
doi = {10.1145/3576915.3623175},
abstract = {Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1865–1879},
numpages = {15},
keywords = {ai safety, code generation, code security, large language models},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@article{10.1007/s10664-023-10374-z,
author = {Tomova, Mihaela and Hofmann, Martin and Hütterer, Constantin and Mäder, Patrick},
title = {Assessing the utility of text-to-SQL approaches for satisfying software developer information needs},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10374-z},
doi = {10.1007/s10664-023-10374-z},
abstract = {Software analytics integrated with complex databases can deliver project intelligence into the hands of software engineering (SE) experts for satisfying their information needs. A new and promising machine learning technique known as text-to-SQL automatically extracts information for users of complex databases without the need to fully understand the database structure nor the accompanying query language. Users pose their request as so-called natural language utterance, i.e., question. Our goal was evaluating the performance and applicability of text-to-SQL approaches on data derived from tools typically used in the workflow of software engineers for satisfying their information needs. We carefully selected and discussed five seminal as well as state-of-the-art text-to-SQL approaches and conducted a comparative assessment using the large-scale, cross-domain Spider dataset and the SE domain-specific SEOSS-Queries dataset. Furthermore, we study via a survey how SE professionals perform in satisfying their information needs and how they perceive text-to-SQL approaches. For the best performing approach, we observe a high accuracy of 94% in query prediction when training specifically on SE data. This accuracy is almost independent of the query’s complexity. At the same time, we observe that SE professionals have substantial deficits in satisfying their information needs directly via SQL queries. Furthermore, SE professionals are open for utilizing text-to-SQL approaches in their daily work, considering them less time-consuming and helpful. We conclude that state-of-the-art text-to-SQL approaches are applicable in SE practice for day-to-day information needs.},
journal = {Empirical Softw. Engg.},
month = {dec},
numpages = {48},
keywords = {Software analytics, Database querying, Natural language processing, Text-to-SQL, Machine learning, Complex queries}
}

@inproceedings{10.1109/ICSE48619.2023.00170,
author = {Yan, Yanyan and Feng, Yang and Fan, Hongcheng and Xu, Baowen},
title = {DLInfer: Deep Learning with Static Slicing for Python Type Inference},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00170},
doi = {10.1109/ICSE48619.2023.00170},
abstract = {Python programming language has gained enormous popularity in the past decades. While its flexibility significantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors.In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type information for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2009–2021},
numpages = {13},
keywords = {type inference, Python, static slicing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3611648,
author = {Buchner, Stefan and Riehle, Dirk},
title = {The Business Impact of Inner Source and How to Quantify It},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3611648},
doi = {10.1145/3611648},
abstract = {Inner source software development is the practice of using open source practices for firm-internal software development. Practitioner reports have shown that inner source can increase flexibility and reduce costs. Despite the potential benefits of inner source, there has been little research on its impact on businesses and their processes. To address this gap, we conducted a systematic literature review that identified which business processes are affected by inner source development, particularly within the accounting and management domain. Our review revealed the need for new dedicated community building processes within companies. In addition, we examined computational tools and techniques that can be used to measure inner source development. We found that existing tools and techniques are insufficiently suitable to manage inner source processes. Based on this, we propose research topics for future work on quantifying inner source.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {47},
numpages = {27},
keywords = {Inner source, open source, internal open source, software engineering, software development, business processes, cost estimation, effort estimation, cost calculation, accounting, taxation, transfer pricing}
}

@article{10.1145/3635712,
author = {Ferrari, Alessio and Huichapa, Thaide and Spoletini, Paola and Novielli, Nicole and Fucci, Davide and Girardi, Daniela},
title = {Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635712},
doi = {10.1145/3635712},
abstract = {Capturing users’ engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users’ feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this paper, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users’ engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1  (sim 70% )  in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {software engineering, requirements engineering, emotion detection, voice analysis, speech analysis, biofeedback analysis, affective requirements engineering.}
}

@inproceedings{10.1145/3587103.3594175,
author = {Oprescu, Ana-Maria and Kokken, Ingrid and Maat, Kyrian and de Geus, Florine},
title = {Introducing Green Thinking Into CS Bachelor Curriculum},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594175},
doi = {10.1145/3587103.3594175},
abstract = {By 2030 greenhouse gas emissions should be reduced by at least 55%. Despite hardware becoming more energy-efficient (Koomey's law), the ever-increasing reliance on computer technology has increased the energy usage due to ICT significantly.In software engineering (SE), there are many choices from programming languages to development patterns that influence energy efficiency. Recent research inspired our work on incorporating energy efficiency into SE education. An international group of researchers formulated initial recommendations for introducing green and sustainable software engineering [4] to students. An EU-US team formulated more refined recommendations for introducing green thinking as modules in existing courses [3]. ICT4S'22 best paper addressed HES eco-anxiety when teaching sustainability [2].In this work we report on the experience of introducing reflection on energy efficiency into the default requirements of a SE course, and how that influenced students to address energy efficiency in their projects. Out of 8 teams, 7 implemented at least a small energy-efficient component. However, as one group donated to a climate charity in bitcoins, we need more ways to raise awareness.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {667},
numpages = {1},
keywords = {software engineering, project-based education, energy efficiency awareness},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@article{10.1145/3641540,
author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Li, Li},
title = {On the Reliability and Explainability of Language Models for Program Generation},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641540},
doi = {10.1145/3641540},
abstract = {Recent studies have adopted pre-trained language models, such as CodeT5 and CodeGPT, for automated program generation tasks like code generation, repair, and translation. Numerous language model-based approaches have been proposed and evaluated on various benchmark datasets, demonstrating promising performance. However, there is still uncertainty about the reliability of these models, particularly their realistic ability to consistently transform code sequences. This raises the question: are these techniques sufficiently trustworthy for automated program generation? Consequently, Further research is needed to understand model logic and assess reliability and explainability. To bridge these research gaps, we conduct a thorough empirical study of eight popular language models on five representative datasets to determine the capabilities and limitations of automated program generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code transformation. We discover that state-of-the-art approaches suffer from inappropriate performance evaluation stemming from severe data duplication, causing over-optimistic results. Our explainability analysis reveals that, in various experimental scenarios, language models can recognize code grammar and structural information, but they exhibit limited robustness to changes in input sequences. Overall, more rigorous evaluation approaches and benchmarks are critical to enhance the reliability and explainability of automated program generation moving forward. Our findings provide important guidelines for this goal.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Automated program generation, empirical analysis, explainable AI}
}

@article{10.1007/s10515-023-00395-9,
author = {Zhang, Xin and Yu, Zhiwen and Liu, Jiaqi and Wang, Hui and Wang, Liang and Guo, Bin},
title = {HMPT: a human–machine cooperative program translation method},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00395-9},
doi = {10.1007/s10515-023-00395-9},
abstract = {Program translation aims to translate one kind of programming language to another, e.g., from Python to Java. Due to the inefficiency of translation rules construction with pure human effort (software engineer) and the low quality of machine translation results with pure machine effort, it is suggested to implement program translation in a human–machine cooperative way. However, existing human–machine program translation methods fail to utilize the human’s ability effectively, which require human to post-edit the results (i.e., statically modified directly on the model generated code). To solve this problem, we propose HMPT (Human-Machine Program Translation), a novel method that achieves program translation based on human–machine cooperation. It can (1) reduce the human effort by introducing a prefix-based interactive protocol that feeds the human’s edit into the model as the prefix and regenerates better output code, and (2) reduce the interactive response time resulted by excessive program length in the regeneration process from two aspects: avoiding duplicate prefix generation with cache attention information, as well as reducing invalid suffix generation by splicing the suffix of the results. The experiments are conducted on two real datasets. Results show compared to the baselines, our method reduces the human effort up to 73.5% at the token level and reduces the response time up to 76.1%.},
journal = {Automated Software Engg.},
month = {aug},
numpages = {38},
keywords = {Program translation, Human–machine cooperation, Neural machine translation, Transformer, Interactive machine learning}
}

@article{10.1145/3607189,
author = {Yang, Yuanhang and He, Wei and Gao, Cuiyun and Xu, Zenglin and Xia, Xin and Liu, Chuanyi},
title = {TopicAns: Topic-informed Architecture for Answer Recommendation on Technical Q&amp;A Site},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607189},
doi = {10.1145/3607189},
abstract = {Technical Q&amp;A sites, such as Stack Overflow and Ask Ubuntu, have been widely utilized by software engineers to seek support for development challenges. However, not all the raised questions get instant feedback, and the retrieved answers can vary in quality. The users can hardly avoid spending much time before solving their problems. Prior studies propose approaches to automatically recommend answers for the question posts on technical Q&amp;A sites. However, the lengthiness and the lack of background knowledge issues limit the performance of answer recommendation on these sites. The irrelevant sentences in the posts may introduce noise to the semantics learning and prevent neural models from capturing the gist of texts. The lexical gap between question and answer posts further misleads current models to make failure recommendations. From this end, we propose a novel neural network named TopicAns for answer selection on technical Q&amp;A sites. TopicAns aims at learning high-quality representations for the posts in Q&amp;A sites with a neural topic model and a pre-trained model. This involves three main steps: (1) generating topic-aware representations of Q&amp;A posts with the neural topic model, (2) incorporating the corpus-level knowledge from the neural topic model to enhance the deep representations generated by the pre-trained language model, and (3) determining the most suitable answer for a given query based on the topic-aware representation and the deep representation. Moreover, we propose a two-stage training technique to improve the stability of our model. We conduct comprehensive experiments on four benchmark datasets to verify our proposed TopicAns’s effectiveness. Experiment results suggest that TopicAns consistently outperforms state-of-the-art techniques by over 30% in terms of Precision@1.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {20},
numpages = {25},
keywords = {Stack overflow, neural networks, answer recommendation}
}

@article{10.1145/3617327,
author = {Zheng, Zhen and Pan, Zaifeng and Wang, Dalin and Zhu, Kai and Zhao, Wenyi and Guo, Tianyou and Qiu, Xiafei and Sun, Minmin and Bai, Junjie and Zhang, Feng and Du, Xiaoyong and Zhai, Jidong and Lin, Wei},
title = {BladeDISC: Optimizing Dynamic Shape Machine Learning Workloads via Compiler Approach},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3617327},
doi = {10.1145/3617327},
abstract = {Compiler optimization plays an increasingly important role to boost the performance of machine learning models for data processing and management. With increasingly complex data, the dynamic tensor shape phenomenon emerges for ML models. However, existing ML compilers either can only handle static shape models or expose a series of performance problems for both operator fusion optimization and code generation in dynamic shape scenes. This paper tackles the main challenges of dynamic shape optimization: the fusion optimization without shape value, and code generation supporting arbitrary shapes. To tackle the fundamental challenge of the absence of shape values, it systematically abstracts and excavates the shape information and designs a cross-level symbolic shape representation. With the insight that what fusion optimization relies upon is tensor shape relationships between adjacent operators rather than exact shape values, it proposes the dynamic shape fusion approach based on shape information propagation. To generate code that adapts to arbitrary shapes efficiently, it proposes a compile-time and runtime combined code generation approach. Finally, it presents a complete optimization pipeline for dynamic shape models and implements an industrial-grade ML compiler, named BladeDISC. The extensive evaluation demonstrates that BladeDISC outperforms PyTorch, TorchScript, TVM, ONNX Runtime, XLA, Torch Inductor (dynamic shape), and TensorRT by up to 6.95×, 6.25×, 4.08×, 2.04×, 2.06×, 7.92×, and 4.16× (3.54×, 3.12×, 1.95×, 1.47×, 1.24×, 2.93×, and 1.46× on average) in terms of end-to-end inference speedup on the A10 and T4 GPU, respectively. BladeDISC's source code is publicly available at https://github.com/alibaba/BladeDISC.},
journal = {Proc. ACM Manag. Data},
month = {nov},
articleno = {206},
numpages = {29},
keywords = {code generation, dynamic shape, machine learning, operator fusion, tensor compiler}
}

@inproceedings{10.1007/978-3-031-48639-5_8,
author = {Kotovich, Julia and Oriol, Manuel},
title = {Is ChatGPT 3 Safe for&nbsp;Students?},
year = {2023},
isbn = {978-3-031-48638-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48639-5_8},
doi = {10.1007/978-3-031-48639-5_8},
abstract = {ChatGPT3 is a chat engine that fulfils the promises of an AI-based chat engine: users can ask a question (prompt) and it answers in a reasonable manner. The coding-related skills of ChatGPT are especially impressive: informal testing shows that it is difficult to find simple questions that ChatGPT3 does not know how to answer properly. Some students are certainly already using it to answer programming assignments.This article studies whether it is safe for students to use ChatGPT3 to answer coding assignments (“safe” means that they will not be caught for plagiarism if they use it). The main result is that it is generally not safe for students to use ChatGPT3. We evaluated the safety of code generated with ChatGPT3, by performing a search with a Codequiry, a plagiarism detection tool, and searching plagiarized code in Google (only considering the first page of results). In 38% of the cases, Codequiry finds a piece of code that is partially copied by the answer of ChatGPT3. In 96% of the cases, the Google search finds a piece of code very similar to the generated code. Overall, it is not safe for students to use ChatGPT3 in 96% of the cases.},
booktitle = {Frontiers in Software Engineering Education: Second International Workshop, FISEE 2023, Villebrumier, France, January 23–25, 2023, Invited Papers},
pages = {100–107},
numpages = {8},
keywords = {ChatGPT, education, programming},
location = {<conf-loc content-type="InPerson">Villebrumier, France</conf-loc>}
}

@inproceedings{10.1007/978-3-031-40953-0_35,
author = {Diemert, Simon and Weber, Jens H.},
title = {Can Large Language Models Assist in Hazard Analysis?},
year = {2023},
isbn = {978-3-031-40952-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-40953-0_35},
doi = {10.1007/978-3-031-40953-0_35},
abstract = {Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable natural language processing and generation capabilities and have been applied to a variety tasks, such as source code generation. This paper explores the potential of integrating LLMs in the hazard analysis for safety-critical systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a human analyst interacts with an LLM via a context-aware chat session and uses the responses to support elicitation of possible hazard causes. In a preliminary experiment, we explore CoHA with three increasingly complex versions of a simple system, using Open AI’s ChatGPT service. The quality of ChatGPT’s responses were systematically assessed to determine the feasibility of CoHA given the current state of LLM technology. The results suggest that LLMs may be useful for supporting human analysts performing hazard analysis.},
booktitle = {Computer Safety, Reliability, and Security. SAFECOMP 2023 Workshops: ASSURE, DECSoS, SASSUR, SENSEI, SRToITS, and WAISE, Toulouse, France, September 19, 2023, Proceedings},
pages = {410–422},
numpages = {13},
keywords = {Hazard Analysis, Artificial Intelligence, Large Language Models, Co-Hazard Analysis},
location = {Toulouse, France}
}

@article{10.1007/s10664-023-10423-7,
author = {Sas, Cezar and Capiluppi, Andrea},
title = {Multi-granular software annotation using file-level weak labelling},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10423-7},
doi = {10.1007/s10664-023-10423-7},
journal = {Empirical Softw. Engg.},
month = {nov},
numpages = {34},
keywords = {File-level labelling, Weak labelling, Software classification, Program comprehension}
}

@article{10.1016/j.infsof.2023.107221,
author = {Chu, Hanting and Zhang, Pengcheng and Dong, Hai and Xiao, Yan and Ji, Shunhui and Li, Wenrui},
title = {A survey on smart contract vulnerabilities: Data sources, detection and repair},
year = {2023},
issue_date = {Jul 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {159},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107221},
doi = {10.1016/j.infsof.2023.107221},
journal = {Inf. Softw. Technol.},
month = {jul},
numpages = {17},
keywords = {Blockchains, Smart contracts, Vulnerability detection, Vulnerability repair, Information security}
}

@inproceedings{10.1007/978-3-031-52183-6_8,
author = {Li, Yi and Sun, Meng},
title = {Challenges Engaging Formal CBSE in&nbsp;Industrial Applications},
year = {2024},
isbn = {978-3-031-52182-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-52183-6_8},
doi = {10.1007/978-3-031-52183-6_8},
abstract = {Component-based software engineering (CBSE) is a widely used software development paradigm. With software systems becoming increasingly sophisticated, CBSE provides an effective approach to construct reusable, extensible, and maintainable software systems. Formal verification provides a rigorous and systematic approach to validate the correctness of software systems by mathematically proving properties or checking them exhaustively against specified requirements. Using formal verification techniques in component-based development can further enhance the correctness of the development process. However, the adoption of component-based development supported by formal methods is hardly widespread in the industry. It serves to a limited extent in domains with stringent requirements for safety and reliability. In this paper, we aim to analyze the successful application scenarios of formal methods in component-based development, identify the challenges faced during their application, and explore methods to further broaden their adoption.},
booktitle = {Formal Aspects of Component Software: 19th International Conference, FACS 2023, Virtual Event, October 19-20, 2023, Revised Selected Papers},
pages = {153–167},
numpages = {15},
keywords = {Formal Methods, Component-based Software Engineering}
}

@inproceedings{10.5555/3615924.3615942,
author = {Michelle, Alexopoulos and Kelly, Lyons and Kaushar, Mahetaji and Marcus Emmanuel, Barnes and Rogan, Gutwillinger},
title = {Gender Inference: Can ChatGPT Outperform Common Commercial Tools?},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {An increasing number of studies use gender information to un-derstand phenomena such as gender bias, inequity in access and participation, or the impact of the Covid pandemic response. Un-fortunately, most datasets do not include self-reported gender in-formation, which makes it necessary for researchers to infer gen-der from other information, such as from names or names and country information. In this paper, we compare the performance of the new generative Artificial Intelligence (AI) tool ChatGPT with three traditional commercially available list-based and ma-chine learning-based gender inference tools—Namsor, Gender-API, and genderize.io—on a unique dataset. Specifically, we use a large Olympic athlete dataset and report how variations in the input (e.g., first name and first &amp; last name, with and without country information) impact the accuracy of their predictions. We find that Namsor is the best traditional commercially available tool. However, ChatGPT performs at least as well as Namsor and often outper-forms it, especially for the female sample when country and/or last name information is available. We conclude ChatGPT may be a cost-effective tool for gender prediction.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {161–166},
numpages = {6},
keywords = {Name-Based Gender Inference, ChatGPT, Performance Evaluation, Data Science and AI},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3613372.3614197,
author = {Pinto, Gustavo and Cardoso-Pereira, Isadora and Monteiro, Danilo and Lucena, Danilo and Souza, Alberto and Gama, Kiev},
title = {Large Language Models for Education: Grading Open-Ended Questions Using ChatGPT},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3614197},
doi = {10.1145/3613372.3614197},
abstract = {As a way of addressing increasingly sophisticated problems, software professionals face the constant challenge of seeking improvement. However, for these individuals to enhance their skills, their process of studying and training must involve feedback that is both immediate and accurate. In the context of software companies, where the scale of professionals undergoing training is large, but the number of qualified professionals available for providing corrections is small, delivering effective feedback becomes even more challenging. To circumvent this challenge, this work presents an exploration of using Large Language Models (LLMs) to support the correction process of open-ended questions in technical training. In this study, we utilized ChatGPT to correct open-ended questions answered by 42 industry professionals on two topics. Evaluating the corrections and feedback provided by ChatGPT, we observed that it is capable of identifying semantic details in responses that other metrics cannot observe. Furthermore, we noticed that, in general, subject matter experts tended to agree with the corrections and feedback given by ChatGPT.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {293–302},
numpages = {10},
keywords = {Automated grading, ChatGPT, Open-ended Questions},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00092,
author = {Dhal, Chandan and Fu, Xiaoqin and Cai, Haipeng},
title = {A Control-Theoretic Approach to Auto-Tuning Dynamic Analysis for Distributed Services},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00092},
doi = {10.1109/ICSE-Companion58688.2023.00092},
abstract = {Traditional dynamic dependence analysis approaches have limited utilities for continuously running distributed systems (i.e., distributed services) because of their low cost-effectiveness. A recent technique, Seads, was developed to improve the cost-effectiveness by adjusting analysis configurations on the fly using a general Q-learning algorithm. However, Seads is unable to utilize the user budget as far as needed for pushing up precision. To overcome this problem, we propose Cadas, an adaptive dynamic dependency analysis framework for distributed services. To realize the adaptation, we are exploring a control-theoretical method which uses a feedback mechanism to predict optimal analysis configurations. Then, we evaluated Cadas against six real-world Java distributed services. We compared Cadas against Seads as the baseline and show that Cadas outperforms the baseline in both precision and budget utilization. Our results suggest a new door opening for future research on adaptive dynamic program analysis.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {330–331},
numpages = {2},
keywords = {distributed system, dynamic analysis, control theory, dependence analysis, cost-effectiveness, auto-tuning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3635439.3635442,
author = {El-Deeb, Ahmed},
title = {Behind OpenAI CEO Dismissal: An Ethical Dilemma And ANew AI Revolution},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3635439.3635442},
doi = {10.1145/3635439.3635442},
abstract = {The world of Artificial Intelligence and Silicon Valley have been rocked in Nov. 2023 by the sudden ouster of OpenAI CEO, Sam Altman, by the company's board. OpenAI is the 2015 start-up behind ChatGPT and GPT-4, which enabled generating content and media-altered by AI. While nothing much was revealed by the board, the official statement mentioned that he wasn't transparent about his communication to the board about the developments of the company, which led the board to lose trust in him and deciding to fire him. Backed up by 700 OpenAI staffers who threatened to resign in solidarity with their fired leader and a supportive offer from Microsoft CEO Satya hiring Altman to lead significant AI Research Lab in Microsoft, Altman was back again as CEO in 5 days along with a revamped new board. While this dramatic story that entertained the software industry for a week could be seen as mere news, it is indeed more than that; and the significance of this story is not Sam Altman himself as an AI business figure. The back story of this fiasco and what happened behind the scene is what interest us as a Software professionals. The purpose of this article is bring some pointers for our learning.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {11–12},
numpages = {2}
}

@inproceedings{10.1007/978-3-031-35320-8_33,
author = {Kondracki, Brendan},
title = {Decoding Strategies for&nbsp;Code Conciseness and&nbsp;Efficiency in&nbsp;Transformer-Generated Programs},
year = {2023},
isbn = {978-3-031-35319-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35320-8_33},
doi = {10.1007/978-3-031-35320-8_33},
abstract = {In recent years, tremendous strides have been made in the area of program synthesis due to the leveraging of highly parameterized transformer models. Such models have demonstrated near human levels of performance on tasks such as bug detection, computer language translation, and competitive programming. Unfortunately, little research has been done in the exploration of decoding methodologies for such models, despite the semantic and structural differences between human and programming languages. In this paper, we propose extensions to commonly used decoding strategies, which incorporate additional constraints on non-concise and inefficient program generations. Our approaches have shown comparable performance on program generation tasks while producing programs requiring fewer lines of code and a reduced number of looping operations on average compared to traditional methods of decoding.},
booktitle = {Natural Language Processing and Information Systems: 28th International Conference on Applications of Natural Language to Information Systems, NLDB 2023, Derby, UK, June 21–23, 2023, Proceedings},
pages = {456–466},
numpages = {11},
keywords = {Program Synthesis, CodeT5, Transformers, Decoding},
location = {Derby, United Kingdom}
}

@inproceedings{10.1145/3613372.3613376,
author = {De Macedo, Gretchen T. and Fontão, Awdren and Gadelha, Bruno},
title = {UIProtoCheck: A Checklist for Semantic Inspection of User Interface Prototypes},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613376},
doi = {10.1145/3613372.3613376},
abstract = {User interface prototypes are widely used in software development to facilitate customer communication and explore ideas, especially in agile development teams. In addition, they often guide subsequent stages of the development process, such as coding, testing, and training. Given their effect on the software development cycle, UI prototypes must be included in quality assurance activities such as inspections. Existing UI inspection approaches aim to detect mainly usability problems and are designed to inspect implemented software. However, development costs could be reduced by early detection of design defects if prototypes were reviewed against software requirements before implementation. For this reason, we developed UIProtoCheck, a comprehensive checklist to inspect UI prototypes semantically according to the software requirements. To evaluate it, we conducted a study where 12 participants used our checklist to inspect three UI prototypes based on a given scenario. The results showed that teams with the best results achieved 67% effectiveness in identifying semantic errors previously included in the prototypes. These promising initial results indicate that UIProtoCheck can support the semantic inspection of UI prototypes.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {485–490},
numpages = {6},
keywords = {checklist-based reading, user interface inspection, user interface prototyping},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@inproceedings{10.1145/3611643.3616332,
author = {Ye, Guixin and Hu, Tianmin and Tang, Zhanyong and Fan, Zhenye and Tan, Shin Hwei and Zhang, Bo and Qian, Wenxiang and Wang, Zheng},
title = {A Generative and Mutational Approach for Synthesizing Bug-Exposing Test Cases to Guide Compiler Fuzzing},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616332},
doi = {10.1145/3611643.3616332},
abstract = {Random test case generation, or fuzzing, is a viable means for uncovering compiler bugs. Unfortunately, compiler fuzzing can be time-consuming and inefficient with purely randomly generated test cases due to the complexity of modern compilers. We present COMFUZZ, a focused compiler fuzzing framework. COMFUZZ aims to improve compiler fuzzing efficiency by focusing on testing components and language features that are likely to trigger compiler bugs. Our key insight is human developers tend to make common and repeat errors across compiler implementations; hence, we can leverage the previously reported buggy-exposing test cases of a programming language to test a new compiler implementation. To this end, COMFUZZ employs deep learning to learn a test program generator from open-source projects hosted on GitHub. With the machine-generated test programs in place, COMFUZZ then leverages a set of carefully designed mutation rules to improve the coverage and bug-exposing capabilities of the test cases. We evaluate COMFUZZ on 11 compilers for JS and Java programming languages. Within 260 hours of automated testing runs, we discovered 33 unique bugs across nine compilers, of which 29 have been confirmed and 22, including an API documentation defect, have already been fixed by the developers. We also compared COMFUZZ to eight prior fuzzers on four evaluation metrics. In a 24-hour comparative test, COMFUZZ uncovers at least 1.5× more bugs than the state-of-the-art baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1127–1139},
numpages = {13},
keywords = {Compiler, Deep learning, Fuzzing, Guided testing, Historical bug},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00139,
author = {Koscinski, Viktoria and Hashemi, Sara and Mirakhorli, Mehdi},
title = {On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00139},
doi = {10.1109/ICSE48619.2023.00139},
abstract = {Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1609–1621},
numpages = {13},
keywords = {software security requirements, requirements engineering, generative adversarial networks},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597926.3598081,
author = {Wang, Jun and Li, Yanhui and Huang, Xiang and Chen, Lin and Zhang, Xiaofang and Zhou, Yuming},
title = {Back Deduction Based Testing for Word Sense Disambiguation Ability of Machine Translation Systems},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598081},
doi = {10.1145/3597926.3598081},
abstract = {Machine translation systems have penetrated our daily lives, providing translation services from source language to target language to millions of users online daily. Word Sense Disambiguation (WSD) is one of the essential functional requirements of machine translation systems, which aims to determine the exact sense of polysemes in the given context. Commercial machine translation systems (e.g., Google Translate) have been shown to fail in identifying the proper sense and consequently cause translation errors. However, to our knowledge, no prior studies focus on testing such WSD bugs for machine translation systems. To tackle this challenge, we propose a novel testing method Back Deduction based Testing for Word Sense Disambiguation (BDTD). Our method’s main idea is to obtain the hidden senses of source words via back deduction from the target language, i.e., employ translation words in the target language to deduce senses of original words identified in the translation procedure. To evaluate BDTD, we conduct an extensive empirical study with millions of sentences under three popular translators, including Google Translate and Bing Microsoft Translator. The experimental results indicate that BDTD can identify a considerable number of WSD bugs with high accuracy, more than 80%, under all three translators.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {601–613},
numpages = {13},
keywords = {Back Deduction, Machine Translation, Software Testing, Word Sense Disambiguation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3611643.3616254,
author = {Souza, Beatriz and Pradel, Michael},
title = {LExecutor: Learning-Guided Execution},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616254},
doi = {10.1145/3611643.3616254},
abstract = {Executing code is essential for various program analysis tasks,  
e.g., to detect bugs that manifest through exceptions or to  
obtain execution traces for further dynamic analysis. How-  
ever, executing an arbitrary piece of code is often difficult in  
practice, e.g., because of missing variable definitions, miss-  
ing user inputs, and missing third-party dependencies. This  
paper presents LExecutor, a learning-guided approach for  
executing arbitrary code snippets in an underconstrained  
way. The key idea is to let a neural model predict missing  
values that otherwise would cause the program to get stuck,  
and to inject these values into the execution. For example,  
LExecutor injects likely values for otherwise undefined vari-  
ables and likely return values of calls to otherwise missing  
functions. We evaluate the approach on Python code from  
popular open-source projects and on code snippets extracted  
from Stack Overflow. The neural model predicts realistic  
values with an accuracy between 79.5% and 98.2%, allowing  
LExecutor to closely mimic real executions. As a result, the  
approach successfully executes significantly more code than  
any available technique, such as simply executing the code  
as-is. For example, executing the open-source code snippets  
as-is covers only 4.1% of all lines, because the code crashes  
early on, whereas LExecutor achieves a coverage of 51.6%.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1522–1534},
numpages = {13},
keywords = {dynamic analysis, execution, neural models},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3597926.3598036,
author = {Shi, Ensheng and Wang, Yanlin and Zhang, Hongyu and Du, Lun and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
title = {Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598036},
doi = {10.1145/3597926.3598036},
abstract = {Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. (3) Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. The extensive experimental results on five various downstream tasks demonstrate that training parameters and the corresponding time cost are greatly reduced, while performances are similar or better.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {39–51},
numpages = {13},
keywords = {Efficient Fine-tuning, Empirical study, Pre-Trained Language Models, Probing Techniques, Representational Similarity Analysis},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1007/978-3-031-51482-1_20,
author = {Sun, Rui and Guo, Yinggang and Wang, Zicheng and Zeng, Qingkai},
title = {AttnCall: Refining Indirect Call Targets in&nbsp;Binaries with&nbsp;Attention},
year = {2024},
isbn = {978-3-031-51481-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-51482-1_20},
doi = {10.1007/978-3-031-51482-1_20},
abstract = {Accurate Control Flow Graphs are crucial for effective binary program analysis, while solving indirect function call targets is its major challenge. Existing static analysis methods heavily rely on domain-specific patterns, resulting in an abundance of false positive edges due to limited expert knowledge. Concurrently, learning-based approaches often depend on heuristic analysis during the code representation stage, which prevents the model from fully comprehending program semantics.To address these limitations, this paper presents AttnCall, a novel neural network learning framework that leverages the attention mechanism to automatically learn the matching relationship between function callsites and callees’ context semantics. AttnCall refines the identification of indirect call targets through the learned matching patterns, eliminating the drawbacks of existing techniques. Additionally, we propose an end-to-end code representation scheme that effectively embeds the semantics of callsites and callees without relying on heuristic rules.The evaluation of AttnCall focuses on the task of predicting indirect function call targets. The results demonstrate that AttnCall surpasses state-of-the-art approaches, achieving 31.4% higher precision and 5% higher recall. Moreover, AttnCall enhances model interpretability, allowing for a better understanding of the underlying analysis process.},
booktitle = {Computer Security – ESORICS 2023: 28th European Symposium on Research in Computer Security, The Hague, The Netherlands, September 25–29, 2023, Proceedings, Part IV},
pages = {391–409},
numpages = {19},
keywords = {Binary Analysis, Control Flow Graph, Indirect Call, Deep Neural Network, Attention Mechanism},
location = {<conf-loc content-type="InPerson">The Hague, The Netherlands</conf-loc>}
}

@article{10.1145/3638531,
author = {Chen, Huaming and Babar, M. Ali},
title = {Security for Machine Learning-based Software Systems: a survey of threats, practices and challenges},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3638531},
doi = {10.1145/3638531},
abstract = {The rapid development of Machine Learning (ML) has demonstrated superior performance in many areas, such as computer vision, video and speech recognition. It has now been increasingly leveraged in software systems to automate the core tasks. However, how to securely develop the machine learning-based modern software systems (MLBSS) remains a big challenge, for which the insufficient consideration will largely limit its application in safety-critical domains. One concern is that the present MLBSS development tends to be rush, and the latent vulnerabilities and privacy issues exposed to external users and attackers will be largely neglected and hard to be identified. Additionally, machine learning-based software systems exhibit different liabilities towards novel vulnerabilities at different development stages from requirement analysis to system maintenance, due to its inherent limitations from the model and data and the external adversary capabilities. The successful generation of such intelligent systems will thus solicit dedicated efforts jointly from different research areas, i.e., software engineering, system security and machine learning. Most of the recent works regarding the security issues for ML have a strong focus on the data and models, which has brought adversarial attacks into consideration. In this work, we consider that security for machine learning-based software systems may arise from inherent system defects or external adversarial attacks, and the secure development practices should be taken throughout the whole lifecycle. While machine learning has become a new threat domain for existing software engineering practices, there is no such review work covering the topic. Overall, we present a holistic review regarding the security for MLBSS, which covers a systematic understanding from a structure review of three distinct aspects in terms of security threats. Moreover, it provides a thorough state-of-the-practice for MLBSS secure development. Finally, we summarise the literature for system security assurance, and motivate the future research directions with open challenges. We anticipate this work provides sufficient discussion and novel insights to incorporate system security engineering for future exploration.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {dec},
keywords = {machine learning, system security, secure development practices, software engineering}
}

@inproceedings{10.1145/3597926.3598050,
author = {Chow, Yiu Wai and Schäfer, Max and Pradel, Michael},
title = {Beware of the Unexpected: Bimodal Taint Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598050},
doi = {10.1145/3597926.3598050},
abstract = {Static analysis is a powerful tool for detecting security vulnerabilities and other programming problems. Global taint tracking, in particular, can spot vulnerabilities arising from complicated data flow across multiple functions. However, precisely identifying which flows are problematic is challenging, and sometimes depends on factors beyond the reach of pure program analysis, such as conventions and informal knowledge. For example, learning that a parameter name of an API function locale ends up in a file path is surprising and potentially problematic. In contrast, it would be completely unsurprising to find that a parameter command passed to an API function execaCommand is eventually interpreted as part of an operating-system command. This paper presents Fluffy, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic. The key idea is to let machine learning models predict from natural language information involved in a taint flow, such as API names, whether the flow is expected or unexpected, and to inform developers only about the latter. We present a general framework and instantiate it with four learned models, which offer different trade-offs between the need to annotate training data and the accuracy of predictions. We implement Fluffy on top of the CodeQL analysis framework and apply it to 250K JavaScript projects. Evaluating on five common vulnerability types, we find that Fluffy achieves an F1 score of 0.85 or more on four of them across a variety of datasets.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {211–222},
numpages = {12},
keywords = {AI4SE, software security},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3584871.3584873,
author = {Ding, Jianwei and Chen, Zhouguo},
title = {How to Find Social Robots exactly?},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584873},
doi = {10.1145/3584871.3584873},
abstract = {With the rapid development of artificial intelligence and natural language processing, there are more and more social robots applied in the social networks such as Twitter, intended to lead public opinion or crawling private information illegally. The problem of detection social robots, which is automated social accounts governed by artificial intelligence software, pretend to be a human user. There are some technologies proposed to detect the social robots automatically applied to the real social network for verification. Hence, conventional social robot detecting technologies proposed before are applied to detect by the account's metadata or account posted tweet content respectively. With the help of pre-trained language model such as BERT, this paper propose a deep neural network model based on contextual long short-term memory (LSTM) architecture named DeepBot, which exploits tweet content and account's metadata features. The architecture of DeepBot contains three phases: (1) it uses the pretrained model such as BERT to extract the embedding vector from the tweet content of the specific account, and (2) it choose more discriminative account metadata to extract a metadata vector, and then (3) it combines the auxiliary embedding vector and metadata vector into decoder layer to train a detecting model. What's more, in this paper, we review the labelling social robots datasets proposed in public, and get a mixture datasets of labelling social datasets to verify and compare the experimental results of our proposed DeepBot and other conventional methods. We also present empirical results of DeepBot and our ongoing experimentation with it, as we have gained experience applying it to the mixture labeling social robot dataset, including over 10000 accounts. The experimental results show that DeepBot outperforms previous state-of-the-art methods, with leveraging a small and interpretable set of features.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {12–18},
numpages = {7},
keywords = {BERT, deep neural network, social robot},
location = {<conf-loc>, <city>Palmerston North</city>, <country>New Zealand</country>, </conf-loc>},
series = {ICSIM '23}
}

@inproceedings{10.1145/3597926.3598126,
author = {Lau, Julia Kaiwen and Kong, Kelvin Kai Wen and Yong, Julian Hao and Tan, Per Hoong and Yang, Zhou and Yong, Zi Qian and Low, Joshua Chern Wey and Chong, Chun Yong and Lim, Mei Kuan and Lo, David},
title = {Synthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598126},
doi = {10.1145/3597926.3598126},
abstract = {Recent studies have proposed the use of Text-To-Speech (TTS) systems to automatically synthesise speech test cases on a scale and uncover a large number of failures in ASR systems. However, the failures uncovered by synthetic test cases may not reflect the actual performance of an ASR system when it transcribes human audio, which we refer to as false alarms. Given a failed test case synthesised from TTS systems, which consists of TTS-generated audio and the corresponding ground truth text, we feed the human audio stating the same text to an ASR system. If human audio can be correctly transcribed, an instance of a false alarm is detected. In this study, we investigate false alarm occurrences in five popular ASR systems using synthetic audio generated from four TTS systems and human audio obtained from two commonly used datasets. Our results show that the least number of false alarms is identified when testing Deepspeech, and the number of false alarms is the highest when testing Wav2vec2. On average, false alarm rates range from 21% to 34% in all five ASR systems. Among the TTS systems used, Google TTS produces the least number of false alarms (17%), and Espeak TTS produces the highest number of false alarms (32%) among the four TTS systems. Additionally, we build a false alarm estimator that flags potential false alarms, which achieves promising results: a precision of 98.3%, a recall of 96.4%, an accuracy of 98.5%, and an F1 score of 97.3%. Our study provides insight into the appropriate selection of TTS systems to generate high-quality speech to test ASR systems. Additionally, a false alarm estimator can be a way to minimise the impact of false alarms and help developers choose suitable test inputs when evaluating ASR systems. The source code used in this paper is publicly available on GitHub at https://github.com/julianyonghao/FAinASRtest.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1169–1181},
numpages = {13},
keywords = {Automated Speech Recognition, False Alarms, Software Testing},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3611643.3616288,
author = {Fronchetti, Felipe and Shepherd, David C. and Wiese, Igor and Treude, Christoph and Gerosa, Marco Aurélio and Steinmacher, Igor},
title = {Do CONTRIBUTING Files Provide Information about OSS Newcomers’ Onboarding Barriers?},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616288},
doi = {10.1145/3611643.3616288},
abstract = {Effectively onboarding newcomers is essential for the success of open source projects. These projects often provide onboarding guidelines in their ’CONTRIBUTING’ files (e.g., CONTRIBUTING.md on GitHub). These files explain, for example, how to find open tasks, implement solutions, and submit code for review. However, these files often do not follow a standard structure, can be too large, and miss barriers commonly found by newcomers. In this paper, we propose an automated approach to parse these CONTRIBUTING files and assess how they address onboarding barriers. We manually classified a sample of files according to a model of onboarding barriers from the literature, trained a machine learning classifier that automatically predicts the categories of each paragraph (precision: 0.655, recall: 0.662), and surveyed developers to investigate their perspective of the predictions’ adequacy (75% of the predictions were considered adequate). We found that CONTRIBUTING files typically do not cover the barriers newcomers face (52% of the analyzed projects missed at least 3 out of the 6 barriers faced by newcomers; 84% missed at least 2). Our analysis also revealed that information about choosing a task and talking with the community, two of the most recurrent barriers newcomers face, are neglected in more than 75% of the projects. We made available our classifier as an online service that analyzes the content of a given CONTRIBUTING file. Our approach may help community builders identify missing information in the project ecosystem they maintain and newcomers can understand what to expect in CONTRIBUTING files.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {16–28},
numpages = {13},
keywords = {FLOSS, novices, onboarding, open source, software engineering},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3568813.3600139,
author = {Hellas, Arto and Leinonen, Juho and Sarsa, Sami and Koutcheme, Charles and Kujanpää, Lilja and Sorva, Juha},
title = {Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600139},
doi = {10.1145/3568813.3600139},
abstract = {Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers’ help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students’ code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {93–105},
numpages = {13},
keywords = {CS1, GPT, OpenAI Codex, automatic feedback, help seeking, introductory programming education, large language models, student questions},
location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
series = {ICER '23}
}

@article{10.1016/j.jksuci.2023.101668,
author = {Cao, Heling and Wang, Fei and Deng, Miaolei and Wang, Xianyong and Liu, Guangen and Wang, Panpan},
title = {Multiple fault localization based on ant colony algorithm via genetic operation},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {35},
number = {8},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2023.101668},
doi = {10.1016/j.jksuci.2023.101668},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = {sep},
numpages = {15},
keywords = {Multiple fault localization, Search-based software engineering, Ant colony algorithm, Genetic operation, Software debugging}
}

@article{10.1145/3640329,
author = {Zhang, Quanjun and Zhai, Juan and Fang, Chunrong and Liu, Jiawei and Sun, Weisong and Hu, Haichuan and Wang, Qingyu},
title = {Machine Translation Testing via Syntactic Tree Pruning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640329},
doi = {10.1145/3640329},
abstract = {Machine translation systems have been widely adopted in our daily life, making life easier and more convenient. Unfortunately, erroneous translations may result in severe consequences, such as financial losses. This requires to improve the accuracy and the reliability of machine translation systems. However, it is challenging to test machine translation systems because of the complexity and intractability of the underlying neural models. To tackle these challenges, we propose a novel metamorphic testing approach by syntactic tree pruning (STP) to validate machine translation systems. Our key insight is that a pruned sentence should have similar crucial semantics compared with the original sentence. Specifically, STP (1) proposes a core semantics-preserving pruning strategy by basic sentence structures and dependency relations on the level of syntactic tree representation; (2) generates source sentence pairs based on the metamorphic relation; (3) reports suspicious issues whose translations break the consistency property by a bag-of-words model. We further evaluate STP on two state-of-the-art machine translation systems (i.e., Google Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs. The results show that STP accurately finds 5,073 unique erroneous translations in Google Translate and 5,100 unique erroneous translations in Bing Microsoft Translator (400% more than state-of-the-art techniques), with 64.5% and 65.4% precision, respectively. The reported erroneous translations vary in types and more than 90% of them found by state-of-the-art techniques. There are 9,393 erroneous translations unique to STP, which is 711.9% more than state-of-the-art techniques. Moreover, STP is quite effective in detecting translation errors for the original sentences with a recall reaching 74.0%, improving state-of-the-art techniques by 55.1% on average.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Software testing, Machine translation, Metamorphic testing}
}

@article{10.1145/3599975.3599982,
author = {Soldani, Jacopo},
title = {An Interview with John Grundy - 2023 SIGSOFT Awardee},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3599975.3599982},
doi = {10.1145/3599975.3599982},
abstract = {John Grundy received the 2023 SIGSOFT Distinguished Service Award for continued outstanding service to the software engineering community, including the repeated successful organization of flagship conferences, and leadership in the Australasian research and education community. He received a Ph.D. in computer science from the University of Auckland (New Zealand), and he is currently Australian Laureate Fellow and Professor of Software Engineering at Monash University. Beyond this, John Grundy is also Fellow of Automated Software Engineering, Fellow of Engineers Australia, Certified Professional Engineer, Engineering Executive, Senior Member of the ACM and Senior Member of the IEEE.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jun},
pages = {18–19},
numpages = {2}
}

@inproceedings{10.1145/3597926.3598066,
author = {Zhang, Zhaoxu and Winn, Robert and Zhao, Yu and Yu, Tingting and Halfond, William G.J.},
title = {Automatically Reproducing Android Bug Reports using Natural Language Processing and Reinforcement Learning},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598066},
doi = {10.1145/3597926.3598066},
abstract = {As part of the process of resolving issues submitted by users via bug reports, Android developers attempt to reproduce and observe the crashes described by the bug reports. Due to the low-quality of bug reports and the complexity of modern apps, the reproduction process is non-trivial and time-consuming. Therefore, automatic approaches that can help reproduce Android bug reports are in great need. However, current approaches to help developers automatically reproduce bug reports are only able to handle limited forms of natural language text and struggle to successfully reproduce crashes for which the initial bug report had missing or imprecise steps. In this paper, we introduce a new fully automated approach to reproduce crashes from Android bug reports that addresses these limitations. Our approach accomplishes this by leveraging natural language processing techniques to more holistically and accurately analyze the natural language in Android bug reports and designing new techniques, based on reinforcement learning, to guide the search for successful reproducing steps. We conducted an empirical evaluation of our approach on 77 real world bug reports. Our approach achieved 67% precision and 77% recall in accurately extracting reproduction steps from bug reports, reproduced 74% of the total bug reports, and reproduced 64% of the bug reports that contained missing steps, significantly outperforming state of the art techniques.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {411–422},
numpages = {12},
keywords = {Android, Bug Reproduction, Natural Language Processing, Reinforcement Learning},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1007/s10515-023-00387-9,
author = {Sangaroonsilp, Pattaraporn and Choetkiertikul, Morakot and Dam, Hoa Khanh and Ghose, Aditya},
title = {An empirical study of automated privacy requirements classification in issue reports},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00387-9},
doi = {10.1007/s10515-023-00387-9},
abstract = {The recent advent of data protection laws and regulations has emerged to protect privacy and personal information of individuals. As the cases of privacy breaches and vulnerabilities are rapidly increasing, people are aware and more concerned about their privacy. These bring a significant attention to software development teams to address privacy concerns in developing software applications. As today’s software development adopts an agile, issue-driven approach, issues in an issue tracking system become a centralised pool that gathers new requirements, requests for modification and all the tasks of the software project. Hence, establishing an alignment between those issues and privacy requirements is an important step in developing privacy-aware software systems. This alignment also facilitates privacy compliance checking which may be required as an underlying part of regulations for organisations. However, manually establishing those alignments is labour intensive and time consuming. In this paper, we explore a wide range of machine learning and natural language processing techniques which can automatically classify privacy requirements in issue reports. We employ six popular techniques namely Bag-of-Words (BoW), N-gram Inverse Document Frequency (N-gram IDF), Term Frequency-Inverse Document Frequency (TF-IDF), Word2Vec, Convolutional Neural Network (CNN) and Bidirectional Encoder Representations from Transformers (BERT) to perform the classification on privacy-related issue reports in Google Chrome and Moodle projects. The evaluation showed that BoW, N-gram IDF, TF-IDF and Word2Vec techniques are suitable for classifying privacy requirements in those issue reports. In addition, N-gram IDF is the best performer in both projects.},
journal = {Automated Software Engg.},
month = {jun},
numpages = {32},
keywords = {Privacy, Issue reports, Issues classification, Machine learning, Natural language processing, Deep learning models, Privacy issues classification}
}

@inproceedings{10.1145/3611643.3617852,
author = {Spiess, Claudio},
title = {STraceBERT: Source Code Retrieval using Semantic Application Traces},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3617852},
doi = {10.1145/3611643.3617852},
abstract = {Software reverse engineering is an essential task in software engineering and security, but it can be a challenging process, especially for adversarial artifacts. To address this challenge, we present STraceBERT, a novel approach that utilizes a Java dynamic analysis tool to record calls to core Java libraries, and pretrain a BERT-style model on the recorded application traces for effective method source code retrieval from a candidate set. Our experiments demonstrate the effectiveness of STraceBERT in retrieving the source code compared to existing approaches. Our proposed approach offers a promising solution to the problem of code retrieval in software reverse engineering and opens up new avenues for further research in this area.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2207–2209},
numpages = {3},
keywords = {neural information retrieval, reverse engineering, tracing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3587102.3588822,
author = {Messer, Marcus and Brown, Neil C. C. and Kölling, Michael and Shi, Miaojing},
title = {Machine Learning-Based Automated Grading and Feedback Tools for Programming: A Meta-Analysis},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588822},
doi = {10.1145/3587102.3588822},
abstract = {Research into automated grading has increased as Computer Science courses grow. Dynamic and static approaches are typically used to implement these graders, the most common implementation being unit testing to grade correctness. This paper expands upon an ongoing systematic literature review to provide an in-depth analysis of how machine learning (ML) has been used to grade and give feedback on programming assignments. We conducted a backward snowball search using the ML papers from an ongoing systematic review and selected 27 papers that met our inclusion criteria. After selecting our papers, we analysed the skills graded, the preprocessing steps, the ML implementation, and the models' evaluations.We find that most the models are implemented using neural network-based approaches, with most implementing some form of recurrent neural network (RNN), including Long Short-Term Memory, and encoder/decoder with attention mechanisms. Some graders implement traditional ML approaches, typically focused on clustering. Most ML-based automated grading, not many use ML to evaluate maintainability, readability, and documentation, but focus on grading correctness, a problem that dynamic and static analysis techniques, such as unit testing, rule-based program repair, and comparison to models or approved solutions, have mostly resolved. However, some ML-based tools, including those for assessing graphical output, have evaluated the correctness of assignments that conventional implementations cannot.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {491–497},
numpages = {7},
keywords = {automated grading, computer science education, machine learning, meta-analysis},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00046,
author = {Liu, Yu and Thurston, Zachary and Han, Alan and Nie, Pengyu and Gligoric, Milos and Legunsen, Owolabi},
title = {pytest-Inline: An Inline Testing Tool for Python},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00046},
doi = {10.1109/ICSE-Companion58688.2023.00046},
abstract = {We present pytest-inline, the first inline testing framework for Python. We recently proposed inline tests to make it easier to test individual program statements. But, there is no framework-level support for developers to write inline tests in Python. To fill this gap, we design and implement pytest-inline as a plugin for pytest, the most popular Python testing framework. Using pytest-inline, a developer can write an inline test by assigning test inputs to variables in a target statement and specifying the expected test output. Then, pytest-inline runs each inline test and fails if the target statement's output does not match the expected output. In this paper, we describe our design of pytest-inline, the testing features that it provides, and the intended use cases. Our evaluation on inline tests that we wrote for 80 target statements from 31 open-source Python projects shows that using pytest-inline incurs negligible overhead, at 0.012x. pytest-inline is integrated into the pytest-dev organization, and a video demo is at https://www.youtube.com/watch?v=pZgiAxR_uJg.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {161–164},
numpages = {4},
keywords = {inline tests, software testing, Python, pytest},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.jss.2023.111859,
author = {Qu, Yubin and Huang, Song and Chen, Xiang and Wang, Xingya and Yao, Yongming},
title = {Detection of backdoor attacks using targeted universal adversarial perturbations for deep neural networks},
year = {2024},
issue_date = {Jan 2024},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {207},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111859},
doi = {10.1016/j.jss.2023.111859},
journal = {J. Syst. Softw.},
month = {jan},
numpages = {11},
keywords = {Software engineering, Deep neural networks, Backdoor attack, Targeted universal adversarial perturbations}
}

@article{10.1145/3643675,
author = {Tao, Wei and Zhou, Yucheng and Wang, Yanlin and Zhang, Hongyu and Wang, Haofen and Zhang, Wenqiang},
title = {KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643675},
doi = {10.1145/3643675},
abstract = {Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {commit message generation, knowledge introducing, denoising training}
}

@inproceedings{10.5555/3620237.3620317,
author = {Hu, Peiwei and Liang, Ruigang and Cao, Ying and Chen, Kai and Zhang, Runze},
title = {AURC: detecting errors in program code and documentation},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Error detection in program code and documentation is a critical problem in computer security. Previous studies have shown promising vulnerability discovery performance by extensive code or document-guided analysis. However, the state-of-the-arts have the following significant limitations: (i) They assume the documents are correct and treat the code that violates documents as bugs, thus cannot find documents' defects and code's bugs if APIs have defective documents or no documents. (ii) They utilize majority voting to judge the inconsistent code snippets and treat the deviants as bugs, thus cannot cope with situations where correct usage is minor or all use cases are wrong.In this paper, we present AURC, a static framework for detecting code bugs of incorrect return checks and document defects. We observe that three objects participate in the API invocation, the document, the caller (code that invokes API), and the callee (the source code of API). Mutual corroboration of these three objects eliminates the reliance on the above assumptions. AURC contains a context-sensitive backward analysis to process callees, a pre-trained model-based document classifier, and a container that collects conditions of if statements from callers. After cross-checking the results from callees, callers, and documents, AURC delivers them to the correctness inference module to infer the defective one. We evaluated AURC on ten popular codebases. AURC discovered 529 new bugs that can lead to security issues like heap buffer overflow and sensitive information leakage, and 224 new document defects. Maintainers acknowledge our findings and have accepted 222 code patches and 76 document patches.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {80},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{10.1145/3635439.3635445,
author = {Gervasi, Vincenzo and Marchetto, Alessandro and Daneva, Maya},
title = {Report of the 8th Workshop on Empirical RequirementsEngineering (EmpiRE 2023)},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3635439.3635445},
doi = {10.1145/3635439.3635445},
abstract = {The Eighth International Workshop on Empirical Requirements Engineering (EmpiRE 2023), co-located with the 31st IEEE International Requirements Engineering conference (RE 2023), was held on September 5, 2023 in Hannover, Germany. This report presents the workshop structure, the keynote speech, the themes of the presented papers, and the panel discussion.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {27–29},
numpages = {3},
keywords = {artificial intelligence, empirical research methods, evidence-based software engineering, requirements engineering}
}

@article{10.1145/3643678,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Ma, Lei and Papadakis, Mike and Le Traon, Yves},
title = {Test Optimization in DNN Testing: A Survey},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643678},
doi = {10.1145/3643678},
abstract = {This paper presents a comprehensive survey on test optimization in deep neural network&nbsp;(DNN) testing. Here, test optimization refers to testing with low data labeling effort. We analyzed 90 papers, including 43 from the software engineering (SE) community, 32 from the machine learning (ML) community, and 15 from other communities. Our study: (i) unifies the problems as well as terminologies associated with low-labeling cost testing, (ii) compares the distinct focal points of SE and ML communities, and (iii) reveals the pitfalls in existing literature. Furthermore, we highlight the research opportunities in this domain.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {test optimization, DNN testing, low-labeling cost}
}

@inproceedings{10.1007/978-3-031-40744-4_1,
author = {Kadosh, Tal and Schneider, Nadav and Hasabnis, Niranjan and Mattson, Timothy and Pinter, Yuval and Oren, Gal},
title = {Advising OpenMP Parallelization via A Graph-Based Approach with Transformers},
year = {2023},
isbn = {978-3-031-40743-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-40744-4_1},
doi = {10.1007/978-3-031-40744-4_1},
abstract = {There is an ever-present need for shared memory parallelization schemes to exploit the full potential of multi-core architectures. The most common parallelization API addressing this need today is OpenMP. Nevertheless, writing parallel code manually is complex and effort-intensive. Thus, many deterministic source-to-source (S2S) compilers have emerged, intending to automate the process of translating serial to parallel code. However, recent studies have shown that these compilers are impractical in many scenarios. In this work, we combine the latest advancements in the field of AI and natural language processing (NLP) with the vast amount of open-source code to address the problem of automatic parallelization. Specifically, we propose a novel approach, called OMPify, to detect and predict the OpenMP pragmas and shared-memory attributes in parallel code, given its serial version. OMPify is based on a Transformer-based model that leverages a graph-based representation of source code that exploits the inherent structure of code. We evaluated our tool by predicting the parallelization pragmas and attributes of a large corpus of (over 54,000) snippets of serial code written in C and C++ languages (Open-OMP-Plus). Our results demonstrate that OMPify outperforms existing approaches — the general-purposed and popular ChatGPT and targeted PragFormer models — in terms of F1 score and accuracy. Specifically, OMPify achieves up to 90% accuracy on commonly-used OpenMP benchmark tests such as NAS, SPEC, and PolyBench. Additionally, we performed an ablation study to assess the impact of different model components and present interesting insights derived from the study. Lastly, we also explored the potential of using data augmentation and curriculum learning techniques to improve the model’s robustness and generalization capabilities. The dataset and source code necessary for reproducing our results are available at .},
booktitle = {OpenMP: Advanced Task-Based, Device and Compiler Programming: 19th International Workshop on OpenMP, IWOMP 2023, Bristol, UK, September 13–15, 2023, Proceedings},
pages = {3–17},
numpages = {15},
keywords = {NLP, Code Completion, OpenMP, Shared Memory Parallelism, Transformers, S2S Compilers, Code Representations},
location = {Bristol, United Kingdom}
}

@article{10.1145/3622856,
author = {Renda, Alex and Ding, Yi and Carbin, Michael},
title = {Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622856},
doi = {10.1145/3622856},
abstract = {Programmers and researchers are increasingly developing surrogates of programs, models of a subset of the observable behavior of a given program, to solve a variety of software development challenges. Programmers train surrogates from measurements of the behavior of a program on a dataset of input examples. A key challenge of surrogate construction is determining what training data to use to train a surrogate of a given program.  

We present a methodology for sampling datasets to train neural-network-based surrogates of programs. We first characterize the proportion of data to sample from each region of a program's input space (corresponding to different execution paths of the program) based on the complexity of learning a surrogate of the corresponding execution path. We next provide a program analysis to determine the complexity of different paths in a program. We evaluate these results on a range of real-world programs, demonstrating that complexity-guided sampling results in empirical improvements in accuracy.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {280},
numpages = {29},
keywords = {neural networks, programming languages, surrogate models}
}

@inproceedings{10.1109/ICSE48619.2023.00217,
author = {Christian, Garrett and Woodlief, Trey and Elbaum, Sebastian},
title = {Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00217},
doi = {10.1109/ICSE48619.2023.00217},
abstract = {Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2604–2616},
numpages = {13},
keywords = {software testing and validation, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.4018/IJHCITP.333857,
author = {Colomo-Palacios, Ricardo and Mikkelsplass, Stine Aurora and Simensen, John Eidar},
title = {Software and Systems Engineers in ICS Security: Graduate-Level Curricula and Industry Needs},
year = {2023},
issue_date = {Nov 2023},
publisher = {IGI Global},
address = {USA},
volume = {14},
number = {1},
issn = {1947-3478},
url = {https://doi.org/10.4018/IJHCITP.333857},
doi = {10.4018/IJHCITP.333857},
abstract = {The introduction of Industry 4.0 and IIoT has enabled the interconnection of information technology (IT) and operational technology (OT) and exposed industrial control systems to cyber threats. Industrial cybersecurity requires knowledge, skill, and collaboration between IT and OT. A comparison of graduate curricula of software engineering and systems engineering identifies competencies related to industrial control systems cybersecurity. Industry experts are interviewed to identify needs for cybersecurity skills and competencies. Results from the mapping are discussed in the context of software and systems engineering challenges in ICS cybersecurity and leveraged against industry experiences and needs expressed through interviews with three OT and IT industry professionals. The curricula mapping reveals variations in both how they are organised and expressed to the extent that subjective interpretation is required for evaluation and comparison. The interviews with the industry experts indicate a gap between graduate competence from the curricula and industry needs.},
journal = {Int. J. Hum. Cap. Inf. Technol. Prof.},
month = {nov},
pages = {1–17},
numpages = {17},
keywords = {Cybersecurity, Industry Needs, Information Technology, Operation Technology, Skills Gap, Software Engineering, Software Engineering Curriculum, Systems Engineering, Systems Engineering Curriculum}
}

@inproceedings{10.1109/ICSE48619.2023.00200,
author = {Wang, Wenxuan and Huang, Jen-tse and Wu, Weibin and Zhang, Jianping and Huang, Yizhan and Li, Shuqing and He, Pinjia and Lyu, Michael R.},
title = {MTTM: Metamorphic Testing for Textual Content Moderation Software},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00200},
doi = {10.1109/ICSE48619.2023.00200},
abstract = {The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness (0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2387–2399},
numpages = {13},
keywords = {software testing, metamorphic relations, NLP software, textual content moderation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-44693-1_46,
author = {Zhang, Tengxun and Xu, Hongfei and van Genabith, Josef and Xiong, Deyi and Zan, Hongying},
title = {NAPG: Non-Autoregressive Program Generation for&nbsp;Hybrid Tabular-Textual Question Answering},
year = {2023},
isbn = {978-3-031-44692-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44693-1_46},
doi = {10.1007/978-3-031-44693-1_46},
abstract = {Hybrid tabular-textual question answering (QA) requires reasoning from heterogeneous information, and the types of reasoning are mainly divided into numerical reasoning and span extraction. Current numerical reasoning methods simply use LSTMs to autoregressively decode program sequences, and each decoding step produces either an operator or an operand. However, step-by-step decoding suffers from exposure bias, and the accuracy of program generation drops sharply as the decoding steps unfold due to error propagation. In this paper, we propose a non-autoregressive program generation framework, which facilitates program generation in parallel. Our framework, which independently generates complete program tuples containing both operators and operands, can address the error propagation issue while significantly boosting the speed of program generation. Experiments on the ConvFinQA and MultiHiertt datasets show that our non-autoregressive program generation method can bring about substantial improvements over the strong FinQANet (+5.06 Exe Acc and +4.80 Prog Acc points) and MT2Net (+7.97 EM and +6.38 F1 points) baselines, establishing the new state-of-the-art performance, while being much faster (∼21x) in program generation. Finally, with increasing numbers of numerical reasoning steps the performance drop of our method is significantly smaller than that of the baselines.},
booktitle = {Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part I},
pages = {591–603},
numpages = {13},
keywords = {Tabular-Textual Question Answering, Numerical Reasoning, Program Generation},
location = {Foshan, China}
}

@inproceedings{10.1145/3597926.3598092,
author = {Liu, Hao and Wang, Yanlin and Wei, Zhao and Xu, Yong and Wang, Juhong and Li, Hui and Ji, Rongrong},
title = {RefBERT: A Two-Stage Pre-trained Framework for Automatic Rename Refactoring},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598092},
doi = {10.1145/3597926.3598092},
abstract = {Refactoring is an indispensable practice of improving the quality and maintainability of source code in software evolution. Rename refactoring is the most frequently performed refactoring that suggests a new name for an identifier to enhance readability when the identifier is poorly named. However, most existing works only identify renaming activities between two versions of source code, while few works express concern about how to suggest a new name. In this paper, we study automatic rename refactoring on variable names, which is considered more challenging than other rename refactoring activities. We first point out the connections between rename refactoring and various prevalent learning paradigms and the difference between rename refactoring and general text generation in natural language processing. Based on our observations, we propose RefBERT, a two-stage pre-trained framework for rename refactoring on variable names. RefBERT first predicts the number of sub-tokens in the new name and then generates sub-tokens accordingly. Several techniques, including constrained masked language modeling, contrastive learning, and the bag-of-tokens loss, are incorporated into RefBERT to tailor it for automatic rename refactoring on variable names. Through extensive experiments on our constructed refactoring datasets, we show that the generated variable names of RefBERT are more accurate and meaningful than those produced by the existing method. Our implementation and data are available at https://github.com/KDEGroup/RefBERT.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {740–752},
numpages = {13},
keywords = {bag-of-tokens loss, contrastive learning, language modeling, rename refactoring},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00051,
author = {Krasniqi, Rrezarta},
title = {Detecting Scattered and Tangled Quality Concerns in Source Code to Aid Maintenance and Evolution Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00051},
doi = {10.1109/ICSE-Companion58688.2023.00051},
abstract = {Quality concerns, such as reliability, security, usability concerns, among others, are typically well-defined and prioritized at the requirement level with the set goal of achieving high quality, robust, user-friendly, and trustworthy systems. However, quality concerns are challenging to address at the implementation level. Often they are scattered across multiple modules in the codebase. In other instances, they are tangled with functional ones within a single module. Reasoning about quality concerns and their interactions with functional ones while being hindered by the effects of scattered and tangled code can only yield to more unseen problems. For example, developers can inadvertently retrofit new bugs or wrongly implement new features that deviate from original system requirement specifications. The goal of this thesis is twofold. First, we aim to detect quality concerns implemented at code level to differentiate them from functional ones when they are scattered across the codebase. Second, we aim to untangle quality concerns from unrelated changes to gain a detailed knowledge about the history of specific quality changes. This knowledge is crucial to support consistency between the requirements-and-design and to verify architecture conformance. From the practical stance, developers could gain a breadth of understanding about quality concerns and their relations with other artifacts. Thus, with more confidence, they could perform code modifications, improve module traceability, and provide a better holistic assessment of change impact analysis.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {184–188},
numpages = {5},
keywords = {quality concerns, quality bugs, tangled quality concerns, scattered quality concerns, software maintenance},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.infsof.2023.107232,
author = {Galster, Matthias and Mitrovic, Antonija and Malinen, Sanna and Holland, Jay and Peiris, Pasan},
title = {Soft skills required from software professionals in New Zealand},
year = {2023},
issue_date = {Aug 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {160},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107232},
doi = {10.1016/j.infsof.2023.107232},
journal = {Inf. Softw. Technol.},
month = {aug},
numpages = {14},
keywords = {Soft skills, Software engineering practice, Job adverts, Exploratory study, Flexiterm}
}

@inproceedings{10.1145/3605764.3623915,
author = {Imgrund, Erik and Ganz, Tom and Härterich, Martin and Pirch, Lukas and Risse, Niklas and Rieck, Konrad},
title = {Broken Promises: Measuring Confounding Effects in Learning-based Vulnerability Discovery},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623915},
doi = {10.1145/3605764.3623915},
abstract = {Several learning-based vulnerability detection methods have been proposed to assist developers during the secure software development life-cycle. In particular, recent learning-based large transformer networks have shown remarkably high performance in various vulnerability detection and localization benchmarks. However, these models have also been shown to have difficulties accurately locating the root cause of flaws and generalizing to out-of-distribution samples. In this work, we investigate this problem and identify spurious correlations as the main obstacle to transferability and generalization, resulting in performance losses of up to 30% for current models. We propose a method to measure the impact of these spurious correlations on learning models and estimate their true, unbiased performance. We present several strategies to counteract the underlying confounding bias, but ultimately our work highlights the limitations of evaluations in the laboratory for complex learning tasks such as vulnerability discovery.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {149–160},
numpages = {12},
keywords = {causal learning, confounding effect, large language models, overfitting, vulnerability discovery},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {AISec '23}
}

@inproceedings{10.1109/ICSE-SEIS58686.2023.00019,
author = {Tizpaz-Niari, Saeid and Monjezi, Verya and Wagner, Morgan and Darian, Shiva and Reed, Krystia and Trivedi, Ashutosh},
title = {Metamorphic Testing and Debugging of Tax Preparation Software},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS58686.2023.00019},
doi = {10.1109/ICSE-SEIS58686.2023.00019},
abstract = {This paper presents a data-driven debugging framework to improve the trustworthiness of US tax preparation software systems. Given the legal implications of bugs in such software on its users, ensuring compliance and trustworthiness of tax preparation software is of paramount importance. The key barriers in developing debugging aids for tax preparation systems are the unavailability of explicit specifications and the difficulty of obtaining oracles. We posit that, since the US tax law adheres to the legal doctrine of precedent, the specifications about the outcome of tax preparation software for an individual taxpayer must be viewed in comparison with individuals that are deemed similar. Consequently, these specifications are naturally available as properties on the software requiring similar inputs provide similar outputs. Inspired by the metamorphic testing paradigm, we dub these relations metamorphic relations as they relate to structurally modified inputs.In collaboration with legal and tax experts, we explicated metamorphic relations for a set of challenging properties from various US Internal Revenue Services (IRS) publications including Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other Dependents), and Form 8863 (Education Credits). While we focus on an open-source tax preparation software for our case study, the proposed framework can be readily extended to other commercial software. We develop a randomized test-case generation strategy to systematically validate the correctness of tax preparation software guided by metamorphic relations. We further aid this test-case generation by visually explaining the behavior of software on suspicious instances using easy-to-interpret decision-tree models. Our tool uncovered several accountability bugs with varying severity ranging from nonrobust behavior in corner-cases (unreliable behavior when tax returns are close to zero) to missing eligibility conditions in the updated versions of software.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
pages = {138–149},
numpages = {12},
location = {Melbourne, Australia},
series = {ICSE-SEIS '23}
}

@inproceedings{10.1007/978-3-031-44725-9_10,
author = {Minjie, Kang and Ran, Ji and Ao, Gui and Xuejiao, Pang and Xiaohu, Fan and Li, Yi and Xing, Lu and Jie, Han},
title = {Enhanced Campus Information Query System based on ChatGPT Interface and Local Content Database},
year = {2023},
isbn = {978-3-031-44724-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44725-9_10},
doi = {10.1007/978-3-031-44725-9_10},
abstract = {With the increasing popularity of AI technology, this paper proposed an Enhanced Campus Information Query System based on ChatGPT Interface and Local Content Database, they developed a campus intelligent dialogue comprehensive service platform to meet the practical information service needs of collage stuff and students. The platform utilizes the latest ChatGPT model API for secondary development. Node.js technology is used as the backend, combined with the ChatGPT model API to achieve natural language interaction. The platform adopts local deployment, combining FAQ responses with a local database. By applying Dynamic Programming algorithm and Levenshtein distance algorithm, the platform implements keyword matching and fuzzy query functions. The dynamic programming algorithm is used to calculate the similarity score of strings by comparing the similarity between two strings and giving a numerical score. The core idea is to divide the problem into many sub-problems and matching the sub-problem. In keyword matching, dynamic programming algorithm can be used to calculate the similarity score between user input and keywords to determine the best match. At the same time, fragmented internal campus information resources are integrated, and users can obtain relevant information through keyword matching queries and enjoy personalized services and recommendations. The platform supports interactive dialogue form, making it convenient for users to quickly obtain the required information. In addition, our algorithm has significantly improved accuracy in keyword matching and fuzzy queries, increasing from 80% to 95%, and efficiency has increased by 50%. Moreover, the new algorithm can handle longer and more complex query strings and more query conditions, meeting the complex query needs of users. Convenient services are provided through universal and user-friendly methods such as WeChat Mini Program and web, improving user experience and satisfaction.},
booktitle = {Big Data – BigData 2023: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2023, Honolulu, HI, USA, September 23–26, 2023, Proceedings},
pages = {131–148},
numpages = {18},
keywords = {Campus Information Interaction System, ChatGPT, Dynamic Programming, Levenshtein Distance},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/3611643.3613090,
author = {Su, Chia-Yi and Bansal, Aakash and Jain, Vijayanta and Ghanavati, Sepideh and McMillan, Collin},
title = {A Language Model of Java Methods with Train/Test Deduplication},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613090},
doi = {10.1145/3611643.3613090},
abstract = {This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2152–2156},
numpages = {5},
keywords = {deduplication, java, language model, research tools},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3581784.3613214,
author = {Schaad, Philipp and Schneider, Timo and Ben-Nun, Tal and Calotoiu, Alexandru and Ziogas, Alexandros Nikolaos and Hoefler, Torsten},
title = {FuzzyFlow: Leveraging Dataflow To Find and Squash Program Optimization Bugs},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3613214},
doi = {10.1145/3581784.3613214},
abstract = {The current hardware landscape and application scale is driving performance engineers towards writing bespoke optimizations. Verifying such optimizations, and generating minimal failing cases, is important for robustness in the face of changing program conditions, such as inputs and sizes. However, isolation of minimal test-cases from existing applications and generating new configurations are often difficult due to side effects on the system state, mostly related to dataflow. This paper introduces FuzzyFlow: a fault localization and test case extraction framework designed to test program optimizations. We leverage dataflow program representations to capture a fully reproducible system state and area-of-effect for optimizations to enable fast checking for semantic equivalence. To reduce testing time, we design an algorithm for minimizing test inputs, trading off memory for recomputation. We demonstrate FuzzyFlow on example use cases in real-world applications where the approach provides up to 528 times faster optimization testing and debugging compared to traditional approaches.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {88},
numpages = {15},
keywords = {software testing, fuzzing, translation verification, test generation},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC '23}
}

@inproceedings{10.1145/3584931.3607020,
author = {Keelawat, Panayu},
title = {NBGuru: Generating Explorable Data Science Flowcharts to Facilitate Asynchronous Communication in Interdisciplinary Data Science Teams},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3607020},
doi = {10.1145/3584931.3607020},
abstract = {Data scientists typically work with domain experts in a Data Science (DS) project, resulting in knowledge gaps between roles. Communication holds an immense and difficult workload due to the complicated content, limited meeting time, vast audience backgrounds, etc. Thus, it is almost impossible to build a common ground within the team. Taking a step back, flowcharts and program descriptions have shown to help programmers learn algorithms. However, drawing a flowchart or writing a description takes time and effort. The novel AI-powered search engines can generate elaborate grounded responses with citations. It is then possible to generate flowcharts with text descriptions from code. Therefore, we studied 92 DS flowcharts and 173 code descriptions from top-voted Kaggle notebooks. We propose NBGuru, a flowchart-based communication tool. Users can explore computation steps asynchronously with generated texts and citations. Furthermore, we also discuss the possibility of AI in other collaborative roles.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {6–11},
numpages = {6},
keywords = {artificial intelligence, asynchronous communication, collaboration, computational notebooks, data science, flowchart, interdisciplinary, large language model, on-the-job training},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@inproceedings{10.1145/3593342.3593348,
author = {Naringrekar, Pranjal Dilip and Akhmetov, Ildar and Stroulia, Eleni},
title = {Generating CS1 Coding Questions using OpenAI},
year = {2023},
isbn = {9798400707896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593342.3593348},
doi = {10.1145/3593342.3593348},
abstract = {In CS1, to assess student knowledge, instructors prepare exam questions that often include code snippets. Due to the significant amount of time and effort required to create high-quality exam questions, instructors often only produce a single version of the exam. This results in all students receiving the same set of questions, which raises the possibility of plagiarism. In this paper, we propose a tool that allows computing science educators to generate a number of variations of a given code snippet, where the pedagogical intent of the code remains the same, but the code is mutated.},
booktitle = {Proceedings of the 25th Western Canadian Conference on Computing Education},
articleno = {11},
numpages = {2},
keywords = {CS1, code generation, coding questions},
location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {WCCCE '23}
}

@inproceedings{10.1145/3605770.3625214,
author = {Singla, Tanmay and Anandayuvaraj, Dharun and Kalu, Kelechi G. and Schorlemmer, Taylor R. and Davis, James C.},
title = {An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605770.3625214},
doi = {10.1145/3605770.3625214},
abstract = {As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing past failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5's categorizations had an average accuracy of 68% and Bard's had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.},
booktitle = {Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {5–15},
numpages = {11},
keywords = {cybersecurity, empirical software engineering, failure analysis, large language models, software security, software supply chain},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {SCORED '23}
}

@inproceedings{10.1007/978-3-031-46674-8_8,
author = {Wang, Cai and Li, Dongyang and He, Xiaofeng},
title = {SE-Prompt: Exploring Semantic Enhancement with&nbsp;Prompt Tuning for&nbsp;Relation Extraction},
year = {2023},
isbn = {978-3-031-46673-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-46674-8_8},
doi = {10.1007/978-3-031-46674-8_8},
abstract = {Compared to traditional supervised learning methods, utilizing prompt tuning for relation extraction tasks is a challenging endeavor in the real world. By inserting a template segment into the input, prompt tuning has proven effective for certain classification tasks. However, applying prompt tuning to relation extraction tasks, which involve mapping multiple words to a single label, poses challenges due to difficulties in precisely defining a template and mapping labels to the appropriate words. Prior approaches do not take full advantage of entities and have also overlooked the semantic connections between words in relation label. To address these limitations, we propose a semantic enhancement with prompt (SE-Prompt) which integrates entity and relation knowledge by incorporating two main contributions: semantic enhancement and subject-object relation refinement. These methods empower our model to effectively leverage relation labels and tap into the knowledge contained in pre-trained models. Our experiments on three datasets, under both fully supervised and low-resource settings demonstrate the effectiveness of our approach for relation extraction.},
booktitle = {Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part IV},
pages = {109–122},
numpages = {14},
keywords = {Relation Extraction, Prompt Tuning, Semantic Enhancement, Entity Expansion},
location = {Shenyang, China}
}

@article{10.1145/3624744,
author = {Cao, Sicong and Sun, Xiaobing and Bo, Lili and Wu, Rongxin and Li, Bin and Wu, Xiaoxue and Tao, Chuanqi and Zhang, Tao and Liu, Wei},
title = {Learning to Detect Memory-related Vulnerabilities},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624744},
doi = {10.1145/3624744},
abstract = {Memory-related vulnerabilities can result in performance degradation or even program crashes, constituting severe threats to the security of modern software. Despite the promising results of deep learning (DL)-based vulnerability detectors, there exist three main limitations: (1) rich contextual program semantics related to vulnerabilities have not yet been fully modeled; (2) multi-granularity vulnerability features in hierarchical code structure are still hard to be captured; and (3) heterogeneous flow information is not well utilized. To address these limitations, in this article, we propose a novel DL-based approach, called MVD+, to detect memory-related vulnerabilities at the statement-level. Specifically, it conducts both intraprocedural and interprocedural analysis to model vulnerability features, and adopts a hierarchical representation learning strategy, which performs syntax-aware neural embedding within statements and captures structured context information across statements based on a novel Flow-Sensitive Graph Neural Networks, to learn both syntactic and semantic features of vulnerable code. To demonstrate the performance, we conducted extensive experiments against eight state-of-the-art DL-based approaches as well as five well-known static analyzers on our constructed dataset with 6,879 vulnerabilities in 12 popular C/C++ applications. The experimental results confirmed that MVD+ can significantly outperform current state-of-the-art baselines and make a great trade-off between effectiveness and efficiency.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {43},
numpages = {35},
keywords = {Statement-level vulnerability detection, abstract syntax tree, graph neural networks, flow analysis}
}

@inproceedings{10.1145/3613372.3613417,
author = {Lima, Vitor Mesaque Alves de and Barbosa, Jacson Rodrigues and Marcacini, Ricardo Marcondes},
title = {MApp-IDEA: Monitoring App for Issue Detection and Prioritization},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613417},
doi = {10.1145/3613372.3613417},
abstract = {Opinion mining for app reviews uses machine learning-based methods to analyze people’s comments on app stores, aiming to support software maintenance and evolution. The challenge of manually analyzing a large amount of textual data can be solved through automatic opinion mining. We present MApp-IDEA tool to detect and classify emerging issues from user feedback in a risk matrix with prioritization levels and monitor evolution over time. The tool includes automatic app review tracking and an analytical data exploration instrument that allows engineers to browse the risk matrix, time series, heat map, issue tree, alerts, and notifications. Additionally, our tool has a performance analysis module, where it was possible to verify that in 6 million processed reviews of 50 popular apps, MApp-IDEA detected approximately 240,000 issues, where the peaks of the time series of issues are related to release dates of app versions. The tool is available on Github 1 and there is a presentation about the tool in Video 2 3.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {180–185},
numpages = {6},
keywords = {app reviews, issue detection, issue prioritization, opinion mining},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@article{10.1145/3637229,
author = {Guo, Hanyang and Chen, Yingye and Chen, Xiangping and Huang, Yuan and Zheng, Zibin},
title = {Smart Contract Code Repair Recommendation based on Reinforcement Learning and Multi-metric Optimization},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637229},
doi = {10.1145/3637229},
abstract = {A smart contract is a kind of code deployed on the blockchain that executes automatically once an event triggers a clause in the contract. Since smart contracts involve businesses such as asset transfer, they are more vulnerable to attacks, so it is crucial to ensure the security of smart contracts. Because a smart contract cannot be tampered with once deployed on the blockchain, for smart contract developers, it is necessary to fix vulnerabilities before deployment. Compared with many vulnerability detection tools for smart contracts, the amount of automatic fix approaches for smart contracts is relatively limited. These approaches mainly use defined pattern-based methods or heuristic search algorithms for vulnerability repairs. In this paper, we propose RLRep, a reinforcement learning-based approach to provide smart contract repair recommendations for smart contract developers automatically. This approach adopts an agent to provide repair action suggestions based on the vulnerable smart contract without any supervision, which can solve the problem of missing labeled data in machine learning-based repair methods. We evaluate our approach on a dataset containing 853 smart contract programs (programming language: Solidity) with different kinds of vulnerabilities. We split them into training and test set. The result shows that our approach can provide 54.97% correct repair recommendations for smart contracts.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {repair recommendation, smart contract}
}

@inproceedings{10.1145/3611643.3613876,
author = {Bendimerad, Anes and Remil, Youcef and Mathonat, Romain and Kaytoue, Mehdi},
title = {On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613876},
doi = {10.1145/3611643.3613876},
abstract = {Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1820–1831},
numpages = {12},
keywords = {AI, AIOps, Enterprise Resource Planning, Predictive Maintenance},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1007/s10664-023-10339-2,
author = {Aman, Hirohisa and Amasaki, Sousuke and Yokogawa, Tomoyuki and Kawahara, Minoru},
title = {An automated detection of confusing variable pairs with highly similar compound names in Java and Python programs},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10339-2},
doi = {10.1007/s10664-023-10339-2},
abstract = {Variable names represent a significant source of information regarding the source code, and a successful naming of variables is key to producing readable code. Programmers often use a compound variable name by concatenating two or more words to make it more informative and enhance the code readability. While each compound variable name is descriptive, a collection of them sometimes produces “confusing” variable pairs if their names are highly similar, e.g., “shippingHeight,” vs. “shippingWeight.” A confusing variable pair would adversely affect the code readability because it can cause a misreading or mix-up of variables during the programming or code review activities. Toward automated support for enhancing code readability, this paper conducts a large-scale investigation of compound variable names in Java and Python programs. The investigation collects 116,921,127 pairs of compound-named variables from 1,876 open-source Java projects and 106,943,523 pairs of such variables from 2,427 open-source Python projects. Then, this study analyzes those variable pairs from two perspectives of name similarity: string similarity and semantic similarity. Through an evaluation study with 30 human participants, the data analyses show that both string and semantic similarity can help detect confusing variable pairs in Java and Python programs. In order to distill confusing variable pairs automatically, support tools for detecting confusing variable pairs are also developed in this study.},
journal = {Empirical Softw. Engg.},
month = {aug},
numpages = {32},
keywords = {Confusing variable names, Compound names, String similarity, Semantic similarity}
}

@inproceedings{10.1109/ICSE48619.2023.00032,
author = {Wang, Xiaoke and Zhao, Lei},
title = {APICad: Augmenting API Misuse Detection through Specifications from Code and Documents},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00032},
doi = {10.1109/ICSE48619.2023.00032},
abstract = {Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICad to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICad can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICad, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {245–256},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3632746,
author = {Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit and Phung, Dinh and Le, Trung},
title = {Vision Transformer-Inspired Automated Vulnerability Repair},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3632746},
doi = {10.1145/3632746},
abstract = {Recently, automated vulnerability repair (AVR) approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. While vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this paper, we propose a novel vulnerability repair framework inspired by the Vision Transformer (VIT)-based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask and integrate it into decoders’ cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our vulnerability mask into encoders’ self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the AVR baseline methods by 2.68%-32.33%. Addtionally, our analysis of the cross-attention map of our approach confirms the design rationale of our vulnerability mask and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {Software Security, Automated Vulnerability Repair}
}

@inproceedings{10.1109/ICSE48619.2023.00113,
author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad},
title = {AI-Based Question Answering Assistance for Analyzing Natural-Language Requirements},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00113},
doi = {10.1109/ICSE48619.2023.00113},
abstract = {By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1% and 96.5%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1277–1289},
numpages = {13},
keywords = {natural-language requirements, question answering (QA), language models, natural language processing (NLP), natural language generation (NLG), BERT, T5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3622830,
author = {Feser, Jack and Dillig, Işıl and Solar-Lezama, Armando},
title = {Inductive Program Synthesis Guided by Observational Program Similarity},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622830},
doi = {10.1145/3622830},
abstract = {We present a new general-purpose synthesis technique for generating programs from input-output examples. Our method, called metric program synthesis, relaxes the observational equivalence idea (used widely in bottom-up enumerative synthesis) into a weaker notion of observational similarity, with the goal of reducing the search space that the synthesizer needs to explore. Our method clusters programs into equivalence classes based on an expert-provided distance metric and constructs a version space that compactly represents “approximately correct” programs. Then, given a “close enough” program sampled from this version space, our approach uses a distance-guided repair algorithm to find a program that exactly matches the given input-output examples. We have implemented our proposed metric program synthesis technique in a tool called SyMetric and evaluate it in three different domains considered in prior work. Our evaluation shows that SyMetric outperforms other domain-agnostic synthesizers that use observational equivalence and that it achieves results competitive with domain-specific synthesizers that are either designed for or trained on those domains.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {254},
numpages = {29},
keywords = {distance metric, inverse csg, program synthesis, regular expression inference}
}

@article{10.1145/3617593,
author = {Weiss, Michael and Tonella, Paolo},
title = {Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617593},
doi = {10.1145/3617593},
abstract = {Recent decades have seen the rise of large-scale Deep Neural Networks (DNNs) to achieve human-competitive performance in a variety of AI tasks. Often consisting of hundreds of million, if not hundreds of billion, parameters, these DNNs are too large to be deployed to or efficiently run on resource-constrained devices such as mobile phones or Internet of Things microcontrollers. Systems relying on large-scale DNNs thus have to call the corresponding model over the network, leading to substantial costs for hosting and running the large-scale remote model, costs which are often charged on a per-use basis. In this article, we propose BiSupervised, a novel architecture, where, before relying on a large remote DNN, a system attempts to make a prediction on a small-scale local model. A DNN supervisor monitors said prediction process and identifies easy inputs for which the local prediction can be trusted. For these inputs, the remote model does not have to be invoked, thus saving costs while only marginally impacting the overall system accuracy. Our architecture furthermore foresees a second supervisor to monitor the remote predictions and identify inputs for which not even these can be trusted, allowing to raise an exception or run a fallback strategy instead. We evaluate the cost savings and the ability to detect incorrectly predicted inputs on four diverse case studies: IMDb movie review sentiment classification, GitHub issue triaging, ImageNet image classification, and SQuADv2 free-text question answering. In all four case studies, we find that BiSupervised allows to reduce cost by at least 30% while maintaining similar system-level prediction performance. In two case studies (IMDb and SQuADv2), we find that BiSupervised even achieves a higher system-level accuracy, at reduced cost, compared to a remote-only model. Furthermore, measurements taken on our setup indicate a large potential of BiSupervised to reduce average prediction latency.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {28},
numpages = {29},
keywords = {Datasets, neural networks, gaze detection, text tagging}
}

@inproceedings{10.1145/3611643.3616322,
author = {Wang, Bo and Li, Ruishi and Li, Mingkai and Saxena, Prateek},
title = {TransMap: Pinpointing Mistakes in Neural Code Translation},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616322},
doi = {10.1145/3611643.3616322},
abstract = {Automated code translation between programming languages can greatly reduce the human effort needed in learning new languages or in migrating code. Recent neural machine translation models, such as Codex, have been shown to be effective on many code generation tasks including translation. However, code produced by neural translators often has semantic mistakes. These mistakes are difficult to eliminate from the neural translator itself because the translator is a black box, which is difficult to interpret or control compared to rule-based transpilers. We propose the first automated approach to pinpoint semantic mistakes in code obtained after neural code translation. Our techniques are implemented in a prototype tool called TransMap which translates Python to JavaScript, both of which are popular scripting languages. On our created micro-benchmarks of Python programs with 648 semantic mistakes in total, TransMap accurately pinpoints the correct location for a fix for 87.96%, often highlighting 1-2 lines for the user to inspect per mistake. We report on our experience in translating 5 Python libraries with up to 1k lines of code with TransMap. Our preliminary user study suggests that TransMap can reduce the time for fixing semantic mistakes by around 70% compared to using a standard IDE with debuggers.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {999–1011},
numpages = {13},
keywords = {Code Translation, Large Language Models, Semantic Mistakes},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00164,
author = {Gao, Shuzheng and Gao, Cuiyun and Wang, Chaozheng and Sun, Jun and Lo, David and Yu, Yue},
title = {Two Sides of the Same Coin: Exploiting the Impact of Identifiers in Neural Code Comprehension},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00164},
doi = {10.1109/ICSE48619.2023.00164},
abstract = {Previous studies have demonstrated that neural code comprehension models are vulnerable to identifier naming. By renaming as few as one identifier in the source code, the models would output completely irrelevant results, indicating that identifiers can be misleading for model prediction. However, identifiers are not completely detrimental to code comprehension, since the semantics of identifier names can be related to the program semantics. Well exploiting the two opposite impacts of identifiers is essential for enhancing the robustness and accuracy of neural code comprehension, and still remains under-explored. In this work, we propose to model the impact of identifiers from a novel causal perspective, and propose a counterfactual reasoning-based framework named CREAM. CREAM explicitly captures the misleading information of identifiers through multitask learning in the training stage, and reduces the misleading impact by counterfactual inference in the inference stage. We evaluate CREAM on three popular neural code comprehension tasks, including function naming, defect detection and code classification. Experiment results show that CREAM not only significantly outperforms baselines in terms of robustness (e.g., +37.9% on the function naming task at F1 score), but also achieve improved results on the original datasets (e.g., +0.5% on the function naming task at F1 score).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1933–1945},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3593663.3593692,
author = {Jell, Lea and List, Corinna and Kipp, Michael},
title = {Towards Automated Interactive Tutoring - Focussing on Misconceptions and Adaptive Level-Specific Feedback},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593692},
doi = {10.1145/3593663.3593692},
abstract = {Programming is an essential cross-disciplinary skill, yet teaching it effectively in large classes can be challenging due to the need for close feedback loops. Identifying and addressing common misconceptions is particularly important during the initial stages of learning to program. While automated interactive tutoring systems have the potential to offer personalized tutoring at scale, current systems tend to emphasize errors and predefined solutions rather than focusing on common misconceptions. In this study, we introduce a novel platform centered on addressing misconceptions in programming education. We describe methods for detecting misconceptions using Abstract Syntax Trees (AST) and providing tailored, level-specific feedback to emulate human-like tutoring. As an empirical basis for this project, we gathered data from various introductory programming courses. Additionally, we advocate for the establishment of a repository of common misconceptions, offering examples derived from both the literature and our own data. Investigating misconceptions can ultimately enhance the teaching strategies of both human educators and AI agents, such as GPT, in guiding learners effectively.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {226–235},
numpages = {10},
keywords = {CS in higher education, intelligent tutoring, programming misconceptions},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.5555/3618408.3620098,
author = {Yu, Zhiyuan and Wu, Yuhao and Zhang, Ning and Wang, Chenguang and Vorobeychik, Yevgeniy and Xiao, Chaowei},
title = {CODEIPPROMPT: intellectual property infringement assessment of code language models},
year = {2023},
publisher = {JMLR.org},
abstract = {Recent advances in large language models (LMs) have facilitated their ability to synthesize programming code. However, they have also raised concerns about intellectual property (IP) rights violations. Despite the significance of this issue, it has been relatively less explored. In this paper, we aim to bridge the gap by presenting CODEIPPROMPT, a platform for automatic evaluation of the extent to which code language models may reproduce licensed programs. It comprises two key components: prompts constructed from a licensed code database to elicit LMs to generate IP-violating code, and a measurement tool to evaluate the extent of IP violation of code LMs. We conducted an extensive evaluation of existing open-source code LMs and commercial products, and revealed the prevalence of IP violations in all these models. We further identified that the root cause is the substantial proportion of training corpus subject to restrictive licenses, resulting from both intentional inclusion and inconsistent license practice in the real world. To address this issue, we also explored potential mitigation strategies, including fine-tuning and dynamic token filtering. Our study provides a testbed for evaluating the IP violation issues of the existing code generation platforms and stresses the need for a better mitigation strategy.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1690},
numpages = {17},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.1145/3597208,
author = {Batoun, Mohamed Amine and Yung, Ka Lai and Tian, Yuan and Sayagh, Mohammed},
title = {An Empirical Study on GitHub Pull Requests’ Reactions},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597208},
doi = {10.1145/3597208},
abstract = {The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., “Thumbs-up”, “Laugh”, “Hooray”, “Heart”, “Rocket”, “Thumbs-down”, “Confused”, and “Eyes”. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests’ comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.To help future studies better leverage reactions as a feedback mechanism, we conduct an empirical study to understand the usage of GitHub reactions and understand their promises and limitations. We investigate in this article how reactions are used, when and who use them on what types of pull requests, and for what purposes. Our study considers a quantitative analysis on a set of 380 k reactions on 63 k pull requests of six popular open-source projects on GitHub and three qualitative analyses on a total number of 989 reactions from the same six projects. We find that the most common used GitHub reactions are the positive ones (i.e., “Thumbs-up”, “Hooray”, “Heart”, “Rocket”, and “Laugh”). We observe that reactors use positive reactions to express positive attitude (e.g., approval, appreciation, and excitement) on the proposed changes in pull requests. A median of just 1.95% of the used reactions are negative ones, which are used by reactors who disagree with the proposed changes for six reasons, such as feature modifications that might have more downsides than upsides or the use of the wrong approach to address certain problems. Most (a median of 78.40%) reactions on a pull request come before the closing of the corresponding pull requests. Interestingly, we observe that non-contributors (i.e., outsiders who potentially are the “end-users” of the software) are also active on reacting to pull requests. On top of that, we observe that core contributors, peripheral contributors, casual contributors and outsiders have different behaviors when reacting to pull requests. For instance, most core contributors react in the early stages of a pull request, while peripheral contributors, casual contributors and outsiders react around the closing time or, in some cases, after a pull request is merged. Contributors tend to react to the pull request’s source code, while outsiders are more concerned about the impact of the pull request on the end-user experience. Our findings shed light on common patterns of GitHub reactions usage on pull requests and provide taxonomies about the intention of reactors, which can inspire future studies better leverage pull requests’ reactions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {146},
numpages = {35},
keywords = {GitHub reactions, pull requests, software collaboration, feedback}
}

@inproceedings{10.1145/3539618.3591783,
author = {Peretz, Gal and Arraf, Mousa and Radinsky, Kira},
title = {What If: Generating Code to Answer Simulation Questions in Chemistry Texts},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591783},
doi = {10.1145/3539618.3591783},
abstract = {Many texts, especially in chemistry and biology, describe complex processes. We focus on texts that describe a chemical reaction process and questions that ask about the process's outcome under different environmental conditions. To answer questions about such processes, one needs to understand the interactions between the different entities involved in the process and simulate their state transitions during the process execution under other conditions. We hypothesize that generating code and executing it to simulate the process will allow answering such questions. We, therefore, define a domain-specific language (DSL) to represent processes. We contribute to the community a unique dataset curated by chemists and annotated by computer scientists. The dataset is composed of process texts, simulation questions, and their corresponding computer codes represented by the DSL. We propose a neural program synthesis approach based on reinforcement learning with a novel state-transition semantic reward. The novel reward is based on the run-time semantic similarity between the predicted code and the reference code. This allows simulating complex process transitions and thus answering simulation questions. Our approach yields a significant boost in accuracy for simulation questions: we achieved 88% accuracy as opposed to 83% accuracy of the state-of-the-art neural program synthesis approaches and 54% accuracy of state-of-the-art end-to-end text-based approaches.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1335–1344},
numpages = {10},
keywords = {chemistry, code generation, neural code generation, question answering, simulation questions},
location = {<conf-loc>, <city>Taipei</city>, <country>Taiwan</country>, </conf-loc>},
series = {SIGIR '23}
}

@article{10.1145/3622874,
author = {Nazari, Amirmohammad and Huang, Yifei and Samanta, Roopsha and Radhakrishna, Arjun and Raghothaman, Mukund},
title = {Explainable Program Synthesis by Localizing Specifications},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622874},
doi = {10.1145/3622874},
abstract = {The traditional formulation of the program synthesis problem is to find a program that meets a logical correctness specification. When synthesis is successful, there is a guarantee that the implementation satisfies the specification. Unfortunately, synthesis engines are typically monolithic algorithms, and obscure the correspondence between the specification, implementation and user intent. In contrast, humans often include comments in their code to guide future developers towards the purpose and design of different parts of the codebase. In this paper, we introduce subspecifications as a mechanism to augment the synthesized implementation with explanatory notes of this form. In this model, the user may ask for explanations of different parts of the implementation; the subspecification generated in response is a logical formula that describes the constraints induced on that subexpression by the global specification and surrounding implementation. We develop algorithms to construct and verify subspecifications and investigate their theoretical properties. We perform an experimental evaluation of the subspecification generation procedure, and measure its effectiveness and running time. Finally, we conduct a user study to determine whether subspecifications are useful: we find that subspecifications greatly aid in understanding the global specification, in identifying alternative implementations, and in debugging faulty implementations.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {298},
numpages = {25},
keywords = {Program synthesis, explainability, program comprehension}
}

@article{10.1007/s10664-023-10407-7,
author = {Castanyer, Roger Creus and Martínez-Fernández, Silverio and Franch, Xavier},
title = {Which design decisions in AI-enabled mobile applications contribute to greener AI?},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10407-7},
doi = {10.1007/s10664-023-10407-7},
journal = {Empirical Softw. Engg.},
month = {nov},
numpages = {34},
keywords = {AI-enabled applications, Mobile applications, Model accuracy, Application performance, Greener AI, Neural networks}
}

@inproceedings{10.1145/3583780.3614869,
author = {Phan, Hung and Jannesari, Ali},
title = {Evaluating and Optimizing the Effectiveness of Neural Machine Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614869},
doi = {10.1145/3583780.3614869},
abstract = {Neural Machine Translation (NMT) is widely applied in software engineering tasks. The effectiveness of NMT for code retrieval relies on the ability to learn from the sequence of tokens in the source language to the sequence of tokens in the target language. While NMT performs well in pseudocode-to-code translation[17], it might have challenges in learning to translate from natural language query to source code in newly curated real-world code documentation/ implementation datasets. In this work, we analyze the performance of NMT in natural language-to-code translation in the newly curated CAT benchmark[31] that includes the optimized versions of three Java datasets TLCodeSum, CodeSearchNet, Funcom, and a Python dataset PCSD. Our evaluation shows that NMT has low accuracy, measured by CrystalBLEU[10] and Meteor[9] metrics in this task. To alleviate the duty of NMT in learning complex representation of source code, we propose ASTTrans Representation, a tailored representation of an Abstract Syntax Tree (AST) using a subset of non-terminal nodes. We show that the classical approach NMT performs significantly better in learning ASTTrans Representation over code tokens with up to 36% improvement on Meteor score. Moreover, we leverage ASTTrans Representation to conduct combined code search processes from the state-of-the-art code search processes using GraphCodeBERT[13], and UniXcoder[12]. Our NMT models of learning ASTTrans Representation can boost the Mean Reciprocal Rank of these state-of-the-art code search processes by up to 3.08% and improve 23.08% of queries' results over the CAT benchmark.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2055–2064},
numpages = {10},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1007/978-3-031-35995-8_23,
author = {Tsigkanos, Christos and Rani, Pooja and Müller, Sebastian and Kehrer, Timo},
title = {Variable Discovery with&nbsp;Large Language Models for&nbsp;Metamorphic Testing of&nbsp;Scientific Software},
year = {2023},
isbn = {978-3-031-35994-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35995-8_23},
doi = {10.1007/978-3-031-35995-8_23},
abstract = {When testing scientific software, it is often challenging or even impossible to craft a test oracle for checking whether the program under test produces the expected output when being executed on a given input – also known as the oracle problem in software engineering. Metamorphic testing mitigates the oracle problem by reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over three case studies of scientific software documentation, and compare variables extracted to ground truth manually labelled by experts.},
booktitle = {Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I},
pages = {321–335},
numpages = {15},
keywords = {Scientific Software, Metamorphic Testing, Large Language Models, Natural Language Processing},
location = {Prague, Czech Republic}
}

@inproceedings{10.1145/3583780.3615493,
author = {Gao, Yue and Piovano, Enrico and Soliman, Tamer and Moniruzzaman, Monir and Kumar, Anoop and Bradford, Melanie and Nandi, Subhrangshu},
title = {Predicting Interaction Quality of Conversational Assistants With Spoken Language Understanding Model Confidences},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615493},
doi = {10.1145/3583780.3615493},
abstract = {In conversational AI assistants, SLU models are part of a complex pipeline composed of several modules working in harmony. Hence, an update to the SLU model needs to ensure improvements not only in the model specific metrics but also in the overall conversational assistant performance. Specifically, the impact on user interaction quality metrics must be factored in, while integrating interactions with distal modules upstream and downstream of the SLU component. We develop a ML model that makes it possible to gauge the interaction quality metrics due to SLU model changes before a production launch. The proposed model is a multi-modal transformer with a gated mechanism that conditions on text embeddings, output of a BERT model pre-trained on conversational data, and the hypotheses of the SLU classifiers with the corresponding confidence scores. We show that the proposed model predicts defect with more than 76% correlation with live interaction quality defects, compared to 46% baseline.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4581–4587},
numpages = {7},
keywords = {defect prediction, dialog response quality, spoken language understanding, transformer-based model},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1145/3611643.3613887,
author = {Aktas, Ethem and Cakmak, Ebru and Inan, Mete and Yilmaz, Cemal},
title = {Issue Report Validation in an Industrial Context},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613887},
doi = {10.1145/3611643.3613887},
abstract = {Effective issue triaging is crucial for software development teams to improve software quality, and thus customer satisfaction. Validating issue reports manually can be time-consuming, hindering the overall efficiency of the triaging process. This paper presents an approach on automating the validation of issue reports to accelerate the issue triaging process in an industrial set-up. We work on 1,200 randomly selected issue reports in banking domain, written in Turkish, an agglutinative language, meaning that new words can be formed with linear concatenation of suffixes to express entire sentences. We manually label these reports for validity, and extract the relevant patterns indicating that they are invalid. Since the issue reports we work on are written in an agglutinative language, we use morphological analysis to extract the features. Using the proposed feature extractors, we utilize a machine learning based approach to predict the issue reports’ validity, performing a 0.77 F1-score.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2026–2031},
numpages = {6},
keywords = {automated issue classification, issue report validation, text analysis},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-3-031-46002-9_25,
author = {Johansson, Moa},
title = {What Can Large Language Models Do for&nbsp;Theorem Proving and&nbsp;Formal Methods?},
year = {2023},
isbn = {978-3-031-46001-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-46002-9_25},
doi = {10.1007/978-3-031-46002-9_25},
abstract = {With the introduction of large language models, AI for natural language have taken a leap. These systems are now also being used for tasks that has previously been dominated by symbolic methods, such as program synthesis and even to support formalising mathematics and assist theorem provers. We survey some recent applications in theorem proving, focusing on how they combine neural networks with symbolic systems, and report on a case-study of using GPT-4 for the task of automated conjecturing a.k.a. theory exploration.},
booktitle = {Bridging the Gap Between AI and Reality: First International Conference, AISoLA 2023, Crete, Greece, October 23–28, 2023, Proceedings},
pages = {391–394},
numpages = {4},
location = {<conf-loc content-type="InPerson">Crete, Greece</conf-loc>}
}

@inproceedings{10.1145/3617555.3617874,
author = {Tihanyi, Norbert and Bisztray, Tamas and Jain, Ridhi and Ferrag, Mohamed Amine and Cordeiro, Lucas C. and Mavroeidis, Vasileios},
title = {The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617874},
doi = {10.1145/3617555.3617874},
abstract = {This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {33–43},
numpages = {11},
keywords = {Artificial Intelligence, Dataset, Formal Verification, Large Language Models, Software Security, Vulnerability Classification},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {PROMISE 2023}
}

@inproceedings{10.1145/3611643.3613083,
author = {Happe, Andreas and Cito, Jürgen},
title = {Getting pwn’d by AI: Penetration Testing with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613083},
doi = {10.1145/3611643.3613083},
abstract = {The field of software security testing, more specifically penetration testing, requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential use of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of AI sparring partners.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2082–2086},
numpages = {5},
keywords = {large language models, penetration testing, security testing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1109/TSE.2023.3277564,
author = {Ojdanic, Milos and Garg, Aayush and Khanfir, Ahmed and Degiovanni, Renzo and Papadakis, Mike and Le Traon, Yves},
title = {Syntactic Versus Semantic Similarity of Artificial and Real Faults in Mutation Testing Studies},
year = {2023},
issue_date = {July 2023},
publisher = {IEEE Press},
volume = {49},
number = {7},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2023.3277564},
doi = {10.1109/TSE.2023.3277564},
abstract = {Fault seeding is typically used in empirical studies to evaluate and compare test techniques. Central to these techniques lies the hypothesis that artificially seeded faults involve some form of realistic properties and thus provide realistic experimental results. In an attempt to strengthen realism, a recent line of research uses machine learning techniques, such as deep learning and Natural Language Processing, to seed faults that look like (syntactically) real ones, implying that fault realism is related to syntactic similarity. This raises the question of whether seeding syntactically similar faults indeed results in semantically similar faults and, more generally whether syntactically dissimilar faults are far away (semantically) from the real ones. We answer this question by employing 4 state-of-the-art fault-seeding techniques (PiTest - a popular mutation testing tool, IBIR - a tool with manually crafted fault patterns, DeepMutation - a learning-based fault seeded framework and &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$mu$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;&amp;#x03BC;&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="ojdanic-ieq1-3277564.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;BERT - a mutation testing tool based on the pre-trained language model CodeBERT) that operate in a fundamentally different way, and demonstrate that syntactic similarity does not reflect semantic similarity. We also show that 65.11&amp;#x0025;, 76.44&amp;#x0025;, 61.39&amp;#x0025; and 9.76&amp;#x0025; of the real faults of Defects4J V2 are semantically resembled by PiTest, IBIR, &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$mu$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;&amp;#x03BC;&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="ojdanic-ieq2-3277564.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;BERT and DeepMutation faults, respectively.},
journal = {IEEE Trans. Softw. Eng.},
month = {jul},
pages = {3922–3938},
numpages = {17}
}

@inproceedings{10.1145/3596454.3597176,
author = {Schmidt, Albrecht},
title = {Speeding Up the Engineering of Interactive Systems with Generative AI},
year = {2023},
isbn = {9798400702068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3596454.3597176},
doi = {10.1145/3596454.3597176},
abstract = {This keynote discusses the opportunities and challenges of using Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) as tools for developing interactive systems. We will look at different stages in the development lifecycle of interactive systems and assess the value of AI support. We explore how GenAI and LLMs can potentially speed-up the ideation, requirements elicitation, architecture development, prototyping, implementation, and testing of interactive systems. The talk will outline emerging practices, such as the use of prompts for code and system generation, to facilitate prototyping and accelerate implementation. We will outline fundamental challenges and suggest emerging research directions, and pose research questions. What will software development tools look like in the future? How can we efficiently use AI to develop interactive systems without compromising quality? We also speculate about the implications of these developments for researchers, practitioners, and society. We believe that it will massively accelerate the digital transformation. Interactive AI-based tools for systems and software development will become a major research direction.},
booktitle = {Companion Proceedings of the 2023 ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {7–8},
numpages = {2},
keywords = {Automation, ChatGPT, Engineering, Interactive Systems, Large Language Models, Software and System Development},
location = {<conf-loc>, <city>Swansea</city>, <country>United Kingdom</country>, </conf-loc>},
series = {EICS '23 Companion}
}

@inproceedings{10.1145/3585059.3611409,
author = {Gumina, Sharon and Dalton, Travis and Gerdes, John},
title = {Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models: Strategies and Techniques for Inclusion of Large Language Models},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611409},
doi = {10.1145/3585059.3611409},
abstract = {This paper argues for the inclusion of tools that utilize Artificial Intelligence (AI) Large Language Models (LLMs) in information technology (IT) undergraduate courses that teach the fundamentals of software. LLM tools have become widely available and disrupt traditional methods for teaching software concepts. Learning objectives are compromised when students submit AI-generated code for a classroom assignment without comprehending or validating the code. Since LLM tools including OpenAI Codex, Copilot by GitHub, and ChatGPT are being used in industry for software development, students need to be familiar with their use without compromising student learning. Incorporating LLM tools into the curriculum prepares students for real-world software development. However, students still need to understand software fundamentals including how to write and debug code. There are many challenges associated with the inclusion of AI tools into the IT curriculum that need to be addressed and mitigated. This paper presents strategies and techniques to integrate student use of LLM tools, assist students’ interaction with the tools, and help prepare students for careers that increasingly use AI tools to design, develop, and maintain software.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {60–65},
numpages = {6},
location = {<conf-loc>, <city>Marietta</city>, <state>GA</state>, <country>USA</country>, </conf-loc>},
series = {SIGITE '23}
}

@article{10.1007/s10664-023-10356-1,
author = {Yu, Liang and Alégroth, Emil and Chatzipetrou, Panagiota and Gorschek, Tony},
title = {Automated NFR testing in continuous integration environments: a multi-case study of Nordic companies},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10356-1},
doi = {10.1007/s10664-023-10356-1},
journal = {Empirical Softw. Engg.},
month = {oct},
numpages = {29},
keywords = {Non-functional requirements, NFR, Continuous integration, CI, Automated testing, Metrics, Case study}
}

@article{10.1007/s10664-023-10361-4,
author = {Obie, Humphrey O. and Du, Hung and Madampe, Kashumi and Shahin, Mojtaba and Ilekura, Idowu and Grundy, John and Li, Li and Whittle, Jon and Turhan, Burak and Khalajzadeh, Hourieh},
title = {Automated detection, categorisation and developers’ experience with the violations of honesty in mobile apps},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10361-4},
doi = {10.1007/s10664-023-10361-4},
abstract = {Human values such as honesty, social responsibility, fairness, privacy, and the like are things considered important by individuals and society. Software systems, including mobile software applications (apps), may ignore or violate such values, leading to negative effects in various ways for individuals and society. While some works have investigated different aspects of human values in software engineering, this mixed-methods study focuses on honesty as a critical human value. In particular, we studied (i) how to detect honesty violations in mobile apps, (ii) the types of honesty violations in mobile apps, and (iii) the perspectives of app developers on these detected honesty violations. We first develop and evaluate 7 machine learning (ML) models to automatically detect violations of the value of honesty in app reviews from an end-user perspective. The most promising was a Deep Neural Network model with F1 score of 0.921. We then conducted a manual analysis of 401 reviews containing honesty violations and characterised honesty violations in mobile apps into 10 categories: unfair cancellation and refund policies; false advertisements; delusive subscriptions; cheating systems; inaccurate information; unfair fees; no service; deletion of reviews; impersonation; and fraudulent-looking apps. A developer survey and interview study with mobile developers then identified 7 key causes behind honesty violations in mobile apps and 8 strategies to avoid or fix such violations. The findings of our developer study also articulate the negative consequences that honesty violations might bring for businesses, developers, and users. Finally, the app developers’ feedback shows that our prototype ML-based models can have promising benefits in practice.},
journal = {Empirical Softw. Engg.},
month = {sep},
numpages = {52},
keywords = {Human values, Honesty, Mobile apps, Machine Learning, App reviews, Mixed-methods, Developer experience}
}

@article{10.1007/s10664-023-10421-9,
author = {Ahasanuzzaman, Md and Oliva, Gustavo A. and Hassan, Ahmed E.},
title = {Using knowledge units of programming languages to recommend reviewers for pull requests: an empirical study},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10421-9},
doi = {10.1007/s10664-023-10421-9},
abstract = {Determining the right code reviewer for a given code change requires understanding the characteristics of the changed code, identifying the skills of each potential reviewer (expertise profile), and finding a good match between the two. To facilitate this task, we design a code reviewer recommender that operates on the knowledge units (KUs) of a programming language. We define a KU as a cohesive set of key capabilities that are offered by one or more building blocks of a given programming language. We operationalize our KUs using certification exams for the Java programming language. We detect KUs from 10 actively maintained Java projects from GitHub, spanning 290K commits and 65K pull requests (PRs). We generate developer expertise profiles based on the detected KUs. We use these KU-based expertise profiles to build a code reviewer recommender (KUREC). We compare KUREC’s performance to that of seven baseline recommenders. KUREC ranked first along with the top-performing baseline recommender (RF) in a Scott-Knott ESD analysis of recommendation accuracy (the top-5 accuracy of KUREC is 0.84 (median) and the MAP@5 is 0.51 (median)). From a practical standpoint, we highlight that KUREC’s performance is more stable (lower interquartile range) than that of RF, thus making it more consistent and potentially more trustworthy. We also design three new recommenders by combining KUREC with our baseline recommenders. These new combined recommenders outperform both KUREC and the individual baselines. Finally, we evaluate how reasonable the recommendations from KUREC and the combined recommenders are when those deviate from the ground truth. We observe that KUREC is the recommender with the highest percentage of reasonable recommendations (63.4%). Overall we conclude that KUREC and one of the combined recommenders (e.g., AD_HYBRID) are overall superior to the baseline recommenders that we studied. Future work in the area should thus (i) consider KU-based recommenders as baselines and (ii) experiment with combined recommenders.},
journal = {Empirical Softw. Engg.},
month = {dec},
numpages = {69},
keywords = {Reviewer recommendation, Pull requests, Knowledge representation, Knowledge units, Java}
}

@article{10.1145/3597206,
author = {Huang, Qing and Liao, Dianshu and Xing, Zhenchang and Zuo, Zhengkang and Wang, Changjing and Xia, Xin},
title = {Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597206},
doi = {10.1145/3597206},
abstract = {Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {147},
numpages = {37},
keywords = {Smart contract, code knowledge graph, knowledge deduction, code recommendation, crowd-scale coding practice checking}
}

@article{10.14778/3632093.3632111,
author = {Singh, Mukul and Cambronero, José and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza, Mohammad and Verbruggen, Gust},
title = {FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language},
year = {2023},
issue_date = {November 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3632093.3632111},
doi = {10.14778/3632093.3632111},
abstract = {Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires understanding and implementing the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {497–510},
numpages = {14}
}

@inproceedings{10.1145/3624032.3624043,
author = {Marques, João Paulo and Lima, Mônica and Souza, Bruno and Miranda, Eloise and Santos, André and Collins, Eliane},
title = {SelectNLTest - Selection and natural language rewriting of test cases generated by the DRL-MOBTEST tool},
year = {2023},
isbn = {9798400716294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624032.3624043},
doi = {10.1145/3624032.3624043},
abstract = {The software testing process is important to ensure quality, especially in mobile applications that have characteristics such as platform diversity, hardware limitations, portability, frequent updates, among others. Software companies need to deliver quickly with increasingly complex functionalities; therefore, the testing process must be efficient and avoid bottlenecks, such as the creation of test cases. Among the solutions found in the literature, the state-of-the-art tool DRL-MOBTEST aims to assist in the automatic generation of test cases for mobile applications using deep reinforcement learning. The experiments show promising results; however, the tool has some limitations, such as generating duplicate and less readable tests. In this article, we present SelectNLTest, a module developed to identify and remove similar test scripts and transcribe the test cases generated by the tool using Natural Language Processing techniques. This allows for the removal of similar tests and improves the readability and understanding of the generated test cases for professionals in the field. The results of the experiments showed that, in 10 Android applications used in the comparative analysis, the proposed module reduced the number of test cases by 58.3% while maintaining code coverage and application functionality.},
booktitle = {Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {77–85},
numpages = {9},
keywords = {automatic test generation, coverage analysis, deep reinforcement learning, natural language},
location = {<conf-loc>, <city>Campo Grande, MS</city>, <country>Brazil</country>, </conf-loc>},
series = {SAST '23}
}

@article{10.1016/j.jksuci.2023.101665,
author = {Saleem, Summra and Asim, Muhammad Nabeel and Elst, Ludger Van and Dengel, Andreas},
title = {FNReq-Net: A hybrid computational framework for functional and non-functional requirements classification},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {35},
number = {8},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2023.101665},
doi = {10.1016/j.jksuci.2023.101665},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = {sep},
numpages = {14},
keywords = {Software development, Functional &amp; non-functional requirements, Feature selection, Feature pruning, Attention mechanism, Hybrid predictor}
}

@inproceedings{10.1145/3611643.3616279,
author = {Benoit, Tristan and Marion, Jean-Yves and Bardin, Sébastien},
title = {Scalable Program Clone Search through Spectral Analysis},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616279},
doi = {10.1145/3611643.3616279},
abstract = {We consider the problem of program clone search, i.e. given a target program and a repository of known programs (all in executable format), the goal is to find the program in the repository most similar to the target program -- with potential applications in terms of reverse engineering, program clustering, malware lineage and software theft detection. Recent years have witnessed a blooming in code similarity techniques, yet most of them focus on function-level similarity and function clone search, while we are interested in program-level similarity and program clone search.  
Actually, our study shows that prior similarity approaches are either too slow to handle large program repositories, or not precise enough, or yet not robust against slight variations introduced by compilers, source code versions or light obfuscations.  
We propose a novel spectral analysis method for program-level similarity and program clone search called Programs Spectral Similarity (PSS).  
In a nutshell, PSS one-time spectral feature extraction is tailored for large repositories, making it a perfect fit for program clone search.  
We have compared the different approaches with extensive benchmarks, showing that PSS reaches a sweet spot in terms of precision, speed and robustness.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {808–820},
numpages = {13},
keywords = {binary code analysis, clone search, spectral analysis},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00196,
author = {Huang, Yuchao and Wang, Junjie and Liu, Zhe and Wang, Song and Chen, Chunyang and Li, Mingyang and Wang, Qing},
title = {Context-Aware Bug Reproduction for Mobile Apps},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00196},
doi = {10.1109/ICSE48619.2023.00196},
abstract = {Bug reports are vital for software maintenance that allow the developers being informed of the problems encountered in the software. Before bug fixing, developers need to reproduce the bugs which is an extremely time-consuming and tedious task, and it is highly expected to automate this process. However, it is challenging to do so considering the imprecise or incomplete natural language described in reproducing steps, and the missing or ambiguous single source of information in GUI components. In this paper, we propose a context-aware bug reproduction approach ScopeDroid which automatically reproduces crashes from textual bug reports for mobile apps. It first constructs a state transition graph (STG) and extracts the contextual information of components. We then design a multi-modal neural matching network to derive the fuzzy matching matrix between all candidate GUI events and reproducing steps. With the STG and matching information, it plans the exploration path for reproducing the bug, and enriches the initial STG iteratively. We evaluate the approach on 102 bug reports from 69 popular Android apps, and it successfully reproduces 63.7% of the crashes, outperforming the state-of-the-art baselines by 32.6% and 38.3%. We also evaluate the usefulness and robustness of ScopeDroid with promising results. Furthermore, to train the neural matching network, we develop a heuristic-based automated training data generation method, which can potentially motivate and facilitate other activities as user interface operations.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2336–2348},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00007,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Ma, Lei and Traon, Yves Le},
title = {CodeS: Towards Code Model Generalization Under Distribution Shift},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00007},
doi = {10.1109/ICSE-NIER58687.2023.00007},
abstract = {Distribution shift has been a longstanding challenge for the reliable deployment of deep learning (DL) models due to unexpected accuracy degradation. Although DL has been becoming a driving force for large-scale source code analysis in the big code era, limited progress has been made on distribution shift analysis and benchmarking for source code tasks. To fill this gap, this paper initiates to propose CodeS, a distribution shift benchmark dataset, for source code learning. Specifically, CodeS supports two programming languages (Java and Python) and five shift types (task, programmer, time-stamp, token, and concrete syntax tree). Extensive experiments based on CodeS reveal that 1) out-of-distribution detectors from other domains (e.g., computer vision) do not generalize to source code, 2) all code classification models suffer from distribution shifts, 3) representation-based shifts have a higher impact on the model than others, and 4) pre-trained bimodal models are relatively more resistant to distribution shifts.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {1–6},
numpages = {6},
keywords = {source code learning, distribution shift},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE48619.2023.00022,
author = {Croft, Roland and Babar, M. Ali and Kholoosi, M. Mehdi},
title = {Data Quality for Software Vulnerability Datasets},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00022},
doi = {10.1109/ICSE48619.2023.00022},
abstract = {The use of learning-based techniques to achieve automated software vulnerability detection has been of longstanding interest within the software security domain. These data-driven solutions are enabled by large software vulnerability datasets used for training and benchmarking. However, we observe that the quality of the data powering these solutions is currently ill-considered, hindering the reliability and value of produced outcomes. Whilst awareness of software vulnerability data preparation challenges is growing, there has been little investigation into the potential negative impacts of software vulnerability data quality. For instance, we lack confirmation that vulnerability labels are correct or consistent. Our study seeks to address such shortcomings by inspecting five inherent data quality attributes for four state-of-the-art software vulnerability datasets and the subsequent impacts that issues can have on software vulnerability prediction models. Surprisingly, we found that all the analyzed datasets exhibit some data quality problems. In particular, we found 20--71% of vulnerability labels to be inaccurate in real-world datasets, and 17--99% of data points were duplicated. We observed that these issues could cause significant impacts on downstream models, either preventing effective model training or inflating benchmark performance. We advocate for the need to overcome such challenges. Our findings will enable better consideration and assessment of software vulnerability data quality in the future.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {121–133},
numpages = {13},
keywords = {software vulnerability, data quality, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.5555/3618408.3619050,
author = {Kandpal, Nikhil and Lester, Brian and Muqeeth, Mohammed and Mascarenhas, Anisha and Evans, Monty and Baskaran, Vishal and Huang, Tenghao and Liu, Haokun and Raffel, Colin},
title = {Git-Theta: a git extension for collaborative development of machine learning models},
year = {2023},
publisher = {JMLR.org},
abstract = {Currently, most machine learning models are trained by centralized teams and are rarely updated. In contrast, open-source software development involves the iterative development of a shared artifact through distributed collaboration using a version control system. In the interest of enabling collaborative and continual improvement of machine learning models (Raffel, 2023), we introduce Git-Theta, a version control system for machine learning models. Git-Theta is an extension to Git, the most widely used version control software, that allows fine-grained tracking of changes to model parameters alongside code and other artifacts. Unlike existing version control systems that treat a model checkpoint as a blob of data, Git-Theta leverages the structure of checkpoints to support communication-efficient updates, automatic model merges, and meaningful reporting about the difference between two versions of a model. In addition, Git-Theta includes a plug-in system that enables users to easily add support for new functionality. In this paper, we introduce Git-Theta's design and features and include an example use-case of Git-Theta where a pre-trained model is continually adapted and modified. We publicly release Git-Theta in hopes of kickstarting a new era of collaborative model development.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {642},
numpages = {12},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1145/3613307.3613318,
author = {Bowen, Zhao and Xiaoxing, Feng and Jianlin, Zhou and Yaxing, Sun},
title = {Diagnosis of Thyroid Nodule Based on Multi-Scale Se-Segnet and Resnet50},
year = {2023},
isbn = {9798400707698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613307.3613318},
doi = {10.1145/3613307.3613318},
abstract = {Thyroid nodule is a common chronic endocrine system disease, which may deteriorate into thyroid cancer without treatment. However, due to the distribution of many blood vessels, nerves and organs near the thyroid, the clarity of ultrasonic image is poor, and human eye recognition has great limitations.We propose a method for thyroid nodule diagnosis based on multi-scale Se-SegNet network and ResNet50. We improved and optimized the SegNet as the main backbone, and constructed a multi-scale Se-SegNet network to segment thyroid ultrasonography. Then we used ResNet50 to classify the segmented images and get the diagnosis results. The accuracy of the multi-scale Se-SegNet image segmentation model is 0.959.The accuracy of classification results was 0.967. This paper proposes a new diagnostic scheme for thyroid nodule, which can diagnose thyroid nodule quickly and accurately. The experimental results show that it has higher diagnostic accuracy than the traditional scheme..},
booktitle = {Proceedings of the 2023 8th International Conference on Biomedical Signal and Image Processing},
pages = {53–60},
numpages = {8},
keywords = {ResNet50, Thyroid nodule, image segmentation, multi-scale Se-SegNet},
location = {<conf-loc>, <city>Chengdu</city>, <country>China</country>, </conf-loc>},
series = {ICBIP '23}
}

@inproceedings{10.1145/3576915.3623143,
author = {Hong, Geng and Wu, Mengying and Chen, Pei and Liao, Xiaojing and Ye, Guoyi and Yang, Min},
title = {Understanding and Detecting Abused Image Hosting Modules as Malicious Services},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623143},
doi = {10.1145/3576915.3623143},
abstract = {As a new type of underground ecosystem, the exploitation of Abused IHMs as MalIcious sErvices (AIMIEs) is becoming increasingly prevalent among miscreants to host illegal images and propagate harmful content. However, there has been little effort to understand this new menace, in terms of its magnitude, impact, and techniques, not to mention any serious effort to detect vulnerable image hosting modules on a large scale. To fulfill this gap, this paper presents the first measurement study of AIMIEs. By collecting and analyzing 89 open-sourced AIMIEs, we reveal the landscape of AIMIEs, report the evolution and evasiveness of abused image hosting APIs from reputable companies such as Alibaba, Tencent, and Bytedance, and identify real-world abused images uploaded through those AIMIEs. In addition, we propose a tool, called Viola, to detect vulnerable image hosting modules (IHMs) in the wild. We find 477 vulnerable IHM upload APIs associated with 338 web services, which integrated vulnerable IHMs, and 207 victim FQDNs. The highest-ranked domain with vulnerable web service is baidu.com, followed by bilibili.com and 163.com. We have reported abused and vulnerable IHM upload APIs and received acknowledgments from 69 of them by the time of paper submission.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3213–3227},
numpages = {15},
keywords = {cybercrime, image hosting module, vulnerability detection, web resource abuse},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@article{10.1109/TSE.2023.3292399,
author = {Liu, Ke and Chen, Xiang and Chen, Chunyang and Xie, Xiaofei and Cui, Zhanqi},
title = {Automated Question Title Reformulation by Mining Modification Logs From Stack Overflow},
year = {2023},
issue_date = {Sept. 2023},
publisher = {IEEE Press},
volume = {49},
number = {9},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2023.3292399},
doi = {10.1109/TSE.2023.3292399},
abstract = {In Stack Overflow, developers may not clarify and summarize the critical problems in the question titles due to a lack of domain knowledge or poor writing skills. Previous studies mainly focused on automatically generating the question titles by analyzing the posts&amp;#x2019; problem descriptions and code snippets. In this study, we aim to improve title quality from the perspective of question title reformulation and propose a novel approach &lt;monospace&gt;QETRA&lt;/monospace&gt; motivated by the findings of our formative study. Specifically, by mining modification logs from Stack Overflow, we first extract title reformulation pairs containing the original title and the reformulated title. Then we resort to multi-task learning by formalizing title reformulation for each programming language as separate but related tasks. Later we adopt a pre-trained model T5 to automatically learn the title reformulation patterns. Automated evaluation and human study both show the competitiveness of &lt;monospace&gt;QETRA&lt;/monospace&gt; after compared with six state-of-the-art baselines. Moreover, our ablation study results also confirm that our studied question title reformulation task is more practical than the direct question title generation task for generating high-quality titles. Finally, we develop a browser plugin based on &lt;monospace&gt;QETRA&lt;/monospace&gt; to facilitate the developers to perform title reformulation. Our study provides a new perspective for studying the quality of post titles and can further generate high-quality titles.},
journal = {IEEE Trans. Softw. Eng.},
month = {sep},
pages = {4390–4410},
numpages = {21}
}

@inproceedings{10.1145/3597926.3598080,
author = {Liu, Zhongxin and Liu, Kui and Xia, Xin and Yang, Xiaohu},
title = {Towards More Realistic Evaluation for Neural Test Oracle Generation},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598080},
doi = {10.1145/3597926.3598080},
abstract = {Unit testing has become an essential practice during software development and maintenance. Effective unit tests can help guard and improve software quality but require a substantial amount of time and effort to write and maintain. A unit test consists of a test prefix and a test oracle. Synthesizing test oracles, especially functional oracles, is a well-known challenging problem. Recent studies proposed to leverage neural models to generate test oracles, i.e., neural test oracle generation (NTOG), and obtained promising results. However, after a systematic inspection, we find there are some inappropriate settings in existing evaluation methods for NTOG. These settings could mislead the understanding of existing NTOG approaches’ performance. We summarize them as 1) generating test prefixes from bug-fixed program versions, 2) evaluating with an unrealistic metric, and 3) lacking a straightforward baseline. In this paper, we first investigate the impacts of these settings on evaluating and understanding the performance of NTOG approaches. We find that 1) unrealistically generating test prefixes from bug-fixed program versions inflates the number of bugs found by the state-of-the-art NTOG approach TOGA by 61.8%, 2) FPR (False Positive Rate) is not a realistic evaluation metric and the Precision of TOGA is only 0.38%, and 3) a straightforward baseline NoException, which simply expects no exception should be raised, can find 61% of the bugs found by TOGA with twice the Precision. Furthermore, we introduce an additional ranking step to existing evaluation methods and propose an evaluation metric named Found@K to better measure the cost-effectiveness of NTOG approaches in terms of bug-finding. We propose a novel unsupervised ranking method to instantiate this ranking step, significantly improving the cost-effectiveness of TOGA. Eventually, based on our experimental results and observations, we propose a more realistic evaluation method TEval+ for NTOG and summarize seven rules of thumb to boost NTOG approaches into their practical usages.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {589–600},
numpages = {12},
keywords = {Neural Network, Realistic Evaluation, Test Oracle Generation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00028,
author = {Barr, Earl and Bell, Jonathan and Hilton, Michael and Mechtaev, Sergey and Timperley, Christopher},
title = {Continuously Accelerating Research},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00028},
doi = {10.1109/ICSE-NIER58687.2023.00028},
abstract = {Science is facing a software reproducibility crisis. Software powers experimentation, and fuels insights, yielding new scientific contributions. Yet, the research software is often difficult for other researchers to reproducibly run. Beyond reproduction, research software that is truly reusable will speed science by allowing other researchers to easily build upon and extend prior work. As software engineering researchers, we believe that it is our duty to create tools and processes that instill reproducibility, reusability, and extensibility into research software. This paper outlines a vision for a community infrastructure that will bring the benefits of continuous integration to scientists developing research software. To persuade researchers to adopt this infrastructure, we will appeal to their self-interest by making it easier for them to develop and evaluate research prototypes. Building better research software is a complex socio-technical problem that requires stakeholders to join forces to solve this problem for the software engineering community, and the greater scientific community. This vision paper outlines an agenda for realizing a world where the reproducibility and reusability barriers in research software are lifted, continuously accelerating research.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {123–128},
numpages = {6},
keywords = {reproducibility, artifact evaluation, continuous integration, scientific software, containers},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@article{10.1016/j.jss.2023.111667,
author = {Dai, Jie and Li, Qingshan and Xue, Hui and Luo, Zhao and Wang, Yinglin and Zhan, Siyuan},
title = {Graph collaborative filtering-based bug triaging},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {200},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111667},
doi = {10.1016/j.jss.2023.111667},
journal = {J. Syst. Softw.},
month = {jun},
numpages = {11},
keywords = {Software reliability engineering, Bug triaging, Deep graph learning, Graph collaborative filtering}
}

@inproceedings{10.1007/978-981-99-8073-4_19,
author = {Akash, Bathini Sai and Kumar, Lov and Singh, Vikram and Patel, Anoop Kumar and Krishna, Aneesh},
title = {Empirical Analysis of&nbsp;Multi-label Classification on&nbsp;GitterCom Using BERT and&nbsp;ML Classifiers},
year = {2023},
isbn = {978-981-99-8072-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8073-4_19},
doi = {10.1007/978-981-99-8073-4_19},
abstract = {To maintain development consciousness, simplify project coordination, and prevent misinterpretation, communication is essential for software development teams. Instant private messaging, group chats, and sharing code are just a few of the capabilities that chat rooms provide to assist and meet the communication demands of software development teams. All of this is capacitated to happen in real-time. Consequently, chat rooms have gained popularity among developers. Gitter is one of these platforms that has gained popularity, and the conversations it contains may be a treasure trove of data for academics researching open-source software systems. This research made use of the GitterCom dataset, The largest collection of Gitter developer messages that have been carefully labelled and curated and perform multi-label classification for the ’Purpose’ category in the dataset. An extensive empirical analysis is performed on 6 feature selection techniques, 14 machine learning classifiers, and BERT transformer layer architecture with layer-by-layer comparison. Consequently, we achieve proficient results through our research pipeline involving Extra Trees Classifier and Random Forest classifiers with AUC (OvR) median performance of 0.94 and 0.92 respectively. Furthermore, The research proposed research pipeline could be utilized for generic multi-label text classification on software developer forum text data.},
booktitle = {Neural Information Processing: 30th International Conference, ICONIP 2023, Changsha, China, November 20–23, 2023, Proceedings, Part V},
pages = {240–252},
numpages = {13},
keywords = {GitterCom, BERT analysis, Data Imbalance Methods, Feature Selection, Ensemble models, Sentence Embedding},
location = {<conf-loc content-type="InPerson">Changsha, China</conf-loc>}
}

@inproceedings{10.1109/ICSE48619.2023.00105,
author = {Yang, Chenyuan and Deng, Yinlin and Yao, Jiayi and Tu, Yuxing and Li, Hanchi and Zhang, Lingming},
title = {Fuzzing Automatic Differentiation in Deep-Learning Libraries},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00105},
doi = {10.1109/ICSE48619.2023.00105},
abstract = {Deep learning (DL) has attracted wide attention and has been widely deployed in recent years. As a result, more and more research efforts have been dedicated to testing DL libraries and frameworks. However, existing work largely overlooked one crucial component of any DL system, automatic differentiation (AD), which is the basis for the recent development of DL. To this end, we propose ∇Fuzz, the first general and practical approach specifically targeting the critical AD component in DL libraries. Our key insight is that each DL library API can be abstracted into a function processing tensors/vectors, which can be differentially tested under various execution scenarios (for computing outputs/gradients with different implementations). We have implemented ∇Fuzz as a fully automated API-level fuzzer targeting AD in DL libraries, which utilizes differential testing on different execution scenarios to test both first-order and high-order gradients, and also includes automated filtering strategies to remove false positives caused by numerical instability. We have performed an extensive study on four of the most popular and actively-maintained DL libraries, PyTorch, TensorFlow, JAX, and OneFlow. The result shows that ∇Fuzz substantially outperforms state-of-the-art fuzzers in terms of both code coverage and bug detection. To date, ∇Fuzz has detected 173 bugs for the studied DL libraries, with 144 already confirmed by developers (117 of which are previously unknown bugs and 107 are related to AD). Remarkably, ∇Fuzz contributed 58.3% (7/12) of all high-priority AD bugs for PyTorch and JAX during a two-month period. None of the confirmed AD bugs were detected by existing fuzzers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1174–1186},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3613872,
author = {Viggiato, Markos and Paas, Dale and Bezemer, Cor-Paul},
title = {Prioritizing Natural Language Test Cases Based on Highly-Used Game Features},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613872},
doi = {10.1145/3611643.3613872},
abstract = {Software testing is still a manual activity in many industries, such as the gaming industry. But manually executing tests becomes impractical as the system grows and resources are restricted, mainly in a scenario with short release cycles. Test case prioritization is a commonly used technique to optimize the test execution. However, most prioritization approaches do not work for manual test cases as they require source code information or test execution history, which is often not available in a manual testing scenario. In this paper, we propose a prioritization approach for manual test cases written in natural language based on the tested application features (in particular, highly-used application features). Our approach consists of (1) identifying the tested features from natural language test cases (with zero-shot classification techniques) and (2) prioritizing test cases based on the features that they test. We leveraged the NSGA-II genetic algorithm for the multi-objective optimization of the test case ordering to maximize the coverage of highly-used features while minimizing the cumulative execution time. Our findings show that we can successfully identify the application features covered by test cases using an ensemble of pre-trained models with strong zero-shot capabilities (an F-score of 76.1%). Also, our prioritization approaches can find test case orderings that cover highly-used application features early in the test execution while keeping the time required to execute test cases short. QA engineers can use our approach to focus the test execution on test cases that cover features that are relevant to users.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1961–1972},
numpages = {12},
keywords = {Feature usage, Multi-objective genetic algorithm, Test case prioritization, Zero- shot classification},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1007/s10664-023-10385-w,
author = {Guizzo, Giovani and Zhang, Jie M. and Sarro, Federica and Treude, Christoph and Harman, Mark},
title = {Mutation analysis for evaluating code translation},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10385-w},
doi = {10.1007/s10664-023-10385-w},
abstract = {Source-to-source code translation automatically translates a program from one programming language to another. The existing research on code translation evaluates the effectiveness of their approaches by using either syntactic similarities (e.g., BLEU score), or test execution results. The former does not consider semantics, the latter considers semantics but falls short on the problem of insufficient data and tests. In this paper, we propose MBTA (Mutation-based Code Translation Analysis), a novel application of mutation analysis for code translation assessment. We also introduce MTS (Mutation-based Translation Score), a measure to compute the level of trustworthiness of a translator. If a mutant of an input program shows different test execution results from its translated version, the mutant is killed and a translation bug is revealed. Fewer killed mutants indicate better code translation. MBTA is novel in the sense that mutants are compared to their translated counterparts, and not to their original program’s translation. We conduct a proof-of-concept case study with 612 Java-Python program pairs and 75,082 mutants on the code translators TransCoder and j2py to evaluate the feasibility of MBTA. The results reveal that TransCoder and j2py fail to translate 70.44% and 70.64% of the mutants, respectively, i.e., more than two-thirds of all mutants are incorrectly translated by these translators. By analysing the MTS results more closely, we were able to reveal translation bugs not captured by the conventional comparison between the original and translated programs.},
journal = {Empirical Softw. Engg.},
month = {dec},
numpages = {23},
keywords = {Mutation testing, Source to source translation, Code translation}
}

@article{10.1145/3622863,
author = {Chen, Qiaochu and Banerjee, Arko and Demiralp, Çağatay and Durrett, Greg and Dillig, Işıl},
title = {Data Extraction via Semantic Regular Expression Synthesis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622863},
doi = {10.1145/3622863},
abstract = {Many data extraction tasks of practical relevance require not only syntactic pattern matching but also semantic reasoning about the content of the underlying text. While regular expressions are very well suited for tasks that require only syntactic pattern matching, they fall short for data extraction tasks that involve both a syntactic and semantic component. To address this issue, we introduce semantic regexes, a generalization of regular expressions that facilitates combined syntactic and semantic reasoning about textual data. We also propose a novel learning algorithm that can synthesize semantic regexes from a small number of positive and negative examples. Our proposed learning algorithm uses a combination of neural sketch generation and compositional type-directed synthesis for fast and effective generalization from a small number of examples.  We have implemented these ideas in a new tool called Smore and evaluated it on representative data extraction tasks involving several textual datasets. Our evaluation shows that semantic regexes can better support complex data extraction tasks than standard regular expressions and that our learning algorithm significantly outperforms existing tools, including state-of-the-art neural networks and program synthesis tools.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {287},
numpages = {30},
keywords = {Program Synthesis, Regular Expression}
}

@article{10.1145/3611666,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Le Traon, Yves},
title = {LaF: Labeling-free Model Selection for Automated Deep Neural Network Reusing},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3611666},
doi = {10.1145/3611666},
abstract = {Applying deep learning (DL) to science is a new trend in recent years, which leads DL engineering to become an important problem. Although training data preparation, model architecture design, and model training are the normal processes to build DL models, all of them are complex and costly. Therefore, reusing the open-sourced pre-trained model is a practical way to bypass this hurdle for developers. Given a specific task, developers can collect massive pre-trained deep neural networks from public sources for reusing. However, testing the performance (e.g., accuracy and robustness) of multiple deep neural networks (DNNs) and recommending which model should be used is challenging regarding the scarcity of labeled data and the demand for domain expertise. In this article, we propose a labeling-free (LaF) model selection approach to overcome the limitations of labeling efforts for automated model reusing. The main idea is to statistically learn a Bayesian model to infer the models’ specialty only based on predicted labels. We evaluate LaF using nine benchmark datasets, including image, text, and source code, and 165 DNNs, considering both the accuracy and robustness of models. The experimental results demonstrate that LaF outperforms the baseline methods by up to 0.74 and 0.53 on Spearman’s correlation and Kendall’s τ, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {25},
numpages = {28},
keywords = {Deep neural network, model selection, labeling-free, Bayesian model}
}

@article{10.1145/3591867,
author = {Hutiri, Wiebke (Toussaint) and Ding, Aaron Yi and Kawsar, Fahim and Mathur, Akhil},
title = {Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3591867},
doi = {10.1145/3591867},
abstract = {Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {155},
numpages = {37},
keywords = {Bias, on-device machine learning, embedded machine learning, design choices, fairness, audio keyword spotting, personal data}
}

@inproceedings{10.5555/3618408.3618843,
author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
title = {PAL: program-aided language models},
year = {2023},
publisher = {JMLR.org},
abstract = {Large language models (LLMs) have demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and others. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using CODEX achieves state-of-the-art few-shot accuracy on GSM8K, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {435},
numpages = {36},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.1109/MS.2023.3265877,
author = {Ebert, Christof and Louridas, Panos and Ebert, Christof},
title = {Generative AI for Software Practitioners},
year = {2023},
issue_date = {July-Aug. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {40},
number = {4},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2023.3265877},
doi = {10.1109/MS.2023.3265877},
abstract = {Generative artificial intelligence (AI) tools, such as Bard, ChatGPT, and CoPilot, have rapidly gained widespread usage. They also have the potential to boost software engineering productivity. In this article, we elaborate technologies and usage of generative AI in the software industry. We address questions, such as: How does generative AI improve software productivity? How to connect generative AI to software development, and what are the risks? Which technologies have what sorts of benefits? Practitioner guidance and case studies are shared from our industry context. I look forward to hearing from you about this column and the technologies that matter most for your work.—Christof Ebert},
journal = {IEEE Softw.},
month = {jul},
pages = {30–38},
numpages = {9}
}

@article{10.1145/3641848,
author = {Huang, Zhenfei and Chen, Junjie and Jiang, Jiajun and Liang, Yihua and You, Hanmo and Li, Fengjie},
title = {Mapping APIs in Dynamic-typed Programs by Leveraging Transfer Learning},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641848},
doi = {10.1145/3641848},
abstract = {Application Programming Interface (API) migration is a common task for adapting software across different programming languages and platforms, where manually constructing the mapping relations between APIs is indeed time-consuming and error-prone. To facilitate this process, many automated API mapping approaches have been proposed. However, existing approaches were mainly designed and evaluated for mapping APIs of statically-typed languages, while their performance on dynamically-typed languages remains unexplored. In this paper, we conduct the first extensive study to explore existing API mapping approaches’ performance for mapping APIs in dynamically-typed languages, for which we have manually constructed a high-quality dataset. According to the empirical results, we have summarized several insights. In particular, the source code implementations of APIs can significantly improve the effectiveness of API mapping. However, due to the confidentiality policy, they may not be available in practice. To overcome this, we propose a novel API mapping approach, named Matl, which leverages the transfer learning technique to learn the semantic embeddings of source code implementations from large-scale open-source repositories and then transfers the learned model to facilitate the mapping of APIs. In this way, Matl can produce more accurate API embedding of its functionality for more effective mapping without knowing the source code of the APIs. To evaluate the performance of Matl, we have conducted an extensive study by comparing Matl with state-of-the-art approaches. The results demonstrate that Matl is indeed effective as it improves the state-of-the-art approach by at least 18.36% for mapping APIs of dynamically-typed language and by 30.77% for mapping APIs of the statically-typed language.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {API mapping, Program transformation, Transfer learning}
}

@article{10.1145/3607181,
author = {Mo, Ran and Zhang, Yao and Wang, Yushuo and Zhang, Siyuan and Xiong, Pu and Li, Zengyang and Zhao, Yang},
title = {Exploring the Impact of Code Clones on Deep Learning Software},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607181},
doi = {10.1145/3607181},
abstract = {Deep learning (DL) is a really active topic in recent years. Code cloning is a common code implementation that could negatively impact software maintenance. For DL software, developers rely heavily on frameworks to implement DL features. Meanwhile, to guarantee efficiency, developers often reuse the steps and configuration settings for building DL models. These may bring code copy-pastes or reuses inducing code clones. However, there is little work exploring code clones’ impact on DL software. In this article, we conduct an empirical study and show that: (1) code clones are prevalent in DL projects, about 16.3% of code fragments encounter clones, which is almost twice larger than the traditional projects; (2) 75.6% of DL projects contain co-changed clones, meaning changes are propagated among cloned fragments, which can bring maintenance difficulties; (3)&nbsp;Percentage of the clones and Number of clone lines are associated with the emergence of co-changes; (4) the prevalence of Code clones varies in DL projects with different frameworks, but the difference is not significant; (5) Type 1 co-changed clones often spread over different folders, but Types 2 and 3 co-changed clones mainly occur within the same files or folders; (6) 57.1% of all co-changed clones are involved in bugs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {153},
numpages = {34},
keywords = {Deep learning software, code clone, co-changed clone}
}

@article{10.1007/s10515-023-00399-5,
author = {Dessureault, Jean-Sébastien and Massicotte, Daniel},
title = {AI2: the next leap toward native language-based and explainable machine learning framework},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00399-5},
doi = {10.1007/s10515-023-00399-5},
abstract = {The machine learning frameworks flourished in the last decades, allowing artificial intelligence to get out of academic circles to be applied to enterprise domains. This field has significantly advanced, but there is still some meaningful improvement to reach the subsequent expectations. The proposed framework, named AI2, uses a natural language interface that allows non-specialists to benefit from machine learning algorithms without necessarily knowing how to program with a programming language. The primary contribution of the AI2 framework allows a user to call the machine learning algorithms in English, making its interface usage easier. The second contribution is greenhouse gas (GHG) awareness. It has some strategies to evaluate the GHG generated by the algorithm to be called and to propose alternatives to find a solution without executing the energy-intensive algorithm. Another contribution is a preprocessing module that helps to describe and to load data properly. Using an English text-based chatbot, this module guides the user to define every dataset so that it can be described, normalized, loaded, and divided appropriately. The last contribution of this paper is about explainability. The scientific community has known that machine learning algorithms imply the famous black-box problem for decades. Traditional machine learning methods convert an input into an output without being able to justify this result. The proposed framework explains the algorithm’s process with the proper texts, graphics, and tables. The results, declined in five cases, present usage applications from the user’s English command to the explained output. Ultimately, the AI2 framework represents the next leap toward native language-based, human-oriented concerns about machine learning framework.},
journal = {Automated Software Engg.},
month = {sep},
numpages = {28},
keywords = {Machine learning, Framework, NLP, AI ethics, Explainability}
}

@inproceedings{10.1145/3622758.3622893,
author = {Avishahar-Zeira, Assaf and Lorenz, David H.},
title = {Could No-Code Be Code? Toward a No-Code Programming Language for Citizen Developers},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622758.3622893},
doi = {10.1145/3622758.3622893},
abstract = {By 2030 for each filled position in Software Engineering,  
two positions would remain unfilled. This already  
apparent loss of productivity has the software  
industry scrambling to fill the missing positions with  
citizen developers---technical people with little or no  
programming skills---who would be using No-Code platforms  
to program various software solutions in specific domains.  
However, currently available platforms have fairly limited  
abstractions, lacking the flexibility of a general purpose  
programming language.  

To break the No-Code abstraction barrier, a very simple yet  
expressive general purpose No-Code programming language  
might provide citizen developers with an alternative to  
domain-specific No-Code platforms. Unfortunately, these  
requirements seem contradictory. Making a language very  
simple and specific might render it crippled, thus limited  
to a certain domain of problems. Conversely, making a  
language very expressive and general, might render it too  
complicated for citizen developers.  

In this work we argue that a multi-paradigm minimalist  
approach can bridge the gap between simplicity and  
expressiveness by including only abstractions considered  
intuitive to citizens. As a concrete proof-of-concept, we  
present a general purpose programming language designed  
for citizen developers that is on the one hand very  
powerful and on the other hand very simple. In fact,  
this language is so simple that the entire development  
is accomplished by flowcharts using mouse actions only,  
without typing a single line of code, thus demonstrating  
a general purpose No-Code programming language candidate  
for citizen developers.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {103–119},
numpages = {17},
keywords = {Citizen Developers, Golang, No-Code Software Development, Programming Language Design, Projectional Editing},
location = {<conf-loc>, <city>Cascais</city>, <country>Portugal</country>, </conf-loc>},
series = {Onward! 2023}
}

@article{10.1016/j.infsof.2023.107304,
author = {Vo, Hieu Dinh and Nguyen, Son},
title = {Can an old fashioned feature extraction and a light-weight model improve vulnerability type identification performance?},
year = {2023},
issue_date = {Dec 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {164},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107304},
doi = {10.1016/j.infsof.2023.107304},
journal = {Inf. Softw. Technol.},
month = {dec},
numpages = {9},
keywords = {Vulnerability type identification, Vulnerability resolution, Software vulnerability}
}

@inproceedings{10.1109/ICSE-SEIS58686.2023.00016,
author = {Rink, Konstantin and Gruschka, Tristan and Palsbröker, Patrick and Baez, Marcos and Becking, Dominic and Seelmeyer, Udo and Dobslaw, Gudrun and Stolz, Patricia},
title = {Walking Down the Road to Independent Mobility: An Adaptive Route Training System for the Cognitively Impaired},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS58686.2023.00016},
doi = {10.1109/ICSE-SEIS58686.2023.00016},
abstract = {In this paper we describe the design and development of a route training system for individuals with cognitive impairments (CIs) living in residential care facilities. Learning to move autonomously in public spaces is a fundamental skill for people with CI, who face several challenges to independently and safely move around. Yet, exploring opportunities for route training support, especially in residential settings, has received very little attention. To explore these opportunities, we followed a design and development process based on inclusive design practices that considered the organisational context and aimed at involving people with CI in the software design. To ensure our solution addressed the identified needs and abilities of this heterogeneous population, we further framed the route training definition as a design process that is enacted by the system, making the trainer and user co-creators of a personalised training. In this paper we report on the needs and challenges for mobility training in residential settings, introduce the design and formative evaluation of the route training system, to conclude with reflections and considerations on our methodological approach.Learning to navigate public spaces without assistance is important for people with cognitive impairments (CIs). It can help them overcome challenges to independently and safely reach places in their daily lives. Yet, the use of technology for route learning has not been fully explored in research, especially for people with CIs living in residential care. In this article, we describe the process of developing a route training system that explores the use of technology support. In our research, we tried different ways to involve people with CIs in the design of the system, which is seen as a more inclusive approach to designing solutions. We identified that people with CIs have different needs and abilities when it comes to route learning and related skills. For this reason, our solution focuses on ways to personalise the training, making sure people with CIs are involved in the personalisation, so that it fits their needs, abilities and learning progress.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
pages = {108–119},
numpages = {12},
keywords = {route training, cognitive impairments, inclusive design},
location = {Melbourne, Australia},
series = {ICSE-SEIS '23}
}

@inproceedings{10.1109/ICSE48619.2023.00090,
author = {Qi, Binhang and Sun, Hailong and Gao, Xiang and Zhang, Hongyu and Li, Zhaotian and Liu, Xudong},
title = {Reusing Deep Neural Network Models through Model Re-Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00090},
doi = {10.1109/ICSE48619.2023.00090},
abstract = {Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time. With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently. Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack. To solve the above problem, we propose SeaM, a tool that re-engineers a trained DNN model to improve its reusability. Specifically, given a target problem and a trained model, SeaM utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem. The re-engineered model that only retains the relevant weights is then reused to solve the target problem. Evaluation results on widely-used models show that the re-engineered models produced by SeaM only contain 10.11% weights of the original models, resulting 42.41% reduction in terms of inference time. For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85%. Moreover, reusing the re-engineered models inherits an average of 57% fewer defects than reusing the entire model. We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {983–994},
numpages = {12},
keywords = {model reuse, deep neural network, re-engineering, DNN modularization},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597926.3598083,
author = {Li, Chengpeng and Khosravi, M. Mahdi and Lam, Wing and Shi, August},
title = {Systematically Producing Test Orders to Detect Order-Dependent Flaky Tests},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598083},
doi = {10.1145/3597926.3598083},
abstract = {Software testing suffers from the presence of flaky tests, which can pass or fail when run on the same version of code. Order- dependent tests (OD tests) are flaky tests whose outcome depends on the order in which they are run. An OD test can be detected if specific tests are run or not run before it, resulting in a difference in test outcome. While prior work has proposed rerunning tests in different random test orders, this approach does not provide guarantees toward detecting all OD tests. Later work that proposed a more systematic approach to ordering tests still fails to account for the relationships between all tests in the test suite.  
We propose three new techniques to detect OD tests through a more systematic means of producing test orders. Our techniques build upon prior work in Tuscan squares to cover test pairs in a minimal set of test orders while also obeying the constraints of how tests can be positioned in a test order w.r.t. their test classes. Further, as there are many test pairs that need to be covered, we develop a technique that can take a specified set of test pairs to cover and produce test orders that aim to cover just those test pairs. Our evaluation with 289 known OD tests across 47 test suites from open-source projects shows that our most cost-effective technique can detect 97.2% of the known OD tests with 104.7 test orders, on average, per subject. While all techniques produce a relatively large number of test orders, our analysis of the minimal set of test orders needed to detect OD tests shows a tremendous reduction in the test orders needed to detect OD tests – representing an opportunity for future work to prioritize test orders.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {627–638},
numpages = {12},
keywords = {flaky test detection, order-dependent flaky test},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.5555/3615924.3615941,
author = {Savas, Yildirim and Mucahit, Cevik and Ayşe, Başar},
title = {Few-shot Learning Approaches to Software Requirement Quality Prediction},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Computer-aided processes can be used to improve the quality of requirements in software projects. These processes, in principle, can be divided into two stages: requirement quality evaluation, essentially a classification task, and requirement correction sugges-tions, fundamentally a language generation problem. This study particularly focuses on the initial phase –requirement quality evalu-ation, for which we develop a machine learning infrastructure. Our approach aligns itself with the universally recognized assumption that ideal requirements should be uncomplicated, precise, atomic, and succinct. We have a two-step solution to evaluate the qual-ity of requirements. The first s tep e mploys f ew-shot techniques, addressing the issue of limited access to labeled data in practi-cal contexts. The second step involves adapting a language model with domain-specific data, given that language models trained on general-purpose texts might struggle to accurately capture patterns and meanings in specific d omains. O ur p ipeline p rimarily lever-ages contrastive learning objectives in both steps of our approach. Furthermore, we consider various robust baselines for compara-tive analysis. Numerical results illustrate that our quality detection model effectively discriminates between good quality and bad qual-ity requirements, thereby providing valuable input for subsequent requirement generation and suggestion models.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {155–160},
numpages = {6},
keywords = {Few-shot Learning, Contrastive Learning, Domain Adaption, Software Requirement Specifications, Transformer Models},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3597926.3598054,
author = {Cui, Lei and Cui, Jiancong and Ji, Yuede and Hao, Zhiyu and Li, Lun and Ding, Zhenquan},
title = {API2Vec: Learning Representations of API Sequences for Malware Detection},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598054},
doi = {10.1145/3597926.3598054},
abstract = {Analyzing malware based on API call sequence is an effective approach as the sequence reflects the dynamic execution behavior of malware.Recent advancements in deep learning have led to the application of these techniques for mining useful information from API call sequences. However, these methods mainly operate on raw sequences and may not effectively capture important information especially for multi-process malware, mainly due to the API call interleaving problem.  

Motivated by that, this paper presents API2Vec, a graph based API embedding method for malware detection. First, we build a graph model to represent the raw sequence. In particular, we design the temporal process graph (TPG) to model inter-process behavior and temporal API graph (TAG) to model intra-process behavior. With such graphs, we design a heuristic random walk algorithm to generate a number of paths that can capture the fine-grained malware behavior. By pre-training the paths using the Doc2Vec model, we are able to generate the embeddings of paths and APIs, which can further be used for malware detection. The experiments on a real malware dataset demonstrate that API2Vec outperforms the state-of-the-art embedding methods and detection methods for both accuracy and robustness, especially for multi-process malware.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–273},
numpages = {13},
keywords = {Deep Learning, Embedding, Malware Detection, Random Walk},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3611643.3616291,
author = {Gao, Haoyu and Treude, Christoph and Zahedi, Mansooreh},
title = {Evaluating Transfer Learning for Simplifying GitHub READMEs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616291},
doi = {10.1145/3611643.3616291},
abstract = {Software documentation captures detailed knowledge about a software product, e.g., code, technologies, and design. It plays an important role in the coordination of development teams and in conveying ideas to various stakeholders. However, software documentation can be hard to comprehend if it is written with jargon and complicated sentence structure. In this study, we explored the potential of text simplification techniques in the domain of software engineering to automatically simplify GitHub README files. We collected software-related pairs of GitHub README files consisting of 14,588 entries, aligned difficult sentences with their simplified counterparts, and trained a Transformer-based model to automatically simplify difficult versions. To mitigate the sparse and noisy nature of the software-related simplification dataset, we applied general text simplification knowledge to this field. Since many general-domain difficult-to-simple Wikipedia document pairs are already publicly available, we explored the potential of transfer learning by first training the model on the Wikipedia data and then fine-tuning it on the README data. Using automated BLEU scores and human evaluation, we compared the performance of different transfer learning schemes and the baseline models without transfer learning. The transfer learning model using the best checkpoint trained on a general topic corpus achieved the best performance of 34.68 BLEU score and statistically significantly higher human annotation scores compared to the rest of the schemes and baselines. We conclude that using transfer learning is a promising direction to circumvent the lack of data and drift style problem in software README files simplification and achieved a better trade-off between simplification and preservation of meaning.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1548–1560},
numpages = {13},
keywords = {GitHub, Software Documentation, Text Simplification, Transfer Learning},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00136,
author = {Monjezi, Verya and Trivedi, Ashutosh and Tan, Gang and Tizpaz-Niari, Saeid},
title = {Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00136},
doi = {10.1109/ICSE48619.2023.00136},
abstract = {The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions---amplifying existing biases or introducing new ones---that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids---such as severity and causal explanations---crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs.The key goal of DICE is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quantitative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that DICE efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances (vis-a-vis the state-of-the-art techniques), and localizes layers/neurons with significant biases.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1571–1582},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3604237.3626867,
author = {Lakkaraju, Kausik and Jones, Sara E and Vuruma, Sai Krishna Revanth and Pallagani, Vishal and Muppasani, Bharath C and Srivastava, Biplav},
title = {LLMs for Financial Advisement: A Fairness and Efficacy Study in Personal Decision Making},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626867},
doi = {10.1145/3604237.3626867},
abstract = {As Large Language Model (LLM) based chatbots are becoming more accessible, users are relying on these chatbots for reliable and personalized recommendations in diverse domains, ranging from code generation to financial advisement. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We test widely used LLM-based chatbots, ChatGPT and Bard, and compare their performance against SafeFinance, a rule-based chatbot built using the Rasa platform. The comparison is across two critical tasks: product discovery and multi-product interaction, where product refers to banking products like Credit Cards, Certificate of Deposits, and Checking Accounts. With this study, we provide interesting insights into the chatbots’ efficacy in financial advisement and their ability to provide fair treatment across different user groups. We find that both Bard and ChatGPT can make errors in retrieving basic online information, the responses they generate are inconsistent across different user groups, and they cannot be relied on for reasoning involving banking products. On the other hand, despite their limited generalization capabilities, rule-based chatbots like SafeFinance provide safe and reliable answers to users that can be traced back to their original source. Overall, although the outputs of the LLM-based chatbots are fluent and plausible, there are still critical gaps in providing consistent and reliable financial information.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {100–107},
numpages = {8},
location = {<conf-loc>, <city>Brooklyn</city>, <state>NY</state>, <country>USA</country>, </conf-loc>},
series = {ICAIF '23}
}

@article{10.1016/j.infsof.2023.107263,
author = {Méndez, Manuel and Benito-Parejo, Miguel and Ibias, Alfredo and Núñez, Manuel},
title = {Metamorphic testing of chess engines},
year = {2023},
issue_date = {Oct 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {162},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107263},
doi = {10.1016/j.infsof.2023.107263},
journal = {Inf. Softw. Technol.},
month = {oct},
numpages = {15},
keywords = {Software testing, Metamorphic testing, Chess engines}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00008,
author = {Chaaben, Meriem Ben and Burgueño, Lola and Sahraoui, Houari},
title = {Towards Using Few-Shot Prompt Learning for Automating Model Completion},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00008},
doi = {10.1109/ICSE-NIER58687.2023.00008},
abstract = {We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {7–12},
numpages = {6},
keywords = {language models, few-shot learning, prompt learning, domain modeling, model completion},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE48619.2023.00204,
author = {Le, Van-Hoang and Zhang, Hongyu},
title = {Log Parsing with Prompt-Based Few-Shot Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00204},
doi = {10.1109/ICSE48619.2023.00204},
abstract = {Logs generated by large-scale software systems provide crucial information for engineers to understand the system status and diagnose problems of the systems. Log parsing, which converts raw log messages into structured data, is the first step to enabling automated log analytics. Existing log parsers extract the common part as log templates using statistical features. However, these log parsers often fail to identify the correct templates and parameters because: 1) they often overlook the semantic meaning of log messages, and 2) they require domain-specific knowledge for different log datasets. To address the limitations of existing methods, in this paper, we propose LogPPT to capture the patterns of templates using prompt-based few-shot learning. LogPPT utilises a novel prompt tuning method to recognise keywords and parameters based on a few labelled log data. In addition, an adaptive random sampling algorithm is designed to select a small yet diverse training set. We have conducted extensive experiments on 16 public log datasets. The experimental results show that LogPPT is effective and efficient for log parsing.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2438–2449},
numpages = {12},
keywords = {log parsing, few-shot learning, prompt-tuning, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1007/s10515-023-00401-0,
author = {Wu, Di and Feng, Yang and Zhang, Hongyu and Xu, Baowen},
title = {Automatic recognizing relevant fragments of APIs using API references},
year = {2023},
issue_date = {May 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00401-0},
doi = {10.1007/s10515-023-00401-0},
abstract = {API tutorials are crucial resources as they often provide detailed explanations of how to utilize APIs. Typically, an API tutorial is segmented into a number of consecutive fragments.. If a fragment explains API usage, we regard it as a relevant fragment of the API. Recognizing relevant fragments can aid developers in comprehending, learning, and using APIs. Recently, some studies have presented relevant fragments recognition approaches, which mainly focused on using API tutorials or Stack Overflow to train the recognition model. API references are also important API learning resources as they contain abundant API knowledge. Considering the similarity between API tutorials and API references (both provide API knowledge), we believe that using API knowledge from API references could help recognize relevant tutorial fragments of APIs effectively. However, it is non-trivial to leverage API references to build a supervised learning-based recognition model. Two major problems are the lack of labeled API references and the unavailability of engineered features of API references. We propose a supervised learning based approach named RRTR (which stands for Recognize Relevant Tutorial fragments using API References) to address the above problems. For the problem of lacking labeled API references, RRTR designs heuristic rules to automatically collect relevant and irrelevant API references for APIs. Regarding the unavailable engineered features issue, we adopt the pre-trained SBERT model (SBERT stands for Sentence-BERT) to automatically learn semantic features for API references. More specifically, we first automatically generate labeled API,ARE pairs (ARE stands for an API reference) via our heuristic rules of API references. We then use SBERT to automatically learn semantic features for the collected pairs and train a supervised learning based recognition model. Finally, we can recognize the relevant tutorial fragments of APIs based on the trained model. To evaluate the effectiveness of RRTR, we collected Java and Android API reference datasets containing a total of 20,680 labeled API,ARE pairs. Experimental results demonstrate that RRTR outperforms state-of-the-art approaches in terms of F-Measure on two datasets. In addition, we conducted a user study to investigate the practicality of RRTR and the results further illustrate the effectiveness of RRTR in practice. The proposed RRTR approach can effectively recognize relevant fragments of APIs with API references by solving the problems of lacking labeled API references and engineered features of API references.},
journal = {Automated Software Engg.},
month = {nov},
numpages = {33},
keywords = {API tutorials, API references, Relevance generation, Learned features}
}

@inproceedings{10.1109/ICSE48619.2023.00081,
author = {Bouzenia, Islem and Pradel, Michael},
title = {When to Say What: Learning to Find Condition-Message Inconsistencies},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00081},
doi = {10.1109/ICSE48619.2023.00081},
abstract = {Programs often emit natural language messages, e.g., in logging statements or exceptions raised on unexpected paths. To be meaningful to users and developers, the message, i.e., what to say, must be consistent with the condition under which it gets triggered, i.e., when to say it. However, checking for inconsistencies between conditions and messages is challenging because the conditions are expressed in the logic of the programming language, while messages are informally expressed in natural language. This paper presents CMI-Finder, an approach for detecting condition-message inconsistencies. CMI-Finder is based on a neural model that takes a condition and a message as its input and then predicts whether the two are consistent. To address the problem of obtaining realistic, diverse, and large-scale training data, we present six techniques to generate large numbers of inconsistent examples to learn from automatically. Moreover, we describe and compare three neural models, which are based on binary classification, triplet loss, and fine-tuning, respectively. Our evaluation applies the approach to 300K condition-message statements extracted from 42 million lines of Python code. The best model achieves a precision of 78% at a recall of 72% on a dataset of past bug fixes. Applying the approach to the newest versions of popular open-source projects reveals 50 previously unknown bugs, 19 of which have been confirmed by the developers so far.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {868–880},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1007/s11219-023-09646-0,
author = {Marijan, Dusica},
title = {Comparative study of machine learning test case prioritization for continuous integration testing},
year = {2023},
issue_date = {Dec 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-023-09646-0},
doi = {10.1007/s11219-023-09646-0},
abstract = {There is a growing body of research indicating the potential of machine learning to tackle complex software testing challenges. One such challenge pertains to continuous integration testing, which is highly time-constrained, and generates a large amount of data coming from iterative code commits and test runs. In such a setting, we can use plentiful test data for training machine learning predictors to identify test cases able to speed up the detection of regression bugs introduced during code integration. However, different machine learning models can have different fault prediction performance depending on the context and the parameters of continuous integration testing, for example, variable time budget available for continuous integration cycles, or the size of test execution history used for learning to prioritize failing test cases. Existing studies on test case prioritization rarely study both of these factors, which are essential for the continuous integration practice. In this study, we perform a comprehensive comparison of the fault prediction performance of machine learning approaches that have shown the best performance on test case prioritization tasks in the literature. We evaluate the accuracy of the classifiers in predicting fault-detecting tests for different values of the continuous integration time budget and with different lengths of test history used for training the classifiers. In evaluation, we use real-world and augmented industrial datasets from a continuous integration practice. The results show that different machine learning models have different performance for different size of test history used for model training and for different time budgets available for test case execution. Our results imply that machine learning approaches for test prioritization in continuous integration testing should be carefully configured to achieve optimal performance.},
journal = {Software Quality Journal},
month = {jul},
pages = {1415–1438},
numpages = {24},
keywords = {Machine learning, Neural networks, Support vector regression, Gradient boosting, Learning to rank, Continuous integration, Software testing, Regression testing, Test prioritization, Test selection, Test optimization}
}

@article{10.1145/3617174,
author = {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Peng, Xin and Xu, Xiwei and Lu, Qinghua},
title = {FQN Inference in Partial Code by Prompt-tuned Language Model of Code},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617174},
doi = {10.1145/3617174},
abstract = {Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a prompt-tuned code masked language model (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {31},
numpages = {32},
keywords = {Type inference, fully qualified names, code masked language model, neural knowledge base}
}

@inproceedings{10.1109/ICSE48619.2023.00076,
author = {Li, Jiawei and Ahmed, Iftekhar},
title = {Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00076},
doi = {10.1109/ICSE48619.2023.00076},
abstract = {Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {806–817},
numpages = {12},
keywords = {commit message quality, software defect proneness, empirical analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.engappai.2023.106110,
author = {Panda, Rama Ranjan and Nagwani, Naresh Kumar},
title = {An intuitionistic fuzzy representation based software bug severity prediction approach for imbalanced severity classes},
year = {2023},
issue_date = {Jun 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106110},
doi = {10.1016/j.engappai.2023.106110},
journal = {Eng. Appl. Artif. Intell.},
month = {jun},
numpages = {18},
keywords = {Software reliability, Intuitionistic fuzzy similarity, Software maintenance, Severity prediction, Machine learning, Topic modeling}
}

@article{10.1016/j.scico.2023.102994,
author = {Moharil, Ambarish and Sharma, Arpit},
title = {TABASCO: A transformer based contextualization toolkit},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {230},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2023.102994},
doi = {10.1016/j.scico.2023.102994},
journal = {Sci. Comput. Program.},
month = {aug},
numpages = {16},
keywords = {Ambiguity, Requirements, Quality, Natural language, Transformer}
}

@inproceedings{10.1145/3579371.3589072,
author = {Liang, Mingyu and Fu, Wenyin and Feng, Louis and Lin, Zhongyi and Panakanti, Pavani and Zheng, Shengbao and Sridharan, Srinivas and Delimitrou, Christina},
title = {Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589072},
doi = {10.1145/3579371.3589072},
abstract = {Building large AI fleets to support the rapidly growing DL workloads is an active research topic for modern cloud providers. Generating accurate benchmarks plays an essential role in designing the fast-paced software and hardware solutions in this space. Two fundamental challenges to make this scalable are (i) workload representativeness and (ii) the ability to quickly incorporate changes to the fleet into the benchmarks.To overcome these issues, we propose Mystique, an accurate and scalable framework for production AI benchmark generation. It leverages the PyTorch execution trace (ET), a new feature that captures the runtime information of AI models at the granularity of operators, in a graph format, together with their metadata. By sourcing fleet ETs, we can build AI benchmarks that are portable and representative. Mystique is scalable, due to its lightweight data collection, in terms of runtime overhead and instrumentation effort. It is also adaptive because ET composability allows flexible control on benchmark creation.We evaluate our methodology on several production AI models, and show that benchmarks generated with Mystique closely resemble original AI models, both in execution time and system-level metrics. We also showcase the portability of the generated benchmarks across platforms, and demonstrate several use cases enabled by the fine-grained composability of the execution trace.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {37},
numpages = {13},
keywords = {artificial intelligence, cloud computing, benchmarking, performance cloning, code generation},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3618305.3623588,
author = {Gomes, Luís},
title = {Transforming Ideas into Code: Visual Sketching for ML Development},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623588},
doi = {10.1145/3618305.3623588},
abstract = {We propose a novel code assistant and generation paradigm aimed at closing the gap between visual sketching and code creation for Machine Learning (ML) development. This approach empowers developers and ML practitioners to translate hand-drawn sketches into functional code with enhanced accuracy and usability. Developers are recruited to assess the tool's performance. This research contributes to the future of low-code approaches, facilitating ML application development, and promoting an intuitive and accessible programming environment.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {10–12},
numpages = {3},
keywords = {Code generation, Machine learning, Synthetic data, Tool development, Visual sketching},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@inproceedings{10.1145/3593434.3593453,
author = {Agbese, Mamia and Mohanani, Rahul and Khan, Arif and Abrahamsson, Pekka},
title = {Implementing AI Ethics: Making Sense of the Ethical Requirements},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593453},
doi = {10.1145/3593434.3593453},
abstract = {Society’s increasing dependence on Artificial Intelligence (AI) and AI-enabled systems require a more practical approach from software engineering (SE) executives in middle and higher-level management to improve their involvement in implementing AI ethics by making ethical requirements part of their management practices. However, research indicates that most work on implementing ethical requirements in SE management primarily focuses on technical development, with scarce findings for middle and higher-level management. We investigate this by interviewing ten Finnish SE executives in middle and higher-level management to examine how they consider and implement ethical requirements. We use ethical requirements from the European Union (EU) Trustworthy Ethics guidelines for Trustworthy AI as our reference for ethical requirements and an Agile portfolio management framework to analyze implementation. Our findings reveal a general consideration of privacy and data governance ethical requirements as legal requirements with no other consideration for ethical requirements identified. The findings also show practicable consideration of ethical requirements as technical robustness and safety for implementation as risk requirements and societal and environmental well-being for implementation as sustainability requirements. We examine a practical approach to implementing ethical requirements using the ethical risk requirements stack employing the Agile portfolio management framework.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {62–71},
numpages = {10},
keywords = {AI, AI ethics, AI ethics principles, Agile portfolio management, Ethical requirements, Ethical requirements stack},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00034,
author = {Luo, Linghui and Mukherjee, Rajdeep and Tripp, Omer and Schäf, Martin and Zhou, Qiang and Sanchez, Daniel},
title = {Long-Term Static Analysis Rule Quality Monitoring Using True Negatives},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00034},
doi = {10.1109/ICSE-SEIP58684.2023.00034},
abstract = {Static application security testing (SAST) tools have found broad adoption in modern software development workflows. These tools employ a variety of static analysis rules to generate recommendations on how to improve the code of an application.Every recommendation consumes the time of the engineer that is investigating it, so it is important to measure how useful these rules are in the long term. But what is a good metric for monitoring rule quality over time? Counting the number of recommendations rewards noisy rules and ignores developers' reactions. Measuring fix rate is not ideal either, because it overemphasizes rules that are easy to fix.In this paper, we report on an experiment where we use the frequency of true negatives to quantify if developers are able to learn a static analysis rule. We consider a static analysis rule to be ideal if its recommendations are not only addressed, but also internalized by the developer in a way that prevents the bug from recurring. That is, the rule contributes to code quality not only at present, but also in the future. We measure how often developers produce true negatives, that is, code changes that are relevant to a rule but do not trigger a recommendation, and we compare true-negative rate against other metrics. Our results show that measuring true negatives provides insights that cannot be provided by metrics such as fix rate or developer feedback.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {315–326},
numpages = {12},
keywords = {software security, static analysis},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3587716.3587760,
author = {Usami, Yoshiyuki and Kitaoka, Kosuke and Shindo, Koichi},
title = {Integrated Artificial Intelligence for Making Digital Human},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587760},
doi = {10.1145/3587716.3587760},
abstract = {Artificial intelligence is actively researched in various fields such as image recognition, image detection, audio recognition, natural language processing, face expression recognition, and facial expression generation. If we want to create artificial intelligence in the original sense, it will be necessary to integrate these many research results and create a system that can exactly imitate the functions of the human brain. Commercially, the current situation is that integrated AI such as Ameria [2], Uneeq [3], Neon [13], LaMDA [29] and the system using GPT-3 [9] have entered the market. However, there is no research that creates integrated AI with open source in the academic field. This work is an attempt to construct such an integrated AI as an academic research which is in an form of open source. Furthermore, this work is described in a form of multi-processing job with socket connection. Then, execution of the program can be accomplished by multiple computers. For the visual input, object detection is performed by Redman’s YOLO [14]. Next, the system accomplishes Image2text which generates sentences describing the image [34]. The system recognizes the meaning of visual input. As for speech recognition, the question and answering task is activated, and it is possible to give an accurate answer to the question through the microphone [7]. In addition, text generation enables this system to respond to human chattering [5]. This work combines four different sources: visual, text, audio, and scraping outworld news sources. We believe that attempts like this work will become more common in future AI studies.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {267–273},
numpages = {7},
keywords = {mage2text, question and answering, text generation, visual object detection, visual object recognition},
location = {<conf-loc>, <city>Zhuhai</city>, <country>China</country>, </conf-loc>},
series = {ICMLC '23}
}

@inproceedings{10.1145/3597926.3598047,
author = {Yu, Shiwen and Wang, Ting and Wang, Ji},
title = {Loop Invariant Inference through SMT Solving Enhanced Reinforcement Learning},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598047},
doi = {10.1145/3597926.3598047},
abstract = {Inferring loop invariants is one of the most challenging problems in program verification. It is highly desired to incorporate machine learning when inferring. This paper presents a Reinforcement Learning (RL) pruning framework to infer loop invariants over a general nonlinear hypothesis space. The key idea is to synergize the RL-based pruning and SMT solving to generate candidate invariants efficiently. To address the sparse reward problem in learning, we design a novel two-dimensional reward mechanism that enables the RL pruner to recognize the capability boundary of SMT solvers and learn the pruning heuristics in a few rounds. We have implemented our approach with Z3 SMT solver in the tool called LIPuS and conducted extensive experiments over the linear and nonlinear benchmarks. Experiment results show that LIPuS can solve the most cases compared to the state-of-the-art loop invariant inference tools such as Code2Inv, ICE-DT, GSpacer, SymInfer, ImplCheck, and Eldarica. Especially, LIPuS outperforms them significantly on nonlinear benchmarks.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {175–187},
numpages = {13},
keywords = {loop invariant, program verification, reinforcement learning},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1007/s10766-023-00758-5,
author = {Hakimi, Yacine and Baghdadi, Riyadh and Challal, Yacine},
title = {A Hybrid Machine Learning Model for Code Optimization},
year = {2023},
issue_date = {Dec 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-023-00758-5},
doi = {10.1007/s10766-023-00758-5},
abstract = {The complexity of programming modern heterogeneous systems raises huge challenges. Over the past two decades, researchers have aimed to alleviate these difficulties by employing classical Machine Learning and Deep Learning techniques within compilers to optimize code automatically. This work presents a novel approach to optimize code using at the same time Classical Machine Learning and Deep Learning techniques by maximizing their benefits while mitigating their drawbacks. Our proposed model extracts features from the code using Deep Learning and then applies Classical Machine Learning to map these features to specific outputs for various tasks. The effectiveness of our model is evaluated on three downstream tasks: device mapping, optimal thread coarsening, and algorithm classification. Our experimental results demonstrate that our model outperforms previous models in device mapping with an average accuracy of 91.60% on two datasets and in&nbsp;optimal thread coarsening task where we are the first to achieve a positive speedup on all four platforms while achieving a comparable result of 91.48% in the algorithm classification task. Notably, our approach yields better results even with a small dataset without requiring a pre-training phase or a complex code representation, offering the advantage of reducing training time and data volume requirements.},
journal = {Int. J. Parallel Program.},
month = {sep},
pages = {309–331},
numpages = {23},
keywords = {Machine learning, Deep learning, Code optimizations, LLVM-IR, Heterogeneous platforms, Thread coarsening, Algorithm classification, Code mapping}
}

@inproceedings{10.5555/3618408.3619073,
author = {Khakhar, Adam and Mell, Stephen and Bastani, Osbert},
title = {PAC prediction sets for large language models of code},
year = {2023},
publisher = {JMLR.org},
abstract = {Prediction sets have recently been shown to be a promising strategy for quantifying the uncertainty of deep neural networks in a way that provides theoretical guarantees. However, existing techniques have largely targeted settings where the space of labels is simple, so prediction sets can be arbitrary subsets of labels. For structured prediction problems where the space of labels is exponential in size, even prediction sets containing a small fraction of all labels can be exponentially large. In the context of code generation, we propose a solution that considers a restricted set of prediction sets that can compactly be represented as partial programs, which are programs with portions replaced with holes. Given a trained code generation model, our algorithm leverages a programming language's abstract syntax tree to generate a set of programs such that the correct program is in the set with high-confidence. Valuable applications of our algorithm include a Codex-style code generator with holes in uncertain parts of the generated code, which provides a partial program with theoretical guarantees. We evaluate our approach on PICARD (a T5 model for SQL semantic parsing) and Codex (a GPT model for over a dozen programming languages, including Python), demonstrating that our approach generates compact PAC prediction sets. This is the first research contribution that generates PAC prediction sets for generative code models},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {665},
numpages = {13},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1145/3593434.3593476,
author = {Ho, Anh and Bui, Anh M. T. and Nguyen, Phuong T. and Di Salle, Amleto},
title = {Fusion of deep convolutional and LSTM recurrent neural networks for automated detection of code smells},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593476},
doi = {10.1145/3593434.3593476},
abstract = {Code smells is the term used to signal certain patterns or structures in software code that may contain a potential design or architecture problem, leading to maintainability or other software quality issues. Detecting code smells early in the software development process helps prevent these problems and improve the overall software quality. Existing research concentrates on the process of collecting and handling dataset, then exploring the potential of utilizing deep learning models to detect smells, while ignoring extensive feature engineering. Though these approaches obtained promising results, the following issues need to be tackled: (i) extracting both structural and semantic features from the software units; (ii) mitigating the effects of imbalanced data distribution on the performance.In this paper, we propose DeepSmells as a novel approach to code smells detection. To learn the complex hierarchical representations of the code fragment, we apply a deep convolutional neural network (CNN). Then, in order to improve the quality of the context encoding and preserve semantic information, long short-term memory networks (LSTM) is placed immediately after the CNN. The final classification is conducted by deep neural networks with weighted loss function to reduce the impact of skewed data distribution. We performed an empirical study using the existing code smell benchmark datasets to assess the performance of our proposed approach, and compare it with state-of-the-art baselines. The results demonstrate the effectiveness of our proposed method for all kinds of code smells with outperformed evaluation metrics in terms of F1 score and MCC.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {229–234},
numpages = {6},
location = {Oulu, Finland},
series = {EASE '23}
}

@article{10.1007/s10515-023-00397-7,
author = {Alturayeif, Nouf and Aljamaan, Hamoud and Hassine, Jameleddine},
title = {An automated approach to aspect-based sentiment analysis of apps reviews using machine and deep learning},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00397-7},
doi = {10.1007/s10515-023-00397-7},
abstract = {Apps reviews hold a huge amount of informative user feedback that may be used to assist software practitioners in better understanding users’ needs, identify issues related to quality, such as privacy concerns and low efficiency, and evaluate the perceived users’ satisfaction with the app features. One way to efficiently extract this information is by using Aspect-Based Sentiment Analysis (ABSA). The role of ABSA of apps reviews is to identify all app’s aspects being reviewed and assign a sentiment polarity towards each aspect. This paper aims to build ABSA models using supervised Machine Learning (ML) and Deep Learning (DL) approaches. Our automated technique is intended to (1) identify the most useful and effective text-representation and task-specific features in both Aspect Category Detection (ACD) and Aspect Category Polarity, (2) empirically investigate the performance of conventional ML models when utilized for ABSA task of apps reviews, and (3) empirically compare the performance of ML models and DL models in the context of ABSA task. We built the models using different algorithms/architectures and performed hyper-parameters tuning. In addition, we extracted a set of relevant features for the ML models and performed an ablation study to analyze their contribution to the performance. Our empirical study showed that the ML model trained using Logistic Regression algorithm and BERT embeddings outperformed the other models. Although ML outperformed DL, DL models do not require hand-crafted features and they allow for a better learning of features when trained with more data.},
journal = {Automated Software Engg.},
month = {sep},
numpages = {37},
keywords = {Aspect-based sentiment analysis (ABSA), Apps reviews, Users feedback, Machine learning, Deep learning}
}

@inproceedings{10.1007/978-3-031-49269-3_7,
author = {Ardimento, Pasquale},
title = {Enhancing Bug-Fixing Time Prediction with&nbsp;LSTM-Based Approach},
year = {2023},
isbn = {978-3-031-49268-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49269-3_7},
doi = {10.1007/978-3-031-49269-3_7},
abstract = {This work presents an approach based on Long short-term memory (LSTM) for estimating the bug-fixing time in the bug triage process. Existing bug-fixing time predictor approaches underutilize useful semantic information and long-term dependencies between activities in the bug-fixing sequence. Therefore, the proposed approach is a deep learning-based model that converts activities into vectors of real numbers based on their semantic meaning. It then uses LSTM to identify long-term dependencies between activities and classifies sequences as having either short fixing time or long fixing time. The evaluation on bug reports from the Eclipse project shows that this approach performs slightly better than the current best in the literature, boasting improved metrics such as accuracy, precision, f-score, and recall.},
booktitle = {Product-Focused Software Process Improvement: 24th International Conference, PROFES 2023, Dornbirn, Austria, December 10–13, 2023, Proceedings, Part II},
pages = {68–79},
numpages = {12},
keywords = {bug-fixing time, prediction, LSTM, software maintenance, software repository mining, deep learning},
location = {<conf-loc content-type="InPerson">Dornbirn, Austria</conf-loc>}
}

@inproceedings{10.1145/3611643.3616317,
author = {Du, Xueying and Lou, Yiling and Liu, Mingwei and Peng, Xin and Yang, Tianyong},
title = {KG4CraSolver: Recommending Crash Solutions via Knowledge Graph},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616317},
doi = {10.1145/3611643.3616317},
abstract = {Fixing crashes is challenging, and developers often discuss their encountered crashes and refer to similar crashes and solutions on online Q&amp;A forums (e.g., Stack Overflow). However, a crash often involves very complex contexts, which includes different contextual elements, e.g., purposes, environments, code, and crash traces. Existing crash solution recommendation or general solution 
recommendation techniques only use an incomplete context or treat the entire context as pure texts to search relevant solutions for a given crash, resulting in inaccurate recommendation results. In this work, we propose a novel crash solution knowledge graph (KG) to summarize the complete crash context and its solution with a graph-structured representation. To construct the crash solution KG automatically, we propose to leverage prompt learning to construct the KG from SO threads with a small set of labeled data. Based on the constructed KG, we further propose a novel KG-based crash solution recommendation technique KG4CraSolver, which precisely finds the relevant SO thread for an encountered crash by finely analyzing and matching the complete crash context based on the crash 
solution KG. The evaluation results show that the constructed KG is of high quality and KG4CraSolver outperforms baselines in terms of all metrics (e.g., 13.4%-113.4% MRR improvements). Moreover, we perform a user study and find that KG4CraSolver helps participants find crash solutions 34.4% faster and 63.3% more accurately.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1242–1254},
numpages = {13},
keywords = {Crash Solution Recommendation, Knowledge Graph, Stack Overflow},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3594671.3594685,
author = {Tanimoto, Steven L.},
title = {Five Futures with AI Coding Agents},
year = {2023},
isbn = {9798400707551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594671.3594685},
doi = {10.1145/3594671.3594685},
abstract = {Many computer programmers are beginning to use computational agents to help them develop software. This article raises questions about the nature of programmer-to-agent relationships. The author’s intent is to foster thought that will help human programmers best prepare for such relationships and perhaps design the relationships, ultimately keeping their jobs and improving their programming experience.},
booktitle = {Companion Proceedings of the 7th International Conference on the Art, Science, and Engineering of Programming},
pages = {32–38},
numpages = {7},
keywords = {ChatGPT, Copilot, artificial intelligence, code golf, coding agents, coding service, collaboration, flow, future, interactive intelligent development environment, live coding, liveness, pair programming, programming experience, software development, soloing, trust, verification},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {Programming '23}
}

@inproceedings{10.1145/3590777.3590780,
author = {Ozturk, Omer Said and Ekmekcioglu, Emre and Cetin, Orcun and Arief, Budi and Hernandez-Castro, Julio},
title = {New Tricks to Old Codes: Can AI Chatbots Replace Static Code Analysis Tools?},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590777.3590780},
doi = {10.1145/3590777.3590780},
abstract = {The prevalence and significance of web services in our daily lives make it imperative to ensure that they are – as much as possible – free from vulnerabilities. However, developing a complex piece of software free from any security vulnerabilities is hard, if not impossible. One way to progress towards achieving this holy grail is by using static code analysis tools to root out any common or known vulnerabilities that may accidentally be introduced during the development process. Static code analysis tools have significantly contributed to addressing the problem above, but are imperfect. It is conceivable that static code analysis can be improved by using AI-powered tools, which have recently increased in popularity. However, there is still very little work in analysing both types of tools’ effectiveness, and this is a research gap that our paper aims to fill. We carried out a study involving 11 static code analysers, and one AI-powered chatbot named ChatGPT, to assess their effectiveness in detecting 92 vulnerabilities representing the top 10 known vulnerability categories in web applications, as classified by OWASP. We particularly focused on PHP vulnerabilities since it is one of the most widely used languages in web applications. However, it has few security mechanisms to help its software developers. We found that the success rate of ChatGPT in terms of finding security vulnerabilities in PHP is around 62-68%. At the same time, the best traditional static code analyser tested has a success rate of 32%. Even combining several traditional static code analysers (with the best features on certain aspects of detection) would only achieve a rate of 53%, which is still significantly lower than ChatGPT’s success rate. Nonetheless, ChatGPT has a very high false positive rate of 91%. In comparison, the worst false positive rate of any traditional static code analyser is 82%. These findings highlight the promising potential of ChatGPT for improving the static code analysis process but reveal certain caveats (especially regarding accuracy) in its current state. Our findings suggest that one interesting possibility to explore in future works would be to pick the best of both worlds by combining traditional static code analysers with ChatGPT to find security vulnerabilities more effectively.},
booktitle = {Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
pages = {13–18},
numpages = {6},
keywords = {ChatGPT · AI · Static code analysis · PHP vulnerabilities · Tools evaluation · Vulnerability detection · AI in cyber security},
location = {Stavanger, Norway},
series = {EICC '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00050,
author = {Cui, Xing and Wu, Jingzheng and Wu, Yanjun and Wang, Xu and Luo, Tianyue and Qu, Sheng and Ling, Xiang and Yang, Mutian},
title = {An Empirical Study of License Conflict in Free and Open Source Software},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00050},
doi = {10.1109/ICSE-SEIP58684.2023.00050},
abstract = {Free and Open Source Software (FOSS) has become the fundamental infrastructure of mainstream software projects. FOSS is subject to various legal terms and restrictions, depending on the type of open source license in force. Hence it is important to remain compliant with the FOSS license terms. Identifying the licenses that provide FOSS and understanding the terms of those licenses is not easy, especially when dealing with a large amount of reuse that is common in modern software development. Since reused software is often large, automated license analysis is needed to address these issues and support users in license compliant reuse of FOSS. However, existing license assessment tools can only identify the name and quantity of licenses embedded in software and thus cannot identify whether the licenses are being used safely and correctly. Moreover, they cannot provide a comprehensive analysis of the compatibility and potential risk that come with the term conflicts.In this paper, we propose DIKE, an automated tool that can perform license detection and conflict analysis for FOSS. First, DIKE extracts 12 terms under 3,256 unique open source licenses by manual analysis and Natural Language Processing (NLP) and constructs a license knowledge base containing the responsibilities of the terms. Second, DIKE scans all licenses from the code snippet for the input software and outputs the scan results in a tree structure. Third, the scan results match the license knowledge base to detect license conflicts from terms and conditions. DIKE designs two solutions for software with license conflicts: license replacement and code replacement. To demonstrate the effectiveness of DIKE, we first evaluate with the term extraction and responsibility classification, and the results show that their F1-scores reach 0.816 and 0.948, respectively. In addition, we conduct a measurement study of 16,341 popular projects from GitHub based on our proposed DIKE to explore the conflict of license usage in FOSS. The results show that 1,787 open source licenses are used in the project, and 27.2% of licenses conflict. Our new findings suggest that conflicts are prevalent in FOSS, warning the open source community about intellectual property risks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {495–505},
numpages = {11},
keywords = {free and open source software, license analysis, license conflict, natural language processing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@article{10.1145/3599975.3599976,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3599975.3599976},
doi = {10.1145/3599975.3599976},
abstract = {ChatBots and Generative AI more generally have really opened up a deluge of RISKS items in the past few months, along with an increased awareness of risks relating to arti - cial intelligence. This issue includes just the tip of the iceberg with one-liners on that subject, where it is almost impossible to summarize all the risks. PGN},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jun},
pages = {4–7},
numpages = {4}
}

@inproceedings{10.5555/978-3-031-48796-5_fm,
title = {Front Matter},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {i–xx},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@inproceedings{10.1007/978-3-031-46308-2_22,
author = {Gao, Liang and He, Jinrong and Zhai, Longlong and He, Yiting},
title = {A Segmentation Method Based on SE Attention and U-Net for Apple Image},
year = {2023},
isbn = {978-3-031-46307-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-46308-2_22},
doi = {10.1007/978-3-031-46308-2_22},
abstract = {Apple image segmentation is the basis for apple target recognition and positioning in apple intelligent picking. Traditional apple image segmentation methods have problems such as low accuracy and poor recall rate. Based on the U-Net model, a SE attention mechanism fusion improved U-Net apple image segmentation method is proposed to utilize contextual information of features. First, 344 apple images were collected in the orchard and manually labeled using LabelMe software. The samples were expanded to 1700 using data augmentation. Then the U-type network structure is used to connect the feature maps of the low-level network and the high-level network. The skip connection is used to reduce the network complexity, and the number of feature map channels is superimposed. The SE attention mechanism is added to the decoder part to enhance the channel features of the effective feature maps for apple image segmentation tasks and suppress unimportant channel features to obtain rich contextual information for more refined feature maps. Finally, apple target segmentation is predicted based on the obtained feature maps. The results show that the U-Net model with SE attention fusion can accurately segment the apple region, especially for small-scale apples, and can further optimize the segmentation effect of the edge. The segmentation precision of the model can reach 98.86%, and F1 score is 98.96%, verifying that the proposed model can accurately segment apple targets of different scales in complex environments.},
booktitle = {Image and Graphics: 12th International Conference, ICIG 2023, Nanjing, China, September 22–24, 2023, Proceedings, Part II},
pages = {263–276},
numpages = {14},
keywords = {apple picking, image segmentation, U-Net, feature maps, SE attention mechanism},
location = {Nanjing, China}
}

@inproceedings{10.1145/3609437.3609455,
author = {Wu, Jiadong and Wang, Yanlin and Wang, Ruixin and Chen, Jiachi and Zheng, Zibin},
title = {Can Neural Networks Help Smart Contract Testing? An Empirical Study},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609455},
doi = {10.1145/3609437.3609455},
abstract = {Smart contracts are one of the most successful applications of blockchain technology. In order to guarantee the security of smart contracts, researchers have successively introduced various testing methodologies, including static analysis, symbolic execution, and fuzzing, which contribute to a more rigorous and precise evaluation of smart contract vulnerabilities. Deep learning techniques have been widely applied in traditional software vulnerability detection, while the opposite is true in the field of smart contract testing. Consequently, we anticipate that deep learning can be similarly applied to enhance traditional smart contract vulnerability detection tools. However, there is a lack of empirical study on the performance of deep learning applied to smart contract testing. In order to explore how deep neural networks can help with testing tools on smart contracts, we construct a test framework based on SMARTEST. We manage to train deep learning language models using various neural networks including Transformer, GRU, RNN and test the symbolic execution tool SMARTEST framework with the application of these models on the CVE dataset. Upon analyzing the experimental results, we find that deep neural networks did not surpass traditional language models in enhancing smart contract testing. In terms of accuracy, the SMARTEST tool, which utilizes a statistical 3-gram language model, succeeded in detecting the greatest number of vulnerabilities. Specifically, the 3-gram model was able to identify 69.8% of vulnerabilities in the benchmark set within the first 5 seconds. Based on our experimental findings and thorough analysis, we outline the challenges faced in DNN-assisted smart contract testing and suggest potential directions for improvement.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {79–89},
numpages = {11},
keywords = {neural networks, smart contract, vulnerabilities detection},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@article{10.1007/s10664-023-10340-9,
author = {Wen, Ming and Wang, Yongcong and Xia, Yifan and Jin, Hai},
title = {Evaluating seed selection for fuzzing JavaScript engines},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10340-9},
doi = {10.1007/s10664-023-10340-9},
abstract = {JavaScript (JS), as a platform-independent programming language, remains to be the most popular language over the years. However, popular JavaScript engines that have been widely utilized by web browsers to interpret JS code, have become the most common targets for attackers. Thus ensuring the security and reliability of JS engines is significant. Fuzzing is a simple yet effective method to unveil vulnerabilities. However, existing JS fuzzers focus more on the design of effective mutation mechanisms to generate diverse and valid seeds while they often ignore the importance of the initial seed corpus selected to drive the fuzzing process. In this paper, we performed extensive experiments to systematically evaluate the impact of seed selection on fuzzing JavaScript engines. In particular, we investigate seed selections from three main dimensions, their collected sources (e.g., CVE PoCs, Regression tests, etc.), the number and sizes, as well as a set of concerned code properties. Our major findings reveal that seeds collected from different sources can cast a significant impact on the fuzzing effectiveness (i.e., CVE PoC is significantly better than the other types of seeds), and seed files containing those concerned code structures can lead existing fuzzers to achieve superior results in terms of both code coverage and unique crashes identified. Inspired by our observations, we devised a simple heuristic to prioritize JavaScript files when selecting seed corpus. Our experiments show that when driven by our selected seed corpus, the existing state-of-art fuzzer is able to achieve significantly higher code coverage and identify more crashes.},
journal = {Empirical Softw. Engg.},
month = {sep},
numpages = {35},
keywords = {Fuzzing, JavaScript engines, Seed selection, Empirical study}
}

@inproceedings{10.1145/3593434.3593952,
author = {Manfredi, Gilda and Erra, Ugo and Gilio, Gabriele},
title = {A Mixed Reality Approach for Innovative Pair Programming Education with a Conversational AI Virtual Avatar},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593952},
doi = {10.1145/3593434.3593952},
abstract = {Pair Programming (PP) is an Agile software development methodology that involves two developers working together on a single computer. However, the physical presence of two developers has become a challenge in recent years due to the pandemic, necessitating remote collaboration methods such as Distributed Pair Programming (DPP). DPP has been found to have similar benefits to in-person PP, but the issue of team compatibility remains unresolved. These are more evident in the educational field of Agile methodologies. To address these challenges, we developed a novel approach by creating a Mixed Reality (MR) application that enables users to learn PP with the assistance of a conversational intelligent virtual avatar. The application uses the HoloLens MR device and a Conversational Agent (CA) extension integrated into Visual Studio Code to provide suggestions for improving the code written by the user. The virtual avatar animates these suggestions, making it appear to speak and interact with the user in real time. This system aims to overcome the limitations of common DPP methods, allowing a single developer to learn and apply the PP methodology even when a human partner is unavailable.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {450–454},
numpages = {5},
keywords = {artificial intelligence, conversational agents, extended reality, pair programming},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.5555/3620237.3620345,
author = {Wang, Dawei and Li, Ying and Zhang, Zhiyu and Chen, Kai},
title = {CarpetFuzz: automatic program option constraint extraction from documentation for fuzzing},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {The large-scale code in software supports the rich and diverse functionalities, and at the same time contains potential vulnerabilities. Fuzzing, as one of the most popular vulnerability detection methods, continues evolving in both industry and academy, aiming to find more vulnerabilities by covering more code. However, we find that even with the state-of-the-art fuzzers, there is still some unexplored code that can only be triggered using a specific combination of program options. Simply mutating the options may generate many invalid combinations due to the lack of consideration of constraints (or called relationships) among options. In this paper, we leverage natural language processing (NLP) to automatically extract option descriptions from program documents and analyze the relationship (e.g., conflicts, dependencies) among the options before filtering out invalid combinations and only leaving the valid ones for fuzzing. We implemented a tool called CarpetFuzz and evaluated its performance. The results show that CarpetFuzz accurately extracts the relationships from documents with 96.10% precision and 88.85% recall. Based on these relationships, CarpetFuzz reduced the 67.91% option combinations to be tested. It helps AFL find 45.97% more paths that other fuzzers cannot discover. After analyzing 20 popular open-source programs, CarpetFuzz discovered 57 vulnerabilities, including 43 undisclosed ones. We also successfully obtained CVE IDs for 30 vulnerabilities.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {108},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{10.1145/3632860,
author = {Patton, Noah and Rahmani, Kia and Missula, Meghana and Biswas, Joydeep and Dillig, Işıl},
title = {Programming-by-Demonstration for Long-Horizon Robot Tasks},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632860},
doi = {10.1145/3632860},
abstract = {The goal of programmatic Learning from Demonstration (LfD) is to learn a policy in a programming language that can be used to control a robot’s behavior from a set of user demonstrations. This paper presents a new programmatic LfD algorithm that targets long-horizon robot tasks which require synthesizing programs with complex control flow structures, including nested loops with multiple conditionals. Our proposed method first learns a program sketch that captures the target program’s control flow and then completes this sketch using an LLM-guided search procedure that incorporates a novel technique for proving unrealizability of programming-by-demonstration problems. We have implemented our approach in a new tool called PROLEX and present the results of a comprehensive experimental evaluation on 120 benchmarks involving complex tasks and environments. We show that, given a 120 second time limit, PROLEX can find a program consistent with the demonstrations in 80% of the cases. Furthermore, for 81% of the tasks for which a solution is returned, PROLEX is able to find the ground truth program with just one demonstration. In comparison, CVC5, a syntax-guided synthesis tool, is only able to solve 25% of the cases even when given the ground truth program sketch, and an LLM-based approach, GPT-Synth, is unable to solve any of the tasks due to the environment complexity.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {18},
numpages = {34},
keywords = {Abstract Interpretation, Learning from Demonstrations, Program Synthesis}
}

@article{10.1007/s10664-023-10382-z,
author = {Aghili, Roozbeh and Li, Heng and Khomh, Foutse},
title = {Studying the characteristics of AIOps projects on GitHub},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10382-z},
doi = {10.1007/s10664-023-10382-z},
abstract = {Artificial Intelligence for IT Operations (AIOps) leverages AI approaches to handle the massive amount of data generated during the operations of software systems. Prior works have proposed various AIOps solutions to support different tasks in system operations and maintenance, such as anomaly detection. In this study, we conduct an in-depth analysis of open-source AIOps projects to understand the characteristics of AIOps in practice. We first carefully identify a set of AIOps projects from GitHub and analyze their repository metrics (e.g., the used programming languages). Then, we qualitatively examine the projects to understand their input data, analysis techniques, and goals. Finally, we assess the quality of these projects using different quality metrics, such as the number of bugs. To provide context, we also sample two sets of baseline projects from GitHub: a random sample of machine learning projects and a random sample of general-purposed projects. By comparing different metrics between our identified AIOps projects and these baselines, we derive meaningful insights. Our results reveal a recent and growing interest in AIOps solutions. However, the quality metrics indicate that AIOps projects suffer from more issues than our baseline projects. We also pinpoint the most common issues in AIOps approaches and discuss potential solutions to address these challenges. Our findings offer valuable guidance to researchers and practitioners, enabling them to comprehend the current state of AIOps practices and shed light on different ways of improving AIOps’ weaker aspects. To the best of our knowledge, this work marks the first attempt to characterize open-source AIOps projects.},
journal = {Empirical Softw. Engg.},
month = {oct},
numpages = {49},
keywords = {AIOps, Data mining, Repository mining, Qualitative analysis, Temporal trends, Source code analysis}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00101,
author = {Ye, Ming and Chen, Yuanfan and Zhang, Xin and He, Jinning and Cao, Jicheng and Liu, Dong and Gao, Jing and Dai, Hailiang and Cheng, Shengyu},
title = {Automated Feature Document Review via Interpretable Deep Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00101},
doi = {10.1109/ICSE-Companion58688.2023.00101},
abstract = {A feature in the agile methodology is a function of a product that delivers business value and meets stakeholders' requirements. Developers compile and store the content of features in a structured feature document. Feature documents play a critical role in controlling software development at a macro level. It is therefore important to ensure the quality of feature documents so that defects are not introduced at the outset. Manual review is an effective activity to ensure quality, but it is human-intensive and challenging. In this paper, we propose a feature document review tool to automate the process of manual review (quality classification, and suggestion generation) based on neural networks and interpretable deep learning. Our goal is to reduce human effort in reviewing feature documents and to prompt authors to craft better feature documents. We have evaluated our tool on a real industrial project from ZTE Corporation. The results show that our quality classification model achieved 75.6% precision and 94.4% recall for poor quality feature documents. For the suggestion generation model, about 70% of the poor quality feature documents could be improved to the qualified level in three rounds of revision based on the suggestions. User feedback shows that our tool helps users save an average of 15.9% of their time.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {351–354},
numpages = {4},
keywords = {feature documents, agile methodology, neural networks, interpretable deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00066,
author = {Yin, Likang and Zhang, Xiyu and Filkov, Vladimir},
title = {On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00066},
doi = {10.1109/ICSE48619.2023.00066},
abstract = {Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability?From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {678–689},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1007/s10664-023-10329-4,
author = {Santos, Fabio and Vargovich, Joseph and Trinkenreich, Bianca and Santos, Italo and Penney, Jacob and Britto, Ricardo and Pimentel, João Felipe and Wiese, Igor and Steinmacher, Igor and Sarma, Anita and Gerosa, Marco A.},
title = {Tag that issue: applying API-domain labels in issue tracking systems},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10329-4},
doi = {10.1007/s10664-023-10329-4},
abstract = {Labeling issues with the skills required to complete them can help contributors to choose tasks in Open Source Software projects. However, manually labeling issues is time-consuming and error-prone, and current automated approaches are mostly limited to classifying issues as bugs/non-bugs. We investigate the feasibility and relevance of automatically labeling issues with what we call “API-domains,” which are high-level categories of APIs. Therefore, we posit that the APIs used in the source code affected by an issue can be a proxy for the type of skills (e.g., DB, security, UI) needed to work on the issue. We ran a user study (n=74) to assess API-domain labels’ relevancy to potential contributors, leveraged the issues’ descriptions and the project history to build prediction models, and validated the predictions with contributors (n=20) of the projects. Our results show that (i) newcomers to the project consider API-domain labels useful in choosing tasks, (ii) labels can be predicted with a precision of 84% and a recall of 78.6% on average, (iii) the results of the predictions reached up to 71.3% in precision and 52.5% in recall when training with a project and testing in another (transfer learning), and (iv) project contributors consider most of the predictions helpful in identifying needed skills. These findings suggest our approach can be applied in practice to automatically label issues, assisting developers in finding tasks that better match their skills.},
journal = {Empirical Softw. Engg.},
month = {aug},
numpages = {52},
keywords = {API identification, Labelling, Tagging, Skills, Multi-label classification, Mining software repositories}
}

@article{10.1145/3591300,
author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
title = {Prompting Is Programming: A Query Language for Large Language Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591300},
doi = {10.1145/3591300},
abstract = {Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation.  
On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  

Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.  

To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.  

We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).},
journal = {Proc. ACM Program. Lang.},
month = {jun},
articleno = {186},
numpages = {24},
keywords = {language model programming, prompt programming}
}

@inproceedings{10.1145/3624062.3624147,
author = {Shah, Dhruvil and Speyer, Gil and Yalim, Jason},
title = {Centralized provisioning of large language models for a research community},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624147},
doi = {10.1145/3624062.3624147},
abstract = {Local support for large language models (LLMs) in a research community can address unique technological and procedural challenges that arise in an academic setting. Platforms providing multi-GPU nodes, typically found in a centralized computational resource, such as a university datacenter, can manage the large memory footprint of the open-source LLMs. Customizations employing peripheral frameworks help extend the capabilities of these models. Further, the local implementation addresses the protection of researcher IP and control of restricted data sources. This report describes recent efforts toward provisioning this popular new tool and provides guidance for recreating our approach at Arizona State University.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {704–707},
numpages = {4},
keywords = {HPC, LLMs, ML/AI, Software engineering, large language models, open-source, provisioning},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC-W '23}
}

@inproceedings{10.1145/3585059.3611424,
author = {Wang, Ye Diana},
title = {ChatGPT-Proofing a Web Development Course},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611424},
doi = {10.1145/3585059.3611424},
abstract = {This talk discusses an effective and efficient process that a BSIT program has undergone for transitioning to the new ABET Student Outcomes and some preliminary results, which may benefit other computing programs in preparation for an ABET accreditation visit in 2019 or beyond.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {199–200},
numpages = {2},
keywords = {ABET Accreditation, New Criterion 3, Student Outcomes},
location = {<conf-loc>, <city>Marietta</city>, <state>GA</state>, <country>USA</country>, </conf-loc>},
series = {SIGITE '23}
}

@article{10.1007/s10664-023-10366-z,
author = {Wang, Dong and Kondo, Masanari and Kamei, Yasutaka and Kula, Raula Gaikovina and Ubayashi, Naoyasu},
title = {When conversations turn into work: a taxonomy of converted discussions and issues in GitHub},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10366-z},
doi = {10.1007/s10664-023-10366-z},
abstract = {Popular and large contemporary open-source projects now embrace a diverse set of documentation for communication channels. Examples include contribution guidelines (i.e., commit message guidelines, coding rules, submission guidelines), code of conduct (i.e., rules and behavior expectations), governance policies, and Q&amp;A forum. In 2020, GitHub released Discussion to distinguish between communication and collaboration. However, it remains unclear how developers maintain these channels, how trivial it is, and whether deciding on conversion takes time. We conducted an empirical study on 259 NPM and 148 PyPI repositories, devising two taxonomies of reasons for converting discussions into issues and vice-versa. The most frequent conversion from a discussion to an issue is when developers request a contributor to clarify their idea into an issue (Reporting a Clarification Request –35.1% and 34.7%, respectively), while agreeing that having non actionable topic (QA, ideas, feature requests –55.0% and 42.0%, respectively) is the most frequent reason of converting an issue into a discussion. Furthermore, we show that not all reasons for conversion are trivial (e.g., not a bug), and raising a conversion intent potentially takes time (i.e., a median of 15.2 and 35.1&nbsp;h, respectively, taken from issues to discussions). Our work contributes to complementing the GitHub guidelines and helping developers effectively utilize the Issue and Discussion communication channels to maintain their collaboration.},
journal = {Empirical Softw. Engg.},
month = {oct},
numpages = {27},
keywords = {Communication Channels, GitHub Discussion, Empirical Study}
}

@article{10.1007/s10664-023-10364-1,
author = {Wu, Xingfang and Li, Heng and Khomh, Foutse},
title = {On the effectiveness of log representation for log-based anomaly detection},
year = {2023},
issue_date = {Nov 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10364-1},
doi = {10.1007/s10664-023-10364-1},
abstract = {Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners’ opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datasets (i.e., HDFS, BGL, Spirit and Thunderbird) in the context of log-based anomaly detection.We also examine the impacts of the log parsing process and the different feature aggregation approaches when they are employed with log representation techniques. From the experiments, we provide some heuristic guidelines for future researchers and developers to follow when designing an automated log analysis workflow. We believe our comprehensive comparison of log representation techniques can help researchers and practitioners better understand the characteristics of different log representation techniques and provide them with guidance for selecting the most suitable ones for their ML-based log analysis workflow.},
journal = {Empirical Softw. Engg.},
month = {oct},
numpages = {39},
keywords = {Log representation, Anomaly detection, Automated log analysis}
}

@article{10.1007/s10515-023-00402-z,
author = {Cao, Rong and Bao, Liang and Zhangsun, Panpan and Wu, Chase and Wei, Shouxin and Sun, Ren and Li, Ran and Zhang, Zhe},
title = {PTSSBench: a performance evaluation platform in support of automated parameter tuning of software systems},
year = {2023},
issue_date = {May 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-023-00402-z},
doi = {10.1007/s10515-023-00402-z},
abstract = {As software systems become increasingly large and complex, automated parameter tuning of software systems (PTSS) has been the focus of research and many tuning algorithms have been proposed recently. However, due to the lack of a unified platform for comparing and reproducing existing tuning algorithms, it remains a significant challenge for a user to choose an appropriate algorithm for a given software system. There are multiple reasons for this challenge, including diverse experimental conditions, lack of evaluations for different tasks, and excessive evaluation costs of tuning algorithms. In this paper, we propose an extensible and efficient benchmark, referred to as PTSSBench, which provides a unified platform for supporting a comparative study of different tuning algorithms via surrogate models and actual systems. We demonstrate the usability and efficiency of PTSSBench through comparative experiments of six state-of-the-art tuning algorithms from a holistic perspective and a task-oriented perspective. The experimental results show the necessity and effectiveness of parameter tuning for software systems and indicate that the PTSS problem remains an open problem. Moreover, PTSSBench allows extensive runs and in-depth analyses of parameter tuning algorithms, hence providing an efficient and effective way for researchers to develop new tuning algorithms and for users to choose appropriate tuning algorithms for their systems. The proposed PTSSBench benchmark together with the experimental results is made publicly available online as an open-source project.},
journal = {Automated Software Engg.},
month = {nov},
numpages = {39},
keywords = {Benchmark, Parameter tuning, Comparability, Reproducibility}
}

@proceedings{10.1145/3623476,
title = {SLE 2023: Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 16th ACM SIGPLAN International Conference on Software Language Engineering (SLE) held in October 2023 as part of SPLASH 2023. Software Language Engineering (SLE) is a thriving research discipline targeted at establishing an engineering approach to the development, use, and maintenance of software languages, that is, of languages for the specification, modeling and tooling of software. Key topics of interest for SLE include approaches, methodologies and tools for language design and implementation with a focus on techniques for static and behavioral semantics, generative or interpretative approaches (including transformation languages and code generation) as well as meta-languages and tools (including language workbenches). Techniques enabling the testing, simulation or formal verification for language validation purposes are also of particular interest. SLE also accommodates empirical evaluation and experience reports of language engineering tools, such as user studies evaluating usability, performance benchmarks or industrial applications.},
location = {Cascais, Portugal}
}

@inproceedings{10.1109/ICSE48619.2023.00089,
author = {Sun, Jiamou and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming and Hoang, Thong and Zhao, Dehai},
title = {Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00089},
doi = {10.1109/ICSE48619.2023.00089},
abstract = {Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {970–982},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.5555/3615924.3615951,
author = {Luis F., Rivera and Norha M., Villegas and Gabriel, Tamura and Hausi A., Müller and Ian, Watts and Eric, Erpenbach and Laura, Shwartz and Xiaotong, Liu},
title = {Using Digital Twins for Software Change Risk Assessment Toward Proactive AIOps},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {The increasing structural and behavioural complexity of modern IT systems and environments (IT-Sys|Envs) calls for adopting automated fault anticipation and forecasting mechanisms to mitigate risks, limit system disturbances and damage, and improve problem resolution readiness. Preventing unexpected or undesired ITSys| Envs behaviour is crucial not only to maximize customer value and satisfaction but also to fulfill strict service-level agreements. Given the constant need for operational changes in today’s dynamic IT-Sys|Envs as a response to evolving user requirements and expectations, DevOps teams face several challenges and uncertainties to limit the potential introduction of faults, foresee their occurrence, and comprehend their associated risks. While the accomplishments attained in the application of artificial intelligence to the operation of IT-Sys|Envs (i.e., AIOps) show promise, further research is necessary to facilitate the transition from reactive AIOps to its anticipated proactive, and risk-focused, manifestation. This involves advancing AIOps conceptualization and tooling for enabling improved software fault prognosis and remediation, augmented risk management, and enhanced explainability. We describe our vision for proactive AIOps and report our progress toward its viable realization through the application of relevant notions from the revolutionary concept of Digital Twins. Moreover, we discuss challenges and opportunities regarding this promising integration and its impact on the future of software development and operation life cycles.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {211–216},
numpages = {6},
keywords = {Digital Twins, AIOps, Reference Models, Runtime Experimentation},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3583780.3614974,
author = {Wang, Fanmeng and Xu, Hongteng and Chen, Xi and Lu, Shuqi and Deng, Yuqing and Huang, Wenbing},
title = {MPerformer: An SE(3) Transformer-based Molecular Perceptron},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614974},
doi = {10.1145/3583780.3614974},
abstract = {Molecular perception aims to construct 3D molecules from 3D atom clouds (i.e., atom types and corresponding 3D coordinates), determining bond connections, bond orders, and other molecular attributes within molecules. It is essential for realizing many applications in cheminformatics and bioinformatics, such as modeling quantum chemistry-derived molecular structures in protein-ligand complexes. Additionally, many molecular generation methods can only generate molecular 3D atom clouds, requiring molecular perception as a necessary post-processing. However, existing molecular perception methods mainly rely on predefined chemical rules and fail to leverage 3D geometric information, whose performance is sub-optimal fully. In this study, we propose MPerformer, an SE(3) Transformer-based molecular perceptron exhibiting SE(3)-invariance, to construct 3D molecules from 3D atom clouds efficiently. Besides, we propose a multi-task pretraining-and-finetuning paradigm to learn this model. In the pretraining phase, we jointly minimize an attribute prediction loss and an atom cloud reconstruction loss, mitigating the data imbalance issue of molecular attributes and enhancing the robustness and generalizability of the model. Experiments show that MPerformer significantly outperforms state-of-the-art molecular perception methods in precision and robustness, benefiting various molecular generation scenarios.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2512–2522},
numpages = {11},
keywords = {SE(3) transformer, molecular generation, molecular perception, multi-task pretraining},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@inproceedings{10.1145/3613424.3614248,
author = {Zheng, Bojian and Yu, Cody Hao and Wang, Jie and Ding, Yaoyao and Liu, Yizhi and Wang, Yida and Pekhimenko, Gennady},
title = {Grape: Practical and Efficient Graphed Execution for Dynamic Deep Neural Networks on GPUs},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614248},
doi = {10.1145/3613424.3614248},
abstract = {Achieving high performance in machine learning workloads is a crucial yet difficult task. To achieve high runtime performance on hardware platforms such as GPUs, graph-based executions such as CUDA graphs are often used to eliminate CPU runtime overheads by submitting jobs in the granularity of multiple kernels. However, many machine learning workloads, especially dynamic deep neural networks (DNNs) with varying-sized inputs or data-dependent control flows, face challenges when directly using CUDA graphs to achieve optimal performance. We observe that the use of graph-based executions poses three key challenges in terms of efficiency and even practicability: (1) Extra data movements when copying input values to graphs’ placeholders. (2) High GPU memory consumption due to the numerous CUDA graphs created to efficiently support dynamic-shape workloads. (3) Inability to handle data-dependent control flows. To address those challenges, we propose Grape, a new graph compiler that enables practical and efficient graph-based executions for dynamic DNNs on GPUs. Grape comprises three key components: (1) an alias predictor that automatically removes extra data movements by leveraging code positions at the Python frontend, (2) a metadata compressor that efficiently utilizes the data redundancy in CUDA graphs’ memory regions by compressing them, and (3) a predication rewriter that safely replaces control flows with predication contexts while preserving programs’ semantics. The three components improve the efficiency and broaden the optimization scope of graph-based executions while allowing machine learning practitioners to program dynamic DNNs at the Python level with minimal source code changes. We evaluate Grape on state-of-the-art text generation (GPT-2, GPT-J) and speech recognition (Wav2Vec2) workloads, which include both training and inference, using real systems with modern GPUs. Our evaluation shows that Grape achieves up to 36.43 × less GPU memory consumption and up to 1.26 × better performance than prior works on graph-based executions that directly use CUDA graphs. Furthermore, Grape can optimize workloads that are impractical for prior works due to the three key challenges, achieving 1.78 × and 1.82 × better performance on GPT-J and Wav2Vec2 respectively than the original implementations that do not use graph-based executions.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1364–1380},
numpages = {17},
keywords = {CUDA graphs, dynamic neural networks, machine learning compilers},
location = {<conf-loc>, <city>Toronto</city>, <state>ON</state>, <country>Canada</country>, </conf-loc>},
series = {MICRO '23}
}

@inproceedings{10.1145/3605468.3605498,
author = {Greifenstein, Luisa and Brune, Markus and Fuchs, Tobias and Heuer, Ute and Fraser, Gordon},
title = {Impact of Hint Content on Performance and Learning: A Study with Primary School Children in a Scratch Course},
year = {2023},
isbn = {9798400708510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605468.3605498},
doi = {10.1145/3605468.3605498},
abstract = {The implementation of computational thinking concepts in primary school curricula usually includes programming activities. As primary school teachers often lack subject knowledge, they may struggle to help students during these programming activities. Additional support can be provided by automated program analysis, for example in terms of hints on conceptual knowledge related to bad coding patterns observed, or procedural hints on how to solve the task at hand. However, care has to be taken since these hints need to balance (1) helping students to perform a specific task successfully, while nevertheless (2) ensuring a learning effect beyond the specific task. To understand the effects of different types of hints we therefore conducted a study with 36 children aged 7–12 in 10 programming courses. After being introduced to basic programming structures in three units, the children were tasked to debug six Scratch programs using different types of hints, where we observed that procedural hints have the strongest impact on performance. In order to examine an impact on the transfer of learned knowledge, we observed the children’s difficulties during the successive fifth unit, in which they created their own projects. The results of the fifth unit show that having received a procedural hint on a specific pattern during the fourth unit leads to slightly fewer bad related code patterns but also to slightly fewer good code patterns. Considering these results together with the subjective perceptions of the children, we can derive insights into how to best support performance and learning using (automated) feedback.},
booktitle = {Proceedings of the 18th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {7},
numpages = {10},
keywords = {analysis tools, block-based programming feedback, bug patterns, computational thinking, elementary school},
location = {<conf-loc>, <city>Cambridge</city>, <country>United Kingdom</country>, </conf-loc>},
series = {WiPSCE '23}
}

@inproceedings{10.5555/3620237.3620344,
author = {Fu, Yu-Fu and Lee, Jaehyuk and Kim, Taesoo},
title = {autofz: automated fuzzer composition at runtime},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Fuzzing has gained in popularity for software vulnerability detection by virtue of the tremendous effort to develop a diverse set of fuzzers. Thanks to various fuzzing techniques, most of the fuzzers have been able to demonstrate great performance on their selected targets. However, paradoxically, this diversity in fuzzers also made it difficult to select fuzzers that are best suitable for complex real-world programs, which we call selection burden. Communities attempted to address this problem by creating a set of standard benchmarks to compare and contrast the performance of fuzzers for a wide range of applications, but the result was always a suboptimal decision--the best-performing fuzzer on average does not guarantee the best outcome for the target of a user's interest.To overcome this problem, we propose an automated, yet non-intrusive meta-fuzzer, called autofz1, to maximize the benefits of existing state-of-the-art fuzzers via dynamic composition. To an end user, this means that, instead of spending time on selecting which fuzzer to adopt (similar in concept to hyperparameter tuning in ML), one can simply put all of the available fuzzers to autofz (similar in concept to AutoML), and achieve the best, optimal result. The key idea is to monitor the runtime progress of the fuzzers, called trends (similar in concept to gradient descent), and make a fine-grained adjustment of resource allocation (e.g., CPU time) of each fuzzer. This is a stark contrast to existing approaches that statically combine a set of fuzzers, or via exhaustive pre-training per target program--autofz deduces a suitable set of fuzzers of the active workload in a fine-grained manner at runtime. Our evaluation shows that, given the same amount of computation resources, autofz outperforms any best-performing individual fuzzers in 11 out of 12 available benchmarks and beats the best, collaborative fuzzing approaches in 19 out of 20 benchmarks without any prior knowledge in terms of coverage. Moreover, on average, autofz found 152% more bugs than individual fuzzers on UNIFUZZ and FTS, and 415% more bugs than collaborative fuzzing on UNIFUZZ.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {107},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@inproceedings{10.1145/3597926.3598088,
author = {Shi, Jingyi and Xiao, Yang and Li, Yuekang and Li, Yeting and Yu, Dongsong and Yu, Chendong and Su, Hui and Chen, Yufeng and Huo, Wei},
title = {ACETest: Automated Constraint Extraction for Testing Deep Learning Operators},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598088},
doi = {10.1145/3597926.3598088},
abstract = {Deep learning (DL) applications are prevalent nowadays as they can help with multiple tasks. DL libraries are essential for building DL applications. Furthermore, DL operators are the important building blocks of the DL libraries, that compute the multi-dimensional data (tensors). Therefore, bugs in DL operators can have great impacts. Testing is a practical approach for detecting bugs in DL operators. In order to test DL operators effectively, it is essential that the test cases pass the input validity check and are able to reach the core function logic of the operators. Hence, extracting the input validation constraints is required for generating high-quality test cases. Existing techniques rely on either human effort or documentation of DL library APIs to extract the constraints. They cannot extract complex constraints and the extracted constraints may differ from the actual code implementation.  
To address the challenge, we propose ACETest, a technique to automatically extract input validation constraints from the code to build valid yet diverse test cases which can effectively unveil bugs in the core function logic of DL operators. For this purpose, ACETest can automatically identify the input validation code in DL operators, extract the related constraints and generate test cases according to the constraints. The experimental results on popular DL libraries, TensorFlow and PyTorch, demonstrate that ACETest can extract constraints with higher quality than state-of-the-art (SOTA) techniques. Moreover, ACETest is capable of extracting 96.4% more constraints and detecting 1.95 to 55 times more bugs than SOTA techniques. In total, we have used ACETest to detect 108 previously unknown bugs on TensorFlow and PyTorch, with 87 of them confirmed by the developers. Lastly, five of the bugs were assigned with CVE IDs due to their security impacts.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {690–702},
numpages = {13},
keywords = {Constraint Extraction, Deep Learning Library Testing, Symbolic Execution, Test Generation},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1007/978-3-031-36889-9_29,
author = {Keim, Jan and Hey, Tobias and Sauer, Bjarne and Koziolek, Anne},
title = {A Taxonomy for&nbsp;Design Decisions in&nbsp;Software Architecture Documentation},
year = {2023},
isbn = {978-3-031-36888-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36889-9_29},
doi = {10.1007/978-3-031-36889-9_29},
abstract = {A software system is the result of all design decisions that were made during development and maintenance. Documentation, such as software architecture documentation, captures a variety of different design decisions. Classifying the kinds of design decisions facilitates various downstream tasks by enabling more targeted analyses. In this paper, we propose a taxonomy for design decisions in software architecture documentation to primarily support consistency checking. Existing taxonomies about design decisions have different purposes and do not fit well because they are too coarse. We take an iterative approach, starting with an initial taxonomy based on literature and considerations regarding consistency checking. Then, we mine open-source repositories to extract 17 software architecture documentations that we use to refine the taxonomy. We evaluate the resulting taxonomy with regard to purpose, structure, and application. Additionally, we explore the automatic identification and classification of design decisions in software architecture documentation according to the taxonomy. We apply different machine learning techniques, such as Logistic Regression, Decision Trees, Random Forests, and BERT to the 17 software architecture documentations. The evaluation yields a F1-score of up to 92.1% for identifying design decisions and a F1-score of up to 55.2% for the classification of the kind of design decision.},
booktitle = {Software Architecture. ECSA 2022 Tracks and Workshops: Prague, Czech Republic, September 19–23, 2022, Revised Selected Papers},
pages = {439–454},
numpages = {16},
keywords = {Design Decisions, Software Architecture, Documentation, Decision-Making, Software Design, Mining Software Repositories},
location = {Prague, Czech Republic}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00040,
author = {Yang, Chengran and Xu, Bowen and Liu, Jiakun and Lo, David},
title = {TECHSUMBOT: A Stack Overflow Answer Summarization Tool for Technical Query},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00040},
doi = {10.1109/ICSE-Companion58688.2023.00040},
abstract = {Stack Overflow is a popular platform for developers to seek solutions to programming-related problems. However, prior studies identified that developers may suffer from the redundant, useless, and incomplete information retrieved by the Stack Overflow search engine. To help developers better utilize the Stack Overflow knowledge, researchers proposed tools to summarize answers to a Stack Overflow question. However, existing tools use hand-craft features to assess the usefulness of each answer sentence and fail to remove semantically redundant information in the result. Besides, existing tools only focus on a certain programming language and cannot retrieve up-to-date new posted knowledge from Stack Overflow. In this paper, we propose TechSumBot, an automatic answer summary generation tool for a technical problem. Given a question, TechSumBot first retrieves answers using the Stack Overflow search engine, then TechSumBot 1) ranks each answers sentence based on the sentence's usefulness, 2) estimates the centrality of each sentence to all candidates, and 3) removes the semantic redundant information. Finally, TechSumBot returns the top 5 ranked answer sentences as the answer summary. We implement TechSumBot in the form of a search engine website. To evaluate TechSumBot in both automatic and manual manners, we construct the first Stack Overflow multi-answer summarization benchmark and design a manual evaluation study to assess the effectiveness of TechSumBot and state-of-the-art baselines from the NLP and SE domain. Both results indicate that the summaries generated by TechSumBot are more diverse, useful, and similar to the ground truth summaries.Tool Link: www.techsumbot.comVideo Link: https://youtube.coni/watch7v-ozuJOp_vILMReplication Package: https://github.com/TechSumBot/TechSumBot},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {132–135},
numpages = {4},
keywords = {summarization, question retrieval},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1007/s10009-023-00734-x,
author = {Wirsing, Martin and Jähnichen, Stefan and De Nicola, Rocco},
title = {Rigorous engineering of collective adaptive systems – 2nd special section},
year = {2023},
issue_date = {Dec 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5–6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-023-00734-x},
doi = {10.1007/s10009-023-00734-x},
abstract = {An adaptive system is able to adapt at runtime to dynamically changing environments and to new requirements. Adaptive systems can be single adaptive entities or collective ones that consist of several collaborating entities. Rigorous engineering requires appropriate methods and tools that help guaranteeing that an adaptive system lives up to its intended purpose. This paper introduces the special section on “Rigorous Engineering of Collective Adaptive Systems.” It presents the 11&nbsp;contributions of the section categorizing them into five distinct research lines: correctness by design and synthesis, computing with bio-inspired communication, new system models, machine learning, and programming and analyzing ensembles.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = {nov},
pages = {617–624},
numpages = {8},
keywords = {Adaptive systems, Collective adaptive systems, Software engineering, Formal methods, Rigorous methods}
}

@inproceedings{10.1145/3611643.3616347,
author = {Du, Xiaohu and Chen, Xiao and Cao, Jialun and Wen, Ming and Cheung, Shing-Chi and Jin, Hai},
title = {Understanding the Bug Characteristics and Fix Strategies of Federated Learning Systems},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616347},
doi = {10.1145/3611643.3616347},
abstract = {Federated learning (FL) is an emerging machine learning paradigm that aims to address the problem of isolated data islands. To preserve privacy, FL allows machine learning models and deep neural networks to be trained from decentralized data kept privately at individual devices. FL has been increasingly adopted in missioncritical fields such as finance and healthcare. However, bugs in FL systems are inevitable and may result in catastrophic consequences such as financial loss, inappropriate medical decision, and violation of data privacy ordinance. While many recent studies were conducted to understand the bugs in machine learning systems, there is no existing study to characterize the bugs arising from the unique nature of FL systems. To fill the gap, we collected 395 real bugs from six popular FL frameworks (Tensorflow Federated, PySyft, FATE, Flower, PaddleFL, and Fedlearner) in GitHub and StackOverflow, and then manually analyzed their symptoms and impacts, prone stages, root causes, and fix strategies. Furthermore, we report a series of findings and actionable implications that can potentially facilitate the detection of FL bugs.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1358–1370},
numpages = {13},
keywords = {Bug Characteristics, Empirical Study, Federated Learning},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3609468.3609474,
author = {Guerzhoy, Michael and Neumann, Marion and Virtue, Pat and Anderson, Carolyn Jane and Singla, Yaman K and Orchard, Alexi and Shah, Rajiv Ratn},
title = {EAAI-23 Blue Sky Ideas in Artificial Intelligence Education from the AAAI/ACM SIGAI New and Future AI Educator Program},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3609468.3609474},
doi = {10.1145/3609468.3609474},
abstract = {The 13th Symposium on Educational Advances in Artificial Intelligence (EAAI-23), co-chaired by Michael Guerzhoy, Marion Neumann, and Pat Virtue, continued the tradition of the AAAI/ACM SIGAI New and Future AI Educator (NFAIED) Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia).This paper is a collection of the "blue sky" essays of the 2023 NFAIED awardees, intended to help motivate discussion around various current and important issues in AI education.},
journal = {AI Matters},
month = {oct},
pages = {24–29},
numpages = {6}
}

@article{10.1007/s10664-023-10348-1,
author = {Chen, Yihao and Fernandes, Eduardo and Adams, Bram and Hassan, Ahmed E.},
title = {On practitioners’ concerns when adopting service mesh frameworks},
year = {2023},
issue_date = {Sep 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10348-1},
doi = {10.1007/s10664-023-10348-1},
journal = {Empirical Softw. Engg.},
month = {aug},
numpages = {54},
keywords = {Service mesh, Microservices, Mixed-methods empirical study, Discussion forum, Technology adoption, Software architecture}
}

@article{10.1007/s10664-023-10424-6,
author = {Murali, Aniruddhan and Sahu, Gaurav and Thangarajah, Kishanthan and Zimmerman, Brian and Rodríguez-Pérez, Gema and Nagappan, Meiyappan},
title = {Diversity in issue assignment: humans vs bots},
year = {2024},
issue_date = {Mar 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-023-10424-6},
doi = {10.1007/s10664-023-10424-6},
abstract = {Issue assignment process is a common practice in open source projects for managing incoming and existing issues. While traditionally performed by humans, the adoption of software bots for automating this process has become prevalent in recent years. The objective of this paper is to examine the diversity in issue assignments between bots and humans in open source projects, with the aim of understanding how open source communities can foster diversity and inclusivity. To achieve this, we conducted a quantitative analysis on three major open source projects hosted on GitHub, focusing on the most likely racial and ethnic diversity of both human and bot assignors during the issue assignment process. We analyze how issues are assigned by humans and bots, as well as the distribution of issue types among White and Non-White open source collaborators. Additionally, we explore how the diversity in issue assignments evolves over time for human and bot assignors. Our results reveal that both human and bot assignors majorly assign issues to developers of the same most likely race and ethnicity. Notably, we find bots assign more issues to perceived White developers than Non-White developers. In conclusion, our findings suggest that bots display higher levels of bias than humans in most cases, although humans also demonstrate significant bias in certain instances. Thus, open source communities must actively address these potential biases in their GitHub issue assignment process to promote diversity and inclusivity.},
journal = {Empirical Softw. Engg.},
month = {jan},
numpages = {28},
keywords = {Equitable technology, Software inclusiveness, Bias, Software bots}
}

@inproceedings{10.5555/3618408.3620070,
author = {Ye, Jiacheng and Wu, Zhiyong and Feng, Jiangtao and Yu, Tao and Kong, Lingpeng},
title = {Compositional exemplars for in-context learning},
year = {2023},
publisher = {JMLR.org},
abstract = {Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language inference, commonsense reasoning, open-domain question answering, code generation, and semantic parsing. Extensive experiments demonstrate not only the state-of-the-art performance but also the transferability and compositionality of CEIL, shedding new light on in-context learning. Our code is released at https://github.com/HKUNLP/icl-ceil.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1662},
numpages = {16},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.1016/j.cose.2023.103490,
author = {V S, Devi Priya and Chakkaravarthy Sethuraman, Sibi and Khan, Muhammad Khurram},
title = {Container security: Precaution levels, mitigation strategies, and research perspectives},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {135},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103490},
doi = {10.1016/j.cose.2023.103490},
journal = {Comput. Secur.},
month = {dec},
numpages = {26},
keywords = {Microservices, Software development, Container security- root-based and rootless, Threat modeling-attack trees, DREAD}
}

@inproceedings{10.1145/3597926.3598085,
author = {Xu, Sihan and Gao, Ya and Fan, Lingling and Li, Linyu and Cai, Xiangrui and Liu, Zheli},
title = {LiResolver: License Incompatibility Resolution for Open Source Software},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598085},
doi = {10.1145/3597926.3598085},
abstract = {Open source software (OSS) licenses regulate the conditions under which OSS can be legally reused, distributed, and modified. However, a common issue arises when incorporating third-party OSS accompanied with licenses, i.e., license incompatibility, which occurs when multiple licenses exist in one project and there are conflicts between them. Despite being problematic, fixing license incompatibility issues requires substantial efforts due to the lack of license understanding and complex package dependency. In this paper, we propose LiResolver, a fine-grained, scalable, and flexible tool to resolve license incompatibility issues for open source software. Specifically, it first understands the semantics of licenses through fine-grained entity extraction and relation extraction. Then, it detects and resolves license incompatibility issues by recommending official licenses in priority. When no official licenses can satisfy the constraints, it generates a custom license as an alternative solution. Comprehensive experiments demonstrate the effectiveness of LiResolver, with 4.09% false positive (FP) rate and 0.02% false negative (FN) rate for incompatibility issue localization, and 62.61% of 230 real-world incompatible projects resolved by LiResolver. We discuss the feedback from OSS developers and the lessons learned from this work. All the datasets and the replication package of LiResolver have been made publicly available to facilitate follow-up research.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {652–663},
numpages = {12},
keywords = {License, License Incompatibility Resolution, Open Source Software},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3611643.3613898,
author = {Ganatra, Vaibhav and Parayil, Anjaly and Ghosh, Supriyo and Kang, Yu and Ma, Minghua and Bansal, Chetan and Nath, Suman and Mace, Jonathan},
title = {Detection Is Better Than Cure: A Cloud Incidents Perspective},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613898},
doi = {10.1145/3611643.3613898},
abstract = {Cloud providers use automated watchdogs or monitors to continuously observe service availability and to proactively report incidents when system performance degrades. Improper monitoring can lead to delays in the detection and mitigation of production incidents, which can be extremely expensive in terms of customer impacts and manual toil from engineering resources. Therefore, a systematic understanding of the pitfalls in current monitoring practices and how they can lead to production incidents is crucial for ensuring continuous reliability of cloud services. In this work, we carefully study the production incidents from the past year at Microsoft to understand the monitoring gaps in a hyperscale cloud platform. We conduct an extensive empirical study to answer: (1) What are the major causes of failures in early detection of production incidents and what are the steps taken for mitigation, (2) What is the impact of failures in early detection, (3) How do we recommend best monitoring practices for different services, and (4) How can we leverage the insights from this study to enhance the reliability of the cloud services. This study provides a deeper understanding of existing monitoring gaps in cloud platforms, uncover interesting insights and provide guidance for best monitoring practices for ensuring continuous reliability.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1891–1902},
numpages = {12},
keywords = {Cloud Services, Empirical Study, Reliability},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00046,
author = {Sawant, Neela and Sengamedu, Srinivasan H.},
title = {Code Compliance Assessment as a Learning Problem},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00046},
doi = {10.1109/ICSE-SEIP58684.2023.00046},
abstract = {Manual code reviews and static code analyzers are the traditional mechanisms to verify if source code complies with coding policies. However, they are hard to scale. We formulate code compliance assessment as a machine learning (ML) problem, to take as input a natural language policy and code, and generate a prediction on the code's compliance, non-compliance, or irrelevance. Our intention for ML-based automation is to scale the development of Amazon CodeGuru, a commercial code analyzer. We explore key research questions on model formulation, training data, and evaluation setup. We obtain a joint code-text representation space (embeddings) which preserves compliance relationships via the vector distance of code and policy embeddings. As there is no task-specific data, we re-interpret and filter commonly available software datasets with additional pretraining and pre-finetuning tasks that reduce the semantic gap. We benchmarked our approach on two listings of coding policies (CWE and CBP). This is a zero-shot evaluation as none of the policies occur in the training set. On CWE and CBP respectively, our tool Policy2Code achieves classification accuracies of (59%, 71%) and search MRR of (0.05, 0.21) compared to CodeBERT with classification accuracies of (37%, 54%) and MRR of (0.02, 0.02). In a user study, 24% Policy2Code detections were accepted compared to 7% for CodeBERT. Policy2Code is considered a useful ML-based aid to supplement manual efforts.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {445–454},
numpages = {10},
keywords = {code embeddings, natural language analysis},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3622759.3628225,
author = {Marron, Mark},
title = {Programming Languages for AI Programing Agents (Invited Talk)},
year = {2023},
isbn = {9798400703898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622759.3628225},
doi = {10.1145/3622759.3628225},
abstract = {Over the past decade software development has shifted from a process centered around writing code to a process that increasingly involves composition of external packages and managing the integration of code from other team members. The next decade-plus will be defined by the shift from a process where humans are the central developers of code into one where AI agents, likely Large Language Model (LLM) based, will be the major creators of code and humans will shift to a supervisory role as curators, integrating rich framework-functionality and code developed by AI programming agents.  

In this new world we must ask ourselves – are programming languages as they exist today fit for purpose and how do they evolve to meet the needs of this future programming model. This talk represents an opinionated take on the question and attempts to outline specific areas of investigation that need to be addressed by the PL community as part of this journey including:  

What programming language features help/hinder AI agents when understanding and generating code?  

What programming language features help/hinder human agents when working with an AI Copilot?  

What programming language tools are needed to empower AI agents in creating grounded and reliable outputs?  

How can intents be expressed as part of the program representation – examples, constraints, natural language, external documents?  

How do we empower end-users as part of this transformation?  

What programming language features are needed to support new AI driven workflows – live coding, interactive requirement gathering, AI TDD?  

Effectively answering these questions plays a key role in determining if AI driven programming represents a revolution in how software is developed or is limited to being a programming productivity aid for existing development workflows. As such our community should play a central role in understanding this space and leading in the development of this technological transformation!},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Symposium on Dynamic Languages},
pages = {7},
numpages = {1},
location = {Cascais, Portugal},
series = {DLS 2023}
}

@inproceedings{10.1145/3573381.3596463,
author = {Hagio, Yuta and Okuda, Makoto and Kamimura, Marina and Kaneko, Yutaka and Ohmata, Hisayuki},
title = {Generating Utterances for Companion Robots using Television Program Subtitles},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596463},
doi = {10.1145/3573381.3596463},
abstract = {This study presents a method for generating utterances for companion robots that watch TV with people, using TV program subtitles. To enable the robot to automatically generate relevant utterances while watching TV, we created a dataset of approximately 12,000 utterances that were manually added to the collected TV subtitles. Using this dataset, we fine-tuned a large-scale language model to construct an utterance generation model. The proposed model generates utterances based on multiple keywords extracted from the subtitles as topics, while also taking into account the context of the subtitles by inputting them. The evaluation of the generated utterances revealed that approximately 88% of the sentences were natural Japanese, and approximately 75% were relevant and natural in the context of the TV program. Moreover, approximately 99% of the sentences contained the extracted keywords, indicating that our proposed method can generate diverse and contextually appropriate utterances containing the targeted topics. These findings provide evidence of the effectiveness of our approach in generating natural utterances for companion robots that watch TV with people.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {254–261},
numpages = {8},
keywords = {Companion Robots, Datasets, Fine-tuning, Human-Robot Interaction, Large-scale Language Model, Utterance Generation},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3573051.3593387,
author = {Park, Jungkook and Oh, Alice},
title = {EliRank: A Code Editing History Based Ranking Model for Early Detection of Students in Need},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593387},
doi = {10.1145/3573051.3593387},
abstract = {Research on programming education shows that novice programming students benefit significantly from one-to-one tutoring. While many systems propose to replicate the effectiveness of one-to-one tutoring in large-scale classes, it remains a challenge to develop systems with an approach to finding students who need the tutors' help the most. In this paper, we explore the idea of predicting the priority of students in need with a data-driven approach. Among various metrics to calculate the priority of students in need, we adopt time-on-task metric. Previous studies have found that excessively long time-on-task can be used as an indication of students' struggling. Aligned with this, we reduce the problem of finding students with the highest priority to the problem of finding students with the longest time-on-task. To solve the reduced problem, we present EliRank, a ranking model that finds students with the longest estimated time-on-task, using the students' first few minutes of fine-grained code editing history. EliRank recommends students in the descending order of estimated time-on-task, enabling tutors to efficiently monitor and find the students in need at scale in real time. To evaluate the performance of EliRank, we build and publish a new real-world dataset consisting of 15 programming exercises solved by 4000+ students in an introduction to programming class at a university. Unlike the currently available open code editing history datasets, our dataset contains code editing operations at a character-level granularity to minimize the loss of contextual information from students. We also introduce diff-augmented abstract syntax tree (DAST), a novel structured code representation that minimizes the loss of fine-grained code change information during code parsing. The evaluation of EliRank on our dataset shows that EliRank effectively finds students with the longest estimated time-on-task, for early detection of students in need. Also, we illustrate in depth (i) the effectiveness of DAST, (ii) the potential to control the tradeoff between early detection and the prediction accuracy of the model, and (iii) the transferability to unseen programming exercise via zero-shot transfer learning.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {204–214},
numpages = {11},
keywords = {code editing history, machine learning, programming education, recommendation system},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@article{10.1016/j.infsof.2023.107202,
author = {Alhoshan, Waad and Ferrari, Alessio and Zhao, Liping},
title = {Zero-shot learning for requirements classification: An exploratory study},
year = {2023},
issue_date = {Jul 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {159},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107202},
doi = {10.1016/j.infsof.2023.107202},
journal = {Inf. Softw. Technol.},
month = {jul},
numpages = {15},
keywords = {0000, 1111, Zero-shot learning, Language models, Contextual word-embeddings, Requirements classification, Zero-shot text classification, Unsupervised learning, Multi-label classification, Transfer learning, Deep learning, Requirements engineering, AI for requirements engineering, AI for software engineering}
}

@inproceedings{10.1145/3622758.3622895,
author = {Marron, Mark},
title = {Toward Programming Languages for Reasoning: Humans, Symbolic Systems, and AI Agents},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622758.3622895},
doi = {10.1145/3622758.3622895},
abstract = {Integration, composition, mechanization, and AI assisted development are the driving themes in the future of software development. At their core these concepts are rooted in the increasingly important role of computing in our world, the desire to deliver functionality faster, with higher quality, and to empower more people to benefit from programmatic automation. These themes, and how they impact the human developers driving them, are the foundations for the next generation of programming languages. At first glance the needs of mechanization tools, AI agents, and human developers along with the various goals around development velocity, software quality, and software democratization are a broad and seemingly diverse set of needs. However, at their core is a single challenge that, once resolved, enables us to make radical progress in all of these areas. Our hypothesis is that, fundamentally, software development is a problem of reasoning about code and semantics. This is true for human developers implementing a feature, symbolic tools building models of application behaviour, and even for language based AI agents as they perform tasks. While the particular aspects of reasoning that each agent struggles with varies to some degree, they share many common themes and, surprisingly, most mainstream languages extensively employ (anti)features that make this task harder or infeasible! This paper proposes a novel approach to this challenge – instead of new language features or logical constructs, that add more complexity to what is already a problem of complexity, we propose radical simplification in the form of the Bosque platform and language.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {136–152},
numpages = {17},
keywords = {Programming Language Design, Reasoning},
location = {<conf-loc>, <city>Cascais</city>, <country>Portugal</country>, </conf-loc>},
series = {Onward! 2023}
}

@inproceedings{10.1145/3597926.3598071,
author = {Gao, Xuanqi and Zhai, Juan and Ma, Shiqing and Shen, Chao and Chen, Yufei and Wang, Shiwei},
title = {CILIATE: Towards Fairer Class-Based Incremental Learning by Dataset and Training Refinement},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598071},
doi = {10.1145/3597926.3598071},
abstract = {Due to the model aging problem, Deep Neural Networks (DNNs) need updates to adjust them to new data distributions. The common practice leverages incremental learning (IL), e.g., Class-based Incremental Learning (CIL) that updates output labels, to update the model with new data and a limited number of old data. This avoids heavyweight training (from scratch) using conventional methods and saves storage space by reducing the number of old data to store. But it also leads to poor performance in fairness. In this paper, we show that CIL suffers both dataset and algorithm bias problems, and existing solutions can only partially solve the problem. We propose a novel framework, CILIATE, that fixes both dataset and algorithm bias in CIL. It features a novel differential analysis guided dataset and training refinement process that identifies unique and important samples overlooked by existing CIL and enforces the model to learn from them. Through this process, CILIATE improves the fairness of CIL by 17.03%, 22.46%, and 31.79% compared to state-of-the-art methods, iCaRL, BiC, and WA, respectively, based on our evaluation on three popular datasets and widely used ResNet models. Our code is available at https://github.com/Antimony5292/CILIATE.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {475–487},
numpages = {13},
keywords = {fairness, incremental learning, neural network},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.24963/ijcai.2023/376,
author = {Zhang, Ming-Liang and Yin, Fei and Liu, Cheng-Lin},
title = {A multi-modal neural geometric solver with textual clauses parsed from diagram},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/376},
doi = {10.24963/ijcai.2023/376},
abstract = {Geometry problem solving (GPS) is a high-level mathematical reasoning requiring the capacities of multi-modal fusion and geometric knowledge application. Recently, neural solvers have shown great potential in GPS but still be short in diagram presentation and modal fusion. In this work, we convert diagrams into basic textual clauses to describe diagram features effectively, and propose a new neural solver called PGPSNet to fuse multimodal information efficiently. Combining structural and semantic pre-training, data augmentation and self-limited decoding, PGPSNet is endowed with rich knowledge of geometry theorems and geometric representation, and therefore promotes geometric understanding and reasoning. In addition, to facilitate the research of GPS, we build a new large-scale and fine-annotated GPS dataset named PGPS9K, labeled with both fine-grained diagram annotation and interpretable solution program. Experiments on PGPS9K and an existing dataset Geometry3K validate the superiority of our method over the state-of-the-art neural solvers. Our code, dataset and appendix material are available at https://github.com/mingliangzhang2018/PGPS.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {376},
numpages = {9},
location = {<conf-loc>, <city>Macao</city>, <country>P.R.China</country>, </conf-loc>},
series = {IJCAI '23}
}

@article{10.1145/3626234,
author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Zowghi, Didar and Jacquet, Aurelie},
title = {Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626234},
doi = {10.1145/3626234},
abstract = {Responsible AI is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of AI. Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. Also, significant efforts have been placed at algorithm-level rather than system-level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize responsible AI from a system perspective, in this paper, we present a Responsible AI Pattern Catalogue based on the results of a Multivocal Literature Review (MLR). Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The Responsible AI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and responsible-AI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement responsible AI.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {oct},
keywords = {Responsible AI, ethical AI, trustworthy AI, AI governance, AI engineering, MLOps, software engineering, software architecture, pattern, best practice}
}

@article{10.1145/3583067,
author = {Govers, Jarod and Feldman, Philip and Dant, Aaron and Patros, Panos},
title = {Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583067},
doi = {10.1145/3583067},
abstract = {Social media is a modern person’s digital voice to project and engage with new ideas and mobilise communities—a power shared with extremists. Given the societal risks of unvetted content-moderating algorithms for Extremism, Radicalisation, and Hate speech (ERH) detection, responsible software engineering must understand the who, what, when, where, and why such models are necessary to protect user safety and free expression. Hence, we propose and examine the unique research field of ERH context mining to unify disjoint studies. Specifically, we evaluate the start-to-finish design process from socio-technical definition-building and dataset collection strategies to technical algorithm design and performance. Our 2015–2021 51-study Systematic Literature Review (SLR) provides the first cross-examination of textual, network, and visual approaches to detecting extremist affiliation, hateful content, and radicalisation towards groups and movements. We identify consensus-driven ERH definitions and propose solutions to existing ideological and geographic biases, particularly due to the lack of research in Oceania/Australasia. Our hybridised investigation on Natural Language Processing, Community Detection, and visual-text models demonstrates the dominating performance of textual transformer-based algorithms. We conclude with vital recommendations for ERH context mining researchers and propose an uptake roadmap with guidelines for researchers, industries, and governments to enable a safer cyberspace.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {319},
numpages = {35},
keywords = {Extremism, radicalisation, machine learning, community detection, Natural Language Processing, neural networks, hate speech, sociolinguistics}
}

@article{10.1145/3643677,
author = {Shang, Xiuwei and Zhang, Shuai and Zhang, Yitong and Guo, Shikai and Li, Yulong and Chen, Rong and Li, Hui and Li, Xiaochen and Jiang, He},
title = {Analyzing and Detecting Information Types of Developer Live Chat Threads},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643677},
doi = {10.1145/3643677},
abstract = {Online chatrooms serve as vital platforms for information exchange among software developers. With multiple developers engaged in rapid communication and diverse conversation topics, the resulting chat messages often manifest complexity and lack structure. To enhance the efficiency of extracting information from chat threads, automatic mining techniques are introduced for thread classification. However, previous approaches still grapple with unsatisfactory classification accuracy, due to two primary challenges that they struggle to adequately capture long-distance dependencies within chat threads and address the issue of category imbalance in labeled datasets. To surmount these challenges, we present a topic classification approach for chat information types named EAEChat. Specifically, EAEChat comprises three core components: the text feature encoding component captures contextual text features using a multi-head self-attention mechanism-based text feature encoder, and a siamese network is employed to mitigate overfitting caused by limited data; the data augmentation component expands a small number of categories in the training dataset using a technique tailored to developer chat messages, effectively tackling the challenge of imbalanced category distribution; the non-text feature encoding component employs a feature fusion model to integrate deep text features with manually extracted non-text features. Evaluation across three real-world projects demonstrates that EAEChat respectively achieves an average precision, recall, and F1-score of 0.653, 0.651, and 0.644, and it marks a significant 7.60% improvement over the state-of-the-art approachs. These findings confirm the effectiveness of our method in proficiently classifying developer chat messages in online chatrooms.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Developer Chatroom, Information Type Classification, Data Augmentation, Deep Learning}
}

@inproceedings{10.1007/978-981-99-8076-5_35,
author = {Du, Gewangzi and Chen, Liwei and Wu, Tongshuai and Zhu, Chenguang and Shi, Gang},
title = {Identify Vulnerability Types: A Cross-Project Multiclass Vulnerability Classification System Based on&nbsp;Deep Domain Adaptation},
year = {2023},
isbn = {978-981-99-8075-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8076-5_35},
doi = {10.1007/978-981-99-8076-5_35},
abstract = {Software Vulnerability Detection(SVD) is a important means to ensure system security due to the ubiquity of software. Deep learning-based approaches achieve state-of-the-art performance in SVD but one of the most crucial issues is coping with the scarcity of labeled data in projects to be detected. One reliable solution is to employ transfer learning skills to leverage labeled data from other software projects. However, existing cross-project approaches only focused on detecting whether the function code is vulnerable or not. The requirement to identify vulnerability types is essential because it offers information to patch the vulnerabilities. Our aim in this paper is to propose the first system for cross-project multiclass vulnerability classification. We detect at the granularity of code snippet, which is finer-grained compare to function and effective to catch inter-procedure vulnerability patterns. After generating code snippets, we define several principles to extract snippet attentions and build a deep model to obtain the fused deep features; We then extend different domain adaptation approaches to reduce feature distributions of different projects. Experimental results indicate that our system outperforms other state-of-the-art systems.},
booktitle = {Neural Information Processing: 30th International Conference, ICONIP 2023, Changsha, China, November 20–23, 2023, Proceedings, Part VI},
pages = {481–499},
numpages = {19},
keywords = {cyber security, multiclass classification, snippet attention, deep learning, domain adaptation},
location = {<conf-loc content-type="InPerson">Changsha, China</conf-loc>}
}

@inproceedings{10.1145/3597926.3598070,
author = {Zhang, Xudong and Cai, Yan},
title = {Building Critical Testing Scenarios for Autonomous Driving from Real Accidents},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598070},
doi = {10.1145/3597926.3598070},
abstract = {One of the aims of the development and spread of autonomous driving technology is to reduce traffic accidents caused by human factors.  
But recently reported data on fatal accidents involving autonomous driving system (ADS) shows that this important goal has not been achieved.  
So there is an emerge requirement on more comprehensive and targeted testing especially on safe driving.  
In this paper, we propose an approach to automatically building critical testing scenarios from real-world accident data.  
Firstly, we propose a new model called M-CPS (Multi-channel Panoptic Segmentation) to extract the effective information from the accident record (such as images or videos), and separate the independent individuals of different traffic participants for further scene recovery.  
Compared with the traditional panoramic segmentation models, M-CPS model is able to effectively handle segmentation challenges due to the shooting angle, image quality, pixel overlap and other problems existing in the accident record.  
Next, the extracted core information is then connected with the virtual testing platform to generate the original scene set.  
Besides, we also design a mutation testing solution on the basis of the original scene set, thus greatly enriching the scene library for testing.  
In our experiments,  
the M-CPS model reaches a result of 66.1% PQ on CityScapes test set, shows that our model has only slight fluctuations on performance compared with the best benchmark model on pure panoptic segmentation task.  
It also reaches a result of 84.5% IoU for semantic segmentation branch and 40.3% mAP for instance segmentation branch on SHIFT dataset.  
Then we use UCF-Crime, CADP and US-Accidents datasets to generate the original and mutated scene set.  
Those generated scene sets are connected to Apollo and Carla simulation platforms to test ADS prototypes.  
We find three types of scenarios that can lead to accidents of ADS prototypes, which indicates that the existing ADS prototype has defects.  
Our solution provides a new possible direction for the recovery of key scenarios in ADS testing, and can improve the efficiency in related fields.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {462–474},
numpages = {13},
keywords = {Autonomous Driving Systems, Panoptic Segmentation, Parameter Mutation, Scene Recovery, Testing},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1145/3617177,
author = {Hu, Han and Huang, Yujin and Chen, Qiuyuan and Zhuo, Terry Yue and Chen, Chunyang},
title = {A First Look at On-device Models in iOS Apps},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617177},
doi = {10.1145/3617177},
abstract = {Powered by the rising popularity of deep learning techniques on smartphones, on-device deep learning models are being used in vital fields such as finance, social media, and driving assistance. Because of the transparency of the Android platform and the on-device models inside, on-device models on Android smartphones have been proven to be extremely vulnerable. However, due to the challenge in accessing and analyzing iOS app files, despite iOS being a mobile platform as popular as Android, there are no relevant works on on-device models in iOS apps. Since the functionalities of the same app on Android and iOS platforms are similar, the same vulnerabilities may exist on both platforms. In this article, we present the first empirical study about on-device models in iOS apps, including their adoption of deep learning frameworks, structure, functionality, and potential security issues. We study why current developers use different on-device models for one app between iOS and Android. We propose a more general attack against white-box models that does not rely on pre-trained models and a new adversarial attack approach based on our findings to target iOS’s gray-box on-device models. Our results show the effectiveness of our approaches. Finally, we successfully exploit the vulnerabilities of on-device models to attack real-world iOS apps.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {26},
numpages = {30},
keywords = {On-device models, iOS, adversarial attack, mobile, iPhone}
}

@article{10.1145/3599975.3599979,
author = {El-Deeb, Ahmed},
title = {The Recent Wave of Generative AI Systems: What Does This Tell Us About What AI Can Do Now?},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3599975.3599979},
doi = {10.1145/3599975.3599979},
abstract = {No, this paper content is not generated by AI. Not sure if that question came across my editor's mind while reviewing submissions. It's a rightful question nowadays after the wave of generative AI apps that swamped our lives between chatGPT to King Charles coronation afterparty videos. Finally, AI is here in our hands that everyone of us can touch and integrate in daily life; after long being a fancy research topic. The question now is how this is going to shape our industry? Or more proactively, how generative AI can help us improve our craft and processes. This paper surveys key areas where generative AI can help us shape our industry and the way we work.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jun},
pages = {13},
numpages = {1}
}

@article{10.1145/3617946.3617956,
author = {Langdon, William B. and Nowack, Vesna and Petke, Justyna and Wagner, Markus and Lee, Hyeonseok and Fredericks, Erik M. and An, Gabin and Blot, Aymeric},
title = {Genetic Improvement @ ICSE 2023},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617956},
doi = {10.1145/3617946.3617956},
abstract = {Following the formal presentations, which included keynotes by Prof. Myra B. Cohen of Iowa State University and Dr. Sebastian Baltes of SAP as well as six papers (which are recorded in the pro- ceedings) there was a wide ranging discussion at the twelfth inter- national Genetic Improvement workshop, GI-2023 @ ICSE held on Saturday 20th May 2023 in Melbourne and online via Zoom. Topics included GI to improve testing, and remove unpleasant surprises in cloud computing costs, incorporating novelty search, large language models (LLM ANN) and GI benchmarks.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {51–59},
numpages = {9}
}

@inproceedings{10.1109/ICSE48619.2023.00138,
author = {Chang, Zhiyuan and Li, Mingyang and Wang, Qing and Li, Shoubin and Wang, Junjie},
title = {Cross-Domain Requirements Linking via Adversarial-Based Domain Adaptation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00138},
doi = {10.1109/ICSE48619.2023.00138},
abstract = {Requirements linking is the core of software system maintenance and evolution, and it is critical to assuring software quality. In practice, however, the requirements links are frequently absent or incorrectly labeled, and reconstructing such ties is time-consuming and error-prone. Numerous learning-based approaches have been put forth to address the problem. However, these approaches will lose effectiveness for the Cold-Start projects with few labeled samples. To this end, we propose RADIATION, an adversarial-based domain adaptation approach for cross-domain requirements linking. Generally, RADIATION firstly adopts an IDF-based Masking strategy to filter the domain-specific features. Then it pre-trains a linking model in the source domain with sufficient labeled samples and adapts the model to target domains using a distance-enhanced adversarial technique without using any labeled target samples. Evaluation on five public datasets shows that RADIATION could achieve 66.4% precision, 89.2% recall, and significantly outperform state-of-the-art baselines by 13.4%-42.9% F1. In addition, the designed components, i.e., IDF-based Masking and Distance-enhanced Loss, could significantly improve performance.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1596–1608},
numpages = {13},
keywords = {cross-domain requirements linking, domain adaptation, adversarial learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3616258,
author = {Cao, Jialun and Lu, Yaojie and Wen, Ming and Cheung, Shing-Chi},
title = {Testing Coreference Resolution Systems without Labeled Test Sets},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616258},
doi = {10.1145/3611643.3616258},
abstract = {Coreference resolution (CR) is a task to resolve different expressions  
(e.g., named entities, pronouns) that refer to the same real-world en-  
tity/event. It is a core natural language processing (NLP) component  
that underlies and empowers major downstream NLP applications  
such as machine translation, chatbots, and question-answering. De-  
spite its broad impact, the problem of testing CR systems has rarely  
been studied. A major difficulty is the shortage of a labeled dataset  
for testing. While it is possible to feed arbitrary sentences as test  
inputs to a CR system, a test oracle that captures their expected  
test outputs (coreference relations) is hard to define automatically.  
To address the challenge, we propose Crest, an automated testing  
methodology for CR systems. Crest uses constituency and depen-  
dency relations to construct pairs of test inputs subject to the same  
coreference. These relations can be leveraged to define the meta-  
morphic relation for metamorphic testing. We compare Crest with  
five state-of-the-art test generation baselines on two popular CR  
systems, and apply them to generate tests from 1,000 sentences  
randomly sampled from CoNLL-2012, a popular dataset for corefer-  
ence resolution. Experimental results show that Crest outperforms  
baselines significantly. The issues reported by Crest are all true  
positives (i.e., 100% precision), compared with 63% to 75% achieved  
by the baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {107–119},
numpages = {13},
keywords = {Coreference resolution testing, Metamorphic testing, SE4AI},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3597926.3598091,
author = {Fang, Pengcheng and Zou, Zhenhua and Xiao, Xusheng and Liu, Zhuotao},
title = {iSyn: Semi-automated Smart Contract Synthesis from Legal Financial Agreements},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598091},
doi = {10.1145/3597926.3598091},
abstract = {Embracing software-driven smart contracts to fulfill legal agreements is a promising direction for digital transformation in the legal sector. Existing solutions mostly consider smart contracts as simple add-ons, without leveraging the programmability of smart contracts to realize complex semantics of legal agreements. In this paper, we propose iSyn, the first end-to-end system that synthesizes smart contracts to fulfill the semantics of financial legal agreements, with minimal human interventions. The design of iSyn centers around a novel intermediate representation (SmartIR) that closes the gap between the natural language sentences and smart contract statements. Specifically, iSyn includes a synergistic pipeline that unifies multiple NLP-techniques to accurately construct SmartIR instances given legal agreements, and performs template-based synthesis based on the SmartIR instances to synthesize smart contracts. We also design a validation framework to verify the correctness and detect known vulnerabilities of the synthesized smart contracts.We evaluate iSyn using legal agreements centering around financial transactions. The results show that iSyn-synthesized smart contracts are syntactically similar and semantically correct (or within a few edits), compared with the “ground truth” smart contracts manually developed by inspecting the legal agreements.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {727–739},
numpages = {13},
keywords = {Natural Language Processing, Program Synthesis, Smart Contracts},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3624918.3625335,
author = {Kasela, Pranav and Pasi, Gabriella and Perego, Raffaele},
title = {SE-PEF: a Resource for Personalized Expert Finding},
year = {2023},
isbn = {9798400704086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624918.3625335},
doi = {10.1145/3624918.3625335},
abstract = {The problem of personalization in Information Retrieval has been under study for a long time. A well-known issue related to this task is the lack of publicly available datasets to support a comparative evaluation of personalized search systems. To contribute in this respect, this paper introduces SE-PEF (StackExchange - Personalized Expert Finding), a resource useful for designing and evaluating personalized models related to the Expert Finding (EF) task. The contributed dataset includes more than 250k queries and 565k answers from 3 306 experts, which are annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. The results of the preliminary experiments conducted show the appropriateness of SE-PEF to evaluate and to train effective EF models.},
booktitle = {Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region},
pages = {288–309},
numpages = {22},
keywords = {Expert Finding, Personalization., Question Answering, User Model},
location = {<conf-loc>, <city>Beijing</city>, <country>China</country>, </conf-loc>},
series = {SIGIR-AP '23}
}

@inproceedings{10.1145/3611643.3616355,
author = {Yu, Siyu and Wu, Yifan and Li, Zhijing and He, Pinjia and Chen, Ningjiang and Liu, Changjian},
title = {Log Parsing with Generalization Ability under New Log Types},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616355},
doi = {10.1145/3611643.3616355},
abstract = {Log parsing, which converts semi-structured logs into structured logs, is the first step for automated log analysis.  
Existing parsers are still unsatisfactory in real-world systems due to new log types in new-coming logs.  
In practice, available logs collected during system runtime often do not contain all the possible log types of a system because log types related to infrequently activated system states are unlikely to be recorded and new log types are frequently introduced with system updates.  
Meanwhile, most existing parsers require preprocessing to extract variables in advance, but preprocessing is based on the operator’s prior knowledge of available logs and therefore may not work well on new log types.  
In addition, parser parameters set based on available logs are difficult to generalize to new log types.  
To support new log types, we propose a variable generation imitation strategy to craft a novel log parsing approach with generalization ability, called Log3T. Log3T employs a pre-trained transformer encoder-based model to extract log templates and can update parameters at parsing time to adapt to new log types by a modified test-time training.  
Experimental results on 16 benchmark datasets show that Log3T outperforms the state-of-the-art parsers in terms of parsing accuracy. In addition, Log3T can automatically adapt to new log types in new-coming logs.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {425–437},
numpages = {13},
keywords = {generalization, log parsing, self supervised, test-time training},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1016/j.jss.2023.111650,
author = {Qin, Rong and Wang, Zeyu and Huang, Sheng and Huangfu, Luwen},
title = {MSTIL: Multi-cue Shape-aware Transferable Imbalance Learning for effective graphic API recommendation},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {200},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111650},
doi = {10.1016/j.jss.2023.111650},
journal = {J. Syst. Softw.},
month = {jun},
numpages = {14},
keywords = {API recommendation, Data augmentation, Transfer learning, Data visualization, Semantic similarity}
}

@inproceedings{10.1145/3631991.3631999,
author = {Asadi, Amir Reza},
title = {LLMs in Design Thinking: Autoethnographic Insights and Design Implications},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3631999},
doi = {10.1145/3631991.3631999},
abstract = {This article presents an autoethnographic exploration of the use of Large Language Models (LLMs) in the context of design thinking. Through personal narratives and reflections, the author examines his experiences integrating LLMs as tools to support and enhance the design thinking process. The article discusses the benefits, challenges, and transformative potential of ChatGPT and Google Bard in facilitating ideation, prototyping, and user-centered design. Drawing on personal anecdotes and observations, the author offers insights into the impact of LLMs on idea generation, problem-solving, and collaboration within the design thinking framework. This autoethnographic approach provides a unique perspective on the integration of LLMs in design thinking, shedding light on their potentials as tools for innovation and fostering the insights of their implications for design practitioners and UX researchers. These insights were also used to develop design implications for designing interactions for LLMs, including the concept of Dynamic LLM Enabled Documents.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {55–60},
numpages = {6},
keywords = {Autoethnography, Design Thinking, LLMs, User Experience Research},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@inproceedings{10.1145/3593434.3593448,
author = {Doan, Thu T. H. and Nguyen, Phuong T. and Di Rocco, Juri and Di Ruscio, Davide},
title = {Too long; didn’t read: Automatic summarization of GitHub README.MD with Transformers},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593448},
doi = {10.1145/3593434.3593448},
abstract = {The ability to allow developers to share their source code and collaborate on software projects has made GitHub a widely used open source platform. Each repository in GitHub is generally equipped with a README.MD file to exhibit an overview of the main functionalities. Nevertheless, while offering useful information, README.MD is usually lengthy, requiring time and effort to read and comprehend. Thus, besides README.MD, GitHub also allows its users to add a short description called “About,” giving a brief but informative summary about the repository. This enables visitors to quickly grasp the main content and decide whether to continue reading. Unfortunately, due to various reasons–not excluding laziness–oftentimes this field is left blank by developers. This paper proposes GitSum as a novel approach to the summarization of README.MD. GitSum is built on top of BART and T5, two cutting-edge deep learning techniques, learning from existing data to perform recommendations for repositories with a missing description. We test its performance using two datasets collected from GitHub. The evaluation shows that GitSum can generate relevant predictions, outperforming a well-established baseline.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {267–272},
numpages = {6},
keywords = {GitHub, README.MD, mining software repositories, recommender systems, summarization},
location = {Oulu, Finland},
series = {EASE '23}
}

@proceedings{10.1145/3622758,
title = {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2023), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet fully proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.  

Onward! 2023 is co-located with SPLASH 2023, running from Sunday 22nd of October till Friday 27th of October, in Cascais, Portugal. We are delighted to have Felienne Hermans giving the Onward! keynote, on Wednesday 25th of October, on "Creating a learnable and inclusive programming language".  

All papers and essays that lie here before you received at least three reviews, leading to a decision of accept, reject, or conditional accept. Authors of conditionally accepted papers were provided with explicit requirements for acceptance, and were carefully re-reviewed in the second phase. The essays track received six submissions, out of which four were accepted. The papers track accepted nine out of nineteen submissions.  

We hope that the papers and essays in these proceedings will stimulate and challenge your thinking about programming and software engineering, and we are looking forward to many discussions at the conference.},
location = {<conf-loc>, <city>Cascais</city>, <country>Portugal</country>, </conf-loc>}
}

@article{10.1145/3617946.3617947,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617947},
doi = {10.1145/3617946.3617947},
abstract = {Where to begin? Things seem to be getting wildly out of control. I make a comment in the arti cial intelligence subsection here, relating to risks of autonomous robotic airplane pilots (Pibot, Wingman). That comment is also applicable to self-driving vehicles, hospital AI, and other life-critical or mission-critical automated and semi-automated systems: We desperately need evidence-based assurance rather than over-hyped assertions that we should simply trust the developers and operating managers. Oth- erwise, the integrity and credibility of our rampantly increasing overdependence on untrustworthy technol- ogy may cause a collapse of trust in our technology. Undoubtedly, avoiding such a systemic failure may require radical changes in how computer science and system engineering should be taught and practiced, along with corresponding oversight, and serious penalties for failures. I once again invoke the Einstein Principle that Everything should be made as simple as possible, but no simpler. It is just one of many important principles that are needed to make our system development into a solid discipline. However, beware of simplistic legislation, charlatans, frauds, and other recurring risks.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {8–12},
numpages = {5}
}

@inproceedings{10.1145/3611643.3616243,
author = {First, Emily and Rabe, Markus and Ringer, Talia and Brun, Yuriy},
title = {Baldur: Whole-Proof Generation and Repair with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616243},
doi = {10.1145/3611643.3616243},
abstract = {Formally verifying software is a highly desirable but labor-intensive task.  
Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.  
This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once.  
We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power.  
This paper:  
(1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques.  
(2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation.  
(3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis.  
We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs,  
empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7% of the theorems. Together, Baldur and Thor can prove 65.7% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1229–1241},
numpages = {13},
keywords = {Proof assistants, automated formal verification, large language models, machine learning, proof repair, proof synthesis},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00161,
author = {Nam, Daye and Myers, Brad and Vasilescu, Bogdan and Hellendoorn, Vincent},
title = {Improving API Knowledge Discovery with ML: A Case Study of Comparable API Methods},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00161},
doi = {10.1109/ICSE48619.2023.00161},
abstract = {Developers constantly learn new APIs, but often lack necessary information from documentation, resorting instead to popular question-and-answer platforms such as Stack Overflow. In this paper, we investigate how to use recent machine-learning-based knowledge extraction techniques to automatically identify pairs of comparable API methods and the sentences describing the comparison from Stack Overflow answers. We first built a prototype that can be stocked with a dataset of comparable API methods and provides tool-tips to users in search results and in API documentation. We conducted a user study with this tool based on a dataset of TensorFlow comparable API methods spanning 198 hand-annotated facts from Stack Overflow posts. This study confirmed that providing comparable API methods can be useful for helping developers understand the design space of APIs: developers using our tool were significantly more aware of the comparable API methods and better understood the differences between them. We then created SOREL, an comparable API methods knowledge extraction tool trained on our hand-annotated corpus, which achieves a 71% precision and 55% recall at discovering our manually extracted facts and discovers 433 pairs of comparable API methods from thousands of unseen Stack Overflow posts. This work highlights the merit of jointly studying programming assistance tools and constructing machine learning techniques to power them.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1890–1906},
numpages = {17},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3632410.3632504,
author = {Bhowmick, Archisman and Mishra, Mayank and Singhal, Rekha},
title = {TASCA : Tool for Automatic SCalable Acceleration of ML pipelines✱},
year = {2024},
isbn = {9798400716348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632410.3632504},
doi = {10.1145/3632410.3632504},
abstract = {Data Scientists use Python for building ML pipelines including pre-processing on data for cleansing and transformation. The computational overheads, due to performance anti-patterns, on data processing can be expensive, especially on large data size. FASCA&nbsp;[10] is a framework to identify such performance anti-patterns, with their corresponding performant versions, from ML pipelines. However, it needs human intervention to generate the performant code. Recent growth in maturity of Large Language Models (LLM) for code generation motivated us to exploit them for automating the process of transforming performance anti-patterns with their performant versions, the feasibility of which has been discussed in &nbsp;[5]. This paper presents, TASCA, a tool which automatically detects potential performance anti-patterns for large data size in ML pipelines and replaces them with their performant version using Large Language Models like GPTNeo3.5/4. The tool has been tested empirically on three real-world workloads, showing substantial performance improvements, including a 70% speedup for a Netflix Series Recommendation pipeline, a 50% boost for a Movie Recommendation pipeline, and a 40% enhancement for an In-house recommendation system training pipeline.},
booktitle = {Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)},
pages = {514–518},
numpages = {5},
keywords = {Code Acceleration, ML Pipeline, Scalability Bottlenecks.},
location = {<conf-loc>, <city>Bangalore</city>, <country>India</country>, </conf-loc>},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3611643.3616295,
author = {Yu, Boxi and Hu, Yiyan and Mang, Qiuyang and Hu, Wenhan and He, Pinjia},
title = {Automated Testing and Improvement of Named Entity Recognition Systems},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616295},
doi = {10.1145/3611643.3616295},
abstract = {Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context. We use TIN to test two SOTA NER models and two commercial NER APIs, i.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues reported by TIN and find that 702 are erroneous issues, leading to high precision (85.0%-93.4%) across four categories of NER errors: omission, over-labeling, incorrect category, and range error. For automated repairing, TIN achieves a high error reduction rate (26.8%-50.6%) over the four systems under test, which successfully repairs 1,056 out of the 1,877 reported NER errors.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {883–894},
numpages = {12},
keywords = {AI software, Metamorphic testing, named entity recognition, software repairing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3613885,
author = {Hu, Yongxiang and Gu, Jiazhen and Hu, Shuqing and Zhang, Yu and Tian, Wenjie and Guo, Shiyu and Chen, Chaoyi and Zhou, Yangfan},
title = {Appaction: Automatic GUI Interaction for Mobile Apps via Holistic Widget Perception},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613885},
doi = {10.1145/3611643.3613885},
abstract = {In industrial practice, GUI (Graphic User Interface) testing of mobile apps still inevitably relies on huge manual efforts. The major efforts are those on understanding the GUIs, so that testing scripts can be written accordingly. Quality assurance could therefore be very labor-intensive, especially for modern commercial mobile apps, where one may include tremendous, diverse, and complex GUIs, e.g., those for placing orders of different commercial items. To reduce such human efforts, we propose Appaction, a learning-based automatic GUI interaction approach we developed for Meituan, one of the largest E-commerce providers with over 600 million users. Appaction can automatically analyze the target GUI and understand what each input of the GUI is about, so that corresponding valid inputs can be entered accordingly. To this end, Appaction adopts a multi-modal model to learn from human experiences in perceiving a GUI. This allows it to infer corresponding valid input events that can properly interact with the GUI. In this way, the target app can be effectively exercised. We present our experiences in Meituan on applying Appaction to popular commercial apps. We demonstrate the effectiveness of Appaction in GUI analysis, and it can perform correct interactions for numerous form pages.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1786–1797},
numpages = {12},
keywords = {GUI Interaction, Mobile Apps, Testing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3624740,
author = {Ding, Zishuo and Tang, Yiming and Cheng, Xiaoyu and Li, Heng and Shang, Weiyi},
title = {
LoGenText-Plus: Improving Neural Machine Translation Based Logging Texts Generation with Syntactic Templates},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624740},
doi = {10.1145/3624740},
abstract = {Developers insert logging statements in the source code to collect important runtime information about software systems. The textual descriptions in logging statements (i.e., logging texts) are printed during system executions and exposed to multiple stakeholders including developers, operators, users, and regulatory authorities. Writing proper logging texts is an important but often challenging task for developers. Prior studies find that developers spend significant efforts modifying their logging texts. However, despite extensive research on automated logging suggestions, research on suggesting logging texts rarely exists. To fill this knowledge gap, we first propose LoGenText (initially reported in our conference paper), an automated approach that uses neural machine translation (NMT) models to generate logging texts by translating the related source code into short textual descriptions. LoGenText takes the preceding source code of a logging text as the input and considers other context information, such as the location of the logging statement, to automatically generate the logging text. LoGenText’s evaluation on 10 open source projects indicates that the approach is promising for automatic logging text generation and significantly outperforms the state-of-the-art approach. Furthermore, we extend LoGenText to LoGenText-Plus by incorporating the syntactic templates of the logging texts. Different from LoGenText, LoGenText-Plus decomposes the logging text generation process into two stages. LoGenText-Plus first adopts an NMT model to generate the syntactic template of the target logging text. Then LoGenText-Plus feeds the source code and the generated template as the input to another NMT model for logging text generation. We also evaluate LoGenText-Plus on the same 10 projects and observe that it outperforms LoGenText on 9 of them. According to a human evaluation from developers’ perspectives, the logging texts generated by LoGenText-Plus have a higher quality than those generated by LoGenText and the prior baseline approach. By manually examining the generated logging texts, we then identify five aspects that can serve as guidance for writing or generating good logging texts. Our work is an important step toward the automated generation of logging statements, which can potentially save developers’ efforts and improve the quality of software logging. Our findings shed light on research opportunities that leverage advances in NMT techniques for automated generation and suggestion of logging statements.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {38},
numpages = {45},
keywords = {Software logging, logging text, neural machine translation}
}

@inproceedings{10.1007/978-981-99-7584-6_7,
author = {Ma, Zhongkui and Feng, Xinguo and Wang, Zihan and Liu, Shuofeng and Ma, Mengyao and Guan, Hao and Meng, Mark Huasong},
title = {Formalizing Robustness Against Character-Level Perturbations for&nbsp;Neural Network Language Models},
year = {2023},
isbn = {978-981-99-7583-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-7584-6_7},
doi = {10.1007/978-981-99-7584-6_7},
abstract = {The remarkable success of neural networks has led to a growing demand for robustness verification and guarantee. However, the discrete nature of text data processed by language models presents challenges in measuring robustness, impeding verification efforts. To address this challenge, this work focuses on formalizing robustness specification against character-level perturbations for neural network language models. We introduce a key principle of three metrics, namely probability distribution, density, and diversity, for generalizing neural network language model perturbations and meanwhile, formulate the robustness specification against character-level perturbed text inputs. Based on the specification, we propose a novel approach to augment existing text datasets with specified perturbations, aiming to guide the robustness training of language models. Experimental results demonstrate that the training with our generated text datasets can enhance the overall robustness of the language model. Our contributions advance the field of neural network verification and provide a promising approach for handling robustness challenges in neural network language models.},
booktitle = {Formal Methods and Software Engineering: 24th International Conference on Formal Engineering Methods, ICFEM 2023, Brisbane, QLD, Australia, November 21–24, 2023, Proceedings},
pages = {100–117},
numpages = {18},
keywords = {Neural network, Language model, Character-level perturbations, Adversarial training, Robustness},
location = {<conf-loc content-type="InPerson">Brisbane, QLD, Australia</conf-loc>}
}

@inproceedings{10.1109/ICSE48619.2023.00054,
author = {Wu, Weibin and Zhang, Jianping and Wei, Victor Junqiu and Chen, Xixian and Zheng, Zibin and King, Irwin and Lyu, Michael R.},
title = {Practical and Efficient Model Extraction of Sentiment Analysis APIs},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00054},
doi = {10.1109/ICSE48619.2023.00054},
abstract = {Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {524–536},
numpages = {13},
keywords = {model extraction, sentiment analysis APIS, active learning, architecture search},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1109/TSE.2023.3294971,
author = {Wang, Yihui and Liu, Huaxiao and Gao, Shanquan and Tang, Xiao},
title = {Animation2API: API Recommendation for the Implementation of Android UI Animations},
year = {2023},
issue_date = {Sept. 2023},
publisher = {IEEE Press},
volume = {49},
number = {9},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2023.3294971},
doi = {10.1109/TSE.2023.3294971},
abstract = {UI animations, such as card movement and menu slide in/out, provide appealing user experience and enhance the usability of mobile applications. In the process of UI animation implementation, it is difficult for developers to identify suitable APIs for the animation to be implemented from a large number of APIs. Fortunately, the huge app market contains millions of apps, and they can provide valuable data resources for solving this problem. By summarizing the API usage for the same or similar animations in apps, reusable knowledge can be mined for the API recommendation. In this paper, we propose a novel method Animation2API, which mines the knowledge about APIs from existing apps and recommends APIs for UI animations. Different from existing text-based API recommendation approaches, Animation2API takes the UI animation in GIF/video format as query input. Firstly, we construct a database containing mappings between UI animations and APIs by analyzing a broad set of apps. Then, we build a UI animation feature extractor, which can be used to gain temporal-spatial feature vectors of UI animations. By comparing the temporal-spatial feature vectors between UI animations, we identify animations that are similar to the query animation from the database. Finally, we summarize the APIs used for implementing these animations and recommend a list of APIs for developers. The empirical evaluation results show that our method can achieve 82.66% &lt;bold&gt;&lt;italic&gt;Success&amp;#x00A0;rate&lt;/italic&gt;&lt;/bold&gt; and outperform the baseline Guru by 230.77% and 184.95% in terms of &lt;bold&gt;&lt;italic&gt;Precision&lt;/italic&gt;&lt;/bold&gt; and &lt;bold&gt;&lt;italic&gt;Recall&lt;/italic&gt;&lt;/bold&gt; when considering twenty APIs. In the user study, we take the scenarios of using web search and ChatGPT to implement animations as baselines, and the results show that participants can complete animations faster (14.54%) after using Animation2API. Furthermore, participants&amp;#x2019; positive feedbacks on the questionnaire indicate the usefulness of Animation2API.},
journal = {IEEE Trans. Softw. Eng.},
month = {sep},
pages = {4411–4428},
numpages = {18}
}

@article{10.1145/3635439.3635440,
author = {Neumann, Peter G.},
title = {Risks to the Public},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3635439.3635440},
doi = {10.1145/3635439.3635440},
abstract = {Edited by PGN (Risks Forum Moderator, with contribu- tions by others as indicated. Opinions are individual rather than organizational, with usual disclaimers implied. We ad- dress problems relating to software, hardware, people, and other circumstances relevant to computer systems. Ref- erences (R i j) to the online Risks Forum denote RISKS vol i number j. Cited RISKS items generally identify contributors and sources, together with URLs. Official RISKS archives are available at www.risks.org, with nice html formatting and search engine courtesy of Lindsay Mar- shall at Newcastle: http://catless.ncl.ac.uk/Risks/i.j.html (also ftp://www.sri.com/risks). CACM Inside Risks: http://www.csl.sri.com/neumann/insiderisks.html},
journal = {SIGSOFT Softw. Eng. Notes},
month = {dec},
pages = {4–9},
numpages = {6}
}

@inproceedings{10.1109/ICSE48619.2023.00158,
author = {Kou, Bonan and Chen, Muhao and Zhang, Tianyi},
title = {Automated Summarization of Stack Overflow Posts},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00158},
doi = {10.1109/ICSE48619.2023.00158},
abstract = {Software developers often resort to Stack Overflow (SO) to fill their programming needs. Given the abundance of relevant posts, navigating them and comparing different solutions is tedious and time-consuming. Recent work has proposed to automatically summarize SO posts to concise text to facilitate the navigation of SO posts. However, these techniques rely only on information retrieval methods or heuristics for text summarization, which is insufficient to handle the ambiguity and sophistication of natural language.This paper presents a deep learning based framework called Assort for SO post summarization. Assort includes two complementary learning methods, AssortS and AssortIS, to address the lack of labeled training data for SO post summarization. AssortS is designed to directly train a novel ensemble learning model with BERT embeddings and domain-specific features to account for the unique characteristics of SO posts. By contrast, AssortIS is designed to reuse pre-trained models while addressing the domain shift challenge when no training data is present (i.e., zero-shot learning). Both AssortS and AssortIS outperform six existing techniques by at least 13% and 7% respectively in terms of the F1 score. Furthermore, a human study shows that participants significantly preferred summaries generated by AssortS and AssortIS over the best baseline, while the preference difference between AssortS and AssortIS was small.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1853–1865},
numpages = {13},
keywords = {stack overflow, text summarization, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3583131.3590477,
author = {Li Calsi, Davide and Duran, Matias and Laurent, Thomas and Zhang, Xiao-Yi and Arcaini, Paolo and Ishikawa, Fuyuki},
title = {Adaptive Search-based Repair of Deep Neural Networks},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590477},
doi = {10.1145/3583131.3590477},
abstract = {Deep Neural Networks (DNNs) are finding a place at the heart of more and more critical systems, and it is necessary to ensure they perform in as correct a way as possible. Search-based repair methods, that search for new values for target neuron weights in the network to better process fault-inducing inputs, have shown promising results. These methods rely on fault localisation to determine what weights the search should target. However, as the search progresses and the network evolves, the weights responsible for the faults in the system will change, and the search will lose in effectiveness. In this work, we propose an adaptive search method for DNN repair that adaptively updates the target weights during the search by performing fault localisation on the current state of the model. We propose and implement two methods to decide when to update the target weights, based on the progress of the search's fitness value or on the evolution of fault localisation results. We apply our technique to two image classification DNN architectures against a dataset of autonomous driving images, and compare it with a state-of-the art search-based DNN repair approach.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1527–1536},
numpages = {10},
keywords = {deep neural networks, DNN repair, search-based software engineering, fault localisation},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@article{10.1145/3599975.3599977,
author = {Groce, Alex},
title = {Passages},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3599975.3599977},
doi = {10.1145/3599975.3599977},
abstract = {John Sladek's The Complete Roderick [1] collects both of Sladek's novels about the eponymous (though almost everyone gets his name wrong throughout the books) robot: Roderick, or The Education of a Young Machine and sequel, Roderick at Random, or Further Education of a Young Machine. Presumably named for the hero of Tobias Smollet's picaresque 18th century novel The Adventures of Roderick Random, Roderick has his own series of random and absurd adventures.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jun},
pages = {8},
numpages = {1}
}

@inproceedings{10.1145/3584871.3584908,
author = {Qu, Haicheng and Zhang, Xuecong},
title = {Pruning Networks Using Filters Similarity Stability},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584908},
doi = {10.1145/3584871.3584908},
abstract = {Current filter pruning methods rely too much on pretrained weights and have many super parameters, resulting in obvious performance degradation and too long parameters adjustment time. In our research, we found that the cosine similarity distribution between filters can achieve stable in a few epochs during training. Therefore, a cluster pruning method named ECP(Early Cluster Pruning) based on the cosine similarity between filters in the early stage of training is proposed to compress the deep neural networks. First, in the early stage of training, the filters were clustered with a gradually increasing threshold, and then the reserved filters were selected randomly in each cluster. The pruned models could be obtained with only a few super parameters and a single training progress, leading to an obvious reduction in algorithmic complexity and large savings in training time. The experimental results on CIFAR-10 and CIFAR-100 datasets show that ECP method outperforms recent pruning methods in terms of model accuracy maintenance, training time, and model compression rate.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {256–260},
numpages = {5},
keywords = {Clustering, Cosine similarity, Distribution stability, Filter pruning},
location = {<conf-loc>, <city>Palmerston North</city>, <country>New Zealand</country>, </conf-loc>},
series = {ICSIM '23}
}

@inproceedings{10.1109/ICSE48619.2023.00214,
author = {Zhou, Jiayuan and Pacheco, Michael and Chen, Jinfu and Hu, Xing and Xia, Xin and Lo, David and Hassan, Ahmed E.},
title = {CoLeFunDa: Explainable Silent Vulnerability Fix Identification},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00214},
doi = {10.1109/ICSE48619.2023.00214},
abstract = {It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (e.g., the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important.However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (i.e., code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, i.e., silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5% (25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2565–2577},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00092,
author = {Ren, Xiaoning and Lin, Yun and Xue, Yinxing and Liu, Ruofan and Sun, Jun and Feng, Zhiyong and Dong, Jin Song},
title = {DeepArc: Modularizing Neural Networks for the Model Maintenance},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00092},
doi = {10.1109/ICSE48619.2023.00092},
abstract = {Neural networks are an emerging data-driven programming paradigm widely used in many areas. Unlike traditional software systems consisting of decomposable modules, a neural network is usually delivered as a monolithic package, raising challenges for some maintenance tasks such as model restructure and re-adaption. In this work, we propose DeepArc, a novel modularization method for neural networks, to reduce the cost of model maintenance tasks. Specifically, DeepArc decomposes a neural network into several consecutive modules, each of which encapsulates consecutive layers with similar semantics. The network modularization facilitates practical tasks such as refactoring the model to preserve existing features (e.g., model compression) and enhancing the model with new features (e.g., fitting new samples). The modularization and encapsulation allow us to restructure or retrain the model by only pruning and tuning a few localized neurons and layers. Our experiments show that (1) DeepArc can boost the runtime efficiency of the state-of-the-art model compression techniques by 14.8%; (2) compared to the traditional model retraining, DeepArc only needs to train less than 20% of the neurons on average to fit adversarial samples and repair under-performing models, leading to 32.85% faster training performance while achieving similar model prediction performance.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1008–1019},
numpages = {12},
keywords = {architecture, modularization, neural networks},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3624734,
author = {Sun, Jiamou and Xing, Zhenchang and Xia, Xin and Lu, Qinghua and Xu, Xiwei and Zhu, Liming},
title = {Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624734},
doi = {10.1145/3624734},
abstract = {Vulnerable third-party libraries pose significant threats to software applications that reuse these libraries. At an industry scale of reuse, manual analysis of third-party library vulnerabilities can be easily overwhelmed by the sheer number of vulnerabilities continually collected from diverse sources for thousands of reused libraries. Our study of four large-scale, actively maintained vulnerability databases (NVD, IBM X-Force, ExploitDB, and Openwall) reveals the wide presence of information discrepancies, in terms of seven vulnerability aspects, i.e., product, version, component, vulnerability type, root cause, attack vector, and impact, between the reports for the same vulnerability from heterogeneous sources. It would be beneficial to integrate and cross-validate multi-source vulnerability information, but it demands automatic aspect extraction and aspect discrepancy detection. In this work, we experimented with a wide range of NLP methods to extract named entities (e.g., product) and free-form phrases (e.g., root cause) from textual vulnerability reports and to detect semantically different aspect mentions between the reports. Our experiments confirm the feasibility of applying NLP methods to automate aspect-level vulnerability analysis and identify the need for domain customization of general NLP methods. Based on our findings, we propose a discrepancy-aware, aspect-level vulnerability knowledge graph and a KG-based web portal that integrates diversified vulnerability key aspect information from heterogeneous vulnerability databases. Our conducted user study proves the usefulness of our web portal. Our study opens the door to new types of vulnerability integration and management, such as vulnerability portraits of a product and explainable prediction of silent vulnerabilities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {49},
numpages = {38},
keywords = {Vulnerability key aspect, information discrepancy, hetergeneous vulnerability reports}
}

@inproceedings{10.5555/978-981-99-7584-6_fm,
title = {Front Matter},
year = {2023},
isbn = {978-981-99-7583-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Formal Methods and Software Engineering: 24th International Conference on Formal Engineering Methods, ICFEM 2023, Brisbane, QLD, Australia, November 21–24, 2023, Proceedings},
pages = {i–xxviii},
location = {<conf-loc content-type="InPerson">Brisbane, QLD, Australia</conf-loc>}
}

@inproceedings{10.1145/3597926.3605231,
author = {Downing, Mara},
title = {Quantitative Robustness Analysis of Neural Networks},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3605231},
doi = {10.1145/3597926.3605231},
abstract = {Neural networks are an increasingly common tool for solving problems that require complex analysis and pattern matching, such as identifying stop signs or processing medical imagery. Accordingly, verification of neural networks for safety and correctness is of great importance, as mispredictions can have catastrophic results in safety critical domains. One metric for verification is robustness, which answers whether or not a misclassified input exists in a given input neighborhood. I am focusing my research at quantitative robustness---finding not only if there exist misclassified inputs within a given neighborhood but also how many exist as a proportion of the neighborhood size. My overall goal is to expand the research on quantitative neural network robustness verification and create a variety of quantitative verification tools geared towards expanding our understanding of neural network robustness.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1527–1531},
numpages = {5},
keywords = {Neural Network Verification, Quantitative Verification, Robustness},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1007/978-3-031-48796-5_10,
author = {Berger, Harel and Dakhama, Aidan and Ding, Zishuo and Even-Mendoza, Karine and Kelly, David and Menendez, Hector and Moussa, Rebecca and Sarro, Federica},
title = {StableYolo: Optimizing Image Generation for&nbsp;Large Language Models},
year = {2023},
isbn = {978-3-031-48795-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48796-5_10},
doi = {10.1007/978-3-031-48796-5_10},
abstract = {AI-based image generation is bounded by system parameters and the way users define prompts. Both prompt engineering and AI tuning configuration are current open research challenges and they require a significant amount of manual effort to generate good quality images. We tackle this problem by applying evolutionary computation to Stable Diffusion, tuning both prompts and model parameters simultaneously. We guide our search process by using Yolo. Our experiments show that our system, dubbed StableYolo, significantly improves image quality (52% on average compared to the baseline), helps identify relevant words for prompts, reduces the number of GPU inference steps per image (from 100 to 45 on average), and keeps the length of the prompt short (≈ 7 keywords).},
booktitle = {Search-Based Software Engineering: 15th International Symposium, SSBSE 2023, San Francisco, CA, USA, December 8, 2023, Proceedings},
pages = {133–139},
numpages = {7},
keywords = {LLMS, SBSE, Image Generation, Stable Diffusion, Yolo},
location = {<conf-loc content-type="Hybrid">San Francisco, CA, USA</conf-loc>}
}

@inproceedings{10.1145/3611643.3613880,
author = {Feng, Sidong and Lu, Haochuan and Xiong, Ting and Deng, Yuetang and Chen, Chunyang},
title = {Towards Efficient Record and Replay: A Case Study in WeChat},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613880},
doi = {10.1145/3611643.3613880},
abstract = {WeChat, a widely-used messenger app boasting over 1 billion monthly active users, requires effective app quality assurance for its complex features. Record-and-replay tools are crucial in achieving this goal. Despite the extensive development of these tools, the impact of waiting time between replay events has been largely overlooked. On one hand, a long waiting time for executing replay events on fully-rendered GUIs slows down the process. On the other hand, a short waiting time can lead to events executing on partially-rendered GUIs, negatively affecting replay effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We introduce WeReplay, a lightweight image-based approach that dynamically adjusts inter-event time based on the GUI rendering state. Given the real-time streaming on the GUI, WeReplay employs a deep learning model to infer the rendering state and synchronize with the replaying tool, scheduling the next event when the GUI is fully rendered. Our evaluation shows that our model achieves 92.1% precision and 93.3% recall in discerning GUI rendering states in the WeChat app. Through assessing the performance in replaying 23 common WeChat usage scenarios, WeReplay successfully replays all scenarios on the same and different devices more efficiently than the state-of-the-practice baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1681–1692},
numpages = {12},
keywords = {Efficient record and replay, GUI rendering, Machine Learning},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3597926.3598146,
author = {Ren, Kunlun and Qiang, Weizhong and Wu, Yueming and Zhou, Yi and Zou, Deqing and Jin, Hai},
title = {An Empirical Study on the Effects of Obfuscation on Static Machine Learning-Based Malicious JavaScript Detectors},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598146},
doi = {10.1145/3597926.3598146},
abstract = {Machine learning is increasingly being applied to malicious JavaScript detection in response to the growing number of Web attacks and the attendant costly manual identification. In practice, to hide their malicious behaviors or protect intellectual copyrights, both malicious and benign scripts tend to obfuscate their own code before uploading. While obfuscation is beneficial, it also introduces some additional code features (e.g., dead code) into the code. When machine learning is employed to learn a malicious JavaScript detector, these additional features can affect the model to make it less effective. However, there is still a lack of clear understanding of how robust existing machine learning-based detectors are on different obfuscators.  
In this paper, we conduct the first empirical study to figure out how obfuscation affects machine learning detectors based on static features. Through the results, we observe several findings: 1) Obfuscation has a significant impact on the effectiveness of detectors, causing an increase both in false negative rate (FNR) and false positive rate (FPR), and the bias of obfuscation in the training  
set induces detectors to detect obfuscation rather than malicious behaviors. 2) The common measures such as improving the quality of the training set by adding relevant obfuscated samples and leveraging state-of-the-art deep learning models can not work well.3) The root cause of obfuscation effects on these detectors is that feature spaces they use can only reflect shallow differences in code, not about the nature of benign and malicious, which can be easily affected by the differences brought by obfuscation. 4) Obfuscation has a similar effect on realistic detectors in VirusTotal, indicating that this is a common real-world problem.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1420–1432},
numpages = {13},
keywords = {JavaScript obfuscation, machine learning, malicious JavaScript detector, web security},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1145/3582571,
author = {Xu, Qinghua and Ali, Shaukat and Yue, Tao},
title = {Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3582571},
doi = {10.1145/3582571},
abstract = {Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {113},
numpages = {32},
keywords = {Cyber-physical system, digital twin, curriculum learning, deep learning, anomaly detection}
}

@inproceedings{10.1145/3586182.3615825,
author = {Aveni, Timothy J. and Fox, Armando and Hartmann, Björn},
title = {Bringing Context-Aware Completion Suggestions to Arbitrary Text Entry Interfaces},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3615825},
doi = {10.1145/3586182.3615825},
abstract = {Large language models (LLMs) can predict “obvious” next steps that users will take in text entry fields, especially the tedious components of tasks like software engineering or email composition. These models are not only useful in large, unbroken text fields, however. We present OmniFill, a browser extension that detects text entry fields and offers “autofill”-style suggestions based on context from the browsing session. The system constructs an LLM prompt that includes three main components: (a) a description of the active tab’s text fields and their current values, (b) information from the user’s recent web browsing context, and (c) a history, if available, of the user’s prior submissions to the web form (alongside those submissions’ associated browsing context). Suggestions from the LLM’s response are offered to the user to be automatically typed into each corresponding text field. We offer a motivating example of a time-saving interaction and discuss the broader utility of interface-agnostic LLM integrations.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {77},
numpages = {3},
keywords = {Web accessibility, intelligent user interfaces, large language models},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {UIST '23 Adjunct}
}

@inproceedings{10.5555/3618408.3619598,
author = {Ramé, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, Léon and Lopez-Paz, David},
title = {Model ratatouille: recycling diverse models for out-of-distribution generalization},
year = {2023},
publisher = {JMLR.org},
abstract = {Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models. Our code is released here.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1190},
numpages = {24},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1145/3597926.3598074,
author = {Huang, Huaxun and Xu, Chi and Wen, Ming and Liu, Yepang and Cheung, Shing-Chi},
title = {ConfFix: Repairing Configuration Compatibility Issues in Android Apps},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598074},
doi = {10.1145/3597926.3598074},
abstract = {XML configuration files are widely-used to specify the user interfaces (UI) of Android apps. Configuration compatibility (CC) issues are induced owing to the inconsistent handling of such XML configuration files across different Android framework versions. CC issues can cause software crashes and inconsistent look-and-feels, severely impacting the user experience of Android apps. However, there is no universal solution to resolve CC issues and app developers need to handle CC issues case by case. Existing tools are designed based on predefined rules or visual features that are possibly manifested by CC issues. Unfortunately, they can fail or generate overfitting patches when the CC issues are beyond their capabilities. To fill the above research gaps, we first empirically studied the app developers' common strategies in patching real-world CC issues. Based on the findings, we propose ConfFix, an automatic approach to repair CC issues in Android apps. ConfFix is driven by the knowledge of how an XML element is handled inconsistently in different versions of the Android framework and generates patches to eliminate such inconsistencies. We evaluated ConfFix on a set of 77 reproducible CC issues in 13 open-source Android apps. The results show that ConfFix outperforms baselines in successfully repairing 64 CC issues with a high precision. Encouragingly, the patches for 38 CC issues have been confirmed and merged by app developers.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {514–525},
numpages = {12},
keywords = {Android, Automated Repair, Compatibility, XML Configuration},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1007/s11704-023-2678-8,
author = {Gou, Qianwen and Dong, Yunwei and Wu, YuJiao and Ke, Qiao},
title = {Semantic similarity-based program retrieval: a multi-relational graph perspective},
year = {2023},
issue_date = {Jun 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-023-2678-8},
doi = {10.1007/s11704-023-2678-8},
abstract = {In this paper, we formulate the program retrieval problem as a graph similarity problem. This is achieved by first explicitly representing queries and program snippets as AMR and CPG, respectively. Then, through intra-level and inter-level attention mechanisms to infer fine-grained correspondence by propagating node correspondence along the graph edge. Moreover, such a design can learn correspondence of nodes at different levels, which were mostly ignored by previous works. Experiments have demonstrated the superiority of USRAE.},
journal = {Front. Comput. Sci.},
month = {dec},
numpages = {3}
}

@inproceedings{10.1145/3597926.3598072,
author = {Cheng, Mingfei and Zhou, Yuan and Xie, Xiaofei},
title = {BehAVExplor: Behavior Diversity Guided Testing for Autonomous Driving Systems},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598072},
doi = {10.1145/3597926.3598072},
abstract = {Testing Autonomous Driving Systems (ADSs) is a critical task for ensuring the reliability and safety of autonomous vehicles. Existing methods mainly focus on searching for safety violations while the diversity of the generated test cases is ignored, which may generate many redundant test cases and failures. Such redundant failures can reduce testing performance and increase failure analysis costs. In this paper, we present a novel behavior-guided fuzzing technique (BehAVExplor) to explore the different behaviors of the ego vehi- cle (i.e., the vehicle controlled by the ADS under test) and detect diverse violations. Specifically, we design an efficient unsupervised model, called BehaviorMiner, to characterize the behavior of the ego vehicle. BehaviorMiner extracts the temporal features from the given scenarios and performs a clustering-based abstraction to group behaviors with similar features into abstract states. A new test case will be added to the seed corpus if it triggers new behav- iors (e.g., cover new abstract states). Due to the potential conflict between the behavior diversity and the general violation feedback, we further propose an energy mechanism to guide the seed selec- tion and the mutation. The energy of a seed quantifies how good it is. We evaluated BehAVExplor on Apollo, an industrial-level ADS, and LGSVL simulation environment. Empirical evaluation results show that BehAVExplor can effectively find more diverse violations than the state-of-the-art.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {488–500},
numpages = {13},
keywords = {Apollo, Autonomous driving systems, behavior diversity, critical scenarios, fuzzing},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1016/j.infsof.2023.107287,
author = {Ye, Tong and Zhuang, Yi and Qiao, Gongzhe},
title = {MDSSED: A safety and security enhanced model-driven development approach for smart home apps},
year = {2023},
issue_date = {Nov 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {163},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107287},
doi = {10.1016/j.infsof.2023.107287},
journal = {Inf. Softw. Technol.},
month = {nov},
numpages = {18},
keywords = {Security, Safety, Smart home apps, Model-driven software development, Formal method}
}

@article{10.1145/3597203,
author = {Wang, Shangwen and Wen, Ming and Lin, Bo and Liu, Yepang and Bissyandé, Tegawendé F. and Mao, Xiaoguang},
title = {Pre-implementation Method Name Prediction for Object-oriented Programming},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597203},
doi = {10.1145/3597203},
abstract = {Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {157},
numpages = {35},
keywords = {Method name prediction, naming convention}
}

@article{10.1145/3641847,
author = {Liu, Puzhuo and Zheng, Yaowen and Sun, Chengnian and Li, Hong and Li, Zhi and Sun, Limin},
title = {Battling against Protocol Fuzzing: Protecting Networked Embedded Devices from Dynamic Fuzzers},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3641847},
doi = {10.1145/3641847},
abstract = {Networked Embedded Devices (NEDs) are increasingly targeted by cyberattacks, mainly due to their widespread use in our daily lives. Vulnerabilities in NEDs are the root causes of these cyberattacks. Although deployed NEDs go through thorough code audits, there can still be considerable exploitable vulnerabilities. Existing mitigation measures like code encryption and obfuscation adopted by vendors can resist static analysis on deployed NEDs, but are ineffective against protocol fuzzing. Attackers can easily apply protocol fuzzing to discover vulnerabilities and compromise deployed NEDs. Unfortunately, prior anti-fuzzing techniques are impractical as they significantly slow down NEDs, hampering NED availability. To address this issue, we propose Armor—the first anti-fuzzing technique specifically designed for NEDs. First, we design three adversarial primitives—delay, fake coverage, and forged exception—to break the fundamental mechanisms on which fuzzing relies to effectively find vulnerabilities. Second, based on our observation that inputs from normal users consistent with the protocol specification and certain program paths are rarely executed with normal inputs, we design static and dynamic strategies to decide whether to activate the adversarial primitives. Extensive evaluations show that Armor incurs negligible time overhead and effectively reduces the code coverage (e.g., line coverage by 22%-61%) for fuzzing, significantly outperforming the state-of-the-art.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Internet of Things, protocol fuzzing, anti-fuzzing}
}

@inproceedings{10.1145/3597926.3598094,
author = {Yu, Boxi and Zhong, Zhiqing and Li, Jiaqi and Yang, Yixing and He, Shilin and He, Pinjia},
title = {ROME: Testing Image Captioning Systems via Recursive Object Melting},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598094},
doi = {10.1145/3597926.3598094},
abstract = {Image captioning (IC) systems aim to generate a text description of the salient objects in an image. In recent years, IC systems have been increasingly integrated into our daily lives, such as assistance for visually-impaired people and description generation in Microsoft Powerpoint. However, even the cutting-edge IC systems (e.g., Microsoft Azure Cognitive Services) and algorithms (e.g., OFA) could produce erroneous captions, leading to incorrect captioning of important objects, misunderstanding, and threats to personal safety. The existing testing approaches either fail to handle the complex form of IC system output (i.e., sentences in natural language) or generate unnatural images as test cases. To address these problems, we introduce Recursive Object MElting (ROME), a novel metamorphic testing approach for validating IC systems. Different from existing approaches that generate test cases by inserting objects, which easily make the generated images unnatural, ROME melts (i.e., remove and inpaint) objects. ROME assumes that the object set in the caption of an image includes the object set in the caption of a generated image after object melting. Given an image, ROME can recursively remove its objects to generate different pairs of images. We use ROME to test one widely-adopted image captioning API and four state-of-the-art (SOTA) algorithms. The results show that the test cases generated by ROME look much more natural than the SOTA IC testing approach and they achieve comparable naturalness to the original images. Meanwhile, by generating test pairs using 226 seed images, ROME reports a total of 9,121 erroneous issues with high precision (86.47%-92.17%). In addition, we further utilize the test cases generated by ROME to retrain the Oscar, which improves its performance across multiple evaluation metrics.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {766–778},
numpages = {13},
keywords = {AI software, Metamorphic testing, image captioning, testing},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1007/978-3-031-44699-3_1,
author = {Qu, Qian and Wang, Jian and Yang, Kexin and Zhang, Hang and Lv, Jiancheng},
title = {Fantastic Gradients and&nbsp;Where to&nbsp;Find Them: Improving Multi-attribute Text Style Transfer by&nbsp;Quadratic Program},
year = {2023},
isbn = {978-3-031-44698-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44699-3_1},
doi = {10.1007/978-3-031-44699-3_1},
abstract = {Unsupervised text style transfer (TST) is an important task with extensive implications in natural language generation (NLG). A prevalent approach involves editing the latent representations of text, guided by gradients from an attribute classifier. However, in multi-attribute TST, the simultaneous satisfaction of all required attributes remains challenging. In this paper, we unveil that the gradient direction during editing might conflict with certain attribute representations through empirical analysis. To tackle this problem, we introduce a mathematical programming method to impose constraints on the editing direction of multiple attributes, effectively mitigating potential attribute conflicts during the inference stage. Our proposed method considers the potential conflict between different attributes for the first time. Experimental results from the YELP benchmark showcase that our method can effectively improve the multi-attribute-transfer accuracy and quality without compromising single attribute performance. Moreover, our method can be readily integrated with pre-trained auto-encoders, providing an effective and scalable solution for multi-attribute scenarios.},
booktitle = {Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part III},
pages = {3–14},
numpages = {12},
keywords = {Text Style Transfer, Multiple-Attribute Text Generation, Auto-Encoder, Quadratic Program},
location = {Foshan, China}
}

@inproceedings{10.1145/3628797.3628996,
author = {Nguyen Hung, Thinh and Nguyen Phuc, Hai and Tran Dinh, Khoa and Le Tran Thanh, Nhan and To Trong, Nghia and Ngo Khanh, Khoa and Phan The, Duy and Pham Van, Hau},
title = {Binary Representation Embedding and Deep Learning For Binary Code Similarity Detection in Software Security Domain},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628996},
doi = {10.1145/3628797.3628996},
abstract = {Binary Code Similarity Detection (BCSD) is the process of analyzing the binary representations of two functions, programs, or related entities to generate a quantitative output that signifies the similarity score between them. This task encompasses a wide range of applications, including addressing the binary search problem, which involves searching for code segments within a binary file that are similar to a specified binary code segment. These capabilities open up numerous potential applications within the domain of binary code analysis such as software vulnerability detection, clone detection, and malware analysis. In this paper, we introduce BiSim-Inspector, a BCSD tool based on Deep Learning (DL). This tool leverages the Bytes2vec method, which we develop to transform the bytecode of binary functions into vectors, which are then fed into the Convolutional Neural Network - Gated Recurrent Unit (CNN-GRU) model. Additionally, we conducted a series of experiments to assess the effectiveness of our method by comparing it with existing state-of-the-art (SOTA) tools. We use a large-scale, well-structured, and diversified dataset, BinaryCorp, for the task of BCSD. The results show that our framework achieves a Recall rate of 89%, which is 25% higher than existing SOTA methods, without compromising the training and prediction time.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {785–792},
numpages = {8},
keywords = {Binary Code Similarity Detection, Deep Learning, Software Security},
location = {<conf-loc>, <city>Ho Chi Minh</city>, <country>Vietnam</country>, </conf-loc>},
series = {SOICT '23}
}

@article{10.1145/3604611,
author = {Yang, Shouguo and Dong, Chaopeng and Xiao, Yang and Cheng, Yiran and Shi, Zhiqiang and Li, Zhi and Sun, Limin},
title = {Asteria-Pro: Enhancing Deep Learning-based Binary Code Similarity Detection by Incorporating Domain Knowledge},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3604611},
doi = {10.1145/3604611},
abstract = {Widespread code reuse allows vulnerabilities to proliferate among a vast variety of firmware. There is an urgent need to detect these vulnerable codes effectively and efficiently. By measuring code similarities, AI-based binary code similarity detection is applied to detecting vulnerable code at scale. Existing studies have proposed various function features to capture the commonality for similarity detection. Nevertheless, the significant code syntactic variability induced by the diversity of IoT hardware architectures diminishes the accuracy of binary code similarity detection. In our earlier study and the tool Asteria, we adopted a Tree-LSTM network to summarize function semantics as function commonality, and the evaluation result indicates an advanced performance. However, it still has utility concerns due to excessive time costs and inadequate precision while searching for large-scale firmware bugs.To this end, we propose a novel deep learning-enhancement architecture by incorporating domain knowledge-based pre-filtration and re-ranking modules, and we develop a prototype named Asteria-Pro based on Asteria. The pre-filtration module eliminates dissimilar functions, thus reducing the subsequent deep learning-model calculations. The re-ranking module boosts the rankings of vulnerable functions among candidates generated by the deep learning model. Our evaluation indicates that the pre-filtration module cuts the calculation time by 96.9%, and the re-ranking module improves MRR and Recall by 23.71% and 36.4%, respectively. By incorporating these modules, Asteria-Pro outperforms existing state-of-the-art approaches in the bug search task by a significant margin. Furthermore, our evaluation shows that embedding baseline methods with pre-filtration and re-ranking modules significantly improves their precision. We conduct a large-scale real-world firmware bug search, and Asteria-Pro manages to detect 1,482 vulnerable functions with a high precision 91.65%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {1},
numpages = {40},
keywords = {Binary code similarity detection, pre-fitering, re-ranking, abstract syntactic tree, graph neural network}
}

@inproceedings{10.1145/3605157.3605171,
author = {Schiller, Nico and Xu, Xinyi and Bernhard, Lukas and Bars, Nils and Schloegel, Moritz and Holz, Thorsten},
title = {Novelty Not Found: Adaptive Fuzzer Restarts to Improve Input Space Coverage (Registered Report)},
year = {2023},
isbn = {9798400702471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605157.3605171},
doi = {10.1145/3605157.3605171},
abstract = {Feedback-driven greybox fuzzing is one of the cornerstones of modern bug detection techniques. Its flexibility, automated nature, and effectiveness render it an indispensable tool for making software more secure. A key feature that enables its impressive performance is coverage feedback, which guides the fuzzer to explore different parts of the program. The most prominent way to use this feedback is novelty search, in which the fuzzer generates new inputs and only keeps those that have exercised a new program edge. This is grounded in the assumption that novel coverage is a proxy for interestingness. Bolstered by its widespread success, it is easy to overlook its limitations. Particularly the phenomenon of input shadowing, situations in which an “interesting” input is discarded because it does not contribute novel coverage, needs to be considered. This phenomenon limits the explorable input space and risks missing bugs when shadowed inputs are more amenable to mutations that would trigger bugs. In this work, we analyze input shadowing in more detail and find that multiple fuzzing runs of the same target exhibit a different basic block hit frequency despite overlapping code coverage. In other words, different fuzzing runs may find the same set of basic blocks but one might exercise specific basic blocks significantly more often than the other, and vice versa. To better distribute the frequency, we propose restarting the fuzzer to reset the fuzzing state, diversifying the fuzzer’s attention across basic blocks. Our preliminary evaluation of three Fuzzbench targets finds that fuzzer restarts effectively distribute the basic block hit frequencies and boost the achieved coverage by up to 9.3%.},
booktitle = {Proceedings of the 2nd International Fuzzing Workshop},
pages = {12–20},
numpages = {9},
keywords = {Fuzzing, Software Security},
location = {Seattle, WA, USA},
series = {FUZZING 2023}
}

@inproceedings{10.1007/978-3-031-47115-5_13,
author = {Nivon, Quentin and Salaün, Gwen},
title = {Refactoring of&nbsp;Multi-instance BPMN Processes with&nbsp;Time and&nbsp;Resources},
year = {2023},
isbn = {978-3-031-47114-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-47115-5_13},
doi = {10.1007/978-3-031-47115-5_13},
abstract = {Business process optimisation is a strategic activity in organisations because of its potential to increase profit margins and reduce operational costs. In this paper, we focus on a specific technique used for process optimisation known as process refactoring. In this work, a process is described using BPMN extended with quantitative aspects for modelling execution times and resources associated with tasks. A process is not executed once but multiple times, and multiple concurrent executions of a process compete for using the shared resources. In this context, we propose a refactoring approach whose goal is to reduce the total execution time of the process and optimise the usage of the shared resources. To do so, we first analyse the given process in terms of task dependency and resource usage, and then rely on these results to restructure the process and return an optimal version of it. This process refactoring technique is fully automated by a tool that we implemented and applied on several examples for validation purposes.},
booktitle = {Software Engineering and Formal Methods: 21st International Conference, SEFM 2023, Eindhoven, The Netherlands, November 6-10, 2023, Proceedings},
pages = {226–245},
numpages = {20},
location = {<conf-loc content-type="InPerson">Eindhoven, The Netherlands</conf-loc>}
}

@inproceedings{10.1145/3600006.3613148,
author = {Gong, Sishuai and Peng, Dinglan and Altınbüken, Deniz and Fonseca, Pedro and Maniatis, Petros},
title = {Snowcat: Efficient Kernel Concurrency Testing using a Learned Coverage Predictor},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613148},
doi = {10.1145/3600006.3613148},
abstract = {Random-based approaches and heuristics are commonly used in kernel concurrency testing due to the massive scale of modern kernels and corresponding interleaving space. The lack of accurate and scalable approaches to analyze concurrent kernel executions makes existing testing approaches heavily rely on expensive dynamic executions to measure the effectiveness of a new test. Unfortunately, the high cost incurred by dynamic executions limits the breadth of the exploration and puts latency pressure on finding effective concurrent test inputs and schedules, hindering the overall testing effectiveness.This paper proposes Snowcat, a kernel concurrency testing framework that generates effective test inputs and schedules using a learned kernel block-coverage predictor. Using a graph neural network, the coverage predictor takes a concurrent test input and scheduling hints and outputs a prediction on whether certain important code blocks will be executed. Using this predictor, Snowcat can skip concurrent tests that are likely to be fruitless and prioritize the promising ones for actual dynamic execution.After testing the Linux kernel for over a week, Snowcat finds ~17% more potential data races, by prioritizing tests of more fruitful schedules than existing work would have chosen. Snowcat can also find effective test inputs that expose new concurrency bugs with higher probability (1.4×~2.6×), or reproduce known bugs more quickly (15×) than state-of-art testing tools. More importantly, Snowcat is shown to be more efficient at reaching a desirable level of race coverage in the continuous setting, as the Linux kernel evolves from version to version. In total, Snowcat discovered 17 new concurrency bugs in Linux kernel 6.1, of which 13 are confirmed and 6 are fixed.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {35–51},
numpages = {17},
keywords = {kernel concurrency bugs, operating systems security, software testing and debugging, concurrency programming},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings{10.1145/3611643.3616289,
author = {Jain, Kush and Alon, Uri and Groce, Alex and Le Goues, Claire},
title = {Contextual Predictive Mutation Testing},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616289},
doi = {10.1145/3611643.3616289},
abstract = {Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them. However, it is also computationally expensive and thus does not scale to large systems and projects. One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests. However, existing predictive mutation testing approaches still misclassify 33% of detection outcomes on a randomly sampled set of mutant-test suite pairs. We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation. Thanks to its higher precision, MutationBERT saves 33% of the time spent by a prior approach on checking/verifying live mutants. MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score. We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance. MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants that prior approaches do not.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {250–261},
numpages = {12},
keywords = {code coverage, mutation analysis, test oracles},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3617570.3617866,
author = {Caivano, Danilo and De Vincentiis, Mirko and Pal, Anibrata and Scalera, Michele},
title = {Extending Developer Support: Quantum Artificial Intelligence for Automotive Security},
year = {2023},
isbn = {9798400703768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617570.3617866},
doi = {10.1145/3617570.3617866},
abstract = {With the adoption of advanced technology in the automotive field, managing the risks of attack in modern vehicles becomes essential. Some research works substantially exploit Machine Learning algorithms to identify threats conducted on vehicles, particularly on the Controller Area Network (CAN) bus. Therefore, it is necessary not only to use Intrusion Detection Systems (IDSs) to identify attacks but also to help the engineers in the automotive field understand the dangerousness of the attack and help them resolve the vulnerability. With the increasing attention to Quantum Computing (QC), QC-based Artificial Intelligence algorithms have become very popular among many researchers for improving the prediction and the time performance to identify an attack. This paper proposes a methodology, SeQuADE (Secure Quantum Automotive Development and Engineering), to identify CAN attacks and to support developers by proposing associated automotive vulnerabilities and solutions obtained from National Vulnerability Database (NVD).},
booktitle = {Proceedings of the 2nd International Workshop on Quantum Programming for Software Engineering},
pages = {7–12},
numpages = {6},
keywords = {NLP, QBoost, automotive, multi-class IDS},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {QP4SE 2023}
}

@inproceedings{10.1007/978-981-99-7584-6_8,
author = {Evangelou-Oost, Naso and Meinicke, Larissa and Bannister, Callum and Hayes, Ian J.},
title = {Trace Models of&nbsp;Concurrent Valuation Algebras},
year = {2023},
isbn = {978-981-99-7583-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-7584-6_8},
doi = {10.1007/978-981-99-7584-6_8},
abstract = {This paper introduces Concurrent Valuation Algebras (CVAs), a novel extension of ordered valuation algebras (OVAs). CVAs include two combine operators representing parallel and sequential products, adhering to a weak exchange law. This development offers theoretical and practical benefits for the specification and modelling of concurrent and distributed systems. As a presheaf on a space of domains, CVAs enable localised specifications, supporting modularity, compositionality, and the ability to represent large and complex systems. Furthermore, CVAs align with lattice-based refinement reasoning and are compatible with established methodologies such as Hoare and Rely-Guarantee logics. The flexibility of CVAs is explored through three trace models, illustrating distinct paradigms of concurrent/distributed computing, interrelated by morphisms. The paper also highlights the potential to incorporate a powerful local computation framework from valuation algebras for model checking in concurrent and distributed systems. The foundational results presented have been verified with the proof assistant Isabelle/HOL.},
booktitle = {Formal Methods and Software Engineering: 24th International Conference on Formal Engineering Methods, ICFEM 2023, Brisbane, QLD, Australia, November 21–24, 2023, Proceedings},
pages = {118–136},
numpages = {19},
keywords = {Concurrent valuation algebras, Concurrent systems, Distributed systems},
location = {<conf-loc content-type="InPerson">Brisbane, QLD, Australia</conf-loc>}
}

@inproceedings{10.1145/3555041.3589360,
author = {Alonso, Gustavo and Ailamaki, Natassa and Krishnamurthy, Sailesh and Madden, Sam and Sivasubramanian, Swami and Ramakrishnan, Raghu},
title = {Future of Database System Architectures},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555041.3589360},
doi = {10.1145/3555041.3589360},
abstract = {Over the past two decades, we have experienced major technology disruptions on multiple fronts, none bigger than the emergence of cloud computing, which has led to fundamental changes in how database software is architected. We are seeing several new trends that are similarly shaping the future of data management.With the demise of Moore's Law, we are now seeing a lot of interest (and start-ups with significant investments) in hardware database accelerators, exploring FPGAs, GPUs, and more. Economies of scale in the cloud make it possible to move to hardware many things that were done in software, the trend will continue and increase.Modern data estates are spread across data located on premises, on the edge and in one or more public clouds, spread across various sources like multiple relational databases, file and storage systems, and no-SQL systems, both operational and analytic. This phenomenon is referred to as data sprawl.We are also seeing the emergence of many novel data workloads. For example, rich data pipelines are an increasingly common workload. And finally, Machine Learning is having a rapidly increasing role in every aspect of the database software lifecycle.This SIGMOD panel will discuss the impact of the above changes and trends on database hardware and software architectures. How will these changes impact DB system design, how will DB systems look like in the near future? Where are the hardest research challenges? What learnings from the past will guide us through these disruptions?},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {261–262},
numpages = {2},
keywords = {cloud computing, data governance, data warehouses, database architecture, distributed computing, hardware acceleration, lakehouses, machine learning, parallel databases, privacy, security},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@inproceedings{10.1007/978-3-031-47115-5_18,
author = {Song, Xidan and Sun, Youcheng and Mustafa, Mustafa A. and Cordeiro, Lucas C.},
title = {QNNRepair: Quantized Neural Network Repair},
year = {2023},
isbn = {978-3-031-47114-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-47115-5_18},
doi = {10.1007/978-3-031-47115-5_18},
abstract = {We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks, together with a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a MILP, solving neuron weight parameters, which corrects the QNN’s performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant&nbsp;[22]. According to the experiment results, we conclude that QNNRepair is effective in improving the quantized model’s performance in most cases. Its repaired models have 24% higher accuracy than SQuant’s in the independent validation set, especially for the ImageNet dataset.},
booktitle = {Software Engineering and Formal Methods: 21st International Conference, SEFM 2023, Eindhoven, The Netherlands, November 6-10, 2023, Proceedings},
pages = {320–339},
numpages = {20},
keywords = {neural network repair, quantization, fault localization, constraints solving},
location = {<conf-loc content-type="InPerson">Eindhoven, The Netherlands</conf-loc>}
}

@inproceedings{10.1145/3611643.3616366,
author = {Kim, Soomin and Kim, Hyungseok and Cha, Sang Kil},
title = {FunProbe: Probing Functions from Binary Code through Probabilistic Analysis},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616366},
doi = {10.1145/3611643.3616366},
abstract = {Current function identification techniques have been mostly focused on a specific set of binaries compiled for a specific CPU architecture. While recent deep-learning-based approaches theoretically can handle binaries from different architectures, they require significant computation resources for training and inference, making their use less practical. Furthermore, due to the lack of interpretability of such models, it is fundamentally difficult to gain insight from them. Hence, in this paper, we propose FunProbe, an efficient system for identifying functions from binaries using probabilistic inference. In particular, we identify 16 architecture-neutral hints for function identification, and devise an effective method to combine them in a probabilistic framework. We evaluate our tool on a large dataset consisting of 19,872 real-world binaries compiled for six major CPU architectures. The results are promising. FunProbe shows the best accuracy compared to five state-of-the-art tools we tested, while it takes only 6 seconds on average to analyze a single binary. Notably, FunProbe is 6× faster on average in identifying functions than XDA, a state-of-the-art deep-learning tool that leverages GPU in its inference phase.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1419–1430},
numpages = {12},
keywords = {binary code analysis, function identification, probabilistic analysis},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616345,
author = {Yin, Yining and Feng, Yang and Weng, Shihao and Liu, Zixi and Yao, Yuan and Zhang, Yichi and Zhao, Zhihong and Chen, Zhenyu},
title = {Dynamic Data Fault Localization for Deep Neural Networks},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616345},
doi = {10.1145/3611643.3616345},
abstract = {Rich datasets have empowered various deep learning (DL) applications, leading to remarkable success in many fields.  
However, data faults hidden in the datasets could result in DL applications behaving unpredictably and even cause massive monetary and life losses.  
To alleviate this problem, in this paper, we propose a dynamic data fault localization approach, namely DFauLo, to locate the mislabeled and noisy data in the deep learning datasets.  
DFauLo is inspired by the conventional mutation-based code fault localization, but utilizes the differences between DNN mutants to amplify and identify the potential data faults.  
Specifically, it first generates multiple DNN model mutants of the original trained model. Then it extracts features from these mutants and maps them into a suspiciousness score indicating the probability of the given data being a data fault.  
Moreover, DFauLo is the first dynamic data fault localization technique, prioritizing the suspected data based on user feedback, and providing the generalizability to unseen data faults during training.  
To validate DFauLo, we extensively evaluate it on 26 cases with various fault types, data types, and model structures.  
We also evaluate DFauLo on three widely-used benchmark datasets.  
The results show that DFauLo outperforms the state-of-the-art techniques in almost all cases and locates hundreds of different types of real data faults in benchmark datasets.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1345–1357},
numpages = {13},
keywords = {data quality, deep learning testing, fault localization},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3585005,
author = {Qi, Hua and Wang, Zhijie and Guo, Qing and Chen, Jianlang and Juefei-Xu, Felix and Zhang, Fuyuan and Ma, Lei and Zhao, Jianjun},
title = {ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3585005},
doi = {10.1145/3585005},
abstract = {Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we need to further consider techniques to address two key technical challenges, i.e., block localization, where we should localize the targeted block that we need to fix; and how to perform joint architecture and weight repairing. Specifically, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons’ status and weights’ gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy and robustness, outperforming the state-of-the-art DNN repair techniques.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {129},
numpages = {31},
keywords = {Deep learning, DNN repair, neural architecture search}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00057,
author = {Fu, Michael},
title = {Toward More Effective Deep Learning-Based Automated Software Vulnerability Prediction, Classification, and Repair},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00057},
doi = {10.1109/ICSE-Companion58688.2023.00057},
abstract = {Software vulnerabilities are prevalent in software systems and the unresolved vulnerable code may cause system failures or serious data breaches. To enhance security and prevent potential cyberattacks on software systems, it is critical to (1) early detect vulnerable code, (2) identify its vulnerability type, and (3) suggest corresponding repairs. Recently, deep learning-based approaches have been proposed to predict those tasks based on source code. In particular, software vulnerability prediction (SVP) detects vulnerable source code; software vulnerability classification (SVC) identifies vulnerability types to explain detected vulnerable programs; neural machine translation (NMT)-based automated vulnerability repair (AVR) generates patches to repair detected vulnerable programs. However, existing SVPs require much effort to inspect their coarse-grained predictions; SVCs encounter an unresolved data imbalance issue; AVRs are still inaccurate. I hypothesize that by addressing the limitations of existing SVPs, SVCs and AVRs, we can improve the accuracy and effectiveness of DL-based approaches for the aforementioned three prediction tasks. To test this hypothesis, I will propose (1) a finer-grained SVP approach that can point out vulnerabilities at the line level; (2) an SVC approach that mitigates the data imbalance issue; (3) NMT-based AVR approaches to address limitations of previous NMT-based approaches. Finally, I propose integrating these novel approaches into an open-source software security framework to promote the adoption of the DL-powered security tool in the industry.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {208–212},
numpages = {5},
keywords = {cybersecurity, software vulnerability, software security},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3586183.3606719,
author = {Angert, Tyler and Suzara, Miroslav and Han, Jenny and Pondoc, Christopher and Subramonyam, Hariharan},
title = {Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606719},
doi = {10.1145/3586183.3606719},
abstract = {Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a “stained glass filter” and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don’t lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst’s potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {100},
numpages = {22},
keywords = {creative coding, exploratory programming, generative art, large language models, prompt engineering},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {UIST '23}
}

@inproceedings{10.1145/3584871.3584876,
author = {Badhon, Ariful Islam Mahmud and Hasan, Md Sadman and Haque, Md. Samiul and Pranto, Md. Shafayat Hossain and Ghosh, Saurav and Alam, Md. Golam Rabiul},
title = {Diagnosing Prostate Cancer: An Implementation of Deep Machine Learning Fusion Network in MRI Using a Transfer Learning Approach},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584876},
doi = {10.1145/3584871.3584876},
abstract = {Of all the terminal cancers that plague men, prostate cancer remains one of the most prevalent and ubiquitous. Data shows prostate cancer is the second leading cause of cancer death worldwide among men. About 11% of men have prostate cancer at some time during their lives. As it happens, we have dedicated our entire research to developing an approach that can improve the existing precision of prostate cancer diagnosis. In our research, we have dedicated a Transfer Learning approach for the Deep Learning model to compare the accuracy in results using Machine Learning classifiers. In addition, we evaluated individual performance in classifications with different evaluation measures using a Deep Learning pre-trained network, VGG16. During our evaluation, we assessed several performance metrics such as Precision, Recall, F1 Score, and Loss Vs. Accuracy for performance analysis. Upon implementing the Transfer Learning approach, we recorded the optimum performance using the VGG16 architecture compared to other popular Deep learning models such as MobileNet and ResNet. It is important to note that we have used the convolutional block and dense layers of VGG16 architecture to extract features from our image dataset. Afterward, we forwarded those features to Machine Learning classifiers to tabulate the final classification result. Upon successful tabulation, we have secured significant accuracy in prognostication using the Deep Machine Learning method in our research.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {33–43},
numpages = {11},
keywords = {Deep Learning, Image Classification, ImageNet, Machine Learning classifier, Prostate Cancer, Transfer Learning, VGG16},
location = {<conf-loc>, <city>Palmerston North</city>, <country>New Zealand</country>, </conf-loc>},
series = {ICSIM '23}
}

@article{10.1016/j.jksuci.2023.101676,
author = {Cao, Heling and Chu, Yonghe and Zhao, Chenyang and Deng, Miaolei},
title = {Software multi-fault localization via Chameleon clustering in parallel},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {35},
number = {8},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2023.101676},
doi = {10.1016/j.jksuci.2023.101676},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = {sep},
numpages = {12},
keywords = {Program debugging, Fault localization, Cluster analysis, Multiple faults}
}

@article{10.1145/3583564,
author = {Tian, Yongqiang and Zhang, Wuqi and Wen, Ming and Cheung, Shing-Chi and Sun, Chengnian and Ma, Shiqing and Jiang, Yu},
title = {Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583564},
doi = {10.1145/3583564},
abstract = {Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.To this end, we propose Dflare, a novel, search-based, black-box testing technique to automatically find triggering inputs that result in deviated behaviors in image classification tasks. Dflare iteratively applies a series of mutation operations to a given seed image until a triggering input is found. For better efficacy and efficiency, Dflare models the search problem as Markov Chains and leverages the Metropolis-Hasting algorithm to guide the selection of mutation operators in each iteration. Further, Dflare utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models’ outputs or trigger previously unobserved models’ probability vectors. We evaluated Dflare on 21 compressed models for image classification tasks with three datasets. The results show that Dflare not only constantly outperforms the baseline in terms of efficacy but also significantly improves the efficiency: Dflare is 17.84×–446.06× as fast as the baseline in terms of time; the number of queries required by Dflare to find one triggering input is only 0.186–1.937% of those issued by the baseline. We also demonstrated that the triggering inputs found by Dflare can be used to repair up to 48.48% deviated behaviors in image classification tasks and further decrease the effectiveness of Dflare on the repaired models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {128},
numpages = {32},
keywords = {Model dissemination, model compression, neural networks, image classification models}
}

@article{10.1145/3640336,
author = {Gao, Kai and He, Runzhi and Xie, Bing and Zhou, Minghui},
title = {Characterizing Deep Learning Package Supply Chains in PyPI: Domains, Clusters, and Disengagement},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3640336},
doi = {10.1145/3640336},
abstract = {Deep learning (DL) frameworks have become the cornerstone of the rapidly developing DL field. Through installation dependencies specified in the distribution metadata, numerous packages directly or transitively depend on DL frameworks, layer after layer, forming DL package supply chains (SCs), which are critical for DL frameworks to remain competitive. However, vital knowledge on how to nurture and sustain DL package SCs is still lacking. Achieving this knowledge may help DL frameworks formulate effective measures to strengthen their SCs to remain competitive and shed light on dependency issues and practices in the DL SC for researchers and practitioners. In this paper, we explore the domains, clusters, and disengagement of packages in two representative PyPI DL package SCs to bridge this knowledge gap. We analyze the metadata of nearly six million PyPI package distributions and construct version-sensitive SCs for two popular DL frameworks: TensorFlow and PyTorch. We find that popular packages (measured by the number of monthly downloads) in the two SCs cover 34 domains belonging to eight categories. Applications, Infrastructure, and Sciences categories account for over 85% of popular packages in either SC and TensorFlow and PyTorch SC have developed specializations on Infrastructure and Applications packages respectively. We employ the Leiden community detection algorithm and detect 131 and 100 clusters in the two SCs. The clusters mainly exhibit four shapes: Arrow, Star, Tree, and Forest with increasing dependency complexity. Most clusters are Arrow or Star, while Tree and Forest clusters account for most packages (Tensorflow SC: 70.7%, PyTorch SC: 92.9%). We identify three groups of reasons why packages disengage from the SC (i.e., remove the DL framework and its dependents from their installation dependencies): dependency issues, functional improvements, and ease of installation. The most common reason in TensorFlow SC is dependency incompatibility and in PyTorch SC is to simplify functionalities and reduce installation size. Our study provides rich implications for DL framework vendors, researchers, and practitioners on the maintenance and dependency management practices of PyPI DL SCs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Software Supply Chain, PyPI Ecosystem, Deep Learning, Software Structure and Evolution}
}

@inproceedings{10.1145/3611643.3616337,
author = {Liu, Jiawei and Peng, Jinjun and Wang, Yuyao and Zhang, Lingming},
title = {NeuRI: Diversifying DNN Generation via Inductive Rule Inference},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616337},
doi = {10.1145/3611643.3616337},
abstract = {Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications.  
As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints.  
To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process:  
(i) collecting valid and invalid API traces from various sources;  
(ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and  
(iii) using hybrid model generation which incorporates both symbolic and concrete operators.  
Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24% and 15% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10% of all high-priority bugs of the period.  
Open-source developers regard error-inducing tests reported by us as "high-quality" and "common in practice".},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {657–669},
numpages = {13},
keywords = {Compiler Testing, Deep Learning Compilers, Fuzzing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.5555/3615924.3615949,
author = {Komal, Sarda and Zakeya, Namrud and Raphael, Rouf and Harit, Ahuja and Mohammadreza, Rasolroveicy and Marin, Litoiu and Larisa, Shwartz and Ian, Watts},
title = {ADARMA Auto-Detection and Auto-Remediation of Microservice Anomalies by Leveraging Large Language Models},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {In microservice architecture, anomalies can cause slow response times or poor user experience if not detected early. Manual detection can be time-consuming and error-prone, making real-time anomaly detection necessary. By implementing runtime performance anomaly detection models, microservice systems can become more stable and reliable. However, anomaly detection alone is not enough, and complementary auto-remediation techniques are required to automatically detect and fix issues. Auto-remediation techniques can optimize resource allocation, tune code, or trigger automatic recovery mechanisms. The combination of anomaly detection and auto-remediation reduces downtime and enhances system reliability, resulting in increased productivity and customer satisfaction, which, in turn, drives higher revenue. Prior works have overlooked auto-remediation. In this work in progress paper, we propose a pipeline for automatic anomaly detection and remediation based on Large Language Models (LLMs).},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {200–205},
numpages = {6},
keywords = {AIOps, Anomaly Detection, Root-cause Analysis, Auto-remediation, Microservice},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3611643.3613879,
author = {Xu, Qinghua and Ali, Shaukat and Yue, Tao and Nedim, Zaimovic and Singh, Inderjeet},
title = {KDDT: Knowledge Distillation-Empowered Digital Twin for Anomaly Detection},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613879},
doi = {10.1145/3611643.3613879},
abstract = {Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.915, respectively, demonstrating the effectiveness of KDDT. We also explored individual contributions of the DT model, LM, and KD to the overall performance of KDDT, via a comprehensive empirical study, and observed average F1 score improvements of 12.4%, 3%, and 6.05%, respectively.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1867–1878},
numpages = {12},
keywords = {Train Control and Management System, anomaly detection, digital twin, knowledge distillation},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3613897,
author = {Lu, Chengjie and Xu, Qinghua and Yue, Tao and Ali, Shaukat and Schwitalla, Thomas and Nygård, Jan},
title = {EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613897},
doi = {10.1145/3611643.3613897},
abstract = {The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for the previous version of GURI as the pretrained model and fine-tunes it with the dataset labelled by querying a new GURI version. EvoCLINICAL employs a genetic algorithm to select an optimal subset of cancer messages from a candidate dataset and query GURI with it. We evaluate EvoCLINICAL on three evolution processes. The precision, recall, and F1 score are all greater than 91%, demonstrating the effectiveness of EvoCLINICAL. Furthermore, we replace the active learning part of EvoCLINICAL with random selection to study the contribution of transfer learning to the overall performance of EvoCLINICAL. Results show that employing active learning in EvoCLINICAL increases its performances consistently.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1973–1984},
numpages = {12},
keywords = {active learning, cyber-cyber digital twin, digital twin, neural network, transfer learning, validation system},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.14778/3611540.3611635,
author = {Cheung, Alvin and Ahmad, Maaz Bin Safeer and Haynes, Brandon and Kittivorawong, Chanwut and Laddad, Shadaj and Liu, Xiaoxuan and Wang, Chenglong and Yan, Cong},
title = {Towards Auto-Generated Data Systems},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611635},
doi = {10.14778/3611540.3611635},
abstract = {After decades of progress, database management systems (DBMSs) are now the backbones of many data applications that we interact with on a daily basis. Yet, with the emergence of new data types and hardware, building and optimizing new data systems remain as difficult as the heyday of relational databases. In this paper, we summarize our work towards automating the building and optimization of data systems. Drawing from our own experience, we further argue that any automation technique must address three aspects: user specification, code generation, and result validation. We conclude by discussing a case study using videos data processing, along with opportunities for future research towards designing data systems that are automatically generated.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {4116–4129},
numpages = {14}
}

@inproceedings{10.1145/3611643.3616364,
author = {Zhao, Zhongkai and Kou, Bonan and Ibrahim, Mohamed Yilmaz and Chen, Muhao and Zhang, Tianyi},
title = {Knowledge-Based Version Incompatibility Detection for Deep Learning},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616364},
doi = {10.1145/3611643.3616364},
abstract = {Version incompatibility issues are rampant when reusing or reproducing deep learning models and applications. Existing techniques are limited to library dependency specifications declared in PyPI. Therefore, these techniques cannot detect version issues due to undocumented version constraints or issues involving hardware drivers or OS. To address this challenge, we propose to leverage the abundant discussions of DL version issues from Stack Overflow to facilitate version incompatibility detection. We reformulate the problem of knowledge extraction as a Question-Answering (QA) problem and use a pre-trained QA model to extract version compatibility knowledge from online discussions. The extracted knowledge is further consolidated into a weighted knowledge graph to detect potential version incompatibilities when reusing a DL project. Our evaluation results show that (1) our approach can accurately extract version knowledge with 84% accuracy, and (2) our approach can accurately identify 65% of known version issues in 10 popular DL projects with a high precision (92%), while two state-of-the-art approaches can only detect 29% and 6% of these issues with 33% and 17% precision respectively.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {708–719},
numpages = {12},
keywords = {Deep Learning, Knowledge Extraction, Version Compatibility},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3579638,
author = {Yitagesu, Sofonias and Xing, Zhenchang and Zhang, Xiaowang and Feng, Zhiyong and Li, Xiaohong and Han, Linyi},
title = {Extraction of Phrase-based Concepts in Vulnerability Descriptions through Unsupervised Labeling},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3579638},
doi = {10.1145/3579638},
abstract = {Software vulnerabilities, once disclosed, can be documented in vulnerability databases, which have great potential to advance vulnerability analysis and security research. People describe the key characteristics of software vulnerabilities in natural language mixed with domain-specific names and concepts. This textual nature poses a significant challenge for the automatic analysis of vulnerability knowledge embedded in text. Automatic extraction of key vulnerability aspects is highly desirable but demands significant effort to manually label data for model training. In this article, we propose unsupervised methods to label and extract important vulnerability concepts in textual vulnerability descriptions (TVDs). We focus on six types of phrase-based vulnerability concepts (vulnerability type, vulnerable component, root cause, attacker type, impact, and attack vector) as they are much more difficult to label and extract than name- or number-based entities (i.e., vendor, product, and version). Our approach is based on a key observation that the same-type of phrases, no matter how they differ in sentence structures and phrase expressions, usually share syntactically similar paths in the sentence parsing trees. Specifically, we present a source-target neural architecture that learns the Part-of-Speech (POS) tagging to identify a token’s functional role within TVDs, where the source neural model is trained to capture common features found in the TVD corpus, and the target model is trained to identify linguistically malformed words specific to the security domain. Our evaluation confirms that the proposed tagger outperforms (4.45%–5.98%) the taggers designed on natural language notions and identifies a broad set of TVDs and natural language contents. Then, based on the key observations, we propose two path representations (absolute paths and relative paths) and use an auto-encoder to encode such syntactic similarities. To address the discrete nature of our paths, we enhance the traditional Variational Auto-encoder (VAE) with Gumble-Max trick for categorical data distribution and thus create a Categorical VAE (CaVAE). In the latent space of absolute and relative paths, we further apply unsupervised clustering techniques to generate clusters of the same-type of concepts. Our evaluation confirms the effectiveness of our CaVAE, which achieves a small (85.85) log-likelihood for encoding path representations and the accuracy (83%–89%) of vulnerability concepts in the resulting clusters. The resulting clusters accurately label six types of vulnerability concepts from a TVD corpus in an unsupervised way. Furthermore, these labeled vulnerability concepts can be mapped back to the corresponding phrases in the original TVDs, which produce labels of six types of vulnerability concepts. The resulting labeled TVDs can be used to train concept extraction models for other TVD corpora. In this work, we present two concept extraction methods (concept classification and sequence labeling model) to demonstrate the utility of the unsupervisedly labeled concepts. Our study shows that models trained with our unsupervisedly labeled vulnerability concepts outperform (3.9%–5.14%) those trained with the two manually labeled TVD datasets from previous work due to the consistent boundary and typing by our unsupervised labeling method.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {112},
numpages = {45},
keywords = {Textual vulnerability descriptions, phrase-based vulnerability concepts, unsupervised representation learning, clustering and concept labeling, supervised concept extraction}
}

@inproceedings{10.1145/3631991.3632034,
author = {Thongda, Nattha and Songpan, Wararat},
title = {Enhancing Education Process Through Intelligent Association Mining (IAM) System Conversations: Chatbot Services in Educational Settings},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3632034},
doi = {10.1145/3631991.3632034},
abstract = {Nowadays, chatbots are growth in terms of business, and it is necessary for businesses to implement innovative approaches to repository big data and provide customer service twenty-four hours. Moreover, chatbots system is explored the efficacy and potential of applied services to enhance education through intelligent conversations and AI techniques. The study aims to investigate how chatbots based on intelligent association mining method called IAM can contribute to educational settings by providing personalized and interactive questions and answering. In aspects of cluster are divided into 6 clusters as 1) Enrollment 2) Academic service 3) Student profile 4) Admission 5) Check and tracking grade and 6) Finance. The implementation of enhancing education process through intelligent association mining found out the overall accuracy of answering correctly to 92.16% compared with traditional way was cost human time-consuming investment to answer the question via chat in social media. The benefit of deployed IAM chatbot system used the several of educational services’ question in university which is huge number of around 45,000 students per year.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {261–266},
numpages = {6},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@inproceedings{10.1145/3611643.3616315,
author = {Wang, Chong and Lou, Yiling and Peng, Xin and Liu, Jianan and Zou, Baihan},
title = {Mining Resource-Operation Knowledge to Support Resource Leak Detection},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616315},
doi = {10.1145/3611643.3616315},
abstract = {Resource leaks, which are caused by acquired resources not being released, often result in performance degradation and system crashes. Resource leak detection relies on two essential components: identifying potential Resource Acquisition and Release (RAR) API pairs, and subsequently analyze code to uncover instances where the corresponding release API call is absent after an acquisition API call. Yet, existing techniques confine themselves to an incomplete pair pool, either pre-defined manually or mined from project-specific code corpus, thus limiting coverage across libraries/APIs and po- 
tentially overlooking latent resource leaks. 

In this work, we propose to represent resource-operation knowledge as abstract resource acquisition/release operation pairs (Abs-RAR pairs for short), and present a novel approach called 
MiROK to mine such Abs-RAR pairs to construct a better RAR pair pool. Given a large code corpus, MiROK first mines Abs-RAR pairs with rule-based pair expansion and learning-based pair identification strategies, and then instantiates these Abs-RAR pairs into concrete RAR pairs. We implement MiROK and apply it to mine RAR pairs from a large code corpus of 1,454,224 Java methods and 20,000 Maven libraries. We then perform an extensive evaluation to investigate the mining effectiveness of MiROK and the practical usage of its mined RAR pairs for supporting resource leak detection. Our results show that MiROK mines 1,313 new Abs-RAR pairs and instantiates them into 6,314 RAR pairs with a high precision (i.e., 93.3%). In addition, by feeding our mined RAR pairs, existing approaches detect more resource leak defects in both online code examples and open-source projects},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {986–998},
numpages = {13},
keywords = {defect detection, knowledge mining, knowledge representation, resource leaks},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3631991.3632036,
author = {Toan, Pham Van and Sang, Dinh Viet},
title = {M3C-Polyp: Mixed Momentum Model Committee for Improved Semi-Supervised Learning in Polyp Segmentation},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3632036},
doi = {10.1145/3631991.3632036},
abstract = {We propose a novel approach for semi-supervised learning (SSL) in polyp segmentation using a Mixed Momentum Model Committee. Our method addresses limited labeled data and uncertainty estimation challenges. The committee M3C consists of K momentum models derived from a teacher model T, enabling effective pseudo-label generation and uncertainty estimation at the pixel level. Experiments on five benchmark datasets, including Kvarsir, CVC-ClinicDB, ETIS-LaribPolypDB, CVC-ColonDB, and CVC-300, show the superiority of our method, achieving an average Dice score of 0.862 compared to the supervised baseline (0.826). Notably, our approach outperforms the supervised method on out-of-domain datasets. The proposed Mixed Momentum Model Committee advances SSL for polyp segmentation, offering improved accuracy and uncertainty estimation. The findings have significant implications for clinical applications, aiding in accurate diagnosis and treatment of colorectal polyps.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {274–279},
numpages = {6},
keywords = {Medical Imaging, Mixed Momentum Model Committee, Polyp Segmentation, Pseudo-Labels, Semi-Supervised Learning, Uncertainty Estimation},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@article{10.1145/3637228,
author = {Cañizares, Pablo C. and López-Morales, Jose María and Pérez-Soler, Sara and Guerra, Esther and de Lara, Juan},
title = {Measuring and Clustering Heterogeneous Chatbot Designs},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637228},
doi = {10.1145/3637228},
abstract = {Conversational agents, or chatbots, have become popular to access all kind of software services. They provide an intuitive natural language interface for interaction, available from a wide range of channels including social networks, web pages, intelligent speakers or cars. In response to this demand, many chatbot development platforms and tools have emerged. However, they typically lack support to statically measure properties of the chatbots being built, as indicators of their size, complexity, quality or usability. Similarly, there are hardly any mechanisms to compare and cluster chatbots developed with heterogeneous technologies. To overcome this limitation, we propose a suite of 21 metrics for chatbot designs, as well as two clustering methods that help in grouping chatbots along their conversation topics and design features. Both the metrics and the clustering methods are defined on a neutral chatbot design language, becoming independent of the implementation platform. We provide automatic translations of chatbots defined on some major platforms into this neutral notation to perform the measurement and clustering. The approach is supported by our tool Asymob, which we have used to evaluate the metrics and the clustering methods over a set of 259 Dialogflow and Rasa chatbots from open-source repositories. The results open the door to incorporating the metrics within chatbot development processes for the early detection of quality issues, and to exploit clustering to organise large collections of chatbots into significant groups to ease chatbot comprehension, search and comparison.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {chatbot design, metrics, clustering, quality assurance, model-driven engineering}
}

@inproceedings{10.1145/3631991.3632039,
author = {Samonte, Mary Jane C. and Dong, Ting and Wu, Minning and Wang, Yong},
title = {Detection of Defects in Bottled Liquor Using Deep Learning},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3632039},
doi = {10.1145/3631991.3632039},
abstract = {In this paper, we aim at the problem of low detection accuracy in bottle defect detection; combined with the characteristics of the bottle defect dataset, this paper adopts the method of data enhancement to enable the model to be trained through a small amount of information, improve the model's ability to capture defects, and thus improve the generalization ability of the model. Secondly, the algorithm is improved for the false detection and missed detection problems in the defect detection of wine bottles. Due to its unique construction method, the feature pyramid network can fuse the high-level feature information to improve detection accuracy. The study used the concept of feature pyramid network (FPN) and applied ResNet-65 network is used as the backbone network structure. Through experimental verification, this method can improve detection accuracy.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {292–297},
numpages = {6},
keywords = {Feature Pyramid Network, Network model optimization, Raster R-CNN, Surface defects of Bottled Liquor},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@article{10.1145/3631971,
author = {Pizzolotto, Davide and Berlato, Stefano and Ceccato, Mariano},
title = {Mitigating Debugger-based Attacks to Java Applications with Self-Debugging},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631971},
doi = {10.1145/3631971},
abstract = {Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode. In this paper, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-Middle attacks and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them. We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Anti-debugging, Maliciuos reverse engineering, Tampering attacks, Man at the end attacks}
}

@article{10.1145/3591870,
author = {Huang, Yuheng and Ma, Lei and Li, Yuanchun},
title = {PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3591870},
doi = {10.1145/3591870},
abstract = {In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g.,&nbsp;67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {154},
numpages = {34},
keywords = {Adversarial patch, neural networks, robustness certification, vision transformer, certified accuracy, deep learning}
}

@inproceedings{10.1109/ICSE48619.2023.00198,
author = {Chen, Binger and Abedjan, Ziawasch},
title = {DUETCS: Code Style Transfer through Generation and Retrieval},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00198},
doi = {10.1109/ICSE48619.2023.00198},
abstract = {Coding style has direct impact on code comprehension. Automatically transferring code style to user's preference or consistency can facilitate project cooperation and maintenance, as well as maximize the value of open-source code. Existing work on automating code stylization is either limited to code formatting or requires human supervision in pre-defining style checking and transformation rules. In this paper, we present unsupervised methods to assist automatic code style transfer for arbitrary code styles. The main idea is to leverage Big Code database to learn style and content embedding separately to generate or retrieve a piece of code with the same functionality and the desired target style. We carefully encode style and content features, so that a style embedding can be learned from arbitrary code. We explored the capabilities of novel attention-based style generation models and meta-learning and implemented our ideas in DUETCS. We complement the learning-based approach with a retrieval mode, which uses the same embeddings to directly search for the desired piece of code in Big Code. Our experiments show that DUETCS captures more style aspects than existing baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2362–2373},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3643676,
author = {Li, Yinghua and Dang, Xueqi and Ma, Lei and Klein, Jacques and Traon, Yves LE and Bissyandé, Tegawendé F.},
title = {Test Input Prioritization for 3D Point Clouds},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3643676},
doi = {10.1145/3643676},
abstract = {Three-dimensional (3D) point cloud applications have become increasingly prevalent in diverse domains, showcasing their efficacy in various software systems. However, testing such applications presents unique challenges due to the high-dimensional nature of 3D point cloud data and the vast number of possible test cases. Test input prioritization has emerged as a promising approach to enhance testing efficiency by prioritizing potentially misclassified test cases during the early stages of the testing process. Consequently, this enables the early labeling of critical inputs, leading to a reduction in the overall labeling cost. However, applying existing prioritization methods to 3D point cloud data is constrained by several factors: 1) Inadequate consideration of crucial spatial information, and 2) susceptibility to noises inherent in 3D point cloud data. In this paper, we propose PCPrior, the first test prioritization approach specifically designed for 3D point cloud test cases. The fundamental concept behind PCPrior is that test inputs closer to the decision boundary of the model are more likely to be predicted incorrectly. To capture the spatial relationship between a point cloud test and the decision boundary, we propose transforming each test (a point cloud) into a low-dimensional feature vector, towards indirectly revealing the underlying proximity between a test and the decision boundary. To achieve this, we carefully design a group of feature generation strategies, and for each test input, we generate four distinct types of features, namely, spatial features, mutation features, prediction features, and uncertainty features. Through a concatenation of the four feature types, PCPrior assembles a final feature vector for each test. Subsequently, a ranking model is employed to estimate the probability of misclassification for each test based on its feature vector. Finally, PCPrior ranks all tests based on their misclassification probabilities. We conducted an extensive study based on 165 subjects to evaluate the performance of PCPrior, encompassing both natural and noisy datasets. The results demonstrate that PCPrior outperforms all the compared test prioritization approaches, with an average improvement of 10.99%~66.94% on natural datasets and 16.62%~53% on noisy datasets.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Test Input Prioritization, Deep Neural Network, Learning to Rank, Labeling}
}

@inproceedings{10.1109/ICSE48619.2023.00152,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Ma, Lei and Traon, Yves Le},
title = {Aries: Efficient Testing of Deep Neural Networks via Labeling-Free Accuracy Estimation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00152},
doi = {10.1109/ICSE48619.2023.00152},
abstract = {Deep learning (DL) plays a more and more important role in our daily life due to its competitive performance in industrial application domains. As the core of DL-enabled systems, deep neural networks (DNNs) need to be carefully evaluated to ensure the produced models match the expected requirements. In practice, the de facto standard to assess the quality of DNNs in the industry is to check their performance (accuracy) on a collected set of labeled test data. However, preparing such labeled data is often not easy partly because of the huge labeling effort, i.e., data labeling is labor-intensive, especially with the massive new incoming unlabeled data every day. Recent studies show that test selection for DNN is a promising direction that tackles this issue by selecting minimal representative data to label and using these data to assess the model. However, it still requires human effort and cannot be automatic. In this paper, we propose a novel technique, named Aries, that can estimate the performance of DNNs on new unlabeled data using only the information obtained from the original test data. The key insight behind our technique is that the model should have similar prediction accuracy on the data which have similar distances to the decision boundary. We performed a large-scale evaluation of our technique on two famous datasets, CIFAR-10 and Tiny-ImageNet, four widely studied DNN models including ResNet101 and DenseNet121, and 13 types of data transformation methods. Results show that the estimated accuracy by Aries is only 0.03% -- 2.60% off the true accuracy. Besides, Aries also outperforms the state-of-the-art labeling-free methods in 50 out of 52 cases and selection-labeling-based methods in 96 out of 128 cases.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1776–1787},
numpages = {12},
keywords = {deep learning testing, performance estimation, distribution shift},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3581961.3609828,
author = {Choe, Mungyeong and Bosch, Esther and Dong, Jiayuan and Alvarez, Ignacio and Oehl, Michael and Jallais, Christophe and Alsaid, Areen and Nadri, Chihab and Jeon, Myounghoon},
title = {Emotion GaRage Vol. IV: Creating Empathic In-Vehicle Interfaces with Generative AIs for Automated Vehicle Contexts},
year = {2023},
isbn = {9798400701122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581961.3609828},
doi = {10.1145/3581961.3609828},
abstract = {This workshop aims to design advanced empathic user interfaces for in-vehicle displays, particularly for high-level automated vehicles (SAE level 3 or higher). Incorporating model-based approaches for understanding human emotion regulation, it seeks to enhance the user-vehicle interaction. A unique aspect of this workshop is the integration of generative artificial intelligence (AI) tools in the design process. The workshop will explore generative AI’s potential in crafting contextual responses and its impact on user experience and interface design. The agenda includes brainstorming on various driving scenarios, developing emotion-oriented intervention methods, and rapid prototyping with AI tools. The anticipated outcome includes practical prototypes of affective user interfaces and insights on the role of AI in designing human-machine interactions. Through this workshop, we hope to contribute to making automated driving more accessible and enjoyable.},
booktitle = {Adjunct Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {234–236},
numpages = {3},
keywords = {ChatGPT, affective computing, emotions, empathic vehicles, interaction design},
location = {Ingolstadt, Germany},
series = {AutomotiveUI '23 Adjunct}
}

@inproceedings{10.5555/3618408.3619637,
author = {Sachidananda, Vin and Yang, Ziyi and Zhu, Chenguang},
title = {Global selection of contrastive batches via optimization on sample permutations},
year = {2023},
publisher = {JMLR.org},
abstract = {Contrastive Learning has recently achieved state-of-the-art performance in a wide range of unimodal and multimodal tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, LGlobal – LTrain, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbor indices, is more computationally efficient than the most minimal hard negative mining approaches, and makes no changes to the model being trained. Code is available at https://github.com/vinayak1/GCBS.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1229},
numpages = {21},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00049,
author = {Shen, Kedi and Zhang, Yun and Bao, Lingfeng and Wan, Zhiyuan and Li, Zhuorong and Wu, Minghui},
title = {Patchmatch: A Tool for Locating Patches of Open Source Project Vulnerabilities},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00049},
doi = {10.1109/ICSE-Companion58688.2023.00049},
abstract = {With the rapid development of open source projects, the continuous emergence of vulnerabilities in the project brings great challenges to the security of the project. Security patches are one of the best ways to deal with vulnerabilities, but are not well applied currently. Although there are sites like CVE/NVD that provide information about vulnerabilities, many of the vulnerabilities disclosed by CVE/NVD are not accompanied by security patches. This makes it difficult for developers to apply patches. In the present study, a sorting method based on extracting multidimensional features from auxiliary information in CVE/NVD was proposed. And we made a further step, we proposed VCmatch, a model for mining semantic information in vulnerability description and code commit messages, which has good recall rate and applicability across projects. On this basis, we established Patchmatch, a tool for helping developers to quickly locate patches. Given a vulnerability, Patchmatch can forecast the implicit patches in the code repository's commits. Patchmatch also has a visual webpage for information statistics and a display web page to help developers manage all kinds of information in the code repository. A demo video of Patch-match is at https://www.youtube.com/watch?v=nOBSMFtZV8A. Patchmatch is in https://github.com/Sklud1456/patchmatch.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {175–179},
numpages = {5},
keywords = {vulnerability, model application, manage tool},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3628096.3628753,
author = {Ngaruiya, Njeri and Donner, Jonathan and Baru, Joshua Kinuthia and Chege, Babra Wanjiku},
title = {The domestication of AI by Kenyan digital creators},
year = {2024},
isbn = {9798400708879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628096.3628753},
doi = {10.1145/3628096.3628753},
abstract = {This note explores the adoption and use of artificial intelligence (AI) in Kenya's digital creative sectors. Guided by a lens of domestication theory, and informed by interviews with 21 practitioners, the study documents ways in which AI is transforming traditional workflows, job roles, and skill requirements, enabling increased efficiency, automation, and creativity possibilities. Digital marketers leverage AI-powered analytics tools for data-driven insights and personalized marketing campaigns. Coders utilize AI algorithms to optimize code development, enhance software testing, and streamline debugging processes. Graphic designers incorporate AI tools for image recognition, automated design generation, and enhanced visual effects. Ghostwriters embrace AI-based writing assistants for generating content, improving productivity, and meeting client demands. Importantly, the study identifies concerns among professionals regarding job security, ethical implications, and the need for upskilling to effectively collaborate with AI technologies.},
booktitle = {Proceedings of the 4th African Human Computer Interaction Conference},
pages = {71–75},
numpages = {5},
keywords = {AI, Domestication, ICT4D, Livelihoods},
location = {<conf-loc>, <city>East London</city>, <country>South Africa</country>, </conf-loc>},
series = {AfriCHI '23}
}

@inproceedings{10.1145/3611643.3613092,
author = {Shanbhag, Shriram and Chimalakonda, Sridhar and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
title = {DENT: A Tool for Tagging Stack Overflow Posts with Deep Learning Energy Patterns},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613092},
doi = {10.1145/3611643.3613092},
abstract = {Energy efficiency has become an important consideration in deep learning systems. However, it remains a largely under-emphasized aspect during the development. Despite the emergence of energy-efficient deep learning patterns, their adoption remains a challenge due to limited awareness. To address this gap, we present DENT (Deep Learning Energy Pattern Tagger, a Chrome extension used to add "energy pattern tags" to the deep learning related questions from Stack Overflow. The idea of DENT is to hint to the developers about the possible energy-saving opportunities associated with the Stack Overflow post through energy pattern labels. We hope this will increase awareness about energy patterns in deep learning and improve their adoption. A preliminary evaluation of DENT achieved an average precision of 0.74, recall of 0.66, and an F1-score of 0.65 with an accuracy of 66%. The demonstration of the tool is available at https://youtu.be/S0Wf_w0xajw and the related artifacts are available at https://rishalab.github.io/DENT/},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2157–2161},
numpages = {5},
keywords = {deep learning, energy patterns, energy tags, stack overflow},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00027,
author = {Humbatova, Nargiz and Jabangirova, Gunel and Tonella, Paolo},
title = {DeepCrime: From Real Faults to Mutation Testing Tool for Deep Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00027},
doi = {10.1109/ICSE-Companion58688.2023.00027},
abstract = {The recent advance of Deep Learning (DL) due to its human-competitive performance in complex and often safety-critical tasks, reveals many gaps in their testing. There exist a number of DL-specific testing approaches, and yet none has presented the possibility of simulating the occurrence of real DL faults for the mutation testing of DL systems. We propose 35 and implement 24 mutation operators that were systematically extracted from the existing studies on real DL faults. Our evaluation shows that the proposed operators produce non-redundant, killable, and non-trivial mutations while being more sensitive to the change in the quality of test data than the existing mutation testing approaches. Video demonstration is available at: https://youtu.be/WOvuPaXH6Jk},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {68–72},
numpages = {5},
keywords = {deep learning, mutation testing, real faults},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3616324,
author = {Qi, Xiaofang and Qian, Xiang and Li, Yanhui},
title = {Semantic Test Repair for Web Applications},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616324},
doi = {10.1145/3611643.3616324},
abstract = {Automation testing is widely used in the functional testing of web applications. However, during the evolution of web applications, such web test scripts tend to break. It is essential to repair such broken test scripts to make regression testing run successfully. As manual repairing is time-consuming and expensive, researchers focus on automatic repairing techniques. Empirical study shows that the web element locator is the leading cause of web test breakages. Most existing repair techniques utilize Document Object Model attributes or visual appearances of elements to find their location but neglect their semantic information. This paper proposes a novel semantic repair technique called Semantic Test Repair (Semter) for web test repair. Our approach captures relevant semantic information from test executions on the application’s basic version and locates target elements by calculating semantic similarity between elements to repair tests. Our approach can also repair test workflow due to web page additions or deletions by a local exploration in the updated version. We evaluated the efficacy of our technique on six real-world web applications compared with three baselines. Experimental results show that Semter achieves an 84% average repair ratio within an acceptable time cost, significantly outperforming the state-of-the-art web test repair techniques.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1190–1202},
numpages = {13},
keywords = {GUI Testing, Semantic Similarity, Test Repair, Web Testing},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3635800.3637446,
author = {Pettorossi, Alberto and Proietti, Maurizio and Fioravanti, Fabio and De Angelis, Emanuele},
title = {A Historical Perspective on Program Transformation and Recent Developments (Invited Contribution)},
year = {2024},
isbn = {9798400704871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635800.3637446},
doi = {10.1145/3635800.3637446},
abstract = {This paper presents some ideas concerning  
program manipulation and program transformation from the  
early days of their development.  
Particular emphasis will be given to  
program transformation techniques in the area of functional  
programming and constraint logic programming.  
We will also indicate current applications of program transformation  
techniques to the verification of program properties and program  
synthesis.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Workshop on Partial Evaluation and Program Manipulation},
pages = {16–38},
numpages = {23},
keywords = {Constraint logic programming, Partial evaluation, Program manipulation, Program specialization, Program synthesis, Program transformation, Program validation, Transformation rules, Transformation strategies},
location = {<conf-loc>, <city>London</city>, <country>UK</country>, </conf-loc>},
series = {PEPM 2024}
}

@article{10.1186/s13638-023-02277-w,
author = {Jia, Haohui and Chen, Na and Urakami, Taisei and Gao, Hui and Okada, Minoru},
title = {Spatial attention and quantization-based contrastive learning framework for mmWave massive MIMO beam training},
year = {2023},
issue_date = {Jul 2023},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2023},
number = {1},
issn = {1687-1472},
url = {https://doi.org/10.1186/s13638-023-02277-w},
doi = {10.1186/s13638-023-02277-w},
abstract = {Deep learning (DL)-based beam training schemes have been exploited to improve spectral efficiency with fast optimal beam selection for millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems. To achieve high prediction accuracy, these DL models rely on training with a tremendous amount of labeled environmental measurements, such as mmWave channel state information (CSI). However, demanding a large volume of ground truth labels for beam training is inefficient and infeasible due to the high labeling cost and the requirement for expertise in practical mmWave massive MIMO systems. Meanwhile, a complex environment incurs critical performance degradation in the continuous output of beam training. In this paper, we propose a novel contrastive learning framework, named self-enhanced quantized phase-based transformer network (SE-QPTNet), for reliable beam training with only a small fraction of the labeled CSI dataset. We first develop a quantized phase-based transformer network (QPTNet) with a hierarchical structure to explore the essential features from frequency and spatial views and quantize the environmental components with a latent beam codebook to achieve robust representation. Next, we design the SE-QPTNet including self-enhanced pre-training and supervised beam training. SE-QPTNet pre-trains by the contrastive information of the target user and others with the unlabeled CSI, and then, it is utilized as the initialization to fine-tune with a reduced volume of labeled CSI. Finally, the experimental results show that the proposed framework improves beam prediction accuracy and data rates with 5% labeled data compared to existing solutions. Our proposed framework further enhances flexibility and breaks the limitation of the quantity of label information for practical beam training.},
journal = {EURASIP J. Wirel. Commun. Netw.},
month = {jul},
numpages = {28},
keywords = {MmWave, Massive MIMO, Deep learning, Spatial attention, Feature quantization, Contrastive learning}
}

@article{10.1145/3617594,
author = {Weiss, Michael and Tonella, Paolo},
title = {Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks - RCR Report},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617594},
doi = {10.1145/3617594},
abstract = {This is the Replicated Computational Results (RCR) Report for our TOSEM paper “Adopting Two Supervisors for Efficient Use of Large-Scale Remote Deep Neural Networks”, where we propose a novel client-server architecture allowing to leverage the high accuracy of huge neural networks running on remote servers while reducing the economical and latency costs typically coming from using such models. As part of this RCR, we provide a replication package, which allows the full replication of all our results and is specifically designed to facilitate reuse.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {29},
numpages = {4},
keywords = {Neural networks, replication, artifact}
}

@inproceedings{10.1145/3611643.3616326,
author = {Karimipour, Nima and Pham, Justin and Clapp, Lazaro and Sridharan, Manu},
title = {Practical Inference of Nullability Types},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616326},
doi = {10.1145/3611643.3616326},
abstract = {NullPointerExceptions (NPEs), caused by dereferencing null, fre- quently cause crashes in Java programs. Pluggable type checking is highly effective in preventing Java NPEs. However, this approach is difficult to adopt for large, existing code bases, as it requires manually inserting a significant number of type qualifiers into the code. Hence, a tool to automatically infer these qualifiers could make adoption of type-based NPE prevention significantly easier.  
We present a novel and practical approach to automatic inference of nullability type qualifiers for Java. Our technique searches for a set of qualifiers that maximizes the amount of code that can be successfully type checked. The search uses the type checker as a black box oracle, easing compatibility with existing tools. However, this approach can be costly, as evaluating the impact of a qualifier requires re-running the checker. We present a technique for safely evaluating many qualifiers in a single checker run, dramatically reducing running times. We also describe extensions to make the approach practical in a real-world deployment.  
We implemented our approach in an open-source tool Null- AwayAnnotator, designed to work with the NullAway type checker. We evaluated NullAwayAnnotator’s effectiveness on both open- source projects and commercial code. NullAwayAnnotator re- duces the number of reported NullAway errors by 69.5% on average. Further, our optimizations enable NullAwayAnnotator to scale to large Java programs. NullAwayAnnotator has been highly effective in practice: in a production deployment, it has already been used to add NullAway checking to 160 production modules totaling over 1.3 million lines of Java code.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1395–1406},
numpages = {12},
keywords = {inference, null safety, pluggable type systems, static analysis},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3588195.3592984,
author = {Dutta, Akash and Alcaraz, Jordi and TehraniJamsaz, Ali and Cesar, Eduardo and Sikora, Anna and Jannesari, Ali},
title = {Performance Optimization using Multimodal Modeling and Heterogeneous GNN},
year = {2023},
isbn = {9798400701559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588195.3592984},
doi = {10.1145/3588195.3592984},
abstract = {Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoising Autoencoders for modeling IR-based code representations that serve as separate modalities. This approach is used as part of our pipeline to model a syntax, semantics, and structure-aware IR-based code representation for tuning parallel code regions/kernels. We extensively experiment on OpenMP and OpenCL code regions/kernels obtained from PolyBench, Rodinia, STREAM, DataRaceBench, AMD SDK, NPB, NVIDIA SDK, Parboil, SHOC, LULESH, XSBench, RSBench, miniFE, miniAMR, and Quicksilver benchmarks and applications. We apply our multimodal learning techniques to the tasks of (i) optimizing the number of threads, scheduling policy and chunk size in OpenMP loops and, (ii) identifying the best device for heterogeneous device mapping of OpenCL kernels. Our experiments show that this multimodal learning based approach outperforms the state-of-the-art in almost all experiments.},
booktitle = {Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {45–57},
numpages = {13},
keywords = {OpenCL, OpenMP, auto-tuning, heterogeneous graph neural networks, multimodal learning},
location = {Orlando, FL, USA},
series = {HPDC '23}
}

@inproceedings{10.1145/3611643.3613873,
author = {Mockus, Audris and Rigby, Peter C. and Abreu, Rui and Suresh, Parth and Chen, Yifen and Nagappan, Nachiappan},
title = {Modeling the Centrality of Developer Output with Software Supply Chains},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613873},
doi = {10.1145/3611643.3613873},
abstract = {Raw developer output, as measured by the number of changes a developer makes to the system, is simplistic and potentially misleading measure of productivity as new developers tend to work on peripheral and experienced developers on more central parts of the system. In this work, we use Software Supply Chain (SSC) networks and Katz centrality and PageRank on these networks to suggest a more nuanced measure of developer productivity. Our SSC is a network that represents the relationships between developers and artifacts that make up a system. We combine author-to-file, co-changing files, call hierarchies, and reporting structure into a single SSC and calculate the centrality of each node. The measures of centrality can be used to better understand variations in the impact of developer output at Meta. We start by partially replicating prior work and show that the raw number of developer commits plateaus over a project-specific period. However, the centrality of developer work grows for the entire period of study, but the growth slows after one year. This implies that while raw output might plateau, more experienced developers work on more central parts of the system. Finally, we investigate the incremental contribution of SSC attributes in modeling developer output. We find that local attributes such as the number of reports and the specific project do not explain much variation (𝑅2 = 5.8%). In contrast, adding Katz centrality or PageRank produces a model with an 𝑅2 above 30%. SSCs and their centrality provide valuable insights into the centrality and importance of a developer’s work.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1809–1819},
numpages = {11},
keywords = {Developer productivity, Software supply chains},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3583131.3590502,
author = {Pantridge, Edward and Helmuth, Thomas},
title = {Solving Novel Program Synthesis Problems with Genetic Programming using Parametric Polymorphism},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590502},
doi = {10.1145/3583131.3590502},
abstract = {Contemporary genetic programming (GP) systems for general program synthesis have been primarily concerned with evolving programs that can manipulate values from a standard set of primitive data types and simple indexed data structures. In contrast, human programmers do not limit themselves to a small finite set of data types and use polymorphism to express an unbounded number of types including nested data structures, product types, and generic functions. Code-building Genetic Programming (CBGP) is a recently introduced method that compiles type-safe programs from linear genomes using stack-based compilation and a formal type system. Although prior work with CBGP has shown initial demonstrations of polymorphism inside evolved programs, we have provided a deeper exploration of these capabilities through the evolution of programs which make use of generic data types such as key-value maps, tuples, and sets, as well as higher order functions and functions with polymorphic type signatures. In our experiments, CBGP is able to solve problems with all of these properties, where every other GP system that we know of has restrictions that make it unable to even consider problems with these properties. This demonstration provides a significant step towards fully aligning the expressiveness of GP to real world programming.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1175–1183},
numpages = {9},
keywords = {automatic programming, genetic programming, inductive program synthesis, polymorphism},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1145/3631991.3632045,
author = {Samonte, Mary Jane C. and Bian, Qian and Wang, Zhenduo and Chen, Xiaofan},
title = {Mitosis Detection in Breast Cancer Using Deep Learning},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3632045},
doi = {10.1145/3631991.3632045},
abstract = {Breast cancer is a common gynecological malignant tumor. At the same time, breast cancer is also a cancer that can be detected early and prevented early. Using efficient mitotic cell detection technology for breast cancer screening and diagnosis is significant. The application of computer-aided analysis technology to cell detection tasks in pathological medical images has gradually become a hot research direction. In this study, the object detection technique based on YOLO v5 was first applied to the field of mitotic cell detection in pathological breast images. Aiming at the three difficult problems cell detection faces in pathological breast images, this paper explores the influence of detection models on domain generalization from the perspective of image preprocessing. A series of experimental studies show that the color correction strategy based on CycleGAN adopted in this study applied to the YOLO v5 detection model can effectively improve the model's applicability on the external test set.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {335–341},
numpages = {7},
keywords = {Deep Learning, Domain adaption, Mitosis detection, Object detection},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@article{10.1145/3632744,
author = {Qi, Binhang and Sun, Hailong and Zhang, Hongyu and Gao, Xiang},
title = {Reusing Convolutional Neural Network Models through Modularization and Composition},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3632744},
doi = {10.1145/3632744},
abstract = {With the widespread success of deep learning technologies, many trained deep neural network (DNN) models are now publicly available. However, directly reusing the public DNN models for new tasks often fails due to mismatching functionality or performance. Inspired by the notion of modularization and composition in software reuse, we investigate the possibility of improving the reusability of DNN models in a more fine-grained manner. Specifically, we propose two modularization approaches named CNNSplitter and GradSplitter, which can decompose a trained convolutional neural network (CNN) model for N-class classification into N small reusable modules. Each module recognizes one of the N classes and contains a part of the convolution kernels of the trained CNN model. Then, the resulting modules can be reused to patch existing CNN models or build new CNN models through composition. The main difference between CNNSplitter and GradSplitter lies in their search methods: the former relies on a genetic algorithm to explore search space, while the latter utilizes a gradient-based search method. Our experiments with three representative CNNs on three widely-used public datasets demonstrate the effectiveness of the proposed approaches. Compared with CNNSplitter, GradSplitter incurs less accuracy loss, produces much smaller modules (19.88% fewer kernels), and achieves better results on patching weak models. In particular, experiments on GradSplitter show that (1) by patching weak models, the average improvement in terms of precision, recall, and F1-score is 17.13%, 4.95%, and 11.47%, respectively, and (2) for a new task, compared with the models trained from scratch, reusing modules achieves similar accuracy (the average loss of accuracy is only 2.46%) without a costly training process. Our approaches provide a viable solution to the rapid development and improvement of CNN models.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
keywords = {model reuse, convolutional neural network, CNN modularization, module composition}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00075,
author = {Ronanki, Krishna},
title = {Towards an AI-Centric Requirements Engineering Framework for Trustworthy AI},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00075},
doi = {10.1109/ICSE-Companion58688.2023.00075},
abstract = {Ethical guidelines are an asset for artificial intelligence(AI) development and conforming to them will soon be a procedural requirement once the EU AI Act gets ratified in the European parliament. However, developers often lack explicit knowledge on how to apply these guidelines during the system development process. A literature review of different ethical guidelines from various countries and organizations has revealed inconsistencies in the principles presented and the terminology used to describe such principles. This research begins by identifying the limitations of existing ethical AI development frameworks in performing requirements engineering(RE) processes during the development of trustworthy AI. Recommendations to address those limitations will be proposed to make the frameworks more applicable in the RE process to foster the development of trustworthy AI. This could lead to wider adoption, greater productivity of the AI systems, and reduced workload on humans for non-cognitive tasks. Considering the impact of some of the newer foundation models like GitHub Copilot and ChatGPT, the vision for this research project is to work towards the development of holistic operationalisable RE guidelines for the development and implementation of trustworthy AI not only on a product level but also on process level.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {278–280},
numpages = {3},
keywords = {trustworthy AI, EU AI act, requirements engineering, frameworks, AI co-worker, ethical AI, guidelines},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00100,
author = {Gupta, Saksham and Verbruggen, Gust and Singh, Mukul and Gulwani, Sumit and Le, Vu},
title = {Personalized Action Suggestions in Low-Code Automation Platforms},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00100},
doi = {10.1109/ICSE-Companion58688.2023.00100},
abstract = {Automation platforms aim to automate repetitive tasks using workflows, which start with a trigger and then perform a series of actions. However, with many possible actions, the user has to search for the desired action at each step, which hinders the speed of flow development. We propose a personalized transformer model that recommends the next item at each step. This personalization is learned end-to-end from user statistics that are available at inference time. We evaluated our model on workflows from Power Automate users and show that personalization improves top-1 accuracy by 22%. For new users, our model performs similar to a model trained without personalization.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {346–350},
numpages = {5},
keywords = {transformers, personalization, prediction, decoder, recommendation system},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3616352,
author = {Wang, Yibo and Wang, Ying and Zhang, Tingwei and Yu, Yue and Cheung, Shing-Chi and Yu, Hai and Zhu, Zhiliang},
title = {Can Machine Learning Pipelines Be Better Configured?},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616352},
doi = {10.1145/3611643.3616352},
abstract = {A Machine Learning (ML) pipeline configures the workflow of a learning task using the APIs provided by ML libraries. However, a pipeline’s performance can vary significantly across different configurations of ML library versions. Misconfigured pipelines can result in inferior performance, such as poor execution time and memory usage, numeric errors and even crashes. A pipeline is subject to misconfiguration if it exhibits significantly inconsistent performance upon changes in the versions of its configured libraries or the combination of these libraries. We refer to such performance inconsistency as a pipeline configuration (PLC) issue. There is no prior systematic study on the pervasiveness, impact and root causes of PLC issues. A systematic understanding of these issues helps configure effective ML pipelines and identify misconfigured ones. In this paper, we conduct the first empirical study of PLC issues. To better dig into the problem, we propose Piecer, an infrastructure that automatically generates a set of pipeline variants by varying different version combinations of ML libraries and compares their performance inconsistencies. We apply Piecer to the 3,380 pipelines that can be deployed out of the 11,363 ML pipelines collected from multiple ML competitions at Kaggle platform. The empirical study results show that 1,092 (32.3},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {463–475},
numpages = {13},
keywords = {Empirical Study, Machine Learning Libraries},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@article{10.1145/3593802,
author = {Assi, Maram and Hassan, Safwat and Georgiou, Stefanos and Zou, Ying},
title = {Predicting the Change Impact of Resolving Defects by Leveraging the Topics of Issue Reports in Open Source Software Systems},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3593802},
doi = {10.1145/3593802},
abstract = {Upon receiving a new issue report, practitioners start by investigating the defect type, the potential fixing effort needed to resolve the defect and the change impact. Moreover, issue reports contain valuable information, such as, the title, description and severity, and researchers leverage the topics of issue reports as a collective metric portraying similar characteristics of a defect. Nonetheless, none of the existing studies leverage the defect topic, i.e., a semantic cluster of defects of the same nature, such as Performance, GUI, and Database, to estimate the change impact that represents the amount of change needed in terms of code churn and the number of files changed. To this end, in this article, we conduct an empirical study on 298,548 issue reports belonging to three large-scale open-source systems, i.e., Mozilla, Apache, and Eclipse, to estimate the change impact in terms of code churn or the number of files changed while leveraging the topics of issue reports. First, we adopt the Embedded Topic Model (ETM), a state-of-the-art topic modelling algorithm, to identify the topics. Second, we investigate the feasibility of predicting the change impact using the identified topics and other information extracted from the issue reports by building eight prediction models that classify issue reports requiring small or large change impact along two dimensions, i.e., the code churn size and the number of files changed. Our results suggest that XGBoost is the best-performing algorithm for predicting the change impact, with an AUC of 0.84, 0.76, and 0.73 for the code churn and 0.82, 0.71, and 0.73 for the number of files changed metric for Mozilla, Apache, and Eclipse, respectively. Our results also demonstrate that the topics of issue reports improve the recall of the prediction model by up to 45%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {141},
numpages = {34},
keywords = {Issue reports, topics of issue reports, defect fixing, fixing effort, change impact analysis, amount of change, code churn}
}

@article{10.1145/3585007,
author = {Liu, Xuanzhe and Wen, Jinfeng and Chen, Zhenpeng and Li, Ding and Chen, Junkai and Liu, Yi and Wang, Haoyu and Jin, Xin},
title = {FaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3585007},
doi = {10.1145/3585007},
abstract = {Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.In this article, we propose an approach, namely, FaaSLight, to accelerating the cold start for FaaS applications through application-level optimization. We first conduct a measurement study to investigate the possible root cause of the cold-start problem of FaaS. The result shows that application code loading latency is a significant overhead. Therefore, loading only indispensable code from FaaS applications can be an adequate solution. Based on this insight, we identify code related to application functionalities by constructing the function-level call graph and separate other code (i.e., optional code) from FaaS applications. The separated optional code can be loaded on demand to avoid the inaccurate identification of indispensable code causing application failure. In particular, a key principle guiding the design of FaaSLight is inherently general, i.e., platform- and language-agnostic. In practice, FaaSLight can be effectively applied to FaaS applications developed in different programming languages (Python and JavaScript), and can be seamlessly deployed on popular serverless platforms such as AWS Lambda and Google Cloud Functions, without having to modify the underlying OSes or hypervisors, nor introducing any additional manual engineering efforts to developers. The evaluation results on real-world FaaS applications show that FaaSLight can significantly reduce the code loading latency (up to 78.95%, 28.78% on average), thereby reducing the cold-start latency. As a result, the total response latency of functions can be decreased by up to 42.05% (19.21% on average). Compared with the state-of-the-art, FaaSLight achieves a 21.25× improvement in reducing the average total response latency.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {119},
numpages = {29},
keywords = {Serverless computing, cold start, performance optimization, optional function elimination}
}

@article{10.1145/3638245,
author = {Wang, Han and Yu, Sijia and Chen, Chunyang and Turhan, Burak and Zhu, Xiaodong},
title = {Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3638245},
doi = {10.1145/3638245},
abstract = {Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: 1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests, 2) 68% of the sampled DL projects are not unit tested at all, 3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {Deep Learning, Unit Testing}
}

@article{10.1145/3631968,
author = {Hu, Han and Dong, Ruiqi and Grundy, John and Nguyen, Thai Minh and Liu, Huaxiao and Chen, Chunyang},
title = {Automated Mapping of Adaptive App GUIs from Phones to TVs},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631968},
doi = {10.1145/3631968},
abstract = {With the increasing interconnection of smart devices, users often desire to adopt the same app on quite different devices for identical tasks, such as watching the same movies on both their smartphones and TVs. However, the significant differences in screen size, aspect ratio, and interaction styles make it challenging to adapt Graphical User Interfaces (GUIs) across these devices. Although there are millions of apps available on Google Play, only a few thousand are designed to support smart TV displays. Existing techniques to map a mobile app GUI to a TV either adopt a responsive design, which struggles to bridge the substantial gap between phone and TV, or use mirror apps for improved video display, which requires hardware support and extra engineering efforts. Instead of developing another app for supporting TVs, we propose a semi-automated approach to generate corresponding adaptive TV GUIs, given the phone GUIs as the input. Based on our empirical study of GUI pairs for TVs and phones in existing apps, we synthesize a list of rules for grouping and classifying phone GUIs, converting them to TV GUIs, and generating dynamic TV layouts and source code for the TV display. Our tool is not only beneficial to developers but also to GUI designers, who can further customize the generated GUIs for their TV app development. An evaluation and user study demonstrate the accuracy of our generated GUIs and the usefulness of our tool.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {47},
numpages = {31},
keywords = {Graphic user interface, cross-screen, adaptive GUI}
}

@article{10.1145/3603109,
author = {Zhang, Jingxuan and Luo, Junpeng and Liang, Jiahui and Gong, Lina and Huang, Zhiqiu},
title = {An Accurate Identifier Renaming Prediction and Suggestion Approach},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3603109},
doi = {10.1145/3603109},
abstract = {Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {148},
numpages = {51},
keywords = {Identifier renaming, source code analysis, code refactoring, mining code repository}
}

@inproceedings{10.5555/3615924.3623633,
author = {Raphael, Rouf and Mohammadreza, Rasolroveicy and Harit, Ahuja and Zakeya, Namrud and Ian, Watts and Komal, Sarda and Marin, Litoiu},
title = {Models for Detecting Performance Anomalies and Identifying Root Causes in Microservices Applications},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {As the automation of microservice and cloud computing operations grows, models become crucial for enabling resilient and efficient adaptive architectures and implementations.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {242–244},
numpages = {3},
keywords = {Cloud Computing, Anomaly detection, Resource Overload, Ma-chine Learning, Root Cause Analysis},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@article{10.1016/j.infsof.2023.107169,
author = {AlOmar, Eman Abdullah and Ivanov, Anton and Kurbatova, Zarina and Golubev, Yaroslav and Mkaouer, Mohamed Wiem and Ouni, Ali and Bryksin, Timofey and Nguyen, Le and Kini, Amit and Thakur, Aditya},
title = {Just-in-time code duplicates extraction},
year = {2023},
issue_date = {Jun 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {158},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107169},
doi = {10.1016/j.infsof.2023.107169},
journal = {Inf. Softw. Technol.},
month = {jun},
numpages = {14},
keywords = {Refactoring, Machine learning, Software quality}
}

@inproceedings{10.5555/3615924.3615950,
author = {Xi, Yang and Saurabh, Jha and Paulito, Palmes and Bekir, Turkkan and Gerard, Vanloo and Chandra, Narayanaswami and Larisa, Shwartz},
title = {Meta-learning Generalized AIOps Models for Multi-cloud Computer using Digital Twins},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Multi-cloud computing is a vitally important topic from both busi-ness and technical perspectives since it guarantees resiliency, avail-ability, and security. Due to the vast number of configurations among cloud providers, it is quite challenging to migrate AIOps models across different clouds. Although it is possible to train these models from scratch on the target cloud, this process can be time-consuming and prone to delays. Consequently, the objective of this paper is to create a generalized AIOps model from the original cloud that can be seamlessly applied to target cloud with minimal to zero-shot observations. To achieve this goal, we present a novel framework in this position paper, which harnesses the potential of digital twins to enhance data generalization. Additionally, our proposed framework employs meta-learning techniques to ensure effective model generalization across different cloud environments.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {206–210},
numpages = {5},
keywords = {Multi-cloud Computer, Digital Twins, Distribution Drifts, AIOps},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1109/ICSE48619.2023.00193,
author = {Liu, Jinyang and He, Shilin and Chen, Zhuangbin and Li, Liqun and Kang, Yu and Zhang, Xu and He, Pinjia and Zhang, Hongyu and Lin, Qingwei and Xu, Zhangwei and Rajmohan, Saravan and Zhang, Dongmei and Lyu, Michael R.},
title = {Incident-Aware Duplicate Ticket Aggregation for Cloud Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00193},
doi = {10.1109/ICSE48619.2023.00193},
abstract = {In cloud systems, incidents are potential threats to customer satisfaction and business revenue. When customers are affected by incidents, they often request customer support service (CSS) from the cloud provider by submitting a support ticket. Many tickets could be duplicate as they are reported in a distributed and uncoordinated manner. Thus, aggregating such duplicate tickets is essential for efficient ticket management. Previous studies mainly rely on tickets' textual similarity to detect duplication; however, duplicate tickets in a cloud system could carry semantically different descriptions due to the complex service dependency of the cloud system. To tackle this problem, we propose iPACK, an incident-aware method for aggregating duplicate tickets by fusing the failure information between the customer side (i.e., tickets) and the cloud side (i.e., incidents). We extensively evaluate iPACK on three datasets collected from the production environment of a large-scale cloud platform, Azure. The experimental results show that iPACK can precisely and comprehensively aggregate duplicate tickets, achieving an F1 score of 0.871~0.935 and outperforming state-of-the-art methods by 12.4%~31.2%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2299–2311},
numpages = {13},
keywords = {duplicate tickets, incidents, cloud systems},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3613372.3613405,
author = {Gomes, Anderson and Maia, Paulo Henrique M.},
title = {DoME: An Architecture for Domain Model Evolution at Runtime Using NLP},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613405},
doi = {10.1145/3613372.3613405},
abstract = {In traditional information systems, domain models are represented as database tables with attributes and relationships. Changes in the domain models exist due to system evolution and the emergence of new requirements. In these applications, domain models evolve using CRUD operations requested by users. However, it is necessary to support changes in domain models during the applications’ runtime when new (unforeseen) situations may occur. This work presents an architecture called DoME, which relies on natural language processing (NLP) to allow users to trigger changes in the domain models and self-adaptation techniques to update the models at runtime. It is instantiated in a concrete architecture using a chatbot in Telegram and Transformers Libraries for NLP. The architecture has been preliminary evaluated regarding its assertiveness and user satisfaction, resulting in an 82.55% hit rate and confirming that NL provides good usability and facilitates data manipulation.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {186–195},
numpages = {10},
keywords = {Domain Modelling., Generative Artificial Intelligence, Natural Language Processing, Software Architecture},
location = {<conf-loc>, <city>Campo Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SBES '23}
}

@inproceedings{10.1145/3624062.3624283,
author = {Elia, Donatello and Scardigno, Sonia and Ejarque, Jorge and D’Anca, Alessandro and Accarino, Gabriele and Scoccimarro, Enrico and Donno, Davide and Peano, Daniele and Immorlano, Francesco and Aloisio, Giovanni},
title = {End-to-End Workflows for Climate Science: Integrating HPC Simulations, Big Data Processing, and Machine Learning},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624283},
doi = {10.1145/3624062.3624283},
abstract = {Current scientific workflow systems do not typically integrate simulation-centric and data-centric aspects due to their very different software/infrastructure requirements. A transparent integration of such components into a single end-to-end workflow would lead to a more efficient and automated way for generating insights from large simulation data. This work presents a complex case study related to extreme events analysis of future climate data that integrates in the same workflow numerical simulations, Big Data analytics and Machine Learning models. The case study is being implemented in the context of the eFlows4HPC project using the project’s software stack for deployment and orchestration of the workflow. The solution implemented in the project has shown to simplify the development and execution of end-to-end climate workflows with heterogeneous software requirements. Moreover, such an approach can, in the long term, increase the reuse of workflows by scientists and their portability over different HPC infrastructures.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {2042–2052},
numpages = {11},
keywords = {Scientific workflow management, data-driven models, extreme climate events, high performance data analytics},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC-W '23}
}

@inproceedings{10.1007/978-3-031-36021-3_15,
author = {Lomshakov, Vadim and Kovalchuk, Sergey and Omelchenko, Maxim and Nikolenko, Sergey and Aliev, Artem},
title = {Fine-Tuning Large Language Models for&nbsp;Answering Programming Questions with&nbsp;Code Snippets},
year = {2023},
isbn = {978-3-031-36020-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36021-3_15},
doi = {10.1007/978-3-031-36021-3_15},
abstract = {We study the ability of pretrained large language models (LLM) to answer questions from online question answering fora such as Stack Overflow. We consider question-answer pairs where the main part of the answer consists of source code. On two benchmark datasets—CoNaLa and a newly collected dataset based on Stack Overflow—we investigate how a closed-book question answering system can be improved by fine-tuning the LLM for the downstream task, prompt engineering, and data preprocessing. We use publicly available autoregressive language models such as GPT-Neo, CodeGen, and PanGu-Coder, and after the proposed fine-tuning achieve a BLEU score of 0.4432 on the CoNaLa test set, significantly exceeding previous state of the art for this task.},
booktitle = {Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part II},
pages = {171–179},
numpages = {9},
keywords = {Program synthesis, Question answering, Large language models},
location = {Prague, Czech Republic}
}

@inproceedings{10.1109/ICSE48619.2023.00059,
author = {An, Gabin and Hong, Jingun and Kim, Naryeong and Yoo, Shin},
title = {Fonte: Finding Bug Inducing Commits from Failures},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00059},
doi = {10.1109/ICSE48619.2023.00059},
abstract = {A Bug Inducing Commit (BIC) is a commit that introduces a software bug into the codebase. Knowing the relevant BIC for a given bug can provide valuable information for debugging as well as bug triaging. However, existing BIC identification techniques are either too expensive (because they require the failing tests to be executed against previous versions for bisection) or inapplicable at the debugging time (because they require post hoc artefacts such as bug reports or bug fixes). We propose Fonte, an efficient and accurate BIC identification technique that only requires test coverage. Fonte combines Fault Localisation (FL) with BIC identification and ranks commits based on the suspiciousness of the code elements that they modified. Fonte reduces the search space of BICs using failure coverage as well as a filter that detects commits that are merely style changes. Our empirical evaluation using 130 real-world BICs shows that Fonte significantly outperforms state-of-the-art BIC identification techniques based on Information Retrieval as well as neural code embedding models, achieving at least 39% higher MRR. We also report that the ranking scores produced by Fonte can be used to perform weighted bisection, further reducing the cost of BIC identification. Finally, we apply Fonte to a large-scale industry project with over 10M lines of code, and show that it can rank the actual BIC within the top five commits for 87% of the studied real batch-testing failures, and save the BIC inspection cost by 32% on average.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {589–601},
numpages = {13},
keywords = {bug inducing commit, fault localisation, git, weighted bisection, batch testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00071,
author = {Ma'ayan, Dor and Maoz, Shahar},
title = {Using Reactive Synthesis: An End-to-End Exploratory Case Study},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00071},
doi = {10.1109/ICSE48619.2023.00071},
abstract = {Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Despite its attractiveness and major research progress in the past decades, reactive synthesis is still in early-stage and has not gained popularity outside academia. We conducted an exploratory case study in which we followed students in a semester-long university workshop class on their end-to-end use of a reactive synthesizer, from writing the specifications to executing the synthesized controllers. The data we collected includes more than 500 versions of more than 80 specifications, as well as more than 2500 Slack messages, all written by the class participants. Our grounded theory analysis reveals that the use of reactive synthesis has clear benefits for certain tasks and that adequate specification language constructs assist in the specification writing process. However, inherent issues such as unrealizabilty, non-well-separation, the gap of knowledge between the users and the synthesizer, and considerable running times prevent reactive synthesis from fulfilling its promise. Based on our analysis, we propose action items in the directions of language and specification quality, tools for analysis and execution, and process and methodology, all towards making reactive synthesis more applicable for software engineers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {742–754},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inbook{10.1145/3570361.3615759,
author = {Waskito, Steven and Leow, Kai Jie and Medaranga, Pramuka and Gupta, Tejas and Chakrabarty, Shantanu and Gulati, Manoj and Varshney, Ambuj},
title = {Otter: Simplifying Embedded Sensor Data Collection and Analysis using Large Language Models},
year = {2023},
isbn = {9781450399906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570361.3615759},
abstract = {Wireless embedded systems assist us in collecting data from the physical world, through sensor data analysis, such systems allow us to understand our environment. However, deploying wireless embedded systems and analyzing the collected data remains significantly challenging. This is due to the steep learning curve required to implement custom machine-learning models and other algorithms for data analysis. Furthermore, it is also challenging to program individual embedded devices. The diversity of the available platforms and their capabilities further compounds this problem. In response, we introduce an end-to-end system, called the Otter. It facilitates simple sensor data collection using commodity-embedded platforms. Moreover, it employs a large language model to design a natural language interface for the analysis and extraction of useful information from the sensor data. We present our preliminary work on prototyping this system, applying it to a specific use case of hand gesture detection. Otter represents one of the first systems to leverage the enhanced capabilities of large language models for simplifying wireless embedded system deployments.},
booktitle = {Proceedings of the 29th Annual International Conference on Mobile Computing and Networking},
articleno = {152},
numpages = {3}
}

@inproceedings{10.1109/ICSE48619.2023.00070,
author = {Amram, Gal and Ma'ayan, Dor and Maoz, Shahar and Pistiner, Or and Ringert, Jan Oliver},
title = {Triggers for Reactive Synthesis Specifications},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00070},
doi = {10.1109/ICSE48619.2023.00070},
abstract = {Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Two of the main challenges in bringing reactive synthesis to practice are its very high worst-case complexity and the difficulty of writing declarative specifications using basic LTL operators. To address the first challenge, researchers have suggested the GR(1) fragment of LTL, which has an efficient polynomial time symbolic synthesis algorithm. To address the second challenge, specification languages include higher-level constructs that aim at allowing engineers to write succinct and readable specifications. One such construct is the triggers operator, as supported, e.g., in the Property Specification Language (PSL).In this work we introduce triggers into specifications for reactive synthesis. The effectiveness of our contribution relies on a novel encoding of regular expressions using symbolic finite automata (SFA) and on a novel semantics for triggers that, in contrast to PSL triggers, admits an efficient translation into GR(1). We show that our triggers are expressive and succinct, and prove that our encoding is optimal.We have implemented our ideas on top of the Spectra language and synthesizer. We demonstrate the usefulness and effectiveness of using triggers in specifications for synthesis, as well as the challenges involved in using them, via a study of more than 300 triggers written by undergraduate students who participated in a project class on writing specifications for synthesis.To the best of our knowledge, our work is the first to introduce triggers into specifications for reactive synthesis.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {729–741},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3584871.3584872,
author = {Patel, Manali and Jariwala, Krupa and Chattopadhyay, Chiranjoy},
title = {Deep Learning techniques for stock market forecasting: Recent trends and challenges},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584872},
doi = {10.1145/3584871.3584872},
abstract = {Stock market forecasting has been a very intensive area of research in recent years due to the highly uncertain and volatile nature of stock data which makes this task challenging. By accurately predicting a particular stock's price investors can gain maximum profit out of their investment. With the great success of Deep Learning methods in various domains, it has attracted the research community to apply these models for financial domain also. These DL methods have been proven to achieve better accuracy and predictions compared to econometric and traditional ML methods. This work reviews recent papers according to various Deep Learning models which included: Artificial Neural Networks, Convolution Neural Networks, Sequence to Sequence models, Generative Adversarial Networks, Graph Neural Networks and Transformers applied for stock market forecasting. Furthermore this work also reviews datasets, features, evaluation parameters and results of various methods. From the analysis done on various DL models we found that Graph Neural Networks and Transformer models have potential to interpret dynamic and non-linear patterns of financial time series data with greater accuracy. In addition to this, correlation among various stock indices and investors sentiment along with historical data has great influence on the prediction accuracy. We also identified the benchmark datasets for stock market forecasting based on market capitalization value of an economy. The aim of this paper is to provide insight into most recent work done in the finance domain and identify future directions for more accurate predictions.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {1–11},
numpages = {11},
keywords = {Corporate relationship, Deep Learning, Graph Neural Networks, Sentiment analysis, Stock market forecasting, Transformers},
location = {<conf-loc>, <city>Palmerston North</city>, <country>New Zealand</country>, </conf-loc>},
series = {ICSIM '23}
}

@article{10.4018/JCIT.324762,
author = {Ma, Huimin and Cao, Wenyang and Cheng, Xin and Wang, Chong and Wang, Ke},
title = {Value Creation and Sustainable Project Management: A Case Study on a Leading SOE in China},
year = {2023},
issue_date = {Jul 2023},
publisher = {IGI Global},
address = {USA},
volume = {25},
number = {1},
issn = {1548-7717},
url = {https://doi.org/10.4018/JCIT.324762},
doi = {10.4018/JCIT.324762},
abstract = {The literature emphasizes the crucial role of state-owned enterprises (SOEs) in fostering economic growth and addressing public demands. However, ensuring the sustainable development of SOEs requires further research to investigate sustainable project management with a focus on value creation. Accordingly, the authors conducted a case study of a prominent real estate SOE in China to examine how SOEs can alter their project management practices and achieve a more sustainable business model. The findings reveal that adopting a whole life-cycle management system can promote value refinement, optimization, and co-creation, enabling SOEs to achieve value creation through sustainable management. This study offers useful insights for policymakers and business executives in China and contributes to the literature on sustainable project management and value creation.},
journal = {J. Cases Inf. Technol.},
month = {jun},
pages = {1–16},
numpages = {16},
keywords = {Case Study, China, Life-Cycle Management, Real Estate, SOEs, Sustainable Project Management, Value Creation}
}

@article{10.1016/j.jbi.2023.104384,
author = {Lokker, Cynthia and Bagheri, Elham and Abdelkader, Wael and Parrish, Rick and Afzal, Muhammad and Navarro, Tamara and Cotoi, Chris and Germini, Federico and Linkins, Lori and Haynes, R. Brian and Chu, Lingyang and Iorio, Alfonso},
title = {Deep learning to refine the identification of high-quality clinical research articles from the biomedical literature: Performance evaluation},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {142},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2023.104384},
doi = {10.1016/j.jbi.2023.104384},
journal = {J. of Biomedical Informatics},
month = {jun},
numpages = {9},
keywords = {Bioinformatics, Machine learning, Evidence-based medicine, Literature retrieval, Medical informatics, Natural Language Processing, BERT, HiRU, PLUS, PRLM}
}

@inproceedings{10.1109/ICSE48619.2023.00088,
author = {Pan, Shengyi and Bao, Lingfeng and Xia, Xin and Lo, David and Li, Shanping},
title = {Fine-Grained Commit-Level Vulnerability Type Prediction by CWE Tree Structure},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00088},
doi = {10.1109/ICSE48619.2023.00088},
abstract = {Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that TreeVul significantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {957–969},
numpages = {13},
keywords = {software security, vulnerability type, CWE},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.jss.2023.111796,
author = {Zakeri-Nasrabadi, Morteza and Parsa, Saeed and Ramezani, Mohammad and Roy, Chanchal and Ekhtiarzadeh, Masoud},
title = {A systematic literature review on source code similarity measurement and clone detection: Techniques, applications, and challenges},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {204},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111796},
doi = {10.1016/j.jss.2023.111796},
journal = {J. Syst. Softw.},
month = {oct},
numpages = {33},
keywords = {Source code similarity, Code clone, Plagiarism detection, Code recommendation, Systematic literature review}
}

@inproceedings{10.1109/ICSE48619.2023.00094,
author = {Lyu, Yunbo and Le-Cong, Thanh and Kang, Hong Jin and Widyasari, Ratnadira and Zhao, Zhipeng and Le, Xuan-Bach D. and Li, Ming and Lo, David},
title = {Chronos: Time-Aware Zero-Shot Identification of Libraries from Vulnerability Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00094},
doi = {10.1109/ICSE48619.2023.00094},
abstract = {Tools that alert developers about library vulnerabilities depend on accurate, up-to-date vulnerability databases which are maintained by security researchers. These databases record the libraries related to each vulnerability. However, the vulnerability reports may not explicitly list every library and human analysis is required to determine all the relevant libraries. Human analysis may be slow and expensive, which motivates the need for automated approaches. Researchers and practitioners have proposed to automatically identify libraries from vulnerability reports using extreme multi-label learning (XML).While state-of-the-art XML techniques showed promising performance, their experimental settings do not practically fit what happens in reality. Previous studies randomly split the vulnerability reports data for training and testing their models without considering the chronological order of the reports. This may unduly train the models on chronologically newer reports while testing the models on chronologically older ones. However, in practice, one often receives chronologically new reports, which may be related to previously unseen libraries. Under this practical setting, we observe that the performance of current XML techniques declines substantially, e.g., F1 decreased from 0.7 to 0.24 under experiments without and with consideration of chronological order of vulnerability reports.We propose a practical library identification approach, namely Chronos, based on zero-shot learning. The novelty of Chronos is three-fold. First, Chronos fits into the practical pipeline by considering the chronological order of vulnerability reports. Second, Chronos enriches the data of the vulnerability descriptions and labels using a carefully designed data enhancement step. Third, Chronos exploits the temporal ordering of the vulnerability reports using a cache to prioritize prediction of versions of libraries that recently had reports of vulnerabilities.In our experiments, Chronos achieves an average F1-score of 0.75, 3x better than the best XML-based approach. Data enhancement and the time-aware adjustment improve Chronos over the vanilla zero-shot learning model by 27% in average F1.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1033–1045},
numpages = {13},
keywords = {zero-shot learning, library identification, unseen labels, extreme multi-label classification, vulnerability reports},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00083,
author = {Ran, Dezhi and Wang, Hao and Wang, Wenyu and Xie, Tao},
title = {Badge: Prioritizing UI Events with Hierarchical Multi-Armed Bandits for Automated UI Testing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00083},
doi = {10.1109/ICSE48619.2023.00083},
abstract = {To assure high quality of mobile applications (apps for short), automated UI testing triggers events (associated with UI elements on app UIs) without human intervention, aiming to maximize code coverage and find unique crashes. To achieve high test effectiveness, automated UI testing prioritizes a UI event based on its exploration value (e.g., the increased code coverage of future exploration rooted from the UI event). Various strategies have been proposed to estimate the exploration value of a UI event without considering its exploration diversity (reflecting the variance of covered code entities achieved by explorations rooted from this UI event across its different triggerings), resulting in low test effectiveness, especially on complex mobile apps. To address the preceding problem, in this paper, we propose a new approach named Badge to prioritize UI events considering both their exploration values and exploration diversity for effective automated UI testing. In particular, we design a hierarchical multi-armed bandit model to effectively estimate the exploration value and exploration diversity of a UI event based on its historical explorations along with historical explorations rooted from UI events in the same UI group. We evaluate Badge on 21 highly popular industrial apps widely used by previous related work. Experimental results show that Badge outperforms state-of-the-art/practice tools with 18%-146% relative code coverage improvement and finding 1.19--5.20x unique crashes, demonstrating the effectiveness of Badge. Further experimental studies confirm the benefits brought by Badge's individual algorithms.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {894–905},
numpages = {12},
keywords = {GUI testing, mobile testing, mobile app, android, multi-armed bandits, reinforcement learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-3-031-49266-2_21,
author = {Pandey, Sushant Kumar and Chand, Sivajeet and Horkoff, Jennifer and Staron, Miroslaw},
title = {Design Patterns Understanding and&nbsp;Use in&nbsp;the&nbsp;Automotive Industry: An Interview Study},
year = {2023},
isbn = {978-3-031-49265-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49266-2_21},
doi = {10.1007/978-3-031-49266-2_21},
abstract = {Automotive software is increasing in complexity, leading to new challenges for designers and developers. Design patterns, which offer reusable solutions to common design problems, are a potential way to deal with this complexity. Although design patterns have received much focus in academic publications, it is not clear how they are used in practice. This paper presents an interview-based study that explores the use of design patterns in the automotive industry. The study findings reveal how automotive practitioners view and use design patterns in their software designs. Our study revealed that industry experts have a view of design patterns which often differs from the academic views. They use design patterns in combination with architecture guidelines, principles, and frameworks. Instead of the academic focus on the design patterns, industry professionals focus on the design, architectural tactics, and standards. Such findings highlight the need for a more nuanced understanding of the concept and practical applications of design patterns within the context of industrial software engineering practices.},
booktitle = {Product-Focused Software Process Improvement: 24th International Conference, PROFES 2023, Dornbirn, Austria, December 10–13, 2023, Proceedings, Part I},
pages = {301–319},
numpages = {19},
keywords = {Design patterns, Automotive industry, Software industry},
location = {<conf-loc content-type="InPerson">Dornbirn, Austria</conf-loc>}
}

@inproceedings{10.1109/ICSE48619.2023.00026,
author = {Yan, Jiwei and Wang, Miaomiao and Liu, Yepang and Yan, Jun and Zhang, Long},
title = {Locating Framework-Specific Crashing Faults with Compact and Explainable Candidate Set},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00026},
doi = {10.1109/ICSE48619.2023.00026},
abstract = {Nowadays, many applications do not exist independently but rely on various frameworks or libraries. The frequent evolution and the complex implementation of framework APIs induce lots of unexpected post-release crashes. Starting from the crash stack traces, existing approaches either perform application-level call graph (CG) tracing or construct datasets with similar crash-fixing records to locate buggy methods. However, these approaches are limited by the completeness of CG or dependent on historical fixing records, and some of them only focus on specific manually modeled exception types.To achieve effective debugging on complex framework-specific crashes, we propose a code-separation-based locating approach that weakly relies on CG tracing and does not require any prior knowledge. Our key insight is that one crash trace with the description message can be mapped to a definite exception-thrown point in the framework, the semantics analysis of which can help to figure out the root causes of the crash-triggering procedure. Thus, we can pre-construct reusable summaries for all the framework-specific exceptions to support fault localization in application code. Based on that idea, we design the exception-thrown summary (ETS) that describes both the key variables and key APIs related to the exception triggering. Then, we perform static analysis to automatically compute such summaries and make a data-tracking of key variables and APIs in the application code to get the ranked buggy candidates. In the scenario of locating Android framework-specific crashing faults, our tool CrashTracker exhibited an overall MRR value of 0.91 and outperforms the state-of-the-art tool Anchor with higher precision. It only provides a compact candidate set and gives user-friendly reports with explainable reasons for each candidate.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {172–183},
numpages = {12},
keywords = {fault localization, framework-specific exception, crash stack trace, android application},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3631991.3632001,
author = {Chen, Abbott Po Shun},
title = {Using Conversational AI to Service Organizational Agility and Flexibility: The Dynamic Capability and Option View},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631991.3632001},
doi = {10.1145/3631991.3632001},
abstract = {Artificial intelligence has been changing our lives with the evolution of the times. Although human wisdom brings convenience, the use of conversational artificial intelligence by organizations will expand the scope of their operations. This will give the organization sufficient resources to reduce operating costs and time. On the other hand, conversational artificial intelligence is one of the important dynamic capabilities of organizations and will also allow organizations to respond quickly to market needs. This study examines usage, resilience, and agility in conversational AI. Based on the experience of 132 organizational information personnel, this study used questionnaires, surveys, and statistical analysis. The results show that conversational AI use is related to organizational agility. This dynamic capability will be related to organizational resilience. In conversational artificial intelligence, dynamic capabilities theory can be applied. High organizational resilience will allow organizations to respond quickly to needs when using conversational artificial intelligence. In practical applications, when organizations use conversational AI, they need to consider the breadth of resources generated and use it as an option to make contributions.},
booktitle = {Proceedings of the 2023 5th World Symposium on Software Engineering},
pages = {67–70},
numpages = {4},
keywords = {Conversational AI, Dynamic capabilities, Organizational agility},
location = {<conf-loc>, <city>Tokyo</city>, <country>Japan</country>, </conf-loc>},
series = {WSSE '23}
}

@inproceedings{10.1007/978-981-99-9896-8_11,
author = {Liu, Yuyong and Chen, Zhifei},
title = {NLP-Based Test Co-evolution Prediction for IoT Application Maintenance},
year = {2024},
isbn = {978-981-99-9895-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-9896-8_11},
doi = {10.1007/978-981-99-9896-8_11},
abstract = {The increasing deployment of the Internet of Things (IoT) leads to the diversified development of IoT-based applications. However, due to the fast updates and the growing scale of IoT applications, IoT developers mainly focus on the production code but overlook the co-evolution of the corresponding test code. To facilitate the maintenance of IoT applications, this paper proposes an NLP-based approach to predict whether the test code needs to be co-changed when its production code is updated. We collected data from the most popular projects on GitHub (top 1,000 with the highest stars). Three neural encoders were employed to capture semantic features of commit messages, production code changes, and related test code. We then generated our training samples, in which the features of each sample consist of &lt; Commit Message, Production Code Change, Test Unit Code &gt;. Finally, a neural network model was built by learning the correlations among these features to determine the possibility of test co-evolution. We evaluated the effectiveness of our NLP-based approach on 15 widely used Python projects in the IoT domain. The evaluation result shows that the prediction accuracy of our model achieves 93%, highlighting the practical significance of our approach in the maintenance of IoT applications.},
booktitle = {Green, Pervasive, and Cloud Computing: 18th International Conference, GPC 2023, Harbin, China, September 22–24, 2023, Proceedings; Part II},
pages = {155–171},
numpages = {17},
keywords = {IoT Development, Test Co-evolution, Maintenance, NLP, Prediction Model},
location = {<conf-loc content-type="InPerson">Harbin, China</conf-loc>}
}

@inproceedings{10.1145/3618305.3623612,
author = {Jain, Rijul and Ni, Wode and Sunshine, Joshua},
title = {Generating Domain-Specific Programs for Diagram Authoring with Large Language Models},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623612},
doi = {10.1145/3618305.3623612},
abstract = {Large language models (LLMs) can generate programs in general-purpose languages from prose descriptions, but are not trained on many domain-specific languages (DSLs). Diagram authoring with Penrose, a diagramming system using three DSLs, exemplifies the utility of DSL program generation with LLMs, which enables diagram creation from prose. We provide methods to conceptualize and evaluate the structures of one-shot LLM prompts to generate error-free DSL programs and implement Penrose diagram creation from prose using LLMs. We will evaluate our LLM prompt structures by testing prompt variations across different diagramming domains and plan to run a user study to assess the ease of LLM-augmented Penrose diagramming over other tools.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {70–71},
numpages = {2},
keywords = {domain-specific languages, large language models, visualization},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@article{10.1016/j.infsof.2023.107303,
author = {Kalouptsoglou, Ilias and Siavvas, Miltiadis and Ampatzoglou, Apostolos and Kehagias, Dionysios and Chatzigeorgiou, Alexander},
title = {Software vulnerability prediction: A systematic mapping study},
year = {2023},
issue_date = {Dec 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {164},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107303},
doi = {10.1016/j.infsof.2023.107303},
journal = {Inf. Softw. Technol.},
month = {dec},
numpages = {18},
keywords = {Systematic mapping study, Software security, Vulnerability prediction, Machine learning}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00030,
author = {Talebipour, Saghar and Park, Hyojae and Baral, Kesina and Yee, Leon and Khan, Safwat Ali and Moran, Kevin and Brun, Yuriy and Medvidovic, Nenad and Zhao, Yixue},
title = {Avgust: A Tool for Generating Usage-Based Tests from Videos of App Executions},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00030},
doi = {10.1109/ICSE-Companion58688.2023.00030},
abstract = {Creating UI tests for mobile applications is a difficult and time-consuming task. As such, there has been a considerable amount of work carried out to automate the generation of mobile tests---largely focused upon the goals of maximizing code coverage or finding crashes. However, comparatively fewer automated techniques have been proposed to generate a highly sought after type of test: usage-based tests. These tests exercise targeted app functionalities for activities such as regression testing. In this paper, we present the Avgust tool for automating the construction of usage-based tests for mobile apps. Avgust learns usage patterns from videos of app executions collected by beta testers or crowd-workers, translates these into an app-agnostic state-machine encoding, and then uses this encoding to generate new test cases for an unseen target app. We evaluated Avgust on 374 videos of use cases from 18 popular apps and found that it can successfully exercise the desired usage in 69% of the tests. Avgust is an open-source tool available at https://github.com/felicitia/UsageTesting-Repo/tree/demo. A video illustrating the capabilities of Avgust can be found at: https://youtu.be/LPICxVd0YAg.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {83–87},
numpages = {5},
keywords = {mobile application, UI understanding, mobile testing, test generation, AI/ML},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00084,
author = {Santos, Fabio},
title = {Skill Recommendation for New Contributors in Open-Source Software},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00084},
doi = {10.1109/ICSE-Companion58688.2023.00084},
abstract = {Selecting an appropriate task is challenging for newcomers to Open Source Software (OSS) projects. Therefore, researchers and OSS projects have proposed strategies to label tasks (a.k.a. issues). Several approaches relying on machine learning techniques, historical information, and textual analysis have been submitted. However, the results vary, and these approaches are still far from mainstream adoption, possibly because of a lack of good predictors. Inspired by previous research, we advocate that the prediction models might benefit from leveraging social metrics.In this research, we investigate how to assist the new contributors in finding a task when onboarding a new project. To achieve our goal, we predict the skills needed to solve an open issue by labeling them with the categories of APIs declared in the source code (API-domain labels) that should be updated or implemented. Starting from a case study using one project and an empirical experiment, we found the API-domain labels were relevant to select an issue for a contribution. In the sequence, we investigated employing interviews and a survey of what strategies maintainers the strategies believe the communities have to adopt to assist the new contributors in finding a task. We also studied how maintainers think about new contributors' strategies to pick a task. We found maintainers, frequent contributors, and new contributors diverge about the importance of the communities and new contributors' strategies.The ongoing research works in three directions: 1) generalization of the approach, 2) Use of conversation data metrics for predictions, 3) Demonstration of the approach, and 4) Matching contributors and tasks skills.By addressing the lack of knowledge about the skills in tasks, we hope to assist new contributors in picking tasks with more confidence.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {311–313},
numpages = {3},
keywords = {labelling, skills, mining software repositories, social network analysis, open-source software, machine learning, ontology matching},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1016/j.infsof.2023.107332,
author = {Elias, Paulo and Campos, Heleno de S. and Ogasawara, Eduardo and Murta, Leonardo Gresta Paulino},
title = {Towards accurate recommendations of merge conflicts resolution strategies},
year = {2023},
issue_date = {Dec 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {164},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107332},
doi = {10.1016/j.infsof.2023.107332},
journal = {Inf. Softw. Technol.},
month = {dec},
numpages = {14},
keywords = {Merge, Conflict, Resolution, Recommendation, Data mining, Machine learning}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00025,
author = {Sun, Jiamou and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming},
title = {A Multi-Faceted Vulnerability Searching Website Powered by Aspect-Level Vulnerability Knowledge Graph},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00025},
doi = {10.1109/ICSE-Companion58688.2023.00025},
abstract = {Vulnerabilities can cause damages to users. With heavy dependencies among software, it is particularly important to safely select the dependent libraries and maintain security of software in a targeted manner, which require deep understanding of potential weakness of third-party libraries. Current vulnerability advisories only support rough-level description-based vulnerability information searching, which cannot cater the needs of in-depth investigation and understanding of vulnerabilities. Driven by the real needs, we propose a vulnerability aspect-level vulnerability knowledge graph integrating diversified vulnerability key aspect information from heterogeneous vulnerability databases. Based on the knowledge graph, we implement a multi-faceted vulnerability searching website for statistics and details acquiring of vulnerabilities. Our use cases demonstrate the usefulness of our knowledge graph and website to the software security.Demo Video: https://youtu.be/vYSy7MYIU48Source Code: https://github.com/sjmsjmdsg/Multi_faceted_WebWebsite: see GitHub repository.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {60–63},
numpages = {4},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-981-99-7353-8_9,
author = {Díaz Oporto, Sara and Dueñas Vera, Sergio and Manrique Chalco, Walker and Bragagnini Mendizábal, César and Fernández Del Carpio, Alvaro},
title = {Identifying Master’s Program Web Pages with Machine Learning},
year = {2023},
isbn = {978-981-99-7352-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-7353-8_9},
doi = {10.1007/978-981-99-7353-8_9},
abstract = {In the journey of education and professional growth, there is a wide variety of postgraduate programs and specializations offered by different universities and training centers, many of which have a significant online presence. Simultaneously, for the institutions offering these programs, it is of interest to gather information about the competition in order to develop their study offering strategies accordingly. Likewise, for most individuals interested in pursuing these studies, identifying these programs on various published web pages proves to be challenging. This work presents a tool for detecting relevant information from web page content, allowing the identification of master’s programs. One of the primary challenges addressed in this research is the technological issue of accurately and efficiently extracting information from diverse web sources, given the variability in webpage structures and formats. The tool is based on the analysis of various Machine Learning and Deep Learning models, along with the application of web scraping techniques. Tests and validations were conducted on different models, with the best result being achieved by the deep recurrent neural networks model, which reached a 91.95% F1-score metric. The use of automated tools for selection, gathering, and classification of information enhances learning by enabling a greater focus on points of interest. Therefore, when searching for master’s degree programs, people would avoid the tedious task of manual searching and sorting. Instead, they can focus on deeply analyzing and comparing the options. This focus on exploration and analysis would enhance their understanding of the options and promote more informed decisions.},
booktitle = {Proceedings of the 18th Latin American Conference on Learning Technologies (LACLO 2023)},
pages = {101–115},
numpages = {15},
keywords = {Master’s Program, Machine Learning, Deep Learning, Web Scrapping},
location = {Cuenca, Ecuador}
}

@inproceedings{10.1007/978-3-031-45382-3_21,
author = {Lin, Huei-Fang and Lin, Huei-Yung},
title = {Image Acquisition by&nbsp;Image Retrieval with&nbsp;Color Aesthetics},
year = {2023},
isbn = {978-3-031-45381-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-45382-3_21},
doi = {10.1007/978-3-031-45382-3_21},
abstract = {The objective of this work is to obtained aesthetically pleasing images related based on the location information. To achieve this, we develop a system that can acquire photos using the geographic information and cloud data by image retrieval. This system comprises an insertion module and a search module. The insertion module recognizes the weather conditions, reads the image information and stores the image data in the cloud database. On the other hand, the search module reads the location information of the onboard sensors, searches for images with the similar location from the cloud database using the proposed optimal image selection algorithm. The search module then provides multiple photos with similar geographic information, selects the best image, and provides feedback suggestions. In addition to the software development, we also implement the proposed system on a hardware device that can directly retrieve the outdoor images for display and storage. The experiments with real scenes have demonstrated the feasibility of the proposed system.},
booktitle = {Advanced Concepts for Intelligent Vision Systems: 21st International Conference, ACIVS 2023 Kumamoto, Japan, August 21–23, 2023 Proceedings},
pages = {250–261},
numpages = {12},
keywords = {Image Acquisition, Image Retrieval, Color Aesthetics},
location = {<conf-loc content-type="InPerson">Kumamoto, Japan</conf-loc>}
}

@article{10.1016/j.compbiomed.2023.107336,
author = {Wang, Junjie and Quan, Hao and Wang, Chengguang and Yang, Genke},
title = {Pyramid-based self-supervised learning for histopathological image classification},
year = {2023},
issue_date = {Oct 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {165},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2023.107336},
doi = {10.1016/j.compbiomed.2023.107336},
journal = {Comput. Biol. Med.},
month = {oct},
numpages = {10},
keywords = {Self-supervised learning, Pyramid-based transformer, Histopathological image}
}

@inproceedings{10.1145/3576915.3623157,
author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
title = {Do Users Write More Insecure Code with AI Assistants?},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623157},
doi = {10.1145/3576915.3623157},
abstract = {AI code assistants have emerged as powerful tools that can aid in the software development life-cycle and can improve developer productivity. Unfortunately, such assistants have also been found to produce insecure code in lab environments, raising significant concerns about their usage in practice. In this paper, we conduct a user study to examine how users interact with AI code assistants to solve a variety of security related tasks. Overall, we find that participants who had access to an AI assistant wrote significantly less secure code than those without access to an assistant. Participants with access to an AI assistant were also more likely to believe they wrote secure code, suggesting that such tools may lead users to be overconfident about security flaws in their code. To better inform the design of future AI-based code assistants, we release our user-study apparatus to researchers seeking to build on our work.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2785–2799},
numpages = {15},
keywords = {language models, machine learning, programming assistants, usable security},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@proceedings{10.1145/3605157,
title = {FUZZING 2023: Proceedings of the 2nd International Fuzzing Workshop},
year = {2023},
isbn = {9798400702471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 2nd International Workshop on Fuzzing (FUZZING 2023), co-located with ISSTA in Seattle, Washington, USA on 17 July 2023. This workshop is the continuation of last year's successful inauguration workshop that introduced a preregistration-based publication process to our community. Similar to last year, this workshop hosts the presentations of the accepted drafts of the registered reports that were accepted as part of the first stage in a two-stage publication process. In the first stage, the program committee (PC) evaluates all submissions based on: (i) the significance and novelty of the hypotheses or techniques and 
(ii) the soundness and reproducibility of the methodology specified to validate the claims or hypotheses -- but explicitly not based on the strength of the (preliminary) results. These draft registered reports are presented and improved at the FUZZING 2023 workshop in Seattle.},
location = {Seattle, WA, USA}
}

@inproceedings{10.5555/3631672.3631694,
author = {Rosa, Diego Moreira da and Gnecco, Andrea and Silveira, Milene and Mattjie, Christian and Barros, Rodrigo C. and Apuzzo, Sofia and Pinho, Marcio Sarroglia and Manssour, Isabel},
title = {A Framework for Visualizing HCI Pattern Languages through Network Diagrams},
year = {2023},
isbn = {9781941652183},
publisher = {The Hillside Group},
address = {USA},
abstract = {Defined in the late 1970s by architect Christopher Alexander, the concepts of patterns and pattern language have been adopted by specialists in several areas, including Software Engineering and Human-Computer Interaction (HCI). The hierarchical structure of pattern languages predicted by Alexander favors their representation through network diagrams, a common practice among HCI researchers. Despite the advantages of using graphs to elicit pattern interrelationships within a language, it was not possible to find a structured and documented method for visualizing pattern languages in modern network diagram tools. In this work, we present a framework that describes the steps for the visualization of HCI pattern languages through network diagrams. As part of the framework, a tool capable of converting pattern description XML files into a graph description file ready to be interpreted by feature-rich visualization applications was developed. The framework was tested with data from Welie.com, an online library containing 132 interaction patterns, and the resulting graph was visualized in two well-known applications for designing network structures (Gephi and InfraNodus). The results indicate that the proposed solution is a viable strategy for the visualization of pattern languages.},
booktitle = {Proceedings of the 29th Conference on Pattern Languages of Programs},
articleno = {18},
numpages = {11},
keywords = {HCI, graph, network diagram, pattern language, patterns, visualization},
location = {Virtual Event},
series = {PLoP '22}
}

@inproceedings{10.5555/3615924.3615948,
author = {Hausi A., Müller and Marin, Litoiu and Luis F., Rivera and Mohammadreza, Rasolroveicy and Norha M., Villegas and Gabriel, Tamura and Ian, Watts and Eric, Erpenbach and Laura, Shwartz},
title = {Proactive Continuous Operations using Large Language Models (LLMs) and AIOps},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Advances and synergies between AI and Cloud computing are driv-ing increased levels of automation and autonomy in the strenuous operation of modern and complex IT systems and environments (IT-Sys|Envs). This notion, often termed as AIOps, is transforming the landscape of IT-Sys|Env operations by a providing a framework to augment DevOps teams’ capabilities to cope with inevitable complexities and uncertainties. Through operational optimization and rapid response and risk mitigation mechanisms, AIOps sup-presses factors that might threaten service-level agreement com-pliance or value delivery. A pivotal aspect of this transformation, and its promising future, lies in the integration of AIOps with other groundbreaking technologies such as Large Language Models (LLMs) and Digital Twins (DTs), especially in the context of Cloud environments. The 1st CASCON Workshop on Proactive Continuous Operations Using LLMs and AIOps provided a dynamic platform for researchers and industry practitioners to exchange insights, advancements, and ideas in AI, Cloud, and automation.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {198–199},
numpages = {2},
keywords = {aiops, large language models, digital twins, reference models, run-},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3580305.3599557,
author = {Kenthapadi, Krishnaram and Lakkaraju, Himabindu and Rajani, Nazneen},
title = {Generative AI meets Responsible AI: Practical Challenges and Opportunities},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599557},
doi = {10.1145/3580305.3599557},
abstract = {Generative AI models and applications are being rapidly developed and deployed across a wide spectrum of industries and applications ranging from writing and email assistants to graphic design and art generation to educational assistants to coding to drug discovery. However, there are several ethical and social considerations associated with generative AI models and applications. These concerns include lack of interpretability, bias and discrimination, privacy, lack of model robustness, fake and misleading content, copyright implications, plagiarism, and environmental impact associated with training and inference of generative AI models.In this tutorial, we first motivate the need for adopting responsible AI principles when developing and deploying large language models (LLMs) and other generative AI models, as part of a broader AI model governance and responsible AI framework, from societal, legal, user, and model developer perspectives, and provide a roadmap for thinking about responsible AI for generative AI in practice. We provide a brief technical overview of text and image generation models, and highlight the key responsible AI desiderata associated with these models. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We focus on real-world generative AI use cases spanning domains such as media generation, writing assistants, copywriting, code generation, and conversational assistants, present practical solution approaches / guidelines for applying responsible AI techniques effectively, discuss lessons learned from deploying responsible AI approaches for generative AI applications in practice, and highlight the key open research problems. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on responsible AI in the context of generative AI, and pave the way for building more reliable and trustworthy generative AI applications in the future.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5805–5806},
numpages = {2},
keywords = {case studies from industry, ethics in ai, generative ai models and applications, large language models, responsible ai},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@article{10.1016/j.infsof.2023.107188,
author = {Li, Xun and Liu, Lei and Chen, Zhiqi and Liu, Yuzhou and Liu, Huaxiao},
title = {Describing the APIs comprehensively: Obtaining the holistic representations from multiple modalities data for different tasks},
year = {2023},
issue_date = {Jun 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {158},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107188},
doi = {10.1016/j.infsof.2023.107188},
journal = {Inf. Softw. Technol.},
month = {jun},
numpages = {14},
keywords = {API representation, Multimodal fusion, Code and text mining}
}

@inproceedings{10.5555/3615924.3623635,
author = {Paula, Branco and Nuno, Moniz and Guy-Vincent, Jourdan},
title = {First Workshop on Machine Learning Challenges in Cybersecurity},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cybersecurity events severely impact many individuals, infras-tructures, businesses, and institutions. Machine learning has been playing a key role in many aspects of our daily life and in particular as a cyber defense mechanism. However, the nature of cyber threats poses many unique challenges to machine learning models. The first workshop on Machine Learning Challenges in Cybersecurity held at the CASCON conference focused on the key challenges, recent developments, and open research issues of machine learning in cybersecurity. Two applications of deep learning to phishing and face verification systems are analyzed. Finally, an invited talk from Dr. Shirani covered a machine learning based solution for insider threat detection.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {248–250},
numpages = {3},
keywords = {Cybersecurity, Machine Learning, Deep Learning, Imbalanced Data, Novelty Detection},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.5555/3615924.3615944,
author = {Jingfeng, Pan and Yunfei, Peng and Kaiyu, Li and Aijun, An and Xiaohui, Yu and Dariusz, Jania},
title = {Optimizing Data Migration Using Online Clustering},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Data migration refers to the transfer of data from one location to another, for instance, from a local database to a cloud server or from one cloud to another. To minimize business disruption during this process, it is essential to ensure that data migration has a high throughput. However, current methods only directly compress data into smaller files and transfer them over the network without exploiting data distribution to increase the compression ratio further, ultimately resulting in low overall throughput. In this paper, we present a three-step approach to improve the data migration throughput for relational databases. The proposed approach involves clustering the records into groups, compressing each group, and transmitting the compressed files via the network. By clustering similar records together, the compression ratio within each group is increased, resulting in overall higher compression and lower network transmission time. If the used clustering time is less than the reduced network transmission time, the clustering is worthwhile for the data migration task. We propose to use an online 𝑘-prototype clustering method and a workload-balancing strategy. The experiments conducted on benchmark datasets reveal that our proposed method attains a 4% enhancement in compression ratio and over 3% improvement in throughput as compared to the baseline approach on average},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {173–178},
numpages = {6},
keywords = {Data Migration, Data Compression, Online Clustering},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3593013.3594079,
author = {Lucaj, Laura and van der Smagt, Patrick and Benbouzid, Djalel},
title = {AI Regulation Is (not) All You Need},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594079},
doi = {10.1145/3593013.3594079},
abstract = {The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1267–1279},
numpages = {13},
keywords = {AI regulation, algorithmic auditing, ethical AI, machine learning},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3584871.3584874,
author = {Du, Kai and Lu, Guoming and Qin, Ke},
title = {An Extractive&nbsp;Text&nbsp;Summarization Based on Reinforcement Learning},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584874},
doi = {10.1145/3584871.3584874},
abstract = {Abstract: In recent years, with the rapid development of network information technology, network text information also presents an explosive growth trend. As an efficient information processing technology in the digital age, text summarization can bring the advantage of focusing on key information in all directions in massive text information. However, text summarization is still faced with some problems such as difficulty in extracting long text and information redundancy. Therefore, combining with the deep learning framework, this paper proposes an extractive text summarization that uses reinforcement learning to optimize the long text extraction process and uses the attention mechanism to achieve the effect of redundancy removal. On CNN/Daily Mail datasets, the automatic evaluation shows that our model outperforms the previous on ROUGE, and the ablation experiment proves the effectiveness of the de-redundant attention module.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {19–25},
numpages = {7},
keywords = {Deep learning, Reinforcement learning, Text summarization},
location = {<conf-loc>, <city>Palmerston North</city>, <country>New Zealand</country>, </conf-loc>},
series = {ICSIM '23}
}

@inproceedings{10.1145/3617555.3617871,
author = {Karakas, Umutcan and Tosun, Ayse},
title = {Automated Fairness Testing with Representative Sampling},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617871},
doi = {10.1145/3617555.3617871},
abstract = {The issue of fairness testing in machine learning models has become popular due to rising concerns about potential bias and discrimination, as these models continue to permeate end-user applications. However, achieving an accurate and reliable measurement of the fairness performance of machine learning models remains a substantial challenge. Representative sampling plays a pivotal role in ensuring accurate fairness assessments and providing insight into the underlying dynamics of data, unlike biased or random sampling approaches. In our study, we introduce our approach, namely RSFair, which adopts the representative sampling method to comprehensively evaluate the fairness performance of a trained machine learning model. Our research findings on two datasets indicate that RSFair yields more accurate and reliable results, thus improving the efficiency of subsequent search steps, and ultimately the fairness performance of the model. With the usage of Orthogonal Matching Pursuit (OMP) and K-Singular Value Decomposition (K-SVD) algorithms for representative sampling, RSFair significantly improves the detection of discriminatory inputs by 76% and the fairness performance by 53% compared to other search-based approaches in the literature.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {54–63},
numpages = {10},
keywords = {fairness testing, machine learning, representative sampling},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {PROMISE 2023}
}

@inproceedings{10.1145/3617555.3617875,
author = {Bhandari, Pragya and Rodríguez-Pérez, Gema},
title = {BuggIn: Automatic Intrinsic Bugs Classification Model using NLP and ML},
year = {2023},
isbn = {9798400703751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617555.3617875},
doi = {10.1145/3617555.3617875},
abstract = {Recent studies have shown that bugs can be categorized into in-  
trinsic and extrinsic types. Intrinsic bugs can be backtracked to  
specific changes in the version control system (VCS), while extrin-  
sic bugs originate from external changes to the VCS and lack a  
direct bug-inducing change. Using only intrinsic bugs to train bug  
prediction models has been reported as beneficial to improve the  
performance of such models. However, there is currently no auto-  
mated approach to identify intrinsic bugs. To bridge this gap, our  
study employs Natural Language Processing (NLP) techniques to  
automatically identify intrinsic bugs. Specifically, we utilize two  
embedding techniques, seBERT and TF-IDF, applied to the title and  
description text of bug reports. The resulting embeddings are fed  
into well-established machine learning algorithms such as Support  
Vector Machine, Logistic Regression, Decision Tree, Random Forest,  
and K-Nearest Neighbors. The primary objective of this paper is  
to assess the performance of various NLP and machine learning  
techniques in identifying intrinsic bugs using the textual informa-  
tion extracted from bug reports. The results demonstrate that both  
seBERT and TF-IDF can be effectively utilized for intrinsic bug  
identification. The highest performance scores were achieved by  
combining TF-IDF with the Decision Tree algorithm and utilizing  
the bug titles (yielding an F1 score of 78%). This was closely fol-  
lowed by seBERT, Support Vector Machine, and bug titles (with an  
F1 score of 77%). In summary, this paper introduces an innovative  
approach that automates the identification of intrinsic bugs using  
textual information derived from bug reports.},
booktitle = {Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {classification, extrinsic bugs, intrinsic bugs, natural language processing, software bugs},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {PROMISE 2023}
}

@article{10.1016/j.infsof.2023.107190,
author = {Yu, Jiaojiao and Zhou, Xu and Liu, Xiao and Liu, Jin and Xie, Zhiwen and Zhao, Kunsong},
title = {Detecting multi-type self-admitted technical debt with generative adversarial network-based neural networks},
year = {2023},
issue_date = {Jun 2023},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {158},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107190},
doi = {10.1016/j.infsof.2023.107190},
journal = {Inf. Softw. Technol.},
month = {jun},
numpages = {15},
keywords = {Technical debt, SATD, Generative adversarial network, CodeBERT, Multi-head attention}
}

@inproceedings{10.1007/978-3-031-49266-2_20,
author = {Duszkiewicz, Aleksander G. and Sørensen, Jacob G. and Johansen, Niclas and Edison, Henry and Rocha Silva, Thiago},
title = {Leveraging Historical Data to&nbsp;Support User Story Estimation},
year = {2023},
isbn = {978-3-031-49265-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49266-2_20},
doi = {10.1007/978-3-031-49266-2_20},
abstract = {Accurate and reliable effort and cost estimation are still challenging for agile teams in the industry. It is argued that leveraging historical data regarding the actual time spent on similar past projects could be very helpful to support such an activity before companies embark upon a new project. In this paper, we investigate to what extent user story information retrieved from past projects can help developers estimate the effort needed to develop new similar projects. In close collaboration with a software development company, we applied design science and action research principles to develop and evaluate a tool that employs Natural Language Processing (NLP) algorithms to find past similar user stories and retrieve the actual time spent on them. The tool was then used to estimate a real project that was about to start in the company. A focus group with a team of six developers was conducted to evaluate the tool’s efficacy in estimating similar projects. The results of the focus group with the developers revealed that the tool has the potential to complement the existing estimation process and help different interested parties in the company. Our results contribute both towards a new tool-supported approach to help user story estimation based on historical data and with our lessons learned on why, when, and where such a tool and the estimations provided may play a role in agile projects in the industry.},
booktitle = {Product-Focused Software Process Improvement: 24th International Conference, PROFES 2023, Dornbirn, Austria, December 10–13, 2023, Proceedings, Part I},
pages = {284–300},
numpages = {17},
keywords = {User Stories, Agile Estimation, Natural Language Processing},
location = {<conf-loc content-type="InPerson">Dornbirn, Austria</conf-loc>}
}

@article{10.1007/s11042-023-15626-0,
author = {Shang, Ziyang and Wang, Penghai and Li, Xinfu},
title = {Micro-expression recognition based on differential feature fusion},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {83},
number = {4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-023-15626-0},
doi = {10.1007/s11042-023-15626-0},
abstract = {Micro-expressions (MEs) are natural facial mechanisms with short duration and subtle changes. It has attracted much attention in the real world due to its accuracy and uncontrollability of mental expression. With the development of computer vision, micro-expression Recognition (MER) methods have been continuously proposed and improved by scholars. However, the existing MER methods still have some deficiencies in processing Spatio-temporal redundant information and feature extraction. This paper proposes an MER network based on Differential Feature Fusion (DFF) method to solve this problem. First, inputs the onset frame and apex frame of the face, divide each image into small blocks, and uses part of the SE-ResNet50 model for feature extraction. Second, the Spatio-Temporal information of the features is extracted by using a DFF module composed of a differential feature module, CapsuleNet, and a Fully Connected (FC) layer. Finally, inputs the feature vector to the FC module for classification. This study is based on the Leave One Subject Out (LOSO) cross-validation protocol and uses the CASMEII dataset. Experiments and comparisons show the effectiveness of the algorithm.},
journal = {Multimedia Tools Appl.},
month = {jun},
pages = {11111–11126},
numpages = {16},
keywords = {Micro-expression recognition, CapsuleNet, SE-ResNet50, Deep learning feature, Differential feature fusion}
}

@inproceedings{10.1145/3587102.3588829,
author = {Russell, Seán and Caton, Simon and Becker, Brett A.},
title = {Online Programming Exams - An Experience Report},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588829},
doi = {10.1145/3587102.3588829},
abstract = {When seeking to maximise the authenticity of assessment in programming courses it makes sense to provide students with practical programming problems to solve in an environment that is close to real software development practice, i.e., online, open book, and using their typical development environment. This creates an assessment environment that should afford students sufficient opportunities to evidence what they have learned, but also creates practical challenges in terms of academic integrity, flexibility in the automated grading process, and assumptions surrounding how the student may attempt to solve the problems both in terms of correct and incorrect solutions. In this experience report, we outline two independently observed cohorts of students sitting the same Java programming exam, with different weights, over three years. This is undertaken as a reflective exercise in order to derive a series of recommendations and retrospectively obvious pitfalls to act as guidance for educators considering online programming exams for large (i.e. n &gt; 150) introductory programming courses. After discussing our assessment methodology, we provide 4 high-level observations and centre a set of recommendations around these to aid practitioners in their assessment design.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {436–442},
numpages = {7},
keywords = {authentic assessment, plagiarism, programming, video},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@inproceedings{10.1007/978-3-031-49266-2_4,
author = {Nguyen-Duc, Anh and Cruzes, Daniela Soares and Aalvik, Hege and Iovan, Monica},
title = {Facilitating Security Champions in&nbsp;Software Projects - An Experience Report from&nbsp;Visma},
year = {2023},
isbn = {978-3-031-49265-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49266-2_4},
doi = {10.1007/978-3-031-49266-2_4},
abstract = {The role of security practices is increasingly recognized in fast-paced software development paradigms in contributing to overall software security. Security champions have emerged as a promising role in addressing the shortage of explicit security activities within software teams. Despite the growing awareness of general security practices, there remains limited knowledge regarding security champions, including their establishment, effectiveness, challenges, and best practices. This paper aims to bridge this gap by presenting insights from a survey of 73 security champions and 11 interviews conducted within a large Norwegian software house. Through this study, we explore the diverse activities undertaken by security champions, highlighting notable differences in motivations and task descriptions between voluntary and assigned champions. We also reported challenges with onboarding, communication, and training security champions and how they can be better supported in the organization. Our insight can be relevant for similar software houses in establishing, implementing, and improving their strategic security programs.},
booktitle = {Product-Focused Software Process Improvement: 24th International Conference, PROFES 2023, Dornbirn, Austria, December 10–13, 2023, Proceedings, Part I},
pages = {57–72},
numpages = {16},
keywords = {Security Champion, Agile, Experience Report, Security Training},
location = {<conf-loc content-type="InPerson">Dornbirn, Austria</conf-loc>}
}

@inproceedings{10.1145/3576915.3623174,
author = {Fischer, Felix and Höbenreich, Jonas and Grossklags, Jens},
title = {The Effectiveness of Security Interventions on GitHub},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623174},
doi = {10.1145/3576915.3623174},
abstract = {In 2017, GitHub was the first online open source platform to show security alerts to its users. It has since introduced further security interventions to help developers improve the security of their open source software. In this study, we investigate and compare the effects of these interventions. This offers a valuable empirical perspective on security interventions in the context of software development, enriching the predominantly qualitative and survey-based literature landscape with substantial data-driven insights. We conduct a time series analysis on security-altering commits covering the entire history of a large-scale sample of over 50,000 GitHub repositories to infer the causal effects of the security alert, security update, and code scanning interventions. Our analysis shows that while all of GitHub's security interventions have a significant positive effect on security, they differ greatly in their effect size. By comparing the design of each intervention, we identify the building blocks that worked well and those that did not. We also provide recommendations on how practitioners can improve the design of their interventions to enhance their effectiveness.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2426–2440},
numpages = {15},
keywords = {code security, security warnings, usable security},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@proceedings{10.1145/3635800,
title = {PEPM 2024: Proceedings of the 2024 ACM SIGPLAN International Workshop on Partial Evaluation and Program Manipulation},
year = {2024},
isbn = {9798400704871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the proceedings of the 2024 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation (PEPM 2024), held January 16th, 2024 in London, in affiliation with the annual Symposium on Principles of Programming Languages (POPL 2024).},
location = {<conf-loc>, <city>London</city>, <country>UK</country>, </conf-loc>}
}

@article{10.1145/3639830.3639841,
author = {Huang, Tianhao and Zhu, Xiaozhi and Niu, Mo},
title = {An End-to-End Benchmarking Tool for Analyzing the Hardware-Software Implications of Multi-modal DNNs},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0163-5999},
url = {https://doi.org/10.1145/3639830.3639841},
doi = {10.1145/3639830.3639841},
abstract = {Abstract-Multi-modal deep neural networks (DNNs) have become increasingly pervasive in many machine learning application domains due to their superior accuracy by fusing various modalities together. However, multi-modal DNNs present many unique characteristics such as multi-stage execution, frequent synchronization and high heterogeneity, which are not well understood in the system and architecture community. In this article, we first present and characterize a set of multi-modal DNN workloads of different sizes from five domains and measure metrics like accuracy to ensure the availability of these applications from the algorithm perspective. We then explore their important hardwaresoftware implications from system and architecture aspects by conducting an in-depth analysis on the unique hardware-software characteristics of multimodal DNNs. We hope that our work can help guide future hardware-software design and optimization for efficient inference of multi-modal DNN applications on both cloud and edge computing platforms.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {jan},
pages = {25–27},
numpages = {3}
}

@article{10.1007/s10514-023-10136-2,
author = {Yoshikawa, Naruki and Skreta, Marta and Darvish, Kourosh and Arellano-Rubach, Sebastian and Ji, Zhi and Bjørn&nbsp;Kristensen, Lasse and Li, Andrew Zou and Zhao, Yuchi and Xu, Haoping and Kuramshin, Artur and Aspuru-Guzik, Alán and Shkurti, Florian and Garg, Animesh},
title = {Large language models for chemistry robotics},
year = {2023},
issue_date = {Dec 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {8},
issn = {0929-5593},
url = {https://doi.org/10.1007/s10514-023-10136-2},
doi = {10.1007/s10514-023-10136-2},
abstract = {This paper proposes an approach to automate chemistry experiments using robots by translating natural language instructions into robot-executable plans, using large language models together with task and motion planning. Adding natural language interfaces to autonomous chemistry experiment systems lowers the barrier to using complicated robotics systems and increases utility for non-expert users, but translating natural language experiment descriptions from users into low-level robotics languages is nontrivial. Furthermore, while recent advances have used large language models to generate task plans, reliably executing those plans in the real world by an embodied agent remains challenging. To enable autonomous chemistry experiments and alleviate the workload of chemists, robots must interpret natural language commands, perceive the workspace, autonomously plan multi-step actions and motions, consider safety precautions, and interact with various laboratory equipment. Our approach, CLAIRify, combines automatic iterative prompting with program verification to ensure syntactically valid programs in a data-scarce domain-specific language that incorporates environmental constraints. The generated plan is executed through solving a constrained task and motion planning problem using PDDLStream solvers to prevent spillages of liquids as well as collisions in chemistry labs. We demonstrate the effectiveness of our approach in planning chemistry experiments, with plans successfully executed on a real robot using a repertoire of robot skills and lab tools. Specifically, we showcase the utility of our framework in pouring skills for various materials and two fundamental chemical experiments for materials synthesis: solubility and recrystallization. Further details about CLAIRify can be found at .},
journal = {Auton. Robots},
month = {oct},
pages = {1057–1086},
numpages = {30},
keywords = {Large language models, Constrained task and motion planning, Plan generation verification, Self-driving labs, Chemistry lab automation}
}

@article{10.1145/3610220,
author = {Cheng, Kathy and Cuvin, Phil and Olechowski, Alison and Zhou, Shurui},
title = {User Perspectives on Branching in Computer-Aided Design},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610220},
doi = {10.1145/3610220},
abstract = {Branching is a feature of distributed version control systems that facilitates the "divide and conquer" strategy present in complex and collaborative work domains. Branching has revolutionized modern software development and has the potential to similarly transform hardware product development via CAD (computer-aided design). Yet, contrasting with its status in software, branching as a feature of commercial CAD systems is in its infancy, and little research exists to investigate its use in the digital design and development of physical products. To address this knowledge gap, in this paper, we mine and analyze 719 user-generated posts from online CAD forums to qualitatively study designers' intentions for and preliminary use of branching in CAD. Our work contributes a taxonomy of CAD branching use cases, an identification of deficiencies of existing branching capabilities in CAD, and a discussion of the untapped potential of CAD branching to support a new paradigm of collaborative mechanical design. The insights gained from this study may help CAD tool developers address design shortcomings in CAD branching tools and assist CAD practitioners by raising their awareness of CAD branching to improve design efficiency and collaborative workflows in hardware development teams.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {371},
numpages = {30},
keywords = {computer-aided design, hardware design and development, product data management, software configuration management, version control}
}

@article{10.1145/3610406,
author = {Philbin, Carrie Anne},
title = {Exploring the Potential of Artificial Intelligence Program Generators in Computer Programming Education for Students},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2153-2184},
url = {https://doi.org/10.1145/3610406},
doi = {10.1145/3610406},
journal = {ACM Inroads},
month = {aug},
pages = {30–38},
numpages = {9}
}

@article{10.1016/j.engappai.2023.106485,
author = {Kostić, Marija and Batanović, Vuk and Nikolić, Boško},
title = {Monolingual, multilingual and cross-lingual code comment classification},
year = {2023},
issue_date = {Sep 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {124},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106485},
doi = {10.1016/j.engappai.2023.106485},
journal = {Eng. Appl. Artif. Intell.},
month = {sep},
numpages = {17},
keywords = {Code comments, Code comment classification, Pre-trained language models, Transformers, Annotated dataset}
}

@inproceedings{10.1007/978-3-031-43513-3_11,
author = {Shminke, Boris},
title = {gym-saturation: Gymnasium Environments for&nbsp;Saturation Provers (System description)},
year = {2023},
isbn = {978-3-031-43512-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43513-3_11},
doi = {10.1007/978-3-031-43513-3_11},
abstract = {This work describes a new version of a previously published Python package — gym-saturation: a collection of OpenAI Gym environments for guiding saturation-style provers based on the given clause algorithm with reinforcement learning. We contribute usage examples with two different provers: Vampire and iProver. We also have decoupled the proof state representation from reinforcement learning per se and provided examples of using a known ast2vec Python code embedding model as a first-order logic representation. In addition, we demonstrate how environment wrappers can transform a prover into a problem similar to a multi-armed bandit. We applied two reinforcement learning algorithms (Thompson sampling and Proximal policy optimisation) implemented in Ray RLlib to show the ease of experimentation with the new release of our package.},
booktitle = {Automated Reasoning with Analytic Tableaux and Related Methods: 32nd International Conference, TABLEAUX 2023, Prague, Czech Republic, September 18–21, 2023, Proceedings},
pages = {187–199},
numpages = {13},
keywords = {Automated theorem proving, Reinforcement learning, Saturation-style proving, Machine learning},
location = {Prague, Czech Republic}
}

@inproceedings{10.1145/3638067.3638109,
author = {Nakamura, Walter T. and C. de Oliveira, Edson César and H. T. de Oliveira, Elaine and Conte, Tayana},
title = {UX-MAPPER: A User eXperience Method to Analyze App Store Reviews},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638109},
doi = {10.1145/3638067.3638109},
abstract = {The mobile app market has grown over the last decades. With the rise of app stores, users can easily choose an app from thousands, making them less tolerant of low-quality apps. More than ever, users are looking for apps that provide not only valuable functionalities but pleasurable experiences. Hence, User eXperience (UX) became the differential to stand out from competitors. By understanding what factors affect UX, practitioners could focus on factors that lead to positive UX while mitigating those that affect UX negatively. In this context, reviews from app stores emerged as a valuable source of information to investigate such factors. However, analyzing millions of reviews is costly and time-consuming. This paper presents UX-MAPPER, an approach to analyzing app store reviews and supporting practitioners in identifying factors affecting UX. We applied the Design Science Research method to design UX-MAPPER iteratively and grounded on a solid theoretical background. We performed exploratory studies to investigate the problem, a systematic mapping study to identify factors that affect UX, and an empirical study with 14 participants with experience in requirements engineering to determine the relevance and acceptance of our proposal from practitioners’ perspectives. The participants considered it useful to improve the quality of existing apps and explore the reviews of competing apps to identify functionalities and features that users are requesting, liking, or hating. They were also willing to use it when it became available, highlighting our proposal’s usefulness and relevance in software development.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {44},
numpages = {11},
keywords = {app stores, machine learning, user experience, user reviews},
location = {<conf-loc>, <city>Maceió</city>, <country>Brazil</country>, </conf-loc>},
series = {IHC '23}
}

@article{10.1145/3636341.3636351,
author = {Bauer, Christine and Carterette, Ben and Ferro, Nicola and Fuhr, Norbert and Beel, Joeran and Breuer, Timo and Clarke, Charles L. A. and Crescenzi, Anita and Demartini, Gianluca and Di Nunzio, Giorgio Maria and Dietz, Laura and Faggioli, Guglielmo and Ferwerda, Bruce and Fröbe, Maik and Hagen, Matthias and Hanbury, Allan and Hauff, Claudia and Jannach, Dietmar and Kando, Noriko and Kanoulas, Evangelos and Knijnenburg, Bart P. and Kruschwitz, Udo and Li, Meijie and Maistro, Maria and Michiels, Lien and Papenmeier, Andrea and Potthast, Martin and Rosso, Paolo and Said, Alan and Schaer, Philipp and Seifert, Christin and Spina, Damiano and Stein, Benno and Tintarev, Nava and Urbano, Julián and Wachsmuth, Henning and Willemsen, Martijn C. and Zobel, Justin},
title = {Report on the Dagstuhl Seminar on Frontiers of Information Access Experimentation for Research and Education},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3636341.3636351},
doi = {10.1145/3636341.3636351},
abstract = {This report documents the program and the outcomes of Dagstuhl Seminar 23031 "Frontiers of Information Access Experimentation for Research and Education", which brought together 38 participants from 12 countries. The seminar addressed technology-enhanced information access (information retrieval, recommender systems, natural language processing) and specifically focused on developing more responsible experimental practices leading to more valid results, both for research as well as for scientific education.The seminar featured a series of long and short talks delivered by participants, who helped in setting a common ground and in letting emerge topics of interest to be explored as the main output of the seminar. This led to the definition of five groups which investigated challenges, opportunities, and next steps in the following areas: reality check, i.e. conducting real-world studies, human-machine-collaborative relevance judgment frameworks, overcoming methodological challenges in information retrieval and recommender systems through awareness and education, results-blind reviewing, and guidance for authors.Date: 15--20 January 2023.Website: https://www.dagstuhl.de/23031.},
journal = {SIGIR Forum},
month = {dec},
articleno = {7},
numpages = {28}
}

@inproceedings{10.1145/3617573.3618029,
author = {Westad, Frank and Lodgaard, Lars and Pedersen, Torbjørn},
title = {Data Pre-processing and Sensor-Fusion for Multivariate Statistical Process Control of an Extrusion Process},
year = {2023},
isbn = {9798400703782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617573.3618029},
doi = {10.1145/3617573.3618029},
abstract = {In most manufacturing processes, data related to a product are collected across several process steps. Ensuring good data quality is essential for subsequent process modeling, monitoring, and control. Although data for a given process might already be available in digitized form in the process control systems or industrial databases, it is in most cases not so that the data can directly be used in its original form for process modeling. Pre-processing is often needed before modeling, which may include operations such as time alignment by handling different sampling frequencies and lag time, handling of missing values, and detection of sample outliers. Specific considerations must be made for processes with both continuous and batch process steps due to different data structures. This paper describes an industrial use case for extrusion monitoring starting from structured raw data and ending up with real-time multivariate statistical process control (MSPC) applying a sensor-fusion approach and feature extraction. The MSPC also enables in-depth analysis for identifying process variables in the case of samples lying outside of the normal operating conditions (NOC).},
booktitle = {Proceedings of the 3rd International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things},
pages = {9–15},
numpages = {7},
keywords = {Data pre-processing, MSPC, PCA, outlier detection, sensor fusion, time alignment},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {SEA4DQ 2023}
}

@inproceedings{10.1145/3610548.3618219,
author = {Kodnongbua, Milin and Jones, Benjamin and Ahmad, Maaz Bin Safeer and Kim, Vladimir and Schulz, Adriana},
title = {ReparamCAD: Zero-shot CAD Re-Parameterization for Interactive Manipulation},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610548.3618219},
doi = {10.1145/3610548.3618219},
abstract = {Parametric CAD models encode entire families of shapes that should, in principle, be easy for designers to explore. However, in practice, parametric CAD models can be difficult to manipulate due to implicit semantic constraints among parameter values. Finding and enforcing these semantic constraints solely from geometry or programmatic shape representations is not possible because these constraints ultimately reflect design intent. They are informed by the designer’s experience and semantics in the real world. To address this challenge, we introduce ReparamCAD, a zero-shot pipeline that leverages pre-trained large language and image model to infer meaningful space of variations for a shape We then re-parameterize a new constrained parametric CAD program that captures these variations, enabling effortless exploration of the design space along meaningful design axes. We evaluated our approach through five examples and a user study. The result showed that the inferred spaces are meaningful and comparable to those defined by experts. Code and data are at: https://github.com/milmillin/ReparamCAD.},
booktitle = {SIGGRAPH Asia 2023 Conference Papers},
articleno = {69},
numpages = {12},
keywords = {parametric modeling, program synthesis},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {SA '23}
}

@proceedings{10.1145/3600006,
title = {SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
location = {Koblenz, Germany}
}

@article{10.1016/j.jss.2023.111836,
author = {Aldndni, Waad and Meng, Na and Servant, Francisco},
title = {Automatic prediction of developers’ resolutions for software merge conflicts},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {206},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111836},
doi = {10.1016/j.jss.2023.111836},
journal = {J. Syst. Softw.},
month = {dec},
numpages = {16},
keywords = {Software merge, Textual conflicts, Conflict resolution, Prediction}
}

@article{10.1016/j.knosys.2023.110866,
author = {Sovrano, Francesco and Vitali, Fabio},
title = {An objective metric for Explainable AI: How and why to estimate the degree of explainability},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {278},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2023.110866},
doi = {10.1016/j.knosys.2023.110866},
journal = {Know.-Based Syst.},
month = {oct},
numpages = {23},
keywords = {Degree of explainability, Objective explainability metric, Explainable AI, Theory of explanations}
}

@proceedings{10.1145/3605770,
title = {SCORED '23: Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '23, the second edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Copenhagen, Denmark with extensive support for in-person and virtual attendance.This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>}
}

@proceedings{10.1145/3580305,
title = {KDD '23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining KDD 2023. This year's conference continues its tradition of being the premier forum for presentation of research results and experience reports on leading edge issues of knowledge discovery, data science, and machine learning. The mission of the conference is to provide the premier forum for advancement, education, and adoption of the "science" of knowledge discovery and machine learning from all types of data; to share novel methodologies that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. These ideas have the potential to shape and impact our society and environment, and are becoming particularly important with the emergence of AI in all fields. KDD provides researchers and practitioners a unique opportunity to share their perspectives with others interested in various aspects of data science and machine learning.KDD '23 has a program of three keynotes, 313 research track papers, 184 ADS (Applied Data Science) track papers, 34 workshops, 33 tutorials, nine special days, three panels, and eight ADS invited talks. For the first time, we switched to OpenReview with the mission to further improve the review quality and facilitate the interaction between reviewers and authors. We have introduced several new special days, such as Large Language Model (LLM) Day, Finance Day, AI for Open Society Day, Entertainment, Sports, and Media (ESM) Day, Southern California Data Science; and several new panels, such as AI for Science and LLMs for education &amp; research. The rise of LLMs has been historic and the nature of creativity itself may change. With this in mind, we have emphasized LLMs in our keynotes, special days, and panels. Only time will tell whether we went too far or not far enough!},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>}
}

@article{10.1016/j.csi.2023.103744,
author = {Alvarez-Rodríguez, Jose María and Mendieta, Roy and Cibrián, Eduardo and Llorens, Juan},
title = {Towards a method to quantitatively measure toolchain interoperability in the engineering lifecycle: A case study of digital hardware design},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2023.103744},
doi = {10.1016/j.csi.2023.103744},
journal = {Comput. Stand. Interfaces},
month = {aug},
numpages = {12},
keywords = {Software tools, Software reusability, Web services, Software as a service, Internet}
}

@inproceedings{10.1007/978-3-031-44198-1_41,
author = {Zang, Shunan and Zhang, Chuang and Lin, Jingwen and Chen, Xiaojun and Zhang, Shuai},
title = {Lightweight Reference-Less Summary Quality Evaluation via&nbsp;Key Feature Extraction},
year = {2023},
isbn = {978-3-031-44197-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-44198-1_41},
doi = {10.1007/978-3-031-44198-1_41},
abstract = {One of the main problems with automatic text summarization is the lack of a “gold standard" for summary quality evaluation. ROUGE [9] is the most widely used evaluation metric for summary quality. However, its evaluation merely concentrates on reference summary and overlap features of sentences rather than focusing on more critical semantic features. Some other exiting methods have issues with improper noise handling and high cost. To solve these problems, we propose a lightweight reference-less summary quality evaluation method (SE-tiny), which evaluates the summary from two aspects: the summary’s self-quality and the degree of matching the features of the summary with the key features of the source text. Then, we optimize computational efficiency and space cost. Compared with existing methods, SE-tiny improves the quality of evaluation and reduces the cost. Besides, our method does not rely on reference summaries and can be generalized to evaluation on summarization datasets. For the goal of reproducibility, we make the SE-tiny project’s code and models available.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2023: 32nd International Conference on Artificial Neural Networks, Heraklion, Crete, Greece, September 26–29, 2023, Proceedings, Part VIII},
pages = {498–510},
numpages = {13},
keywords = {Automatic Text Summarization, Reference-Less Summarization Quality Evaluation, Summarization Datasets},
location = {<conf-loc content-type="InPerson">Heraklion, Greece</conf-loc>}
}

@article{10.1007/s11554-023-01331-6,
author = {Sun, Bangyong and Wang, Yu and Wu, Siyuan},
title = {An efficient lightweight CNN model for real-time fire smoke detection},
year = {2023},
issue_date = {Aug 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-023-01331-6},
doi = {10.1007/s11554-023-01331-6},
abstract = {Early fire and smoke detection with computer vision have attracted much attention in recent years, and a lot of fire detectors based on deep neural network have been proposed to improve the detection accuracy. However, most current fire detectors still suffer from low detection accuracy caused by the multi-scale variation of the fire and smoke, or the high false accept rate due to the fire-like or smoke-like objects within the background. In this paper, to address the above challenges, we propose an effective real-time fire detection network (AERNet) with two key functional modules, which achieves a good tradeoff between the detection accuracy and speed. First, we employ a lightweight backbone network Squeeze and Excitation-GhostNet (SE-GhostNet) to extract features, which can make it easier to distinguish the fire and smoke from the background and reduce the model parameters greatly. Second, a Multi-Scale Detection module is constructed to selectively emphasize the contribution of different features by channel and space. Finally, we adopt the decoupled head to predict the classes and locations of fire or smoke respectively. In the experiment, we propose a more challenging dataset “Smoke and Fire-dataset” (“SF-dataset”) to evaluate the proposed algorithm, which includes 18,217 images. And the results show that the proposed method outperforms most SOTA methods in detection accuracy, model size, and detection speed.},
journal = {J. Real-Time Image Process.},
month = {jun},
numpages = {12},
keywords = {Fire/smoke detection, SE-GhostNet, Depthwise separable convolution, MSD subnetwork}
}

@inproceedings{10.1007/978-3-031-48057-7_6,
author = {Hartmann, Benedict and Tamla, Philippe and Hemmje, Matthias},
title = {Supporting Deep Learning-Based Named Entity Recognition Using Cloud Resource Management},
year = {2023},
isbn = {978-3-031-48056-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-48057-7_6},
doi = {10.1007/978-3-031-48057-7_6},
abstract = {This paper presents a system for managing Cloud Resources such as memory and CPU/GPU that is used to develop, train, and customize Deep Learning-based Named Entity Recognition (NER) models in domains like heath care. The increasing digitization of healthcare services has led to the emergence of electronic health records (EHRs) as a significant component of healthcare data management. NER is a machine learning technique that can be applied to EHRs to extract information such as drug and treatment information, helping to support clinical decision making. The paper is addressing the difficulty domain experts face in using Cloud technologies to perform NER tasks, since they often require technical expertise and technical management overhead. The paper presents a system for the configuration of cloud resources for NER training using the spaCy framework and AWS compute services. The research is structured using Nunamaker’s methodology, which provides a structured approach to software development through four phases: observation, theory building, systems development, and experimentation. The paper identifies problem statements and research questions to guide the research and maps them to the objectives of the methodology. The objectives of the methodology include researching the state-of-the-art of NER and cloud technologies, analyzing the architecture of motivating research projects, defining user requirements and the system architecture, and implementing the system. The system is designed using User Centered Systems Design and is based on previously identified user requirements. Two main user groups are considered for the application: NER Experts and Medical Domain Experts. The system is implemented using the Model-View-Controller architecture pattern. It allows for the training of Transformer models, selection of compute resources, and adjusting training configuration and hyperparameters. The system is designed for scalability of compute and storage resources. The paper also discusses the evaluation of the system through experiments and analysis of the results to gain insights. It provides information about the technical implementation and details about the user interface. It is evaluated using cognitive walkthrough and experiments with Transformer-based models.},
booktitle = {HCI International 2023 – Late Breaking Papers: 25th International Conference on Human-Computer Interaction, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part VI},
pages = {84–100},
numpages = {17},
keywords = {Cloud Resource Management, Deep Learning, Named Entity Recognition, Transformer, Cloud Computing, Micro Service Architecture},
location = {<conf-loc content-type="Hybrid">Copenhagen, Denmark</conf-loc>}
}

@proceedings{10.1145/3587259,
title = {K-CAP '23: Proceedings of the 12th Knowledge Capture Conference 2023},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 12th ACM International Conference on Knowledge Capture: K-CAP 2023, held in person on December 5th - 7th in Pensacola, Florida, US.Driven by the increasing demands for knowledge-based applications and the unprecedented availability of information from heterogeneous data sources, the study of knowledge capture is of crucial importance. Knowledge capture involves the extraction of useful knowledge from vast and diverse data sources as well as its acquisition directly from human experts.Nowadays knowledge is derived from an increasingly diverse set of data resources that differ with regard to their domain, format, quality, coverage, specificity, viewpoint, bias, and most importantly, consumers and producers of data. The heterogeneity, amount and complexity of data allow us to answer complex questions that could not be answered in isolation, requiring the interaction of different scientific fields and technologies. A goal of K-CAP is to develop such synergies using systematic and rigorous methodologies.The call for papers attracted 105 submissions from all over the world, covering a diverse range of topics spanning knowledge mining, large language models for information extraction, neuro-symbolic approaches for knowledge capture, knowledge engineering, question-answering, knowledge graphs, natural language processing, reasoning, entity linking, querying and knowledge-based applications. From a competitive set of high-quality submissions, we accepted 27 long research papers, 5 short papers, and 1 vision paper. The high-quality program is divided into 7 research sessions, in addition to 3 tutorials reflecting novel topics of interest in Knowledge Capture.We encourage everyone to attend the keynote talks that we have planned for K-CAP 2023. The highly anticipated talks by Dr. Robert R. Hoffman (Florida Institute for Human and Machine Cognition) and Dr. Jane Pinelis (Johns Hopkins University Applied Physics Laboratory) will guide us to a better understanding of the future of knowledge capture and explainable, resilient AI ecosystems, as they become commonplace in real world applications.},
location = {<conf-loc>, <city>Pensacola</city>, <state>FL</state>, <country>USA</country>, </conf-loc>}
}

@article{10.1007/s10472-023-09894-7,
author = {Buchberger, Bruno},
title = {Automated programming, symbolic computation, machine learning: my&nbsp;personal view},
year = {2023},
issue_date = {Oct 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {91},
number = {5},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-023-09894-7},
doi = {10.1007/s10472-023-09894-7},
abstract = {In this note, I present my personal view on the interaction of the three areas Automated Programming, Symbolic Computation, and Machine Learning. Programming is the activity of finding a (hopefully) correct program (algorithm) for a given problem. Programming is central to automation in all areas and is considered one of the most creative human activities. However, already very early in the history of programming, people started to “jump to the meta-level” of programming, i.e., started to develop procedures that automate, or semi-automate, (various aspects or parts of) the process of programming. This area has various names like “Automated Programming”, “Automated Algorithm Synthesis”, etc. Developing compilers can be considered an early example of a problem in automated programming. Automated reasoners for proving the correctness of programs with respect to a specification is an advanced example of a topic in automated programming. ChatGPT producing (amazingly good) programs from problem specifications in natural language is a recent example of automated programming. Programming tends to become the most important activity as the level of technological sophistication increases. Therefore, automating programming is maybe the most exciting and relevant technological endeavor today. It also will have enormous impact on the global job market in the software industry. Roughly, I see two main approaches to automated programming:symbolic computationand machine learning. In this note, I explain how the two approaches work and that they are fundamentally different because they address two completely different ways of how problems are specified. Together, the two approaches constitute (part of) what some people like to call “artificial intelligence”. In my analysis, both approaches are just part of (algorithmic) mathematics. The approaches, like all non-trivial mathematical methods, need quite some intelligence on the side of the human inventors of the methods. However, applying the methods is just “machine execution” of algorithms. It is misleading to call the application “machine intelligence” or “artificial intelligence”. The analysis of the two approaches to automated programming also suggests that the two approaches, in the future, should be combined to achieve even higher levels of sophistication. At the end of this note, I propose some research questions for this new direction.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = {oct},
pages = {569–589},
numpages = {21},
keywords = {Automated programming, Symbolic computation, Automated reasoning, Machine learning, Artificial intelligence, Artificial general intelligence, Pretrained large language models, Software industry, Programming assistant., .}
}

@article{10.1016/j.eswa.2023.120890,
author = {Gupta, Shubham and Kundu, Suman},
title = {Interaction graph, topical communities, and efficient local event detection from social streams},
year = {2023},
issue_date = {Dec 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {232},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.120890},
doi = {10.1016/j.eswa.2023.120890},
journal = {Expert Syst. Appl.},
month = {dec},
numpages = {13},
keywords = {Event detection, Event extraction, Data stream mining, Unsupervised learning}
}

@inproceedings{10.1007/978-3-031-32883-1_22,
author = {Bredeweg, Bert and Kragten, Marco and Holt, Joanna and Vaendel, Dennis and Hanse, Joris and Bloemen, Steven},
title = {Stargazing Live! Inspiring with Real Data in a Mobile Planetarium and Learning Through Conceptual Modelling},
year = {2023},
isbn = {978-3-031-32882-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-32883-1_22},
doi = {10.1007/978-3-031-32883-1_22},
abstract = {We present the Stargazing Live! program comprising a planetarium experience and supporting lesson activities for pre-university physics education. The mobile planetarium aims to inspire and motivate learners using real telescope data during the experience. Learners then consolidate their learning by creating conceptual models in the DynaLearn software. During development of the program, content experts and stakeholders were consulted. Three conceptual model lesson activities have been created: star properties, star states and the fusion-gravity balance. The present paper evaluates the planetarium experience plus the star properties lesson activity in nine grade 11 and 12 classes across three secondary schools in the Netherlands. Learners are very positive about the planetarium experience, but they are less able to link the topics in the planetarium to the curriculum. The conceptual modelling activity improves the learners understanding of the causal relationship between the various stellar properties. Future work includes classroom testing of the star states and fusion-gravity balance lessons.},
booktitle = {Augmented Intelligence and Intelligent Tutoring Systems: 19th International Conference, ITS 2023, Corfu, Greece, June 2–5, 2023, Proceedings},
pages = {257–269},
numpages = {13},
keywords = {Education, Physics education, Planetarium, Conceptual modelling},
location = {Corfu, Greece}
}

@inproceedings{10.1145/3579856.3582823,
author = {Kim, Hyunjin and Bak, Jinyeong and Cho, Kyunghyun and Koo, Hyungjoon},
title = {A Transformer-based Function Symbol Name Inference Model from an Assembly Language for Binary Reversing},
year = {2023},
isbn = {9798400700989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579856.3582823},
doi = {10.1145/3579856.3582823},
abstract = {Reverse engineering of a stripped binary has a wide range of applications, yet it is challenging mainly due to the lack of contextually useful information within. Once debugging symbols (e.g., variable names, types, function names) are discarded, recovering such information is not technically viable with traditional approaches like static or dynamic binary analysis. We focus on a function symbol name recovery, which allows a reverse engineer to gain a quick overview of an unseen binary. The key insight is that a well-developed program labels a meaningful function name that describes its underlying semantics well. In this paper, we present AsmDepictor, the Transformer-based framework that generates a function symbol name from a set of assembly codes (i.e., machine instructions), which consists of three major components: binary code refinement, model training, and inference. To this end, we conduct systematic experiments on the effectiveness of code refinement that can enhance an overall performance. We introduce the per-layer positional embedding and Unique-softmax for AsmDepictor so that both can aid to capture a better relationship between tokens. Lastly, we devise a novel evaluation metric tailored for a short description length, the Jaccard* score. Our empirical evaluation shows that the performance of AsmDepictor by far surpasses that of the state-of-the-art models up to around 400%. The best AsmDepictor model achieves an F1 of 71.5 and Jaccard* of 75.4.},
booktitle = {Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security},
pages = {951–965},
numpages = {15},
keywords = {Transformer, assembly, function name, neural networks, reversing},
location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
series = {ASIA CCS '23}
}

@inproceedings{10.1007/978-3-031-49187-0_6,
author = {Chen, Wei and Wang, Huaijin and Gu, Weixi and Wang, Shuai},
title = {RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz Testing},
year = {2023},
isbn = {978-3-031-49186-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49187-0_6},
doi = {10.1007/978-3-031-49187-0_6},
abstract = {Securing operating system (OS) kernel is one central challenge in today’s cyber security landscape. The cutting-edge testing technique of OS kernel is software fuzz testing. By mutating the program inputs with random variations for iterations, fuzz testing aims to trigger program crashes and hangs caused by potential bugs that can be abused by the inputs. To achieve high OS code coverage, the de facto OS fuzzer typically composes system call traces as the input seed to mutate and to interact with OS kernels. Hence, quality and diversity of the employed system call traces become the prominent factor to decide the effectiveness of OS fuzzing. However, these system call traces to date are generated with hand-coded rules, or by analyzing system call logs of OS utility programs. Our observation shows that such system call traces can only subsume common usage scenarios of OS system calls, and likely omit hidden bugs.In this research, we propose a deep reinforcement learning-based solution, called RLTrace, to synthesize diverse and comprehensive system call traces as the seed to fuzz OS kernels. During model training, the deep learning model interacts with OS kernels and infers optimal system call traces w.r.t. our learning goal — maximizing kernel code coverage. Our evaluation shows that RLTrace outperforms other seed generators by producing more comprehensive system call traces, subsuming system call corner usage cases and subtle dependencies. By feeding the de facto OS fuzzer, Syzkaller, with system call traces synthesized by RLTrace, we show that Syzkaller can achieve higher code coverage for testing Linux kernels. Furthermore, RLTrace found one vulnerability in the Linux kernel (version 5.5-rc6), which is publicly unknown to the best of our knowledge by the time of writing. We conclude the paper with discussions on the limitations, tentative exploration of technical migration to other OS kernels, and future directions of our work. We believe the proposed RLTrace can be a promising solution to improve the reliability of OS fuzzing in various scenarios, over different OS kernels, and for different reliability purposes.},
booktitle = {Information Security: 26th International Conference, ISC 2023, Groningen, The Netherlands, November 15–17, 2023, Proceedings},
pages = {99–118},
numpages = {20},
location = {<conf-loc content-type="InPerson">Groningen, The Netherlands</conf-loc>}
}

@article{10.1016/j.neucom.2023.03.025,
author = {Moharram, Mohammed Abdulmajeed and Sundaram, Divya Meena},
title = {Land use and land cover classification with hyperspectral data: A comprehensive review of methods, challenges and future directions},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {536},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2023.03.025},
doi = {10.1016/j.neucom.2023.03.025},
journal = {Neurocomput.},
month = {jun},
pages = {90–113},
numpages = {24},
keywords = {LULC, Hyperspectral imaging, Spectral- spatial features, Machine learning, Deep learning}
}

@inproceedings{10.1007/978-3-031-50078-7_25,
author = {Yang, Kuo and Jiang, Wenhao and Shi, Yiqiao and Qin, Rui and Bai, Wanli and Li, Duo and Wu, Yue and Hu, Menghan},
title = {Cup-Disk Ratio Segmentation Joint with&nbsp;Key Retinal Vascular Information Under Diagnostic and&nbsp;Screening Scenarios},
year = {2023},
isbn = {978-3-031-50077-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-50078-7_25},
doi = {10.1007/978-3-031-50078-7_25},
abstract = {Glaucoma is one of the leading causes of irreversible blindness worldwide. Numerous studies have shown that a larger vertical Cup-to-Disc Ratio (CDR) is closely associated with the glaucoma diagnosis. CDR is highly useful in the clinical practice and evaluation of glaucoma. However, the determination of CDR varies among clinicians and is highly dependent on the doctor’s subjectivity. Existing methods only segment the cup and disc features without considering the nearby vascular information. Based on guidance and criteria from experienced clinicians in diagnosing glaucoma, we incorporate segmented essential vascular information to constrain CDR segmentation. We add key vessel information to the network as the prior knowledge to better guide the model to distinguish the boundary of the optic cup. The effectiveness of incorporating essential vascular information has been demonstrated through experiments conducted on the public dataset REFUGE as well as the home-made dataset. The home-made dataset consists of high-quality CDR images and remade CDR images, corresponding to the diagnosis scenario and the screening scenario in which the patient needs to upload the fundus image by taking photos. The model is deployed on the Wechat mini-program for practical glaucoma diagnostic and screening applications.},
booktitle = {Advances in Computer Graphics: 40th Computer Graphics International Conference, CGI 2023, Shanghai, China, August 28 – September 1, 2023, Proceedings, Part IV},
pages = {313–326},
numpages = {14},
keywords = {Cup-disk ratio segmentation, Retinal vascular, Glaucoma diagnostic and screening},
location = {<conf-loc content-type="InPerson">Shanghai, China</conf-loc>}
}

@inproceedings{10.1007/978-3-031-41682-8_4,
author = {Doshi, Chinesh and Shrotiya, Himani and Bhiogade, Rohit and Bhatt, Himanshu S. and Jha, Abhishek},
title = {Analyzing Textual Information from&nbsp;Financial Statements for&nbsp;Default Prediction},
year = {2023},
isbn = {978-3-031-41681-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-41682-8_4},
doi = {10.1007/978-3-031-41682-8_4},
abstract = {Financial statements provide a view of company’s financial status at a specific point in time including the quantitative as well as qualitative view. Besides the quantitative information, the paper asserts that the qualitative information present in the form of textual disclosures have high discriminating power to predict the financial default. Towards this, the paper presents a technique to capture comprehensive 360-∘ features from qualitative textual data at multiple granularities. The paper proposes a new sentence embedding (SE) from large language models specifically built for financial domain to encode the textual data and presents three deep learning models built on SE for financial default prediction. To accommodate unstructured and non-standard financial statements from small and unlisted companies, the paper also presents a document processing pipeline to be inclusive of such companies in the financial text modelling. Finally, the paper presents comprehensive experimental results on two datasets demonstrating the discriminating power of textual features to predict financial defaults.},
booktitle = {Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San José, CA, USA, August 21–26, 2023, Proceedings, Part III},
pages = {48–65},
numpages = {18},
keywords = {Financial Statement Analysis, Document Features Extraction, Document classification},
location = {San José, CA, USA}
}

@inproceedings{10.5555/3618408.3619887,
author = {Wang, Yiping and Chen, Yifang and Jamieson, Kevin and Du, Simon S.},
title = {Improved active multi-task representation learning via Lasso},
year = {2023},
publisher = {JMLR.org},
abstract = {To leverage the copious amount of data from source tasks and overcome the scarcity of the target task samples, representation learning based on multi-task pretraining has become a standard approach in many applications. However, up until now, most existing works design a source task selection strategy from a purely empirical perspective. Recently, Chen et al. (2022) gave the first active multi-task representation learning (A-MTRL) algorithm which adaptively samples from source tasks and can provably reduce the total sample complexity using the L2-regularized-target-source-relevance parameter ν2. But their work is theoretically suboptimal in terms of total source sample complexity and is less practical in some real-world scenarios where sparse training source task selection is desired. In this paper, we address both issues. Specifically, we show the strict dominance of the L1-regularized-relevance-based (ν1-based) strategy by giving a lower bound for the ν2-based strategy. When ν1 is unknown, we propose a practical algorithm that uses the LASSO program to estimate ν1. Our algorithm successfully recovers the optimal result in the known case. In addition to our sample complexity results, we also characterize the potential of our ν1-based strategy in sample-cost-sensitive settings. Finally, we provide experiments on real-world computer vision datasets to illustrate the effectiveness of our proposed method.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1479},
numpages = {31},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.1016/j.cag.2023.05.017,
author = {Liu, Yifan and Chen, Jincai and Lu, Ping and Zhu, Chuanbo and Jian, Yugen and Sun, Chao and Liang, Han},
title = {Snowed autoencoders are efficient snow removers},
year = {2023},
issue_date = {Aug 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {114},
number = {C},
issn = {0097-8493},
url = {https://doi.org/10.1016/j.cag.2023.05.017},
doi = {10.1016/j.cag.2023.05.017},
journal = {Comput. Graph.},
month = {aug},
pages = {73–85},
numpages = {13},
keywords = {Single image snow removal, Masked autoencoder, Vision transformer}
}

@article{10.1016/j.engappai.2023.106795,
author = {Xie, Yuanlun and Tian, Wenhong and Yu, Zitong},
title = {Robust facial expression recognition with Transformer Block Enhancement Module},
year = {2023},
issue_date = {Nov 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {126},
number = {PA},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106795},
doi = {10.1016/j.engappai.2023.106795},
journal = {Eng. Appl. Artif. Intell.},
month = {nov},
numpages = {13},
keywords = {Facial expression recognition, Neural network, Deep learning, Transformer Block Enhancement Module}
}

@article{10.1016/j.compedu.2023.104948,
author = {Cheng, Gary and Zou, Di and Xie, Haoran and Wang, Fu Lee},
title = {Exploring differences in self-regulated learning strategy use between high- and low-performing students in introductory programming: An analysis of eye-tracking and retrospective think-aloud data from program comprehension},
year = {2024},
issue_date = {Jan 2024},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {208},
number = {C},
issn = {0360-1315},
url = {https://doi.org/10.1016/j.compedu.2023.104948},
doi = {10.1016/j.compedu.2023.104948},
journal = {Comput. Educ.},
month = {jan},
numpages = {18},
keywords = {Introductory programming, Self-regulated learning strategies, Eye tracking, Retrospective think aloud, Higher education}
}

@inproceedings{10.1007/978-3-031-51482-1_10,
author = {Seidel, Lukas and Baker Effendi, Sedick David and Pinho, Xavier and Rieck, Konrad and van der Merwe, Brink and Yamaguchi, Fabian},
title = {Learning Type Inference for&nbsp;Enhanced Dataflow Analysis},
year = {2024},
isbn = {978-3-031-51481-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-51482-1_10},
doi = {10.1007/978-3-031-51482-1_10},
abstract = {Statically analyzing dynamically-typed code is a challenging endeavor, as even seemingly trivial tasks such as determining the targets of procedure calls are non-trivial without knowing the types of objects at compile time. Addressing this challenge, gradual typing is increasingly added to dynamically-typed languages, a prominent example being TypeScript that introduces static typing to JavaScript. Gradual typing improves the developer’s ability to verify program behavior, contributing to robust, secure and debuggable programs. In practice, however, users only sparsely annotate types directly. At the same time, conventional type inference faces performance-related challenges as program size grows. Statistical techniques based on machine learning offer faster inference, but although recent approaches demonstrate overall improved accuracy, they still perform significantly worse on user-defined types than on the most common built-in types. Limiting their real-world usefulness even more, they rarely integrate with user-facing applications.We propose CodeTIDAL5, a Transformer-based model trained to reliably predict type annotations. For effective result retrieval and re-integration, we extract usage slices from a program’s code property graph. Comparing our approach against recent neural type inference systems, our model outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore, we present JoernTI, an integration of our approach into Joern, an open source static analysis tool, and demonstrate that the analysis benefits from the additional type information. As our model allows for fast inference times even on commodity CPUs, making our system available through Joern leads to high accessibility and facilitates security research.},
booktitle = {Computer Security – ESORICS 2023: 28th European Symposium on Research in Computer Security, The Hague, The Netherlands, September 25–29, 2023, Proceedings, Part IV},
pages = {184–203},
numpages = {20},
keywords = {Type Inference, Representation Learning, Static Analysis, Static Taint Tracking, Dataflow Analysis},
location = {<conf-loc content-type="InPerson">The Hague, The Netherlands</conf-loc>}
}

@article{10.1145/3640008,
author = {Ikeda, Bryce and Szafir, Daniel},
title = {PRogramAR: Augmented Reality End-User Robot Programming},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640008},
doi = {10.1145/3640008},
abstract = {The field of end-user robot programming seeks to develop methods that empower non-expert programmers to task and modify robot operations. In doing so, researchers may enhance robot flexibility and broaden the scope of robot deployments into the real world. We introduce PRogramAR (Programming Robots using Augmented Reality), a novel end-user robot programming system that combines the intuitive visual feedback of augmented reality (AR) with the simplistic and responsive paradigm of trigger-action programming (TAP) to facilitate human-robot collaboration. Through PRogramAR, users are able to rapidly author task rules and desired reactive robot behaviors, while specifying task constraints and observing program feedback contextualized directly in the real world. PRogramAR provides feedback by simulating the robot’s intended behavior and providing instant evaluation of TAP rule executability to help end-users better understand and debug their programs during development. In a system validation, 17 end-users ranging from ages 18 to 83 used PRogramAR to program a robot to assist them in completing three collaborative tasks. Our results demonstrate how merging the benefits of AR and TAP using elements from prior robot programming research into a single novel system can successfully enhance the robot programming process for non-expert users.},
note = {Just Accepted},
journal = {J. Hum.-Robot Interact.},
month = {jan},
keywords = {End-User Robot Programming, Trigger-Action Programming (TAP), Augmented Reality (AR), Human-Robot Interaction (HRI), Human-Robot Collaboration (HRC)}
}

@inproceedings{10.1145/3624007.3624053,
author = {Magalhães, José Wesley de Souza and Woodruff, Jackson and Polgreen, Elizabeth and O'Boyle, Michael F. P.},
title = {C2TACO: Lifting Tensor Code to TACO},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624053},
doi = {10.1145/3624007.3624053},
abstract = {Domain-specific languages (DSLs) promise a significant performance and portability advantage over traditional languages. DSLs are designed to be high-level and platform-independent, allowing an optimizing compiler significant leeway when targeting a particular device. Such languages are particularly popular with emerging tensor algebra workloads. However, DSLs present their own challenge: they require programmers to learn new programming languages and put in significant effort to migrate legacy code.  

We present C2TACO, a synthesis tool for synthesizing TACO, a well-known tensor DSL, from C code. We develop a smart, enumerative synthesizer that uses automatically generated IO examples and source-code analysis to efficiently generate code. C2TACO is able to synthesize 95% bench marks from a tensor benchmark suite, out-performing an alternative neural machine translation technique, and demonstrates substantially higher levels of accuracy when evaluated against two state-of-the-art existing schemes, TF-Coder and ChatGPT. Our synthesized TACO programs are, by design, portable achieving significant performance improvement when evaluated on a multi-core and GPU platform.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {42–56},
numpages = {15},
keywords = {Program Lifting, Synthesis, TACO, Tensor Algebra},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inproceedings{10.1145/3576915.3623188,
author = {Yang, Su and Xiao, Yang and Xu, Zhengzi and Sun, Chengyi and Ji, Chen and Zhang, Yuqing},
title = {Enhancing OSS Patch Backporting with Semantics},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623188},
doi = {10.1145/3576915.3623188},
abstract = {Keeping open-source software (OSS) up to date is one potential solution to prevent known vulnerabilities. However, it requires frequent and costly testing and may introduce compatibility issues. Consequently, developers often choose to backport security patches to the vulnerable versions instead. Manual backporting is time-consuming, especially for large OSS such as the Linux kernel. Therefore, automating this process is urgently needed to save considerable time. Existing automated approaches for backporting patches involve either automatic patch generation or automatic patch migration. However, these methods are often ineffective and error-prone since they failed to locate the precise patch locations or generate the correct patch, operating only on the syntactic level.In this paper, we propose a patch type-sensitive approach to automatically backport OSS security patches, guided by the patch type and patch semantics. Specifically, our approach identifies patch locations with the aid of program dependency graph-based matching at the semantic level. It further applies fine-grained patch migration and fine-tuning based on patch types. We have implemented our approach in a tool named TSBPORT and evaluated it on a large-scale dataset consisting of 1,815 pairs of real-world security patches for the Linux kernel. The evaluation results show that TSBPORT successfully backported 1,589 (87.59%) patches, out of which 587 (32.34%) could not be backported by any state-of-the-art approaches, significantly outperforming state-of-the-art approaches. In addition, experiments also show that TSBPORT can be generalized to backport patches in other OSS projects with a success rate of 88.18%.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2366–2380},
numpages = {15},
keywords = {patch backporting, patch semantics, patch type},
location = {<conf-loc>, <city>Copenhagen</city>, <country>Denmark</country>, </conf-loc>},
series = {CCS '23}
}

@article{10.1016/j.asoc.2023.110562,
author = {Poczeta, Katarzyna and Płaza, Mirosław and Michno, Tomasz and Krechowicz, Maria and Zawadzki, Michał},
title = {A multi-label text message classification method designed for applications in call/contact centre systems},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {145},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2023.110562},
doi = {10.1016/j.asoc.2023.110562},
journal = {Appl. Soft Comput.},
month = {sep},
numpages = {20},
keywords = {Text classification, Call centre, Contact centre, Multi-label classification}
}

@inproceedings{10.1145/3580305.3599440,
author = {Li, Shuangli and Zhou, Jingbo and Liu, Ji and Xu, Tong and Chen, Enhong and Xiong, Hui},
title = {Multi-Temporal Relationship Inference in Urban Areas},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599440},
doi = {10.1145/3580305.3599440},
abstract = {Finding multiple temporal relationships among locations can benefit a bunch of urban applications, such as dynamic offline advertising and smart public transport planning. While some efforts have been made on finding static relationships among locations, little attention is focused on studying time-aware location relationships. Indeed, abundant location-based human activities are time-varying and the availability of these data enables a new paradigm for understanding the dynamic relationships in a period among connective locations. To this end, we propose to study a new problem, namely multi-Temporal relationship inference among locations (Trial for short), where the major challenge is how to integrate dynamic and geographical influence under the relationship sparsity constraint. Specifically, we propose a solution to Trial with a graph learning scheme, which includes a spatially evolving graph neural network (SEENet) with two collaborative components: spatially evolving graph convolution module (SEConv) and spatially evolving self-supervised learning strategy (SE-SSL). SEConv performs the intra-time aggregation and inter-time propagation to capture the multifaceted spatially evolving contexts from the view of location message passing. In addition, SE-SSL designs time-aware self-supervised learning tasks in a global-local manner with additional evolving constraint to enhance the location representation learning and further handle the relationship sparsity. Finally, experiments on four real-world datasets demonstrate the superiority of our method over several state-of-the-art approaches.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1316–1327},
numpages = {12},
keywords = {graph neural networks, relationship inference, spatial graph},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@article{10.14778/3611540.3611620,
author = {Singh, Mukul and Sanchez, José Cambronero and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Verbruggen, Gust},
title = {Cornet: Learning Spreadsheet Formatting Rules by Example},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611620},
doi = {10.14778/3611540.3611620},
abstract = {Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure". Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce Cornet, a system that automatically learns such conditional formatting rules from user examples. Cornet takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show Cornet in action as a simple add-in to Microsoft's Excel. After the user provides one or two formatted cells as examples, Cornet generates formatting rule suggestions for the user to apply to the spreadsheet.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {4058–4061},
numpages = {4}
}

@article{10.14778/3611479.3611534,
author = {Li, Peng and He, Yeye and Yan, Cong and Wang, Yue and Chaudhuri, Surajit},
title = {Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611534},
doi = {10.14778/3611479.3611534},
abstract = {Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {3391–3403},
numpages = {13}
}

@inproceedings{10.1007/978-3-031-47546-7_1,
author = {Tirsi, Cristina and Proietti, Maurizio and Toni, Francesca},
title = {ABALearn: An Automated Logic-Based Learning System for&nbsp;ABA Frameworks},
year = {2023},
isbn = {978-3-031-47545-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-47546-7_1},
doi = {10.1007/978-3-031-47546-7_1},
abstract = {We introduce ABALearn, an automated algorithm that learns Assumption-Based Argumentation (ABA) frameworks from training data consisting of positive and negative examples, and a given background knowledge. ABALearn’s ability to generate comprehensible rules for decision-making promotes transparency and interpretability, addressing the challenges associated with the black-box nature of traditional machine learning models. This implementation is based on the strategy proposed in a previous work. The learnt ABA frameworks can be mapped onto logic programs with negation as failure. The main advantage of this algorithm is that it requires minimal information about the learning problem and it is also capable of learning circular debates. Our results show that this approach is competitive with state-of-the-art alternatives, demonstrating its potential to be used in real-world applications where low user expertise is available. Overall, this work contributes to the development of automated learning techniques for argumentation frameworks in the context of Explainable AI (XAI) and provides insights into how such learners can be applied to make predictions.},
booktitle = {AIxIA 2023 – Advances in Artificial Intelligence: XXIInd International Conference of the Italian Association for Artificial Intelligence, AIxIA 2023, Rome, Italy, November 6–9, 2023, Proceedings},
pages = {3–16},
numpages = {14},
keywords = {Logic-based learning, Assumption-based argumentation, Logic program transformation},
location = {Rome, Italy}
}

@article{10.1016/j.patrec.2023.07.007,
author = {Liu, Yan and Wei, Li-Fang and Zhang, Chuan-Fei and Zhang, Tian-Hao and Chen, Song-Lu and Yin, Xu-Cheng},
title = {Self-supervised contrastive speaker verification with nearest neighbor positive instances},
year = {2023},
issue_date = {Sep 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {173},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2023.07.007},
doi = {10.1016/j.patrec.2023.07.007},
journal = {Pattern Recogn. Lett.},
month = {sep},
pages = {17–22},
numpages = {6},
keywords = {Self-supervised contrastive learning, Speaker verification, Nearest neighbor positive instances}
}

@article{10.14778/3611540.3611544,
author = {Anneser, Christoph and Tatbul, Nesime and Cohen, David and Xu, Zhenggang and Pandian, Prithviraj and Laptev, Nikolay and Marcus, Ryan},
title = {AutoSteer: Learned Query Optimization for Any SQL Database},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611544},
doi = {10.14778/3611540.3611544},
abstract = {This paper presents AutoSteer, a learning-based solution that automatically drives query optimization in any SQL database that exposes tunable optimizer knobs. AutoSteer builds on the Bandit optimizer (Bao) and extends it with new capabilities (e.g., automated hint-set discovery) to minimize integration effort and facilitate usability in both monolithic and disaggregated SQL systems. We successfully applied AutoSteer on PostgreSQL, PrestoDB, Spark-SQL, MySQL, and DuckDB - five popular open-source database engines with diverse query optimizers. We then conducted a detailed experimental evaluation with public benchmarks (JOB, Stackoverflow, TPC-DS) and a production workload from Meta's PrestoDB deployments. Our evaluation shows that AutoSteer can not only outperform these engines' native query optimizers (e.g., up to 40% improvements for PrestoDB) but can also match the performance of Bao-for-PostgreSQL with reduced human supervision and increased adaptivity, as it replaces Bao's static, expert-picked hint-sets with those that are automatically discovered. We also provide an open-source implementation of AutoSteer together with a visual tool for interactive use by query optimization experts.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {3515–3527},
numpages = {13}
}

@inproceedings{10.1145/3563657.3596051,
author = {Flohr, Lukas A. and Valiyaveettil, Joseph Sebastian and Krüger, Antonio and Wallach, Dieter P.},
title = {Prototyping Autonomous Vehicle Windshields with AR and Real-Time Object Detection Visualization: An On-Road Wizard-of-Oz Study},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596051},
doi = {10.1145/3563657.3596051},
abstract = {Autonomous vehicles (AVs; SAE levels 4 and 5) face substantial challenges regarding acceptance and UX. Novel human-machine interfaces (HMIs) providing transparent system information could account for those and facilitate adoption. However, since the availability of AVs for early concept studies is limited, context-based interface prototyping is required. This paper demonstrates the prototype and wizard-of-oz-based on-road evaluation of a futuristic windshield HMI concept that visualizes real-time object detections via augmented reality (AR). In a mixed-methods within-subjects study (N = 30), participants assessed three early-stage concept variants to explore whether object detection visualization can counteract the aforementioned challenges. The findings confirm that transparent system feedback can increase understandability, perceived usefulness, and hedonic UX, but the amount and the timing of the provided information are crucial. The applied prototyping method proved suitable for investigating HMI concepts with real-time AR on urban roads. Based on a critical discussion, the paper concludes with design and prototyping recommendations.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {2123–2137},
numpages = {15},
keywords = {acceptance, augmented reality, autonomous vehicles, computational interaction, computer vision, context-based prototyping., human-centered artificial intelligence, object detection, user experience, visualization, windshield interface, wizard-of-oz},
location = {<conf-loc>, <city>Pittsburgh</city>, <state>PA</state>, <country>USA</country>, </conf-loc>},
series = {DIS '23}
}

@article{10.1145/3632870,
author = {Pailoor, Shankara and Wang, Yuepeng and Dillig, Işıl},
title = {Semantic Code Refactoring for Abstract Data Types},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632870},
doi = {10.1145/3632870},
abstract = {Modifications to the data representation of an abstract data type (ADT) can require significant semantic refactoring of the code. Motivated by this observation, this paper presents a new method to automate semantic code refactoring tasks. Our method takes as input the original ADT implementation, a new data representation, and a so-called relational representation invariant (relating the old and new data representations), and automatically generates a new ADT implementation that is semantically equivalent to the original version. Our method is based on counterexample-guided inductive synthesis (CEGIS) but leverages three key ideas that allow it to handle real-world refactoring tasks. First, our approach reduces the underlying relational synthesis problem to a set of (simpler) programming-by-example problems, one for each method in the ADT. Second, it leverages symbolic reasoning techniques, based on logical abduction, to deduce code snippets that should occur in the refactored version. Finally, it utilizes a notion of partial equivalence to make inductive synthesis much more effective in this setting. We have implemented the proposed approach in a new tool called Revamp ‍ for automatically refactoring Java classes and evaluated it on 30 Java class mined from Github. Our evaluation shows that Revamp can correctly refactor the entire ADT in 97% of the cases and that it can successfully re-implement 144 out of the 146 methods that require modifications.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {28},
numpages = {32},
keywords = {Abstract Data Types, Program Synthesis, Refactoring}
}

@article{10.1145/3622841,
author = {Crichton, Will and Gray, Gavin and Krishnamurthi, Shriram},
title = {A Grounded Conceptual Model for Ownership Types in Rust},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622841},
doi = {10.1145/3622841},
abstract = {Programmers learning Rust struggle to understand ownership types, Rust’s core mechanism for ensuring memory safety without garbage collection. This paper describes our attempt to systematically design a pedagogy for ownership types. First, we studied Rust developers’ misconceptions of ownership to create the Ownership Inventory, a new instrument for measuring a person’s knowledge of ownership. We found that Rust learners could not connect Rust’s static and dynamic semantics, such as determining why an ill-typed program would (or would not) exhibit undefined behavior. Second, we created a conceptual model of Rust’s semantics that explains borrow checking in terms of flow-sensitive permissions on paths into memory. Third, we implemented a Rust compiler plugin that visualizes programs under the model. Fourth, we integrated the permissions model and visualizations into a broader pedagogy of ownership by writing a new ownership chapter for The Rust Programming Language, a popular Rust textbook. Fifth, we evaluated an initial deployment of our pedagogy against the original version, using reader responses to the Ownership Inventory as a point of comparison. Thus far, the new pedagogy has improved learner scores on the Ownership Inventory by an average of 9},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {265},
numpages = {29},
keywords = {Rust, concept inventory, ownership types, program state visualization}
}

@inproceedings{10.1145/3580305.3599903,
author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and Chen, Jiangui and Zhu, Zuowei and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
title = {Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599903},
doi = {10.1145/3580305.3599903},
abstract = {Recently, a new paradigm called Differentiable Search Index (DSI) has been proposed for document retrieval, wherein a sequence-to-sequence model is learned to directly map queries to relevant document identifiers. The key idea behind DSI is to fully parameterize traditional ''index-retrieve'' pipelines within a single neural model, by encoding all documents in the corpus into the model parameters. In essence, DSI needs to resolve two major questions: (1) how to assign an identifier to each document, and (2) how to learn the associations between a document and its identifier. In this work, we propose a Semantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the area of Cognitive Psychology. Our approach advances original DSI in two ways: (1) For the document identifier, we take inspiration from Elaboration Strategies in human learning. Specifically, we assign each document an Elaborative Description based on the query generation technique, which is more meaningful than a string of integers in the original DSI; and (2) For the associations between a document and its identifier, we take inspiration from Rehearsal Strategies in human learning. Specifically, we select fine-grained semantic features from a document as Rehearsal Contents to improve document memorization. Both the offline and online experiments show improved retrieval performance over prevailing baselines.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4904–4913},
numpages = {10},
keywords = {dsi, elaboration strategies, rehearsal strategies},
location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {KDD '23}
}

@article{10.1016/j.csi.2023.103738,
author = {Pillai, Seema and Sharma, Dr. Anurag},
title = {Hybrid unsupervised web-attack detection and classification – A deep learning approach},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2023.103738},
doi = {10.1016/j.csi.2023.103738},
journal = {Comput. Stand. Interfaces},
month = {aug},
numpages = {11},
keywords = {SQL injection attack, Cross-site scripting attacks, Denoising autoencoder, Deep Boltzmann machine, Binary long term short term memory}
}

@inproceedings{10.1145/3587102.3588787,
author = {Lehtinen, Teemu and Seppälä, Otto and Korhonen, Ari},
title = {Automated Questions About Learners' Own Code Help to Detect Fragile Prerequisite Knowledge},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588787},
doi = {10.1145/3587102.3588787},
abstract = {Students are able to produce correctly functioning program code even though they have a fragile understanding of how it actually works. Questions derived automatically from individual exercise submissions (QLC) can probe if and how well the students understand the structure and logic of the code they just created. Prior research studied this approach in the context of the first programming course. We replicate the study on a follow-up programming course for engineering students which contains a recap of general concepts in CS1. The task was the classic rainfall problem which was solved by 90% of the students. The QLCs generated from each passing submission were kept intentionally simple, yet 27% of the students failed in at least one of them. Students who struggled with questions about their own program logic had a lower median for overall course points than students who answered correctly.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {505–511},
numpages = {7},
keywords = {QLC, online education, prerequisite knowledge, program comprehension},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE 2023}
}

@article{10.1145/3631409,
author = {Ma, Dong and Dang, Ting and Ding, Ming and Balan, Rajesh},
title = {ClearSpeech: Improving Voice Quality of Earbuds Using Both In-Ear and Out-Ear Microphones},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
url = {https://doi.org/10.1145/3631409},
doi = {10.1145/3631409},
abstract = {Wireless earbuds have been gaining increasing popularity and using them to make phone calls or issue voice commands requires the earbud microphones to pick up human speech. When the speaker is in a noisy environment, speech quality degrades significantly and requires speech enhancement (SE). In this paper, we present ClearSpeech, a novel deep-learning-based SE system designed for wireless earbuds. Specifically, by jointly using the earbud's in-ear and out-ear microphones, we devised a suite of techniques to effectively fuse the two signals and enhance the magnitude and phase of the speech spectrogram. We built an earbud prototype to evaluate ClearSpeech under various settings with data collected from 20 subjects. Our results suggest that ClearSpeech can improve the SE performance significantly compared to conventional approaches using the out-ear microphone only. We also show that ClearSpeech can process user speech in real-time on smartphones.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jan},
articleno = {170},
numpages = {25},
keywords = {Audio Processing, Earables, Smart Earbuds, Speech Enhancement}
}

@article{10.1145/3636341.3636352,
author = {Lauw, Hady W. and Chua, Tat-Seng and Si, Luo and Terzi, Evimaria and Tsaparas, Panayiotis and Tomkins, Andrew},
title = {Report on the 16th ACM International Conference on Web Search and Data Mining (WSDM 2023)},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3636341.3636352},
doi = {10.1145/3636341.3636352},
abstract = {The 16th ACM International Conference on Web Search and Data Mining (WSDM 2023) was held in Singapore. It was held as in-person conference that also featured rich virtual elements. This brief report provides an overview of WSDM 2023 with organization and program details and statistics from Conference Chairs and Program Committee Chairs, as well as a message from the WSDM Steering Committee Chair.Date: 27 February-3 March 2023.Website: https://www.wsdm-conference.org/2023.},
journal = {SIGIR Forum},
month = {dec},
articleno = {8},
numpages = {5}
}

@article{10.1016/j.jss.2023.111666,
author = {Stiévenart, Quentin and Binkley, David and De Roover, Coen},
title = {An empirical evaluation of quasi-static executable slices},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {200},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111666},
doi = {10.1016/j.jss.2023.111666},
journal = {J. Syst. Softw.},
month = {jun},
numpages = {13},
keywords = {Program slicing, Static slicing, Dynamic slicing, Program dependence analysis}
}

@article{10.1007/s10462-023-10620-2,
author = {Wang, Xinwei and Wang, Yihui and Su, Xichao and Wang, Lei and Lu, Chen and Peng, Haijun and Liu, Jie},
title = {Deep reinforcement learning-based air combat maneuver decision-making: literature review, implementation tutorial and future direction},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {57},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-023-10620-2},
doi = {10.1007/s10462-023-10620-2},
abstract = {Nowadays, various innovative air combat paradigms that rely on unmanned aerial vehicles (UAVs), i.e., UAV swarm and UAV-manned aircraft cooperation, have received great attention worldwide. During the operation, UAVs are expected to perform agile and safe maneuvers according to the dynamic mission requirement and complicated battlefield environment. Deep reinforcement learning (DRL), which is suitable for sequential decision-making process, provides a powerful solution tool for air combat maneuver decision-making (ACMD), and hundreds of related research papers have been published in the last five years. However, as an emerging topic, there lacks a systematic review and tutorial. For this reason, this paper first provides a comprehensive literature review to help people grasp a whole picture of this field. It starts from the DRL itself and then extents to its application in ACMD. And special attentions are given to the design of reward function, which is the core of DRL-based ACMD. Then, a maneuver decision-making method based on one-to-one dogfight scenarios is proposed to enable UAV to win short-range air combat. The model establishment, program design, training methods and performance evaluation are described in detail. And the associated Python codes are available at gitee.com/wangyyhhh, thus enabling a quick-start for researchers to build their own ACMD applications by slight modifications. Finally, limitations of the considered model, as well as the possible future research direction for intelligent air combat, are also discussed.},
journal = {Artif. Intell. Rev.},
month = {dec},
numpages = {39},
keywords = {Artificial intelligence, Unmanned aerial vehicle (UAV), Deep reinforcement learning (DRL), Air combat maneuver decision-making (ACMD)}
}

@inproceedings{10.5555/3618408.3619263,
author = {Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh and Tian, Yuandong},
title = {Learning compiler pass orders using coreset and normalized value prediction},
year = {2023},
publisher = {JMLR.org},
abstract = {Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to heldout programs from different domains. Website: https://rlcompopt.github.io},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {855},
numpages = {17},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.5555/3618408.3619296,
author = {Liu, Shengchao and Du, Weitao and Ma, Zhiming and Guo, Hongyu and Tang, Jian},
title = {A group symmetric stochastic differential equation model for molecule multi-modal pretraining},
year = {2023},
publisher = {JMLR.org},
abstract = {Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multimodal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous downstream tasks than the previous work. By comparing with 17 pretraining baselines, we empirically verify that MoleculeSDE can learn an expressive representation with state-of-the-art performance on 26 out of 32 downstream tasks. The source codes are available in this repository.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {888},
numpages = {30},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1007/978-3-031-35915-6_28,
author = {Lu, I Xuan and Cai, Yicheng and Peng, Boxu an and Chen, Zhi-Xian and Luo, Tai-Xiang and Wong, Yung-Hao},
title = {Apply CNN Style Transformation on Industry 4.0},
year = {2023},
isbn = {978-3-031-35914-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-35915-6_28},
doi = {10.1007/978-3-031-35915-6_28},
abstract = {This project uses AI (artificial intelligence) rendering technology to realize how to present a photo in different forms of style. In a python environment, we establish a style transfer, inputting an original image for the software to recognize, then putting in a rendered style image for the software to perform the program. After 500 cycles are executed, the style will not have a stable form as the AI is evolving. By the time it reaches 1000 images, the algorithm has already remembered the common points of the images and grasped its own style, indicated by the gradual convergence of the loss function. The software will then produce different styles of rendered images.},
booktitle = {Social Computing and Social Media: 15th International Conference, SCSM 2023, Held as Part of the 25th HCI International Conference, HCII 2023, Copenhagen, Denmark, July 23–28, 2023, Proceedings, Part I},
pages = {384–400},
numpages = {17},
keywords = {AI, style transfer, rendering technology},
location = {Copenhagen, Denmark}
}

@inproceedings{10.1007/978-3-031-37586-6_15,
author = {Abdeen, Basel and Al-Shaer, Ehab and Singhal, Anoop and Khan, Latifur and Hamlen, Kevin},
title = {SMET: Semantic Mapping of&nbsp;CVE to&nbsp;ATT&amp;CK and&nbsp;Its Application to&nbsp;Cybersecurity},
year = {2023},
isbn = {978-3-031-37585-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-37586-6_15},
doi = {10.1007/978-3-031-37586-6_15},
abstract = {Cybercriminals relentlessly pursue vulnerabilities across cyberspace to exploit software, threatening the security of individuals, organizations, and governments. Although security teams strive to establish defense measures to thwart attackers, the complexity of cyber defense and the magnitude of existing threats exceed the capacity of defenders. Therefore, MITRE took the initiative and introduced multiple frameworks to facilitate the sharing of vital knowledge about vulnerabilities, attacks, and defense information. The Common Vulnerabilities and Exposures (CVE) program and ATT&amp;CK Matrix are two significant MITRE endeavors. CVE facilitates the sharing of publicly discovered vulnerabilities, while ATT&amp;CK collects and categorizes adversaries’ Tactics, Techniques, and Procedures (TTP) and recommends appropriate countermeasures.As CVE yields a low-level description of the vulnerability, ATT&amp;CK can complement it by providing more insights into that vulnerability from an attacking perspective, thereby aiding defenders in countering exploitation attempts. Unfortunately, due to the complexity of this mapping and the rapid growth of these frameworks, mapping CVE to ATT&amp;CK is a daunting and time-intensive undertaking. Multiple studies have proposed models that automatically achieve this mapping. However, due to their reliance on annotated datasets, these models exhibit limitations in quality and coverage and fail to justify their decisions. To overcome these challenges, we present SMET—a tool that automatically maps CVE entries to ATT&amp;CK techniques based on their textual similarity. SMET achieves this mapping by leveraging ATT&amp;CK BERT, a model that we trained using the SIAMESE network to learn semantic similarity among attack actions. In inference, SMET utilizes semantic extraction, ATT&amp;CK BERT, and a logistic regression model to map CVE entries to ATT&amp;CK techniques. As a result, SMET has demonstrated superior performance compared to other state-of-the-art models.},
booktitle = {Data and Applications Security and Privacy XXXVII: 37th Annual IFIP WG 11.3 Conference, DBSec 2023, Sophia-Antipolis, France, July 19–21, 2023, Proceedings},
pages = {243–260},
numpages = {18},
location = {Sophia-Antipolis, France}
}

@inproceedings{10.1145/3569951.3597597,
author = {Amaro, Rommie E. and Chen, Jiunn-Yeu and Duarte, Javier M. and Hutton, Thomas E. and Irving, Christopher and Kandes, Martin C. and Majumdar, Amit and Mishin, Dmitry Y. and Nguyen, Mai H. and Rodriguez, Paul and Silva, Fernando and Sinkovits, Robert S. and Strande, Shawn M. and Tatineni, Mahidhar and Tran, Leon Si and Wolter, Nicole},
title = {Voyager – An Innovative Computational Resource for Artificial Intelligence &amp; Machine Learning Applications in Science and Engineering},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569951.3597597},
doi = {10.1145/3569951.3597597},
abstract = {Voyager is an innovative computational resource designed by the San Diego Supercomputer Center in collaboration with technology partners to accelerate the development and performance of artificial intelligence and machine learning applications in science and engineering. Based on Intel’s Habana Labs first-generation deep learning (Gaudi) training and (Goya) inference processors, Voyager is funded by the National Science Foundation’s Advanced Computing Systems &amp; Services Program as a Category II system and will be operated for 5 years, starting with an initial 3-year exploratory test-bed phase that will be followed by a 2-year allocated production phase for the national research community. Its AI-focused hardware features several innovative components, including fully-programmable tensor processing cores, high-bandwidth memory, and integrated, on-chip RDMA over Converged Ethernet network interfaces. In addition, Habana’s SynapseAI software suite provides seamless integration to popular machine learning frameworks like PyTorch and TensorFlow for end users. Here, we describe the design motivation for Voyager, its system architecture, software and user environment, initial benchmarking results, and the early science use cases and applications currently being ported to and deployed on the system.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {278–282},
numpages = {5},
keywords = {AI-focused hardware, benchmarking, deep learning, scientific applications, system deployment},
location = {Portland, OR, USA},
series = {PEARC '23}
}

@article{10.1007/s10055-023-00844-6,
author = {Howard, Matt C. and Davis, Maggie M.},
title = {A Meta-analysis of augmented reality programs for education and training},
year = {2023},
issue_date = {Dec 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {4},
issn = {1359-4338},
url = {https://doi.org/10.1007/s10055-023-00844-6},
doi = {10.1007/s10055-023-00844-6},
abstract = {The application of augmented reality (AR) for education and training has grown dramatically in recent years, resulting in an expansive research domain within a relatively short amount of time. Two primary goals of the current article are to (a) summarize this literature by determining the overall effectiveness of AR programs relative to alternative comparisons and (b) assess the extent that AR program effectiveness is influenced by aspects of hardware, software, outcome, context, and methodology. A meta-analysis of over 250 studies supports that AR programs produce learning outcomes that are, on average, three-fifths of a standard deviation larger than alternative comparisons. Our results surprisingly show that AR programs using head-mounted displays produce significantly smaller effects than those using other output hardware (e.g., smartphones and tablets), and programs using image recognition are no more effective than those using alternative input methods (e.g., QR codes). We further find that most other aspects do not significantly influence observed program effectiveness; however, studies with younger participants produced significantly larger effects, and naturalistic studies produced significantly larger effects than laboratory studies. In our discussion, we utilize these findings to suggest promising theoretical perspectives for the study of AR, and we highlight methodological practices that can produce more accurate research moving forward. Thus, the current article summarizes research on AR education and training programs, identifies aspects that do and do not influence program efficacy, and provides several avenues for future research and practice.},
journal = {Virtual Real.},
month = {aug},
pages = {2871–2894},
numpages = {24},
keywords = {Augmented reality, Mixed reality, Education, Training, Learning, Meta-analysis}
}

@article{10.1007/s11704-023-2570-6,
author = {Liu, Na and Zhang, Fan and Chang, Liang and Duan, Fuqing},
title = {Scattering-based hybrid network for facial attribute classification},
year = {2023},
issue_date = {Jun 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-023-2570-6},
doi = {10.1007/s11704-023-2570-6},
abstract = {Face attribute classification (FAC) is a high-profile problem in biometric verification and face retrieval. Although recent research has been devoted to extracting more delicate image attribute features and exploiting the inter-attribute correlations, significant challenges still remain. Wavelet scattering transform (WST) is a promising non-learned feature extractor. It has been shown to yield more discriminative representations and outperforms the learned representations in certain tasks. Applied to the image classification task, WST can enhance subtle image texture information and create local deformation stability. This paper designs a scattering-based hybrid block, to incorporate frequency-domain (WST) and image-domain features in a channel attention manner (Squeeze-and-Excitation, SE), termed WS-SE block. Compared with CNN, WS-SE achieves a more efficient FAC performance and compensates for the model sensitivity of the small-scale affine transform. In addition, to further exploit the relationships among the attribute labels, we propose a learning strategy from a causal view. The cause attributes defined using the causality-related information can be utilized to infer the effect attributes with a high confidence level. Ablative analysis experiments demonstrate the effectiveness of our model, and our hybrid model obtains state-of-the-art results in two public datasets.},
journal = {Front. Comput. Sci.},
month = {nov},
numpages = {12},
keywords = {wavelet scattering transform, causality-related learning, facial attribute classification}
}

@proceedings{10.1145/3565287,
title = {MobiHoc '23: Proceedings of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ACM MobiHoc is a premier international annual conference with a highly selective single-track technical program dedicated to addressing the challenges emerging from networked systems that must operate in the face of dynamics.},
location = {Washington, DC, USA}
}

@article{10.1016/j.neunet.2023.03.017,
author = {Sun, Hang and Li, Bohui and Dan, Zhiping and Hu, Wei and Du, Bo and Yang, Wen and Wan, Jun},
title = {Multi-level Feature Interaction and Efficient Non-Local Information Enhanced Channel Attention for image dehazing},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {163},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2023.03.017},
doi = {10.1016/j.neunet.2023.03.017},
journal = {Neural Netw.},
month = {jun},
pages = {10–27},
numpages = {18},
keywords = {Image dehazing, Multi-level feature interaction, Non-local information, Channel attention}
}

@article{10.1007/s11042-023-15005-9,
author = {Sitaula, Chiranjibi and Shahi, Tej Bahadur and Marzbanrad, Faezeh and Aryal, Jagannath},
title = {Recent advances in scene image representation and classification},
year = {2023},
issue_date = {Jan 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {83},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-023-15005-9},
doi = {10.1007/s11042-023-15005-9},
abstract = {With the rise of deep learning algorithms nowadays, scene image representation methods have achieved a significant performance boost, particularly in accuracy, in classification. However, the performance is still limited because the scene images are mostly complex having higher intra-class dissimilarity and inter-class similarity problems. To deal with such problems, there have been several methods proposed in the literature with their advantages and limitations. A detailed study of previous works is necessary to understand their advantages and disadvantages in image representation and classification problems. In this paper, we review the existing scene image representation methods that are being widely used for image classification. For this, we, first, devise the taxonomy using the seminal existing methods proposed in the literature to this date using deep learning (DL)-based, computer vision (CV)-based, and search engine (SE)-based methods. Next, we compare their performance both qualitatively (e.g., quality of outputs, pros/cons, etc.) and quantitatively (e.g., accuracy). Last, we speculate on the prominent research directions in scene image representation tasks using keyword growth and timeline analysis. Overall, this survey provides in-depth insights and applications of recent scene image representation methods under three different methods.},
journal = {Multimedia Tools Appl.},
month = {jun},
pages = {9251–9278},
numpages = {28},
keywords = {Computer vision, Classification, Deep learning, Machine learning, Scene image representation}
}

@article{10.1145/3632894,
author = {Li, Xiang and Zhou, Xiangyu and Dong, Rui and Zhang, Yihong and Wang, Xinyu},
title = {Efficient Bottom-Up Synthesis for Programs with Local Variables},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632894},
doi = {10.1145/3632894},
abstract = {We propose a new synthesis algorithm that can efficiently search programs with local variables (e.g., those introduced by lambdas). Prior bottom-up synthesis algorithms are not able to evaluate programs with free local variables, and therefore cannot effectively reduce the search space of such programs (e.g., using standard observational equivalence reduction techniques), making synthesis slow. Our algorithm can reduce the space of programs with local variables. The key idea, dubbed lifted interpretation, is to lift up the program interpretation process, from evaluating one program at a time to simultaneously evaluating all programs from a grammar. Lifted interpretation provides a mechanism to systematically enumerate all binding contexts for local variables, thereby enabling us to evaluate and reduce the space of programs with local variables. Our ideas are instantiated in the domain of web automation. The resulting tool, Arborist, can automate a significantly broader range of challenging tasks more efficiently than state-of-the-art techniques including WebRobot and Helena.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {52},
numpages = {29},
keywords = {Observational Equivalence, Program Synthesis, Web Automation}
}

@article{10.1016/j.compag.2023.107987,
author = {Zeng, Fanguo and Li, Bin and Wang, Haifeng and Zhu, Jun and Jia, Nan and Zhao, Yuliang and Zhao, Wenwen},
title = {Detection of calf abnormal respiratory behavior based on frame difference and improved YOLOv5 method},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {211},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2023.107987},
doi = {10.1016/j.compag.2023.107987},
journal = {Comput. Electron. Agric.},
month = {aug},
numpages = {16},
keywords = {Calf, Abnormal respiratory behavior, Frame difference, YOLOv5, Machine vision}
}

@inproceedings{10.5555/3618408.3618866,
author = {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
title = {Looped transformers as programmable computers},
year = {2023},
publisher = {JMLR.org},
abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. Using this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms to programs that can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {458},
numpages = {45},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.5555/3620237.3620576,
author = {Vyas, Parjanya and Waheed, Asim and Aafer, Yousra and Asokan, N.},
title = {Auditing framework APIs via inferred app-side security specifications},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {In this work, we explore auditing access control implementations of Android private framework APIs by leveraging app-side security specifications. The seemingly straightforward auditing task faces significant challenges. It requires extracting unconventional security indicators and understanding their relevance to private framework APIs. More importantly, addressing these challenges requires relying on uncertain hints. We hence, introduce Bluebird, a security auditing platform for Android APIs, that mimics a human expert. Bluebird seamlessly fuses human-like understanding of app-side logic with statically-derived program semantics using probabilistic inference to detect access control gaps in private APIs.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {339},
numpages = {17},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{10.1016/j.sigpro.2023.109047,
author = {Wei, Shuang and Pan, Heng and He, Di and Tian, Longwei},
title = {A deep-learning-based time of arrival estimation using kernel sparse encoding scheme},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {209},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2023.109047},
doi = {10.1016/j.sigpro.2023.109047},
journal = {Signal Process.},
month = {aug},
numpages = {8},
keywords = {Time of arrival (TOA) estimation, Off-grid estimation, Deep learning, Sparse representation, Kernel encoding}
}

@inproceedings{10.1145/3539618.3591860,
author = {Chaidaroon, Suthee and Zhang, Xiao and Subramaniyam, Shruti and Svajlenko, Jeffrey and Shourya, Tanya and Keivanloo, Iman and Joy, Ria},
title = {Improving Programming Q&amp;A with Neural Generative Augmentation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591860},
doi = {10.1145/3539618.3591860},
abstract = {Knowledge-intensive programming Q&amp;A is an active research area in industry. Its application boosts developer productivity by aiding developers in quickly finding programming answers from the vast amount of information on the Internet. In this study, we propose ProQANS and its variants ReProQANS and ReAugProQANS to tackle programming Q&amp;A. ProQANS is a neural search approach that leverages unlabeled data on the Internet (such as StackOverflow) to mitigate the cold-start problem. ReProQANS extends ProQANS by utilizing reformulated queries with a novel triplet loss. We further use an auxiliary generative model to augment the training queries, and design a novel dual triplet loss function to adapt these generated queries, to build another variant of ReProQANS termed as ReAugProQANS. In our empirical experiments, we show ReProQANS has the best performance when evaluated on the in-domain test set, while ReAugProQANS has the superior performance on the out-of-domain real programming questions, by outperforming the state-of-the-art model by up to 477% lift on the MRR metric respectively. The results suggest their robustness to previously unseen questions and its wide application to real programming questions.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3390–3394},
numpages = {5},
keywords = {deep learning, nlp, q&amp;a, text generation},
location = {<conf-loc>, <city>Taipei</city>, <country>Taiwan</country>, </conf-loc>},
series = {SIGIR '23}
}

@article{10.1016/j.cie.2023.109605,
author = {El-Brawany, Mohamed A. and Adel Ibrahim, Dina and Elminir, Hamdy K. and Elattar, Hatem M. and Ramadan, E.A.},
title = {Artificial intelligence-based data-driven prognostics in industry: A survey},
year = {2023},
issue_date = {Oct 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {184},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2023.109605},
doi = {10.1016/j.cie.2023.109605},
journal = {Comput. Ind. Eng.},
month = {oct},
numpages = {13},
keywords = {Data driven prognostics, Prognostics and health management, Remaining useful life, Machine learning prognostics, Deep learning prognostics, Deep learning architectures, Artificial neural network, Performance metrics, PHM, ML, DL, CBM, RUL, EOL, AI, ANN, GPR, BN, HMM, SVM, RVM, SVR, SOC, EKF, AFSA, HI, FVS, PF, RF, FFBP, DRL, MOP, MOTSP, IIOT, FFNN, RBM, AE, SAE, CNN, RNN, GAN, DAE, DBN, DBM, NM, WPT, TEP, NLP, DCNN, RMSE, MAE, CNC, LSTM, GRU, KPCA, VAE, SGD, BDL, DA, GPU}
}

@article{10.1145/3642979.3643004,
author = {Anand, Avishek and Pera, Maria Soledad and Heuss, Maria and V, Venktesh and Corsi, Matteo},
title = {Report on the 21st Dutch-Belgian Information Retrieval Workshop (DIR 2023)},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3642979.3643004},
doi = {10.1145/3642979.3643004},
abstract = {This year, the 21st edition of the Dutch-Belgian Information Retrieval Workshop (DIR), was hosted by TU Delft. Following a series of online interactions due to the COVID-19 pandemic, this edition returned to its traditional in-person format. In this report, we provide a summary of the program and discuss key insights and lessons learned gleaned from the event.Date: 27 November 2023.Website: https://dir2023.github.io/DIR2023/.},
journal = {SIGIR Forum},
month = {jan},
articleno = {22},
numpages = {5}
}

@inproceedings{10.1145/3583780.3615488,
author = {Zhang, Cong and Liu, Dongyang and Zuo, Lin and Feng, Junlan and Deng, Chao and Sun, Jian and Zeng, Haitao and Zhao, Yaohong},
title = {Multi-gate Mixture-of-Contrastive-Experts with Graph-based Gating Mechanism for TV Recommendation},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615488},
doi = {10.1145/3583780.3615488},
abstract = {With the rapid development of smart TV, TV recommendation is attracting more and more users. TV users usually distribute in multiple regions with different cultures and hence have diverse TV program preferences. From the perspective of engineering practice and performance improvement, it's very essential to model users from multiple regions with one single model. In previous work, Multi-gate Mixture-of-Expert (MMoE) has been widely adopted in multi-task and multi-domain recommendation scenarios. In practice, however, we first observe the embeddings generated by experts tend to be homogeneous which may result in high semantic similarities among embeddings that reduce the capability of Multi-gate Mixture-of-Expert (MMoE) model. Secondly, we also find there are lots of commonalities and differences between multiple regions regarding user preferences. Therefore, it's meaningful to model the complicated relationships between regions. In this paper, we first introduce contrastive learning to overcome the expert representation degeneration problem. The embeddings of two augmented samples generated by the same experts are pushed closer to enhance the alignment, and the embeddings of the same samples generated by different experts are pushed away in vector space to improve uniformity. Then we propose a Graph-based Gating Mechanism to empower typical Multi-gate Mixture-of-Experts. Graph-based MMoE is able to recognize the commonalities and differences among multiple regions by introducing a Graph Neural Network (GNN) with region similarity prior. We name our model Multi-gate Mixture-of-Contrastive-Experts model with Graph-based Gating Mechanism (MMoCEG). Extensive offline experiments and online A/B tests on a commercial TV service provider over 100 million users and 2.3 million items demonstrate the efficacy of MMoCEG compared to the existing models.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4938–4944},
numpages = {7},
keywords = {contrastive learning, graph neural network, multi-gate mixture of experts, multi-region TV recommendation},
location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
series = {CIKM '23}
}

@article{10.1145/3622826,
author = {Wang, Yu and Wang, Ke and Wang, Linzhang},
title = {An Explanation Method for Models of Code},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622826},
doi = {10.1145/3622826},
abstract = {This paper introduces a novel method, called WheaCha, for explaining the predictions of code models. Similar to attribution methods, WheaCha seeks to identify input features that are responsible for a particular prediction that models make. On the other hand, it differs from attribution methods in crucial ways. Specifically, WheaCha separates an input program into "wheat" (i.e., defining features that are the reason for which models predict the label that they predict) and the rest "chaff" for any given prediction. We realize WheaCha in a tool, HuoYan, and use it to explain four prominent code models: code2vec, seq-GNN, GGNN, and CodeBERT. Results show that (1) HuoYan is efficient — taking on average under twenty seconds to compute wheat for an input program in an end-to-end fashion (i.e., including model prediction time); (2) the wheat that all models use to make predictions is predominantly comprised of simple syntactic or even lexical properties (i.e., identifier names); (3) neither the latest explainability methods for code models (i.e., SIVAND and CounterFactual Explanations) nor the most noteworthy attribution methods (i.e., Integrated Gradients and SHAP) can precisely capture wheat. Finally, we set out to demonstrate the usefulness of WheaCha, in particular, we assess if WheaCha’s explanations can help end users to identify defective code models (e.g., trained on mislabeled data or learned spurious correlations from biased data). We find that, with WheaCha, users achieve far higher accuracy in identifying faulty models than SIVAND, CounterFactual Explanations, Integrated Gradients and SHAP.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {250},
numpages = {27},
keywords = {Defining Features, Explainability Method, Models of Code}
}

@article{10.1016/j.knosys.2023.111037,
author = {Liu, Jiaming and Mao, Yiqiao and Huang, Zhen and Ye, Yangdong},
title = {A Bottleneck Network with Light Attention for Multimodal Clustering},
year = {2023},
issue_date = {Nov 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {280},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2023.111037},
doi = {10.1016/j.knosys.2023.111037},
journal = {Know.-Based Syst.},
month = {nov},
numpages = {16},
keywords = {Multimodal clustering, Unsupervised learning, Attention mechanism, Superfluity information}
}

@inproceedings{10.1007/978-981-99-5177-2_8,
author = {Mahaini, Mohamad Imad and Li, Shujun},
title = {Cyber Security Researchers on&nbsp;Online Social Networks: From the&nbsp;Lens of&nbsp;the&nbsp;UK’s ACEs-CSR on&nbsp;Twitter},
year = {2023},
isbn = {978-981-99-5176-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-5177-2_8},
doi = {10.1007/978-981-99-5177-2_8},
abstract = {Much work in the literature has studied different types of cyber security related users and communities on OSNs, such as activists, hacktivists, hackers, cyber criminals. A few studies also covered no-expert users who discussed cyber security related topics, however, to the best of our knowledge, none has studied activities of cyber security researchers on OSNs. This paper fills this gap using a data-driven analysis of the presence of the UK’s Academic Centres of Excellence in Cyber Security Research (ACEs-CSR) on Twitter. We created machine learning classifiers to identify cyber security and research related accounts. Then, starting from 19 seed accounts of the ACEs-CSR, a social network graph of 1,817 research-related accounts that were followers or friends of at least one ACE-CSR was constructed. We conducted a comprehensive analysis of the data we collected: a social structural analysis of the social graph; a topic modelling analysis to identify the main topics discussed publicly by researchers in ACEs-CSR network, and a sentiment analysis of how researchers perceived the ACE-CSR programme and the ACEs-CSR. Our study revealed several findings: 1) graph-based analysis and community detection algorithms are useful in detecting sub-communities of researchers to help understand how they are formed and what they represent; 2) topic modelling can identify topics discussed by cyber security researchers (e.g., cyber security incidents, vulnerabilities, threats, privacy, data protection laws, cryptography, research, education, cyber conflict, and politics); and 3) sentiment analysis showed a generally positive sentiment about the ACE-CSR programme and ACEs-CSR. Our work showed the feasibility and usefulness of large-scale automated analyses of cyber security researchers on Twitter.},
booktitle = {Security and Privacy in Social Networks and Big Data: 9th International Symposium, SocialSec 2023, Canterbury, UK, August 14–16, 2023, Proceedings},
pages = {129–148},
numpages = {20},
keywords = {Cyber Security, Machine Learning, Online Social Network, Community Detection, Natural Language Processing, Topic Modelling, Sentiment Analysis, Twitter},
location = {Canterbury, United Kingdom}
}

@article{10.1145/3642979.3642994,
author = {Litvak, Marina and Rabaev, Irina and Campos, Ricardo and Jorge, Alípio M. and Jatowt, Adam},
title = {Report on the 1st Workshop on Implicit Author Characterization from Texts for Search and Retrieval (IACT 2023) at SIGIR 2023},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3642979.3642994},
doi = {10.1145/3642979.3642994},
abstract = {The first edition of the International Workshop on Implicit Author Characterization from Texts for Search and Retrieval (IACT'23) was held on July 27th, 2023, in conjunction with the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) in Taipei, Taiwan. To support both online and in-person participation, the workshop was held as a hybrid event. Online participation was allowed using the EventX platform. During the course of the day, about 15 attendees had the opportunity to follow up and discuss the recent advances in topics related to identifying and extracting implicit information about authors (e.g., human or AI) from texts and using it in IR tasks. The workshop program included two invited keynotes, four paper presentations, and the report of the ColiE competition which was held as a part of the workshop1. The proceedings of the workshop are available online at https://ceur-ws.org/Vol-3477/.Date: 27 July 2023.Website: https://en.sce.ac.il/news/iact23.},
journal = {SIGIR Forum},
month = {jan},
articleno = {12},
numpages = {6}
}

@inproceedings{10.5555/3618408.3619304,
author = {Liu, Guan-Ting and Hu, En-Pei and Cheng, Pu-Jen and Lee, Hung-Yi and Sun, Shao-Hua},
title = {Hierarchical programmatic reinforcement learning via learning to compose programs},
year = {2023},
publisher = {JMLR.org},
abstract = {Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pregenerated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors. The experimental results in the Karel domain show that our proposed framework outperforms baselines. The ablation studies confirm the limitations of LEAPS and justify our design choices.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {896},
numpages = {26},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.1145/3577190.3614231,
author = {Domingo, Cecilia},
title = {Recording multimodal pair-programming dialogue for reference resolution by conversational agents},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614231},
doi = {10.1145/3577190.3614231},
abstract = {Pair programming is a collaborative technique which has proven highly beneficial in terms of the code produced and the learning gains for programmers. With recent advances in Programming Language Processing (PLP), numerous tools have been created that assist programmers in non-collaborative settings (i.e., where the technology provides users with a solution, instead of discussing the problem to develop a solution together). How can we develop AI that can assist in pair programming, a collaborative setting? To tackle this task, we begin by gathering multimodal dialogue data which can be used to train systems in a basic subtask of dialogue understanding: multimodal reference resolution, i.e., understanding which parts of a program are being mentioned by users through speech or by using the mouse and keyboard.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {731–735},
numpages = {5},
keywords = {dialogue, multimodality, pair programming, reference resolution},
location = {<conf-loc>, <city>Paris</city>, <country>France</country>, </conf-loc>},
series = {ICMI '23}
}

@inproceedings{10.5555/3620237.3620370,
author = {Zhang, Zhuo and Tao, Guanhong and Shen, Guangyu and An, Shengwei and Xu, Qiuling and Liu, Yingqi and Ye, Yapeng and Wu, Yaoxuan and Zhang, Xiangyu},
title = {PELICAN: exploiting backdoors of naturally trained deep learning models in binary code analysis},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Deep Learning (DL) models are increasingly used in many cyber-security applications and achieve superior performance compared to traditional solutions. In this paper, we study backdoor vulnerabilities in naturally trained models used in binary analysis. These backdoors are not injected by attackers but rather products of defects in datasets and/or training processes. The attacker can exploit these vulnerabilities by injecting some small fixed input pattern (e.g., an instruction) called backdoor trigger to their input (e.g., a binary code snippet for a malware detection DL model) such that misclassification can be induced (e.g., the malware evades the detection). We focus on transformer models used in binary analysis. Given a model, we leverage a trigger inversion technique particularly designed for these models to derive trigger instructions that can induce misclassification. During attack, we utilize a novel trigger injection technique to insert the trigger instruction(s) to the input binary code snippet. The injection makes sure that the code snippets' original program semantics are preserved and the trigger becomes an integral part of such semantics and hence cannot be easily eliminated. We evaluate our prototype PELICAN on 5 binary analysis tasks and 15 models. The results show that PELICAN can effectively induce misclassification on all the evaluated models in both white-box and black-box scenarios. Our case studies demonstrate that PELICAN can exploit the backdoor vulnerabilities of two closed-source commercial tools.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {133},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{10.1287/mnsc.2023.4900,
author = {Augustin, Patrick and Rubtsov, Alexey and Shin, Donghwa},
title = {The Impact of Derivatives on Spot Markets: Evidence from the Introduction of Bitcoin Futures Contracts},
year = {2023},
issue_date = {November 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {69},
number = {11},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2023.4900},
doi = {10.1287/mnsc.2023.4900},
abstract = {Cryptocurrencies provide a unique opportunity to identify how derivatives impact spot markets. They are fully fungible and trade across multiple spot exchanges at different prices, and futures contracts were selectively introduced on Bitcoin (BTC) exchange rates against the U.S. dollar (USD) in December 2017. Following the futures introduction, we find a significantly greater increase in cross-exchange price synchronicity for BTC–USD relative to other exchange rate pairs as demonstrated by an increase in price correlations and a reduction in arbitrage opportunities and volatility. We also find support for an increase in price efficiency, market quality, and liquidity. The evidence suggests that futures contracts allowed investors to circumvent arbitrage frictions associated with short-sale constraints, arbitrage risk associated with block confirmation time, and market segmentation. Overall, our analysis supports the view that the introduction of BTC–USD futures was beneficial to the Bitcoin spot market by making the underlying prices more informative.This paper was accepted by Will Cong, Special Section of Management Science: Blockchains and Crypto Economics.Funding: The authors acknowledge financial support from the Global Risk Institute. P. Augustin acknowledges financial support from the Canadian Derivatives Institute and from the Canada Research Chair Program of the Social Sciences and Humanities Research Council Canada. The paper has benefited significantly from a fellow visit of P. Augustin at the Center for Advanced Studies Foundations of Law and Finance funded by the German Research Foundation, project FOR 2774, and from a visiting position of P. Augustin at the finance department of the University of Luxembourg facilitated through the Inter Mobility Programme of the Luxembourg National Research Fund.Supplemental Material: The online appendix and data are available at .},
journal = {Manage. Sci.},
month = {nov},
pages = {6752–6776},
numpages = {25},
keywords = {Bitcoin, blockchain, cryptocurrencies, derivatives, fintech, regulation}
}

@inproceedings{10.1007/978-3-031-45784-5_7,
author = {Carbone, Marco and Marin, Sonia and Schürmann, Carsten},
title = {A Logical Interpretation of&nbsp;Asynchronous Multiparty Compatibility},
year = {2023},
isbn = {978-3-031-45783-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-45784-5_7},
doi = {10.1007/978-3-031-45784-5_7},
abstract = {Session types specify the protocols that communicating processes must follow in a concurrent system. When composing two or more processes, a session typing system must check whether such processes are compatible, i.e., that all sent messages are eventually received and no deadlock ever occurs. After the propositions-as-types paradigm, relating session types to linear logic, previous work has shown that duality, in the binary case, and more generally coherence, in the multiparty case, are sufficient syntactic conditions to guarantee compatibility for two or more processes, yet do not characterise all compatible set of processes.In this work, we generalise duality/coherence to a notion of forwarder compatibility. Forwarders are specified as a restricted family of proofs in linear logic, therefore defining a specific set of processes that can act as middleware by transfering messages without using them. As such, they can guide a network of processes to execute asynchronously. Our main result establishes forwarder compatibility as a sufficient and necessary condition to fully capture all well-typed multiparty compatible processes.},
booktitle = {Logic-Based Program Synthesis and Transformation: 33rd International Symposium, LOPSTR 2023, Cascais, Portugal, October 23-24, 2023, Proceedings},
pages = {99–117},
numpages = {19},
keywords = {Linear Logic, Session Types, Process Compatibility},
location = {Cascais, Portugal}
}

@article{10.1145/3586180,
author = {Walshe, Thomas and Simpson, Andrew},
title = {Towards a Greater Understanding of Coordinated Vulnerability Disclosure Policy Documents},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3586180},
doi = {10.1145/3586180},
abstract = {Bug bounty programmes and vulnerability disclosure programmes, collectively referred to as Coordinated Vulnerability Disclosure (CVD) programmes, open up an organisation’s assets to the inquisitive gaze of (often eager) white-hat hackers. Motivated by the question What information do organisations convey to hackers through public CVD policy documents?, we aim to better understand the information available to hackers wishing to participate in the search for vulnerabilities. As such, in this article we consider three key issues. First, to address the differences in the legal language communicated to hackers, it is necessary to understand the formal constraints by which hackers must abide. Second, it is beneficial to understand the variation that exists in the informal constraints that are communicated to hackers through a variety of institutional elements. Third, for organisations wishing to better understand the commonplace elements that form current policy documents, we offer broad analysis of the components frequently included therein and identify gaps in programme policies.We report the results of a quantitative study, leveraging deep learning based natural language processing models, providing insights into the policy documents that accompany the CVD programmes of thousands of organisations, covering both stand-alone programmes and those hosted on 13 bug bounty programmes. We found that organisations often inadequately convey the formal constraints that are applicable to hackers, requiring hackers to have a deep understanding of the laws that underpin safe and legal security research. Furthermore, a lack of standardisation across similar policy components is prevalent, and may lead to a decreased understanding of the informal constraints placed upon hackers when searching for and disclosing vulnerabilities. Analysis of the institutional elements included in the policy documents of organisations reveals insufficient inclusion of many key components. Namely, legal information and information pertaining to restrictions on the backgrounds of hackers is found to be absent in a majority of policies analysed. Finally, to assist ongoing research, we provide novel annotated policy datasets that include human-labelled annotations at both the sentence and paragraph level, covering a broad range of CVD programme backgrounds.},
journal = {Digital Threats},
month = {aug},
articleno = {29},
numpages = {36},
keywords = {Bug bounty programmes, vulnerability disclosure, software security}
}

@proceedings{10.1145/3624007,
title = {GPCE 2023: Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’23). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming languages research communities.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3633083.3633161,
author = {Todorova, Christina and Sharkov, George and Aldewereld, Huib and Leijnen, Stefan and Dehghani, Alireza and Marrone, Stefano and Sansone, Carlo and Lynch, Maurice and Pugh, John and Singh, Tarry and Mezei, Kitti and Antal, Péter and Hanák, Péter and Barducci, Alessandro and Perez-Tellez, Fernando and Gargiulo, Francesco},
title = {The European AI Tango: Balancing Regulation Innovation and Competitiveness},
year = {2023},
isbn = {9798400716461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633083.3633161},
doi = {10.1145/3633083.3633161},
abstract = {In the past few years, the EU has shown a growing commitment to address the rapid transformations brought about by the latest Artificial Intelligence (AI) developments by increasing efforts in AI regulation. Nevertheless, despite the growing body of technical knowledge and progress, the governance of AI-intensive technologies remains dynamic and challenging. A mounting chorus of experts have been sharing their reservations regarding an overemphasis on regulation in Europe. Among their core arguments is the concern that such an approach might hinder innovation within the AI arena. This concern resonates particularly strongly compared to the United States and Asia, where AI-driven innovation appears to be surging ahead, potentially leaving Europe behind. The current contribution is a position paper emphasising the need for balanced AI governance to foster ethical innovation, reliability, and competitiveness of European technology. This paper only explores recent AI regulations and upcoming European laws relevant to the topic to ensure conciseness while underscoring Europe’s role in the global AI landscape. The authors analyse European governance approaches and their impact, especially on SMEs and startups, offering a comparative view of global regulatory efforts. We address the complexities of creating a comprehensive, human-centred AI master’s programme for higher education and the importance of ethical AI education. Finally, we discuss how Europe can seize opportunities to promote ethical and reliable AI progress through education, fostering a balanced approach to regulation and enhancing young professionals’ understanding of ethical and legal aspects.},
booktitle = {Proceedings of the 2023 Conference on Human Centered Artificial Intelligence: Education and Practice},
pages = {2–8},
numpages = {7},
keywords = {AI Education, Artificial Intelligence, Business, Ethics, Human-Centred, Innovation, Regulation, SMEs},
location = {<conf-loc>, <city>Dublin</city>, <country>Ireland</country>, </conf-loc>},
series = {HCAIep '23}
}

@inproceedings{10.1145/3630970.3631044,
author = {Catalino Piquet, Bryan Usias and Hernandez Cabrera, Horus Alejandro and Hernandez-Calderon, Jose Guillermo and Montane-Jimenez, Luis Gerardo and Soto-Mendoza, Valeria},
title = {Dashboard design for Key Performance Indicators visualization of STEAM government initiatives: A case study},
year = {2024},
isbn = {9798400716577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630970.3631044},
doi = {10.1145/3630970.3631044},
abstract = {The teaching of mathematics at basic educational levels is essential for the development of student’s cognitive abilities. Therefore various programs worldwide have emerged to promote the learning/teaching of mathematics and other complementary disciplines to improve students’ skills. This has motivated educational and local government institutions to design strategies that help raise the level of knowledge in mathematics from an early age based on STEAM methodology. Considering a case study of a Mexican government institution initiative, the program "Math for Everyone", and following a mixed methodology, a dashboard that uses spatial data and performance data is designed to help visualize key performance indicators related to the "Math for Everyone" program activities for authorities in charge of state education policy. Through a series of eight iterative design sessions, an evolved version, 1.5, was synthesized and subsequently validated. This version serves as a valuable tool for program coordinators and academic stakeholders, facilitating their decision-making and oversight responsibilities.},
booktitle = {Proceedings of the XI Latin American Conference on Human Computer Interaction},
articleno = {4},
numpages = {9},
keywords = {Information visualization, Teaching mathematics, Visualization, Visualization systems and tools},
location = {<conf-loc>, <city>Puebla</city>, <country>Mexico</country>, </conf-loc>},
series = {CLIHC '23}
}

@inproceedings{10.1145/3615335.3623016,
author = {Hsiao, Margaret},
title = {Public Comments, Private Interests},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615335.3623016},
doi = {10.1145/3615335.3623016},
abstract = {When rulemaking transitioned online in 2003, the U.S. federal government's e-rulemaking program was designed to make policy deliberations more democratic through Regulations.gov. However, the field of technical and professional communication (TPC) has demonstrated that online spaces can recreate existing social inequities. This research found thousands of public comments that were identical to each other in proposed rules and were traced back to private organizations trying to influence policy, but they may provide incomplete or inaccurate information about the rulemaking process. Technical communicators who understand the policy ecosystem can facilitate a more meaningful dialogue between citizens and government agencies.},
booktitle = {Proceedings of the 41st ACM International Conference on Design of Communication},
pages = {89–95},
numpages = {7},
keywords = {Policymaking, Public comments},
location = {<conf-loc>, <city>Orlando</city>, <state>FL</state>, <country>USA</country>, </conf-loc>},
series = {SIGDOC '23}
}

@inproceedings{10.1007/978-3-031-37105-9_24,
author = {Gaballo, Marika and Mecca, Beatrice and Todella, Elena},
title = {Urban Sustainability Towards European Missions and Challenges: Where Do We Stand?},
year = {2023},
isbn = {978-3-031-37104-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-37105-9_24},
doi = {10.1007/978-3-031-37105-9_24},
abstract = {The centrality of cities and urban processes in sustainable development and the green transition has been recognized by the 2030 Agenda for Sustainable Development, the New Urban Agenda and the Green Deal. In this context, this paper aims to explore the conceptualizations of cities that have received attention, such as “inclusive city”, “knowledge city”, “low carbon city”, “resilient city”, “smart city”, and “sustainable city”. Since measuring progress through the reaching of sustainable development in contemporary cities is a current and relevant issue, the paper reflects on both theory and practice levels: (i) by developing a conceptualization of current challenges in pursuing urban sustainability; (ii) by discussing available (and/or needed) measurement and assessment tools, such as indicators, to support the construction and evaluation of such sustainable urban policies. In doing so, the research specifically focuses on the European Union’s (EU) framework for urban sustainability, outlined in the Horizon Europe Program for Research and Innovation for 2021–2027. Accordingly, the Strategic Plan 2021–2024 and the work programmes 2023–2024 of “Clusters” and “Missions” are analyzed to understand where we stand towards European challenges in urban sustainability. Moreover, the paper explores indicators provided by different European institutions in order to outline a set of indicators useful for monitoring the city conceptualizations encouraged at the European level.},
booktitle = {Computational Science and Its Applications – ICCSA 2023 Workshops: Athens, Greece, July 3–6, 2023, Proceedings, Part I},
pages = {354–373},
numpages = {20},
keywords = {Sustainable Cities, European Union, Urban Sustainability, Indicator framework, Horizon Europe},
location = {Athens, Greece}
}

@inproceedings{10.1007/978-3-031-37765-5_5,
author = {Beillahi, Sidi Mohamed and Bouajjani, Ahmed and Enea, Constantin},
title = {Comparing Causal Convergence Consistency Models},
year = {2023},
isbn = {978-3-031-37764-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-37765-5_5},
doi = {10.1007/978-3-031-37765-5_5},
abstract = {In distributed databases, the CAP theorem establishes that a distributed storage system can only ensure two out of three properties: strong data consistency (i.e., reads returning the most recent writes), availability, or partition tolerance. Modern distributed storage systems prioritize performance and availability over strong consistency and thus offer weaker consistency models such as causal consistency.This paper explores several variations of causal consistency (CC) that guarantee state convergence among replicas, meaning that all distributed replicas converge towards the same consistent state. We investigate a log-based CC model, a commutativity-based CC model, and a global sequence protocol-based CC model. To facilitate our study of the relationships between these models, we use a common formalism to define them. We then show that the log-based CC model is the weakest, while the commutativity-based CC and the global sequence protocol-based CC models are incomparable. We also provide sufficient conditions for a given application program to be robust against one CC model versus another, meaning that the program has the same behavior when executed over databases implementing the two CC models.},
booktitle = {Networked Systems: 11th International Conference, NETYS 2023, Benguerir, Morocco, May 22–24, 2023, Proceedings},
pages = {62–77},
numpages = {16},
location = {Benguerir, Morocco}
}

@article{10.1007/s11390-023-3272-0,
author = {Wang, Chen-Xi and Shan, Yi-Zhou and Zuo, Peng-Fei and Cui, Hui-Min},
title = {Reinvent Cloud Software Stacks for Resource Disaggregation},
year = {2023},
issue_date = {Sep 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {38},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-023-3272-0},
doi = {10.1007/s11390-023-3272-0},
abstract = {Due to the unprecedented development of low-latency interconnect technology, building large-scale disaggregated architecture is drawing more and more attention from both industry and academia. Resource disaggregation is a new way to organize the hardware resources of datacenters, and has the potential to overcome the limitations, e.g., low resource utilization and low reliability, of conventional datacenters. However, the emerging disaggregated architecture brings severe performance and latency problems to the existing cloud systems. In this paper, we take memory disaggregation as an example to demonstrate the unique challenges that the disaggregated datacenter poses to the existing cloud software stacks, e.g., programming interface, language runtime, and operating system, and further discuss the possible ways to reinvent the cloud systems.},
journal = {J. Comput. Sci. Technol.},
month = {sep},
pages = {949–969},
numpages = {21},
keywords = {cloud computing, resource disaggregation, datacenter, program semantics}
}

@inproceedings{10.1145/3629296.3629298,
author = {Wu, Hao and Fa, Daidong and Wu, Xiaoling and Tan, Weijun and Chang, Xiwen and Gao, Ying and Weng, Jinta},
title = {Research on the Construction of Intelligent Programming Platform Based on AI-generated Content},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629298},
doi = {10.1145/3629296.3629298},
abstract = {Programming education can play an important role in cultivating students' higher-order thinking ability and enhancing the competitiveness of an intelligent society. However, a series of problems still exist: the lack of high-quality programming teachers, the low degree of personalization, and the insufficient learning interaction. This study constructs a novel programming learning model based on AI-generated content technology under the guidance of constructivism learning theory and cognitive load theory. The model divides programming learning into the process of learning program development, personalized context creation, learning chain of thought execution and practice exercises, and real-time interaction throughout the process. On this basis, the platform alleviates the shortage of teachers through intelligent programming assistants, realizes personalized programming learning through learning analytics and customized learning settings, as well as using real-time interaction throughout the whole process to change the status quo of insufficient interaction. Compared with traditional programming learning and practice methods, AIGC-based programming learning platforms can provide a more personalized learning experience and more timely learning interactions, which can effectively enhance learners' interest and learning quality.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {9–15},
numpages = {7},
location = {<conf-loc>, <city>Barcelona</city>, <country>Spain</country>, </conf-loc>},
series = {ICETC '23}
}

@inproceedings{10.1145/3585059.3611441,
author = {Servin, Christian and Hawthorne, Elizabeth K. and Postner, Lori and Tang, Cara and Tucker, Cindy S.},
title = {Mathematical Considerations in Two-Year Computing Degrees: The Evolution of Math in Curricular Guidelines},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611441},
doi = {10.1145/3585059.3611441},
abstract = {Incorporating mathematics in computing has been a subject of ongoing deliberation within computing curriculum recommendations spanning several decades. Depending on the specific computing area or program, there is a variation in the advocated number of math contact hours. For instance, two-year programs like community colleges require many math hours due to curriculum alignment and agreements with four-year colleges. In other cases, the regional workforce and the career opportunities provided by local industry partners influence a degree program’s math courses/topics. The debate surrounding the appropriate number of math contact hours in computing programs has persisted over the years and has yet to be a definitive solution. Curricular guidelines such as ACM/IEEE-CS/AAAI CS 2023 and Information Technology 2017 have proposed math education based on competencies and workforce perspectives. Specifically, the former now recommends a tailored math background based on the knowledge area within computer science. This approach allows for a more flexible structure of knowledge units, encompassing specific topics, learning outcomes, and competencies. This paper explores various approaches and solutions proposed in the first two years of computing education programs to address challenges and gaps in computing education. It also highlights the short-term and long-term challenges computing programs and educators face.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {209–212},
numpages = {4},
keywords = {community college, cs2023, curriculum, mathematics, two-year},
location = {<conf-loc>, <city>Marietta</city>, <state>GA</state>, <country>USA</country>, </conf-loc>},
series = {SIGITE '23}
}

@inproceedings{10.1145/3605573.3605596,
author = {Li, Mingzhen and Yang, Hailong and Zhang, Shanjun and Yu, Fengwei and Gong, Ruihao and Liu, Yi and Luan, Zhongzhi and Qian, Depei},
title = {Exploiting Subgraph Similarities for Efficient Auto-tuning of Tensor Programs},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605596},
doi = {10.1145/3605573.3605596},
abstract = {The requirement for deploying deep learning (DL) models efficiently has boosted the research of DL compilers. Especially, the difficulty of generating optimized tensor programs has driven DL compilers to commonly adopt the auto-tuning approaches. Consequently, there are increasing demands to improve the effectiveness of auto-tuning in terms of both search efficiency and search quality. However, existing auto-tuning approaches commonly treat subgraphs individually and overlook the similarities among them, and thus fail to generate better tensor programs under limited time budget. To address the above drawbacks, we propose FamilySeer, an auto-tuning framework that can generate better tensor programs by exploiting the subgraph similarities. Specifically, FamilySeer organizes similar subgraphs into subgraph families, where the cost models are built at family basis with improved accuracy for estimating high potential program candidates. To further leverage the similarity, FamilySeer uses the accurate cost model per family to reduce the number of program candidates for costly hardware measurements without degrading search quality. The experiment results on various DL models demonstrate that FamilySeer can achieve better search efficiency/quality on both CPU and GPU platforms compared to the state-of-the-art auto-tuning framework.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {786–796},
numpages = {11},
keywords = {Auto-tuning, Performance Optimization, Subgraph Similarity, Tensor Compiler},
location = {<conf-loc>, <city>Salt Lake City</city>, <state>UT</state>, <country>USA</country>, </conf-loc>},
series = {ICPP '23}
}

@inproceedings{10.1145/3617650.3624950,
author = {Bouvier, Dennis J. and Lovellette, Ellie and Santos, Eddie Antonio and Becker, Brett A. and Crick, Tom and Dasigi, Venu G. and Forden, Jack and Glebova, Olga and Joshi, Swaroop and Kurkovsky, Stan and Russell, Seán},
title = {Teaching Students To Use Programming Error Messages},
year = {2023},
isbn = {9798400703744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617650.3624950},
doi = {10.1145/3617650.3624950},
abstract = {Research shows many students struggle to use programming error and warning messages effectively. Instead of using these messages as aids to debug and fix their code, some students have negative emotional reactions to seeing 'angry red text'. Not utilizing programming error and warning messages effectively, or at all, increases the difficulty of learning to program.As compiler messages can vary by programming language and/or development environment, lessons on reading them are not typically included in mainstream educational materials. We believe this gap can be filled and that students can learn to use error messages to their advantage. Further, we believe that teaching students how to read and use error messages can have a significant impact on the learning experience for novice programmers.The goal of this working group is to develop educational materials to teach students to use programming error messages, and evaluate the use of these materials. An additional goal is to investigate the role that large language models may play in the interpretation of error messages in the educational environment. We will produce guidelines for developing educational materials and strategies informed by feedback obtained from the community and our experimentation.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 2},
pages = {207–208},
numpages = {2},
keywords = {computer error messages, computing education, error messages, novice programmers, programming error messages, runtime errors, warning messages},
location = {<conf-loc>, <city>Hyderabad</city>, <country>India</country>, </conf-loc>},
series = {CompEd 2023}
}

@inproceedings{10.1007/978-3-031-49611-0_11,
author = {Damerius, Christoph and Kling, Peter and Schneider, Florian},
title = {Improved Scheduling with&nbsp;a&nbsp;Shared Resource},
year = {2023},
isbn = {978-3-031-49610-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-49611-0_11},
doi = {10.1007/978-3-031-49611-0_11},
abstract = {We consider the following shared-resource scheduling problem: Given a set of jobs J, for each j∈J we must schedule a job-specific processing volume of vj&gt;0. A total resource of 1 is available at any time. Jobs have a resource requirement rj∈0,1, and the resources assigned to them may vary over time. However, assigning them less will cause a proportional slowdown.We consider two settings. In the first, we seek to minimize the makespan in an online setting: The resource assignment of a job must be fixed before the next job arrives. Here we give an optimal e/(e-1)-competitive algorithm with runtime O(n,,logn). In the second, we aim to minimize the total completion time. We use a continuous linear programming (CLP) formulation for the fractional total completion time and combine it with a previously known dominance property from malleable job scheduling to obtain a lower bound on the total completion time. We extract structural properties by considering a geometrical representation of a CLP’s primal-dual pair. We combine the CLP schedule with a greedy schedule to obtain a (3/2+ε)-approximation for this setting. This improves upon the so far best-known approximation factor of 2.},
booktitle = {Combinatorial Optimization and Applications: 17th International Conference, COCOA 2023, Hawaii, HI, USA, December 15–17, 2023, Proceedings, Part I},
pages = {154–167},
numpages = {14},
keywords = {Approximation Algorithm, Malleable Job Scheduling, Makespan, List Scheduling, Completion Time, Continuous Linear Program},
location = {<conf-loc content-type="InPerson">Hawai, HI, USA</conf-loc>}
}

@inproceedings{10.1609/aiide.v19i1.27516,
author = {Eberhardinger, Manuel and Maucher, Johannes and Maghsudi, Setareh},
title = {Learning of generalizable and interpretable knowledge in grid-based reinforcement learning environments},
year = {2023},
isbn = {1-57735-883-X},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aiide.v19i1.27516},
doi = {10.1609/aiide.v19i1.27516},
abstract = {Understanding the interactions of agents trained with deep reinforcement learning is crucial for deploying agents in games or the real world. In the former, unreasonable actions confuse players. In the latter, that effect is even more significant, as unexpected behavior cause accidents with potentially grave and long-lasting consequences for the involved individuals. In this work, we propose using program synthesis to imitate reinforcement learning policies after seeing a trajectory of the action sequence. Programs have the advantage that they are inherently interpretable and verifiable for correctness. We adapt the state-of-the-art program synthesis system DreamCoder for learning concepts in grid-based environments, specifically, a navigation task and two miniature versions of Atari games, Space Invaders and Asterix. By inspecting the generated libraries, we can make inferences about the concepts the black-box agent has learned and better understand the agent's behavior. We achieve the same by visualizing the agent's decision-making process for the imitated sequences. We evaluate our approach with different types of program synthesizers based on a search-only method, a neural-guided search, and a language model fine-tuned on code.},
booktitle = {Proceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
articleno = {21},
numpages = {12},
location = {Salt Lake City},
series = {AIIDE '23}
}

@inproceedings{10.24963/ijcai.2023/350,
author = {Chatterjee, Prantik and Campos, José and Abreu, Rui and Roy, Subhajit},
title = {Augmenting automated spectrum based fault localization for multiple faults},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/350},
doi = {10.24963/ijcai.2023/350},
abstract = {Spectrum-based Fault Localization (SBFL) uses the coverage of test cases and their outcome (pass/fail) to predict the "suspiciousness" of program components, e.g., lines of code. SBFL is, perhaps, the most successful fault localization technique due to its simplicity and scalability. However, SBFL heuristics do not perform well in scenarios where a program may have multiple faulty components. In this work, we propose a new algorithm that "augments" previously proposed SBFL heuristics to produce a ranked list where faulty components ranked low by base SBFL metrics are ranked significantly higher. We implement our ideas in a tool, ARTEMIS, that attempts to "bubble up" faulty components which are ranked lower by base SBFL metrics. We compare our technique to the most popular SBFL metrics and demonstrate statistically significant improvement in the developer effort for fault localization with respect to the basic strategies.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {350},
numpages = {9},
location = {<conf-loc>, <city>Macao</city>, <country>P.R.China</country>, </conf-loc>},
series = {IJCAI '23}
}

@inproceedings{10.1145/3624062.3624100,
author = {Hall, Mary and Gopalakrishnan, Ganesh and Eide, Eric and Cohoon, Johanna and Phillips, Jeff and Zhang, Mu and Elhabian, Shireen and Bhaskara, Aditya and Dam, Harvey and Yadrov, Artem and Kataria, Tushar and Tavakkoli, Amir Mohammad and Joshi, Sameeran and Karanam, Mokshagna Sai Teja},
title = {An NSF REU Site Based on Trust and Reproducibility of Intelligent Computation: Experience Report},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624100},
doi = {10.1145/3624062.3624100},
abstract = {This paper presents an overview of an NSF Research Experience for Undergraduate (REU) Site on Trust and Reproducibility of Intelligent Computation, delivered by faculty and graduate students in the Kahlert School of Computing at University of Utah. The chosen themes bring together several concerns for the future in producing computational results that can be trusted: secure, reproducible, based on sound algorithmic foundations, and developed in the context of ethical considerations. The research areas represented by student projects include machine learning, high-performance computing, algorithms and applications, computer security, data science, and human-centered computing. In the first four weeks of the program, the entire student cohort spent their mornings in lessons from experts in these crosscutting topics, and used one-of-a-kind research platforms operated by the University of Utah, namely NSF-funded CloudLab and POWDER facilities; reading assignments, quizzes, and hands-on exercises reinforced the lessons. In the subsequent five weeks, lectures were less frequent, as students branched into small groups to develop their research projects. The final week focused on a poster presentation and final report. Through describing our experiences, this program can serve as a model for preparing a future workforce to integrate machine learning into trustworthy and reproducible applications.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {343–349},
numpages = {7},
keywords = {HPC, ML, artifact evaluation, networking, undergraduate education},
location = {<conf-loc>, <city>Denver</city>, <state>CO</state>, <country>USA</country>, </conf-loc>},
series = {SC-W '23}
}

@inproceedings{10.1007/978-3-031-40725-3_43,
author = {Griol, David and Callejas, Zoraida},
title = {Statistical Dialog Tracking and&nbsp;Management for&nbsp;Task-Oriented Conversational Systems},
year = {2023},
isbn = {978-3-031-40724-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-40725-3_43},
doi = {10.1007/978-3-031-40725-3_43},
abstract = {Conversational interfaces offer users a natural way to interact with a range of applications and devices. Human-machine interaction using these systems involves different components that mimic the mechanisms used by humans when using language and speech interaction. In this paper, we are interested in automatically developing the dialog state tracking component for task-oriented conversational systems to automatically decide the best system response from a set of predefined responses. To do this, we have evaluated several statistical methodologies to develop this component for a conversational system that provides technical program information during conferences. Gradient boosting trees have been selected as the best performing method for this specific domain.},
booktitle = {Hybrid Artificial Intelligent Systems: 18th International Conference, HAIS 2023, Salamanca, Spain, September 5–7, 2023, Proceedings},
pages = {507–518},
numpages = {12},
keywords = {Conversational systems, chatbots, dialog management, statistical methodologies, deep learning, gradient boosting},
location = {Salamanca, Spain}
}

@inproceedings{10.5555/3618408.3619593,
author = {Rahbar, Arman and Panahi, Ashkan and Chehreghani, Morteza Haghir and Dubhashi, Devdatt and Krim, Hamid},
title = {Recovery bounds on class-based optimal transport: a sum-of-norms regularization framework},
year = {2023},
publisher = {JMLR.org},
abstract = {We develop a novel theoretical framework for understating Optimal Transport (OT) schemes respecting a class structure. For this purpose, we propose a convex OT program with a sum-of-norms regularization term, which provably recovers the underlying class structure under geometric assumptions. Furthermore, we derive an accelerated proximal algorithm with a closed-form projection and proximal operator scheme, thereby affording a more scalable algorithm for computing optimal transport plans. We provide a novel argument for the uniqueness of the optimum even in the absence of strong convexity. Our experiments show that the new regularizer not only results in a better preservation of the class structure in the data but also yields additional robustness to the data geometry, compared to previous regularizers.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1185},
numpages = {29},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{10.5555/3618408.3619808,
author = {Tang, Hao and Ellis, Kevin},
title = {From perception to programs: regularize, overparameterize, and amortize},
year = {2023},
publisher = {JMLR.org},
abstract = {We develop techniques for synthesizing neurosymbolic programs. Such programs mix discrete symbolic processing with continuous neural computation. We relax this mixed discrete/continuous problem and jointly learn all modules with gradient descent, and also incorporate amortized inference, overparameterization, and a differentiable strategy for penalizing lengthy programs. Collectedly this toolbox improves the stability of gradient-guided program search, and suggests ways of learning both how to parse continuous input into discrete abstractions, and how to process those abstractions via symbolic code.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1400},
numpages = {16},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.5555/3636517.3636523,
author = {Tribelhorn, Ben and Nuxoll, Andrew},
title = {A Course Model for Ethics Education in Computer Science},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {1},
issn = {1937-4771},
abstract = {This paper presents a course model for teaching ethics to Computer Science students without being linked to a technical topic which helps students think more holistically. The course model proposed allows for students to build on their philosophy of ethics course from the core and to focus on current issues in technology. The assignments and topic generation activities presented can be easily adopted, even into other courses for institutions that don't offer technology focused ethics course. This course assists in aligning the Computer Science major curriculum with one of the five ABET program outcomes and to align with current guidance from ACM. The course was evaluated with a student survey to assess five learning objectives. The initial survey results show that students felt they improved on all five, especially in their ability to identify ethical issues in design decisions.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {48–55},
numpages = {8}
}

@inproceedings{10.1145/3623762.3633494,
author = {Cutts, Quintin and Kallia, Maria and Anderson, Ruth and Crick, Tom and Devlin, Marie and Farghally, Mohammed and Mirolo, Claudio and Runde, Ragnhild Kobro and Seppälä, Otto and Urquiza-Fuentes, Jaime and Vahrenhold, Jan},
title = {Arguments for and Approaches to Computing Education in Undergraduate Computer Science Programmes},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633494},
doi = {10.1145/3623762.3633494},
abstract = {Computing education (CE), the scientific foundation of the teaching and learning of subject matter specific to computing, has matured into a field with its own research journals and conferences as well as graduate programmes. Yet, and unlike other mature subfields of computer science (CS), it is rarely taught as part of undergraduate CS programmes. In this report, we present a gap analysis resulting from semi-structured interviews with various types of stakeholders and derive a set of arguments for teaching CE courses in undergraduate CS programmes. This analysis and the arguments highlight a number of opportunities for the discipline of CS at large, in academia, in industry, and in school education, that would be opened up with undergraduate CE courses, as well as potential barriers to implementation that will need to be overcome. We also report on the results of a Delphi process performed to elicit topics for such a course with various audiences in mind. The Delphi process yielded 19 high-level categories that encompass the subject matter CE courses should incorporate, tailored to the specific needs of their intended student audiences. This outcome underscores the extensive range of content that can be integrated into a comprehensive CE programme. Based on these two stakeholder interactions as well as a systematic literature review aiming to explore the current practices in teaching CE to undergraduate students, we develop two prototypical outlines of such a course, keeping in mind that departments may have different preferences and affordances resulting in different kinds of CE offerings. Overall, input from external stakeholders underscores the clear significance of undergraduate CE courses. We anticipate leveraging this valuable feedback to actively promote these courses on a broader scale.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {160–195},
numpages = {36},
keywords = {argument, computing education, curriculum outline, undergraduate},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE-WGR '23}
}

@inproceedings{10.1007/978-3-031-45382-3_30,
author = {Aissa, Wafa and Ferecatu, Marin and Crucianu, Michel},
title = {Multimodal Representations for&nbsp;Teacher-Guided Compositional Visual Reasoning},
year = {2023},
isbn = {978-3-031-45381-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-45382-3_30},
doi = {10.1007/978-3-031-45382-3_30},
abstract = {Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving training efficiency and final performance. We demonstrate that by incorporating cross-modal features and employing more effective training techniques for NMN, we achieve a favorable balance between performance and transparency in the reasoning process.},
booktitle = {Advanced Concepts for Intelligent Vision Systems: 21st International Conference, ACIVS 2023 Kumamoto, Japan, August 21–23, 2023 Proceedings},
pages = {357–369},
numpages = {13},
keywords = {Visual reasoning, Neural module networks, Multi-modality},
location = {<conf-loc content-type="InPerson">Kumamoto, Japan</conf-loc>}
}

@article{10.1016/j.aei.2023.102033,
author = {Chen, Zhuyun and Xia, Jingyan and Li, Jipu and Chen, Junbin and Huang, Ruyi and Jin, Gang and Li, Weihua},
title = {Generalized open-set domain adaptation in mechanical fault diagnosis using multiple metric weighting learning network},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2023.102033},
doi = {10.1016/j.aei.2023.102033},
journal = {Adv. Eng. Inform.},
month = {aug},
numpages = {18},
keywords = {Fault diagnosis, Domain adaptation, Open-set, Rolling bearings, Weighting learning, DL, DBN, SAE, TL, DA, DACNN, DAN, MLDAN, FC, GRL, OF, IF, CF, IMSB, OPB, PB, FFT, DANN, MCD, MMD, CNN, GAN, AC-GAN, UAN, MFPT, OSFD, SOSFD, POSFD, DATLN, MMWLN, CWRU, EDM, CNC, t-SNE}
}

@inproceedings{10.1145/3578338.3593571,
author = {Kumar, Adithya and Sivasubramaniam, Anand and Zhu, Timothy},
title = {SplitRPC: A {Control + Data} Path Splitting RPC Stack for ML Inference Serving},
year = {2023},
isbn = {9798400700743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578338.3593571},
doi = {10.1145/3578338.3593571},
abstract = {The growing adoption of hardware accelerators driven by their intelligent compiler and runtime system counterparts has democratized ML services and precipitously reduced their execution times. This motivates us to shift our attention to characterize the overheads imposed by the RPC mechanism (`RPC tax') when serving them on accelerators. Conventional RPC implementations implicitly assume the host CPU services the requests, and we focus on expanding such works towards accelerator-based services. While SmartNIC based solutions work well for simple applications, serving complex ML models requires a more nuanced view to optimize both the data-path and the control/orchestration of these accelerators. We program commodity network interface cards (NICs) to split the control and data paths for effective transfer of control while efficiently transferring the payload to the accelerator. As opposed to unified approaches that bundle these paths together, limiting the flexibility in each of these paths, we design and implement SplitRPC - a {control + data} path optimizing RPC mechanism for ML inference serving. SplitRPC allows us to optimize the datapath to the accelerator while simultaneously allowing the CPU to maintain full orchestration capabilities. We implement SplitRPC on both commodity NICs and SmartNICs and demonstrate that SplitRPC is effective in minimizing the RPC tax while providing significant gains in throughput and latency.},
booktitle = {Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {13–14},
numpages = {2},
keywords = {data path, ml inference, orchestration, remote procedure call, smartnic},
location = {Orlando, Florida, United States},
series = {SIGMETRICS '23}
}

@inproceedings{10.24963/ijcai.2023/357,
author = {Fandinno, Jorge and Hecher, Markus},
title = {Treewidth-aware complexity for evaluating epistemic logic programs},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/357},
doi = {10.24963/ijcai.2023/357},
abstract = {Logic programs are a popular formalism for encoding many problems relevant to knowledge representation and reasoning as well as artificial intelligence. However, for modeling rational behavior it is oftentimes required to represent the concepts of knowledge and possibility. Epistemic logic programs (ELPs) is such an extension that enables both concepts, which correspond to being true in all or some possible worlds or stable models. For these programs, the parameter treewidth has recently regained popularity. We present complexity results for the evaluation of key ELP fragments for treewidth, which are exponentially better than known results for full ELPs. Unfortunately, we prove that obtained runtimes can not be significantly improved, assuming the exponential time hypothesis. Our approach defines treewidth-aware reductions between quantified Boolean formulas and ELPs. We also establish that the completion of a program, as used in modern solvers, can be turned treewidth-aware, thereby linearly preserving treewidth.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {357},
numpages = {9},
location = {<conf-loc>, <city>Macao</city>, <country>P.R.China</country>, </conf-loc>},
series = {IJCAI '23}
}

@inproceedings{10.1145/3577193.3593714,
author = {Trümper, Lukas and Ben-Nun, Tal and Schaad, Philipp and Calotoiu, Alexandru and Hoefler, Torsten},
title = {Performance Embeddings: A Similarity-Based Transfer Tuning Approach to Performance Optimization},
year = {2023},
isbn = {9798400700569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577193.3593714},
doi = {10.1145/3577193.3593714},
abstract = {Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit clear correspondences between program characteristics and optimizations, outperforming prior specialized state-of-the-art approaches and generalizing beyond their capabilities.},
booktitle = {Proceedings of the 37th International Conference on Supercomputing},
pages = {50–62},
numpages = {13},
keywords = {compilers, embeddings, transfer tuning, peephole optimization, performance optimization, autotuning},
location = {Orlando, FL, USA},
series = {ICS '23}
}

@article{10.1007/s42979-023-01978-9,
author = {Ramkumar, Gowtham and Misra, Siddharth and Babu, Gadde Raghu and Gottimukkala, Anantha Rao and Siddi, Someshwar and Kumar, Jyothula Sunil},
title = {Optimization of Flexible Manufacturing Production Line System Based on Digital Twin},
year = {2023},
issue_date = {Sep 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {4},
number = {5},
url = {https://doi.org/10.1007/s42979-023-01978-9},
doi = {10.1007/s42979-023-01978-9},
abstract = {This research presents a revolutionary Digital Twin (DT)—driven method aimed at quick customization of computerized industrial processes. The DT includes dual components, the semi-physical replication that transfers system information and gives data input to the subsequent clause, which is enhanced. The outcomes of the optimum section are returned directly to the semi-physical replication used for validation. The term “Open-Architecture Machine Tool” (OAMT) led to a fundamental class of machine tools that consists of a basic unified platform and many individually designed modules that may be quickly added or replaced away. Designers can dynamically modify the production system for responding to process planning by inserting personalized components into its OAMTs. Major enabling approaches, along with how to identical virtual and substantial systems and how to instantly bi-level program the invention size and efficiency of developed structures to accommodate sudden variations of goods, are explained. A real execution is done to demonstrate the efficacy of the method to achieve increased enactment of the system by minimizing the overhead cost of the recompose method by systematizing and quickly enhancing it.},
journal = {SN Comput. Sci.},
month = {jul},
numpages = {12},
keywords = {Digital twin, Manufacturing systems, Production capacity, Effectiveness, Optimization}
}

@inproceedings{10.1145/3573051.3593379,
author = {Demszky, Dorottya and Liu, Jing},
title = {M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1:1 Instruction and Student Outcomes},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593379},
doi = {10.1145/3573051.3593379},
abstract = {Although learners are being connected 1:1 with instructors at an increasing scale, most of these instructors do not receive effective, consistent feedback to help them improve. We deployed M-Powering Teachers, an automated tool based on natural language processing to give instructors feedback on dialogic instructional practices ---including their uptake of student contributions, talk time and questioning practices --- in a 1:1 online learning context. We conducted a randomized controlled trial on Polygence, a research mentorship platform for high schoolers (n=414 mentors) to evaluate the effectiveness of the feedback tool. We find that the intervention improved mentors' uptake of student contributions by 10%, reduced their talk time by 5% and improved student's experience with the program as well as their relative optimism about their academic future. These results corroborate existing evidence that scalable and low-cost automated feedback can improve instruction and learning in online educational contexts.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {59–69},
numpages = {11},
keywords = {automated teacher feedback, natural language processing, randomized controlled trial},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3626111.3628179,
author = {Alcoz, Albert Gran and Vanbever, Laurent},
title = {QVISOR: Virtualizing Packet Scheduling Policies},
year = {2023},
isbn = {9798400704154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626111.3628179},
doi = {10.1145/3626111.3628179},
abstract = {The concept of programmable packet scheduling has been recently introduced, enabling the programming of scheduling algorithms into existing data planes without requiring new hardware designs. Notably, several programmable schedulers have been proposed, which are capable of running directly on existing commodity switches. Unfortunately, though, their focus has been limited to single-tenant traffic scheduling: i.e., scheduling all incoming traffic following one single scheduling policy (e.g., pFabric to minimize flow completion times).In this paper, we emphasize the fact that today's networks are heterogeneous: they are shared by multiple tenants, who run applications with different performance requirements. As such, we introduce a new research challenge: how can we extend scheduling programmability to multi-tenant policies?We envision QVISOR, a scheduling hypervisor that enables multi-tenant programmable scheduling on existing switches. With QVISOR, tenants program the scheduling policies for their traffic; operators define how tenants should share the available resources; and QVISOR does the rest: combines and deploys the scheduling policies to the underlying hardware.},
booktitle = {Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
pages = {238–244},
numpages = {7},
keywords = {Packet Scheduling, Programmable Scheduling, Virtualization},
location = {<conf-loc>, <city>Cambridge</city>, <state>MA</state>, <country>USA</country>, </conf-loc>},
series = {HotNets '23}
}

@inproceedings{10.1007/978-981-99-6489-5_23,
author = {Liu, Yuan and Chen, Mingzhi and Zhu, Daqi},
title = {Design and Development of ROV for Ship Hull Inspection Using ADRC},
year = {2023},
isbn = {978-981-99-6488-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-6489-5_23},
doi = {10.1007/978-981-99-6489-5_23},
abstract = {Remote Operated Vehicles (ROVs) have been widely used in complex underwater environments. One important application of ROVs is in the inspection of underwater ship hulls. This article describes the design and development of an ROV to assist in this task. The ROV is designed as an open-frame type, with many sensors installed to detect the condition of the ship’s submerged structures. The article presents the design of the ROV’s power and control compartments, as well as the control program. The ROV has two modes of operation: manual and automatic. In manual mode, the ROV is operated using a joystick. In automatic mode, it can run at a fixed position, depth and direction. Experimental results show that the use of the Active Disturbance Rejection Control (ADRC) algorithm is effective and can assist in underwater inspections. Underwater detection experiments were also conducted.},
booktitle = {Intelligent Robotics and Applications: 16th International Conference, ICIRA 2023, Hangzhou, China, July 5–7, 2023, Proceedings, Part III},
pages = {287–296},
numpages = {10},
keywords = {Remote Operated Vehicle (ROV), Hull Inspection, ADRC, Design},
location = {Hangzhou, China}
}

@inproceedings{10.1145/3586183.3606781,
author = {Alaboudi, Abdulaziz and Latoza, Thomas D.},
title = {Hypothesizer: A Hypothesis-Based Debugger to Find and Test Debugging Hypotheses},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606781},
doi = {10.1145/3586183.3606781},
abstract = {When software defects occur, developers begin the debugging process by formulating hypotheses to explain the cause. These hypotheses guide the investigation process, determining which evidence developers gather to accept or reject the hypothesis, such as parts of the code and program state developers examine. However, existing debugging techniques do not offer support in finding relevant hypotheses, leading to wasted time testing hypotheses and examining code that ultimately does not lead to a fix. To address this issue, we introduce a new type of debugging tool, the hypothesis-based debugger, and an implementation of this tool in Hypothesizer. Hypothesis-based debuggers support developers from the beginning of the debugging process by finding relevant hypotheses until the defect is fixed. To debug using Hypothesizer, developers first demonstrate the defect, generating a recording of the program behavior with code execution, user interface events, network communications, and user interface changes. Based on this information and the developer’s descriptions of the symptoms, Hypothesizer finds relevant hypotheses, analyzes the code to identify relevant evidence to test the hypothesis, and generates an investigation plan through a timeline view. This summarizes all evidence items related to the hypothesis, indicates whether the hypothesis is likely to be true by showing which evidence items were confirmed in the recording, and enables the developer to quickly check evidence in the recording by viewing code snippets for each evidence item. A randomized controlled experiment with 16 professional developers found that, compared to traditional debugging tools and techniques such as breakpoint debuggers and Stack Overflow, Hypothesizer dramatically improved the success rate of fixing defects by a factor of five and decreased the time to debug by a factor of three.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {73},
numpages = {14},
keywords = {debugging, debugging hypotheses, debugging tools},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {UIST '23}
}

@article{10.1007/s10883-023-09661-1,
author = {dos Santos, Mayk Joaquim and Tonon, Durval José},
title = {Structural Stability for 2-Dimensional Piecewise Smooth Vector Fields Where the Switching Manifold is a Double Discontinuity},
year = {2023},
issue_date = {Oct 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {1079-2724},
url = {https://doi.org/10.1007/s10883-023-09661-1},
doi = {10.1007/s10883-023-09661-1},
abstract = {The main purpose of this work is exhibit the canonical forms for 2D codimension zero piecewise smooth vector fields where the switching manifold is a double discontinuity and therefore the Filippov’s convention is not applied. All possible orientations and codimension zero scenarios were covered. Also the intrinsic objects that characterize each one of the canonical forms were presented. As a consequence, all canonical forms were obtained. Following the Thom-Smale’s program we exhibit the open and dense subset of piecewise smooth vector fields that is structural stable. Besides that, we apply the result obtained to a model that governs the dynamic of a switched boost converter. In this context, we are able to obtain the local dynamics of the model, as well as the local asymptotic stability and the local structural stability of the system.},
journal = {Journal of Dynamical and Control Systems},
month = {aug},
pages = {1775–1808},
numpages = {34},
keywords = {Piecewise smooth vector fields, Canonical forms, Singularity, Double discontinuity, Bifurcation, 34A36, 34D30, 37G05}
}

@article{10.1145/3592416,
author = {Jones, R. Kenny and Guerrero, Paul and Mitra, Niloy J. and Ritchie, Daniel},
title = {ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592416},
doi = {10.1145/3592416},
abstract = {We introduce ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across a dataset, so that programs rewritten with these abstractions are more compact, and suppress spurious degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stringent input assumptions. This is principally made possible by two methodological advancements: (a) a shape-to-program recognition network that learns to solve sub-problems and (b) the use of e-graphs, augmented with a conditional rewrite scheme, to determine when abstractions with complex parametric expressions can be applied, in a tractable manner. We evaluate ShapeCoder on multiple datasets of 3D shapes, where primitive decompositions are either parsed from manual annotations or produced by an unsupervised cuboid abstraction method. In all domains, ShapeCoder discovers a library of abstractions that captures high-level relationships, removes extraneous degrees of freedom, and achieves better dataset compression compared with alternative approaches. Finally, we investigate how programs rewritten to use discovered abstractions prove useful for downstream tasks.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {49},
numpages = {17},
keywords = {procedural modeling, visual programs, shape analysis, shape abstraction, library learning, e-graph}
}

@article{10.1287/opre.2022.2268,
author = {Aouad, Ali and Segev, Danny},
title = {Technical Note—An Approximate Dynamic Programming Approach to the Incremental Knapsack Problem},
year = {2023},
issue_date = {July-August 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {71},
number = {4},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2022.2268},
doi = {10.1287/opre.2022.2268},
abstract = {Integer packing problems have traditionally been some of the most fundamental and well-studied computational questions in discrete optimization. The paper by Aouad and Segev studies the incremental knapsack problem, where one wishes to sequentially pack items into a knapsack whose capacity expands over a finite planning horizon, with the objective of maximizing time-averaged profits. Although various approximation algorithms were developed under mitigating structural assumptions, obtaining nontrivial performance guarantees for this problem in its utmost generality has remained an open question thus far. The authors devise the first polynomial-time approximation scheme for general instances of the incremental knapsack problem, which is the strongest guarantee possible given existing hardness results. Their approach synthesizes various techniques related to approximate dynamic programming, including problem decompositions, counting arguments, and efficient rounding methods, which may be of broader interest.We study the incremental knapsack problem, where one wishes to sequentially pack items into a knapsack whose capacity expands over a finite planning horizon, with the objective of maximizing time-averaged profits. Although various approximation algorithms were developed under mitigating structural assumptions, obtaining nontrivial performance guarantees for this problem in its utmost generality has remained an open question thus far. In this paper, we devise a polynomial-time approximation scheme for general instances of the incremental knapsack problem, which is the strongest guarantee possible given existing hardness results. In contrast to earlier work, our algorithmic approach exploits an approximate dynamic programming formulation. Starting with a simple exponentially sized dynamic program, we prove that an appropriate composition of state pruning ideas yields a polynomially sized state space with negligible loss of optimality. The analysis of this formulation synthesizes various techniques, including new problem decompositions, parsimonious counting arguments, and efficient rounding methods, that may be of broader interest.},
journal = {Oper. Res.},
month = {jul},
pages = {1414–1433},
numpages = {20},
keywords = {Optimization, incremental knapsack, approximate dynamic programming, PTAS}
}

@article{10.1145/3589773,
author = {Zhao, Hanyu and Yang, Zhi and Cheng, Yu and Tian, Chao and Ren, Shiru and Xiao, Wencong and Yuan, Man and Chen, Langshi and Liu, Kaibo and Zhang, Yang and Li, Yong and Lin, Wei},
title = {GoldMiner: Elastic Scaling of Training Data Pre-Processing Pipelines for Deep Learning},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589773},
doi = {10.1145/3589773},
abstract = {Training data pre-processing pipelines are essential to deep learning (DL). As the performance of model training keeps increasing with both hardware advancements (e.g., faster GPUs) and various software optimizations, the data pre-processing on CPUs is becoming more resource-intensive and a severe bottleneck of the pipeline. This problem is even worse in the cloud, where training jobs exhibit diverse CPU-GPU demands that usually result in mismatches with fixed hardware configurations and resource fragmentation, degrading both training performance and cluster utilization.We introduce GoldMiner, an input data processing service for stateless operations used in pre-processing data for DL model training. GoldMiner decouples data pre-processing from model training into a new role called the data worker. Data workers facilitate scaling of data pre-processing to anywhere in a cluster, effectively pooling the resources across the cluster to satisfy the diverse requirements of training jobs. GoldMiner achieves this decoupling in a fully automatic and elastic manner. The key insight is that data pre-processing is inherently stateless, thus can be executed independently and elastically. This insight guides GoldMiner to automatically extract stateless computation out of a monolithic training program, efficiently disaggregate it across data workers, and elastically scale data workers to tune the resource allocations across jobs to optimize cluster efficiency. We have applied GoldMiner to industrial workloads, and our evaluation shows that GoldMiner can transform unmodified training programs to use data workers, accelerating individual training jobs by up to 12.1x. GoldMiner also improves average job completion time and aggregate GPU utilization by up to 2.5x and 2.1x in a 64-GPU cluster, respectively, by scheduling data workers with elasticity.},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {193},
numpages = {25},
keywords = {data pre-processing, deep learning, disaggregation, scheduling}
}

@article{10.1016/j.infsof.2023.107339,
author = {Wang, Xiaolin and Zhang, Sulan},
title = {Cluster-based adaptive test case prioritization},
year = {2024},
issue_date = {Jan 2024},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {165},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2023.107339},
doi = {10.1016/j.infsof.2023.107339},
journal = {Inf. Softw. Technol.},
month = {jan},
numpages = {17},
keywords = {Test case prioritization, Clustering analysis, Requirement, Regression testing}
}

@inproceedings{10.5555/3618408.3619760,
author = {Steiner, Benoit and Elhoushi, Mostafa and Kahn, Jacob and Hegarty, James},
title = {MODeL: memory optimizations for deep learning},
year = {2023},
publisher = {JMLR.org},
abstract = {The size of deep neural networks has grown exponentially in recent years. Unfortunately, hardware devices have not kept pace with the rapidly increasing memory requirements. To cope with this, researchers have proposed various techniques including spilling, recomputation, reduced precision training, model pruning, and so on. However, these approaches suffer from various limitations: they can increase training time, affect model accuracy, or require extensive manual modifications to the neural networks.We present MODeL, an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks. Our method automatically reduces the memory usage of existing neural networks without any of the drawbacks of other techniques.We formulate the problem as a joint integer linear program (ILP). We present several techniques to simplify the encoding of the problem, and enable our approach to scale to the size of state-of-the-art neural networks using an off-the-shelf ILP solver.We experimentally demonstrate that MODeL only takes seconds to allow the training of neural networks using 30% less memory on average. MODeL is an open-source project available at https://github.com/facebookresearch/model_opt.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1352},
numpages = {15},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{10.1007/s10676-023-09740-8,
author = {Montefiore, Thomas and Podosky, Paul-Mikhail Catapang},
title = {The conceptual exportation question: conceptual engineering and the normativity of virtual worlds},
year = {2024},
issue_date = {Mar 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1388-1957},
url = {https://doi.org/10.1007/s10676-023-09740-8},
doi = {10.1007/s10676-023-09740-8},
abstract = {Debate over the normativity of virtual phenomena is now widespread in the philosophical literature, taking place in roughly two distinct but related camps. The first considers the relevant problems to be within the scope of applied ethics, where the general methodological program is to square the intuitive (im)permissibility of virtual wrongdoings with moral accounts that justify their (im)permissibility. The second camp approaches the normativity of virtual wrongdoings as a metaphysical debate. This is done by disambiguating the ‘virtual’ character of ‘virtual wrongdoings’. Doing so is supposed to provide illuminating ontological distinctions that inform ethical aspects of the debate. We argue that each approach faces its own set of issues, and as a result, motivates consideration of an alternative. The alternative we suggest turns inquiry concerning the normativity of virtual wrongdoings into a distinctively conceptual question. Rather than asking whether some action is right or wrong, or whether some virtual phenomenon counts as a particular action at all, we argue that research into the normativity of virtual wrongdoings may be guided by reflecting on whether a concept that originated and developed within a non-virtual context should be exported into a foreign virtual domain. We consider this approach and several objections.},
journal = {Ethics and Inf. Technol.},
month = {jan},
numpages = {13},
keywords = {Conceptual engineering, Conceptual ethics, Virtual action, Videogame ethics, Virtual ethics, Virtual worlds, Virtual reality, Applied ethics, Gamer’s dilemma}
}

@article{10.1287/mnsc.2023.4679,
author = {D’Aeth, Josh C. and Ghosal, Shubhechyya and Grimm, Fiona and Haw, David and Koca, Esma and Lau, Krystal and Liu, Huikang and Moret, Stefano and Rizmie, Dheeya and Smith, Peter C. and Forchini, Giovanni and Miraldo, Marisa and Wiesemann, Wolfram},
title = {Optimal Hospital Care Scheduling During the SARS-CoV-2 Pandemic},
year = {2023},
issue_date = {October 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {69},
number = {10},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2023.4679},
doi = {10.1287/mnsc.2023.4679},
abstract = {The COVID-19 pandemic has seen dramatic demand surges for hospital care that have placed a severe strain on health systems worldwide. As a result, policy makers are faced with the challenge of managing scarce hospital capacity to reduce the backlog of non-COVID patients while maintaining the ability to respond to any potential future increases in demand for COVID care. In this paper, we propose a nationwide prioritization scheme that models each individual patient as a dynamic program whose states encode the patient’s health and treatment condition, whose actions describe the available treatment options, whose transition probabilities characterize the stochastic evolution of the patient’s health, and whose rewards encode the contribution to the overall objectives of the health system. The individual patients’ dynamic programs are coupled through constraints on the available resources, such as hospital beds, doctors, and nurses. We show that the overall problem can be modeled as a grouped weakly coupled dynamic program for which we determine near-optimal solutions through a fluid approximation. Our case study for the National Health Service in England shows how years of life can be gained by prioritizing specific disease types over COVID patients, such as injury and poisoning, diseases of the respiratory system, diseases of the circulatory system, diseases of the digestive system, and cancer.This paper was accepted by Chung-Piaw Teo, optimization.Funding: G. Forchini acknowledges funding from Jan Wallanders and Tom Hedelius Foundation and the Tore Browaldh Foundation, funding from MRC Centre for Global Infectious Disease Analysis [Reference MR/R015600/1], jointly funded by the UK Medical Research Council (MRC) and the UK Foreign, Commonwealth and Development Office (FCDO), under the MRC/FCDO Concordat agreement, part of the EDCTP2 program supported by the European Union; and acknowledges funding by Community Jameel. D. Rizmie acknowledges partial funding from the MRC Centre for Global Infectious Disease Analysis [Reference MR/R015600/1]. J. C. D’Aeth acknowledges funding from the Wellcome Trust [Reference 102169/Z/13/Z]. S. Moret acknowledges partial support from the Swiss National Science Foundation (SNSF) under [Grant P2ELP2_188028]. S. Ghosal was funded by the Imperial College President’s PhD Scholarship. F. Grimm was funded by the Health Foundation as part of core staff member activity. This research was funded in whole, or in part, by the Wellcome Trust [Grant 102169/Z/13/Z].Supplemental Material: The data files are available at .},
journal = {Manage. Sci.},
month = {oct},
pages = {5923–5947},
numpages = {25},
keywords = {COVID, care prioritization, grouped weakly coupled dynamic programs, fluid approximation}
}

@article{10.1145/3635305,
author = {Zhao, Jie and Xu, Jinchen and Di, Peng and Nie, Wang and Hu, Jiahui and Yi, Yanzhi and Yang, Sijia and Geng, Zhen and Zhang, Renwei and Li, Bojie and Gan, Zhiliang and Jin, Xuefeng},
title = {Modeling the Interplay between Loop Tiling and Fusion in Optimizing Compilers Using Affine Relations},
year = {2024},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1–4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3635305},
doi = {10.1145/3635305},
abstract = {Loop tiling and fusion are two essential transformations in optimizing compilers to enhance the data locality of programs. Existing heuristics either perform loop tiling and fusion in a particular order, missing some of their profitable compositions, or execute ad-hoc implementations for domain-specific applications, calling for a generalized and systematic solution in optimizing compilers. In this article, we present a so-called basteln (an abbreviation for backward slicing of tiled loop nests) strategy in polyhedral compilation to better model the interplay between loop tiling and fusion. The basteln strategy first groups loop nests by preserving their parallelism/tilability and next performs rectangular/parallelogram tiling to the output groups that produce data consumed outside the considered program fragment. The memory footprints required by each tile are then computed, from which the upward exposed data are extracted to determine the tile shapes of the remaining fusion groups. Such a tiling mechanism can construct complex tile shapes imposed by the dependences between these groups, which are further merged by a post-tiling fusion algorithm for enhancing data locality without losing the parallelism/tilability of the output groups. The basteln strategy also takes into account the amount of redundant computations and the fusion of independent groups, exhibiting a general applicability. We integrate the basteln strategy into two optimizing compilers, with one a general-purpose optimizer and the other a domain-specific compiler for deploying deep learning models. The experiments are conducted on CPU, GPU, and a deep learning accelerator to demonstrate the effectiveness of the approach for a wide class of application domains, including deep learning, image processing, sparse matrix computation, and linear algebra. In particular, the basteln strategy achieves a mean speedup of 1.8× over cuBLAS/cuDNN and 1.1× over TVM on GPU when used to optimize deep learning models; it also outperforms PPCG and TVM by 11% and 20%, respectively, when generating code for the deep learning accelerator.},
journal = {ACM Trans. Comput. Syst.},
month = {jan},
articleno = {5},
numpages = {45},
keywords = {Tiling, fusion, data locality, parallelism, redundant computation, memory hierarchy, polyhedral model, optimizing compilers}
}

@article{10.1287/trsc.2021.0469,
author = {Wang, Hua and Wang, Jing and Chen, Shukai and Meng, Qiang},
title = {Equilibrium Traffic Dynamics with Mixed Autonomous and Human-Driven Vehicles and Novel Traffic Management Policies: The Effects of Value-of-Time Compensation and Random Road Capacity},
year = {2023},
issue_date = {September-October 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {57},
number = {5},
issn = {1526-5447},
url = {https://doi.org/10.1287/trsc.2021.0469},
doi = {10.1287/trsc.2021.0469},
abstract = {Emerging autonomous vehicles (AVs) are expected to bring about a revolution in both the automotive industry and transportation systems. Introducing AVs into the existing mobility system with human-driven vehicles (HVs) yields mixed traffic with the following new features: in-vehicle compensation on value of time for AV users, distinct road capacities for pure AV and HV flows, and stochastic road capacity for the inseparable AV-HV traffic pattern. In this paper, we aim to investigate equilibrium traffic dynamics for the morning commuting problem where AVs and HVs coexist in a transportation corridor by considering these new features, and also explore several novel mixed AV-HV traffic management strategies. The AV-HV traffic pattern could be either separable (i.e., pure AV flow and pure HV flow depart from home in different periods) or inseparable, depending on the user profile condition. In addition to deriving departure time equilibriums for scenarios with separable traffic flows, significant effort is put into the scenario with an inseparable AV-HV traffic pattern, where stochastic road capacity is taken into account. Based on these equilibrium traffic analyses, we propose and explore some new traffic management strategies, including AV certificate of entitlement management scheme for scenarios with separable traffic flows and departure-period management (DPM) scheme and lane management policies for the scenario with an inseparable AV-HV traffic pattern. Eligibilities for applying these strategies are analytically derived and extensively discussed, and numerical experiments are conducted to demonstrate our theoretical findings and reveal the underlying impacts of road capacity randomness. Some lessons learned from the numerical experiments are (i) overlooking the impact of road capacity uncertainty will lead to an overestimation of system performance and even yield biased policymaking, (ii) the full dedicated-lane policy is the preferred option for the medium-level AV situation and partial dedicated-lane policies are more attractive choices for the early AV era or a market with a high AV share, and (iii) the DPM scheme could be a better substitute for partially dedicated-lane policies.Funding: This study was supported by the Ministry of Education of Singapore [Project T2EP40222-0002 under the MOE Tier 2 Grant] and the National Natural Science Foundation Council of China [Grant 72001133] and the Excellent Young Scientists Fund Program (Overseas).Supplemental Material: The online appendix is available at .},
journal = {Transportation Science},
month = {sep},
pages = {1177–1208},
numpages = {32},
keywords = {autonomous and human-driven vehicles, bottleneck model, equilibrium traffic dynamics, value-of-time compensation, random road capacity, traffic management policies}
}

@inproceedings{10.1145/3626641.3627240,
author = {Tolle, Herman and Brata, Komang Candra and Maryanto, Sukir and Athaya, Nuzulul},
title = {Design of School Watching Mobile Application with Geotagging and Context Aware for Schools Reporting and Monitoring in Mount Semeru Disaster Area},
year = {2023},
isbn = {9798400708503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626641.3627240},
doi = {10.1145/3626641.3627240},
abstract = {Mount Semeru erupted on December 4, 2021, followed by hot clouds falling. The eruption caused significant destruction to one school, moderate damage to five schools, and impacted 19 schools with volcanic ash, resulting in the tragic loss of six students' lives. Subsequently, a disaster mitigation program named "School Watching" was implemented. It involved observing school environments and identifying hazardous elements to prepare for future disasters. Therefore, using the Design Thinking method, the researchers designed the user experience (UX) for the Semeru School Watch application to assist with the school environment's preparedness. This application implements geotagging and context-aware to accommodate information about school location and the impact of the disaster. The final design of the application was evaluated with a usability test on 20 respondents to assess their effectiveness, efficiency, and satisfaction. The results for effectiveness scored 98.89%, the efficiency aspect achieved an overall relative efficiency calculation of 98.5%, and the satisfaction aspect received an SUS calculation of 83 with a grade A (excellent). Additionally, the user experience aspect was tested using the UEQ calculation, yielding a score of 1.99 (very good) for attractiveness, 1.68 (above average) for insight, 1.99 (very good) for efficiency, 2.00 (very good) for dependency, 1.88 (very good) for stimulation, and 1.71 (very good) for novelty. So, the School Watching application design with geotagging and context-aware will provide a model for mitigation reporting with mobile apps.},
booktitle = {Proceedings of the 8th International Conference on Sustainable Information Engineering and Technology},
pages = {673–678},
numpages = {6},
location = {<conf-loc>, <city>Badung, Bali</city>, <country>Indonesia</country>, </conf-loc>},
series = {SIET '23}
}

@article{10.1016/j.artint.2023.104023,
author = {Brewka, Gerhard and Delgrande, James and Romero, Javier and Schaub, Torsten},
title = {A general framework for preferences in answer set programming},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {325},
number = {C},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2023.104023},
doi = {10.1016/j.artint.2023.104023},
journal = {Artif. Intell.},
month = {dec},
numpages = {49},
keywords = {Preferences, Optimization, Answer set programming}
}

@inproceedings{10.5555/3620237.3620515,
author = {Shi, Ji and Wang, Zhun and Feng, Zhiyao and Lan, Yang and Qin, Shisong and You, Wei and Zou, Wei and Payer, Mathias and Zhang, Chao},
title = {AIFORE: smart fuzzing based on automatic input format reverse engineering},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {Knowledge of a program's input format is essential for effective input generation in fuzzing. Automated input format reverse engineering represents an attractive but challenging approach to learning the format. In this paper, we address several challenges of automated input format reverse engineering, and present a smart fuzzing solution AIFORE which makes full use of the reversed format and benefits from it. The structures and semantics of input fields are determined by the basic blocks (BBs) that process them rather than the input specification. Therefore, we first utilize byte-level taint analysis to recognize the input bytes processed by each BB, then identify indivisible input fields that are always processed together with a minimum cluster algorithm, and learn their types with a neural network model that characterizes the behavior of BBs. Lastly, we design a new power scheduling algorithm based on the inferred format knowledge to guide smart fuzzing. We implement a prototype of AIFORE and evaluate both the accuracy of format inference and the performance of fuzzing against state-of-the-art (SOTA) format reversing solutions and fuzzers. AIFORE significantly outperforms SOTA baselines on the accuracy of field boundary and type recognition. With AIFORE, we uncovered 20 bugs in 15 programs that were missed by other fuzzers.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {278},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@article{10.1007/s10458-023-09616-7,
author = {Jalota, Devansh and Solovey, Kiril and Tsao, Matthew and Zoepf, Stephen and Pavone, Marco},
title = {Balancing fairness and efficiency in traffic routing via interpolated traffic assignment},
year = {2023},
issue_date = {Dec 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {37},
number = {2},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-023-09616-7},
doi = {10.1007/s10458-023-09616-7},
abstract = {System optimum (SO) routing, wherein the total travel time of all users is minimized, is a holy grail for transportation authorities. However, SO routing may discriminate against users who incur much larger travel times than others to achieve high system efficiency, i.e., low total travel times. To address the inherent unfairness of SO routing, we study the β-fair SO problem whose goal is to minimize the total travel time while guaranteeing a β≥1 level of unfairness, which specifies the maximum possible ratio between the travel times of different users with shared origins and destinations. To obtain feasible solutions to the β-fair SO problem while achieving high system efficiency, we develop a new convex program, the interpolated traffic assignment problem (I-TAP), which interpolates between a fairness-promoting and an efficiency-promoting traffic-assignment objective. We evaluate the efficacy of I-TAP through theoretical bounds on the total system travel time and level of unfairness in terms of its interpolation parameter, as well as present a numerical comparison between I-TAP and a state-of-the-art algorithm on a range of transportation networks. The numerical results indicate that our approach is faster by several orders of magnitude as compared to the benchmark algorithm, while achieving higher system efficiency for all desirable levels of unfairness. We further leverage the structure of I-TAP to develop two pricing mechanisms to collectively enforce the I-TAP solution in the presence of selfish homogeneous and heterogeneous users, respectively, that independently choose routes to minimize their own travel costs. We mention that this is the first study of pricing in the context of fair routing for general road networks (as opposed to, e.g., parallel road networks).},
journal = {Autonomous Agents and Multi-Agent Systems},
month = {aug},
numpages = {40},
keywords = {Traffic assignment, Congestion games, Nash equilibria}
}

@article{10.1287/mnsc.2022.4558,
author = {Niazadeh, Rad and Golrezaei, Negin and Wang, Joshua and Susan, Fransisca and Badanidiyuru, Ashwinkumar},
title = {Online Learning via Offline Greedy Algorithms: Applications in Market Design and Optimization},
year = {2023},
issue_date = {July 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {69},
number = {7},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2022.4558},
doi = {10.1287/mnsc.2022.4558},
abstract = {Motivated by online decision making in time-varying combinatorial environments, we study the problem of transforming offline algorithms to their online counterparts. We focus on offline combinatorial problems that are amenable to a constant factor approximation using a greedy algorithm that is robust to local errors. For such problems, we provide a general framework that efficiently transforms offline robust greedy algorithms to online ones using Blackwell approachability. We show that the resulting online algorithms have O(T) (approximate) regret under the full information setting. We further introduce a bandit extension of Blackwell approachability that we call Bandit Blackwell approachability. We leverage this notion to transform greedy robust offline algorithms into a O(T2/3) (approximate) regret in the bandit setting. Demonstrating the flexibility of our framework, we apply our offline-to-online transformation to several problems at the intersection of revenue management, market design, and online optimization, including product ranking optimization in online platforms, reserve price optimization in auctions, and submodular maximization. We also extend our reduction to greedy-like first-order methods used in continuous optimization, such as those used for maximizing continuous strong DR monotone submodular functions subject to convex constraints. We show that our transformation, when applied to these applications, leads to new regret bounds or improves the current known bounds. We complement our theoretical studies by conducting numerical simulations for two of our applications, in both of which we observe that the numerical performance of our transformations outperforms the theoretical guarantees in practical instances.This paper was accepted by George Shanthikumar, data science.Funding: R. Niazadeh was supported by research funding from University of Chicago Booth School of Business. N. Golrezaei was supported in part by the Young Investigator Program Award from the Office of Naval Research [N00014-21-1-2776] and the MIT Research Support Award.Supplemental Material: The Online Appendix is available at},
journal = {Manage. Sci.},
month = {jul},
pages = {3797–3817},
numpages = {21},
keywords = {Blackwell approachability, offline-to-online, no regret, submodular maximization, product ranking, reserve price optimization}
}

@inproceedings{10.1145/3623762.3633498,
author = {Ericson, Barbara J. and Pearce, Janice L. and Rodger, Susan H. and Csizmadia, Andrew and Garcia, Rita and Gutierrez, Francisco J. and Liaskos, Konstantinos and Padiyath, Aadarsh and Scott, Michael James and Smith, David H. and Warriem, Jayakrishnan M. and Zavaleta Bernuy, Angela},
title = {Multi-Institutional Multi-National Studies of Parsons Problems},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633498},
doi = {10.1145/3623762.3633498},
abstract = {Students are often asked to learn programming by writing code from scratch. However, many novices struggle to write code and get frustrated when their code does not work. Parsons problems can reduce the difficulty of a coding problem by providing mixed-up blocks the learner rearranges into the correct order. These mixed-up blocks can include distractor blocks that are not needed in a correct solution. Distractor blocks can include common errors, which may help students learn to recognize and fix such errors. Evidence suggests students find Parsons problems engaging, useful for learning to program, and typically easier and faster to solve than writing code from scratch, but with equivalent learning gains. Most research on Parsons problems prior to this work has been conducted at a single institution. This work addresses the need for replication across multiple contexts.A 2022 ITiCSE Parsons Problems Working Group conducted an extensive literature review of Parsons problems, designed several experimental studies for Parsons problems in Python, and created 'study-in-a-box' materials to help instructors run the experimental studies, but the 2022 working group had only sufficient time to pilot two of these studies.Our 2023 ITiCSE Parsons Problems Working Group reviewed these studies, revised some of the studies, expanded both the programming and natural languages used in some of the studies, created new studies, conducted think-aloud observations on some of the studies, and ran both revised as well as new experimental studies. The think-aloud observations and experimental studies provide evidence for using Parsons problems to help students learn common algorithms such as swap, and the usefulness of distractors in helping students learn to recognize, fix, and avoid common errors. In addition, our 2023 ITiCSE Parsons Problems Working Group reviewed Parsons problem papers published after the 2022 literature review and provided a literature review of multi-national (MIMN) studies conducted in computer science education to better understand the motivations and challenges in performing such MIMN studies.In summary, this article contributes an analysis of recent Parsons problem research papers, an itemization of considerations for MIMN studies, the results from our MIMN studies of Parsons problems, and a discussion of recent and future directions for MIMN studies of Parsons problems and more generally.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {57–107},
numpages = {51},
keywords = {code puzzles, multi-institutional multi-national study, multi-institutional study, multi-national study, parson's problems, parson's programming puzzles, parson's puzzles, parsons problems, parsons puzzles},
location = {<conf-loc>, <city>Turku</city>, <country>Finland</country>, </conf-loc>},
series = {ITiCSE-WGR '23}
}

