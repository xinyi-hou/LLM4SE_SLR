
@inproceedings{lin_traceability_2021,
	location = {Madrid, {ES}},
	title = {Traceability Transformed: Generating More Accurate Links with Pre-Trained {BERT} Models},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9402118/},
	doi = {10.1109/ICSE43902.2021.00040},
	shorttitle = {Traceability Transformed},
	eventtitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	pages = {324--335},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
	urldate = {2023-06-10},
	date = {2021-05},
	file = {已提交版本:files/208205/Lin 等 - 2021 - Traceability Transformed Generating More Accurate.pdf:application/pdf},
}

@incollection{wang_extraction_2020,
	location = {Cham},
	title = {Extraction and Portrait of Knowledge Points for Open Learning Resources},
	volume = {12432},
	isbn = {978-3-030-60028-0 978-3-030-60029-7},
	url = {http://link.springer.com/10.1007/978-3-030-60029-7_5},
	pages = {46--56},
	booktitle = {Web Information Systems and Applications},
	publisher = {Springer International Publishing},
	author = {Yu, Jian and Jiang, Tingxu and Xu, Tianyi and Gao, Jie and Chen, Jun and Yu, Mei and Zhao, Mankun},
	editor = {Wang, Guojun and Lin, Xuemin and Hendler, James and Song, Wei and Xu, Zhuoming and Liu, Genggeng},
	urldate = {2023-06-10},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-60029-7_5},
	note = {Series Title: Lecture Notes in Computer Science},
}

@incollection{uchida_how_2022,
	location = {Cham},
	title = {How Confident Was Your Reviewer? Estimating Reviewer Confidence from Peer Review Texts},
	volume = {13237},
	isbn = {978-3-031-06554-5 978-3-031-06555-2},
	url = {https://link.springer.com/10.1007/978-3-031-06555-2_9},
	shorttitle = {How Confident Was Your Reviewer?},
	pages = {126--139},
	booktitle = {Document Analysis Systems},
	publisher = {Springer International Publishing},
	author = {Bharti, Prabhat Kumar and Ghosal, Tirthankar and Agrawal, Mayank and Ekbal, Asif},
	editor = {Uchida, Seiichi and Barney, Elisa and Eglin, Véronique},
	urldate = {2023-06-10},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-06555-2_9},
	note = {Series Title: Lecture Notes in Computer Science},
}

@incollection{mantoro_hierarchical_2021,
	location = {Cham},
	title = {A Hierarchical Graph-Based Neural Network for Malware Classification},
	volume = {13111},
	isbn = {978-3-030-92272-6 978-3-030-92273-3},
	url = {https://link.springer.com/10.1007/978-3-030-92273-3_51},
	pages = {621--633},
	booktitle = {Neural Information Processing},
	publisher = {Springer International Publishing},
	author = {Wang, Shuai and Zhao, Yuran and Liu, Gongshen and Su, Bo},
	editor = {Mantoro, Teddy and Lee, Minho and Ayu, Media Anugerah and Wong, Kok Wai and Hidayanto, Achmad Nizar},
	urldate = {2023-06-10},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-92273-3_51},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{lin_information_2022,
	title = {Information retrieval versus deep learning approaches for generating traceability links in bilingual projects},
	volume = {27},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/10.1007/s10664-021-10050-0},
	doi = {10.1007/s10664-021-10050-0},
	pages = {5},
	number = {1},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Lin, Jinfeng and Liu, Yalin and Cleland-Huang, Jane},
	urldate = {2023-06-10},
	date = {2022-01},
	langid = {english},
}

@incollection{chen_software_2020,
	location = {Cham},
	title = {Software Entity Recognition Method Based on {BERT} Embedding},
	volume = {12488},
	isbn = {978-3-030-62462-0 978-3-030-62463-7},
	url = {http://link.springer.com/10.1007/978-3-030-62463-7_4},
	pages = {33--47},
	booktitle = {Machine Learning for Cyber Security},
	publisher = {Springer International Publishing},
	author = {Sun, Chao and Tang, Mingjing and Liang, Li and Zou, Wei},
	editor = {Chen, Xiaofeng and Yan, Hongyang and Yan, Qiben and Zhang, Xiangliang},
	urldate = {2023-06-10},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-62463-7_4},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{mastropaolo_studying_2021,
	location = {Madrid, {ES}},
	title = {Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},
	isbn = {978-1-66540-296-5},
	url = {https://ieeexplore.ieee.org/document/9401982/},
	doi = {10.1109/ICSE43902.2021.00041},
	eventtitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	pages = {336--347},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Nader Palacio, David and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021-05},
	file = {已提交版本:files/208212/Mastropaolo 等 - 2021 - Studying the Usage of Text-To-Text Transfer Transf.pdf:application/pdf},
}

@article{sharma_self-admitted_2022,
	title = {Self-admitted technical debt in R: detection and causes},
	volume = {29},
	issn = {0928-8910, 1573-7535},
	url = {https://link.springer.com/10.1007/s10515-022-00358-6},
	doi = {10.1007/s10515-022-00358-6},
	shorttitle = {Self-admitted technical debt in R},
	abstract = {Abstract
            
              Self-Admitted Technical Debt ({SATD}) is primarily studied in Object-Oriented ({OO}) languages and traditionally commercial software. However, scientific software coded in dynamically-typed languages such as R differs in paradigm, and the source code comments’ semantics are different (i.e., more aligned with algorithms and statistics when compared to traditional software). Additionally, many Software Engineering topics are understudied in scientific software development, with {SATD} detection remaining a challenge for this domain. This gap adds complexity since prior works determined {SATD} in scientific software does not adjust to many of the keywords identified for {OO} {SATD}, possibly hindering its automated detection. Therefore, we investigated how classification models (traditional machine learning, deep neural networks, and deep neural Pre-Trained Language Models ({PTMs})) automatically detect {SATD} in R packages. This study aims to study the capabilities of these models to classify different {TD} types in this domain and manually analyze the causes of each in a representative sample. Our results show that {PTMs} (i.e., {RoBERTa}) outperform other models and work well when the number of comments labelled as a particular {SATD} type has low occurrences. We also found that some {SATD} types are more challenging to detect. We manually identified sixteen causes, including eight new causes detected by our study. The most common cause was
              failure to remember
              , in agreement with previous studies. These findings will help the R package authors automatically identify {SATD} in their source code and improve their code quality. In the future, checklists for R developers can also be developed by scientific communities such as {rOpenSci} to guarantee a higher quality of packages before submission.},
	pages = {53},
	number = {2},
	journaltitle = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Sharma, Rishab and Shahbazi, Ramin and Fard, Fatemeh H. and Codabux, Zadia and Vidoni, Melina},
	urldate = {2023-06-10},
	date = {2022-11},
	langid = {english},
	file = {全文:files/208176/Sharma 等 - 2022 - Self-admitted technical debt in R detection and c.pdf:application/pdf},
}

@inproceedings{wan_what_2022,
	location = {Pittsburgh Pennsylvania},
	title = {What do they capture?: a structural analysis of pre-trained language models for source code},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510050},
	doi = {10.1145/3510003.3510050},
	shorttitle = {What do they capture?},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2377--2388},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Wan, Yao and Zhao, Wei and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
}

@inproceedings{liu_multi-task_2020,
	location = {Virtual Event Australia},
	title = {Multi-task learning based pre-trained language model for code completion},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416591},
	doi = {10.1145/3324884.3416591},
	eventtitle = {{ASE} '20: 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {473--485},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2020-12-21},
	langid = {english},
	file = {已提交版本:files/208229/Liu 等 - 2020 - Multi-task learning based pre-trained language mod.pdf:application/pdf},
}

@inproceedings{tufano_generating_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Generating accurate assert statements for unit test cases using pretrained transformers},
	isbn = {978-1-4503-9286-0},
	url = {https://dl.acm.org/doi/10.1145/3524481.3527220},
	doi = {10.1145/3524481.3527220},
	eventtitle = {{AST} '22: {IEEE}/{ACM} 3rd International Conference on Automation of Software Test},
	pages = {54--64},
	booktitle = {Proceedings of the 3rd {ACM}/{IEEE} International Conference on Automation of Software Test},
	publisher = {{ACM}},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Sundaresan, Neel},
	urldate = {2023-06-10},
	date = {2022-05-17},
	langid = {english},
	file = {已提交版本:files/208182/Tufano 等 - 2022 - Generating accurate assert statements for unit tes.pdf:application/pdf},
}

@article{uddin_software_2022,
	title = {Software defect prediction employing {BiLSTM} and {BERT}-based semantic feature},
	volume = {26},
	issn = {1432-7643, 1433-7479},
	url = {https://link.springer.com/10.1007/s00500-022-06830-5},
	doi = {10.1007/s00500-022-06830-5},
	pages = {7877--7891},
	number = {16},
	journaltitle = {Soft Computing},
	shortjournal = {Soft Comput},
	author = {Uddin, Md Nasir and Li, Bixin and Ali, Zafar and Kefalas, Pavlos and Khan, Inayat and Zada, Islam},
	urldate = {2023-06-10},
	date = {2022-08},
	langid = {english},
}

@inproceedings{sobania_choose_2022,
	location = {Boston Massachusetts},
	title = {Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming},
	isbn = {978-1-4503-9237-2},
	url = {https://dl.acm.org/doi/10.1145/3512290.3528700},
	doi = {10.1145/3512290.3528700},
	shorttitle = {Choose your programming copilot},
	eventtitle = {{GECCO} '22: Genetic and Evolutionary Computation Conference},
	pages = {1019--1027},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	publisher = {{ACM}},
	author = {Sobania, Dominik and Briesch, Martin and Rothlauf, Franz},
	urldate = {2023-06-10},
	date = {2022-07-08},
	langid = {english},
}

@article{kolthoff_data-driven_2023,
	title = {Data-driven prototyping via natural-language-based {GUI} retrieval},
	volume = {30},
	issn = {0928-8910, 1573-7535},
	url = {https://link.springer.com/10.1007/s10515-023-00377-x},
	doi = {10.1007/s10515-023-00377-x},
	abstract = {Abstract
            
              Rapid {GUI} prototyping has evolved into a widely applied technique in early stages of software development to facilitate the clarification and refinement of requirements. Especially high-fidelity {GUI} prototyping has shown to enable productive discussions with customers and mitigate potential misunderstandings, however, the benefits of applying high-fidelity {GUI} prototypes are accompanied by the disadvantage of being expensive and time-consuming in development and requiring experience to create. In this work, we show
              {RaWi}
              , a data-driven {GUI} prototyping approach that effectively retrieves {GUIs} for reuse from a large-scale semi-automatically created {GUI} repository for mobile apps on the basis of Natural Language ({NL}) searches to facilitate {GUI} prototyping and improve its productivity by leveraging the vast {GUI} prototyping knowledge embodied in the repository. Retrieved {GUIs} can directly be reused and adapted in the graphical editor of
              {RaWi}
              . Moreover, we present a comprehensive evaluation methodology to enable (i) the systematic evaluation of {NL}-based {GUI} ranking methods through a novel high-quality gold standard and conduct an in-depth evaluation of traditional {IR} and state-of-the-art {BERT}-based models for {GUI} ranking, and (ii) the assessment of {GUI} prototyping productivity accompanied by an extensive user study in a practical {GUI} prototyping environment.},
	pages = {13},
	number = {1},
	journaltitle = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo},
	urldate = {2023-06-10},
	date = {2023-05},
	langid = {english},
	file = {全文:files/208175/Kolthoff 等 - 2023 - Data-driven prototyping via natural-language-based.pdf:application/pdf},
}

@article{wang_detecting_2022,
	title = {Detecting coreferent entities in natural language requirements},
	volume = {27},
	issn = {0947-3602, 1432-010X},
	url = {https://link.springer.com/10.1007/s00766-022-00374-8},
	doi = {10.1007/s00766-022-00374-8},
	pages = {351--373},
	number = {3},
	journaltitle = {Requirements Engineering},
	shortjournal = {Requirements Eng},
	author = {Wang, Yawen and Shi, Lin and Li, Mingyang and Wang, Qing and Yang, Yun},
	urldate = {2023-06-10},
	date = {2022-09},
	langid = {english},
}

@inproceedings{jangda_breaking_2022,
	location = {Lausanne Switzerland},
	title = {Breaking the computation and communication abstraction barrier in distributed machine learning workloads},
	isbn = {978-1-4503-9205-1},
	url = {https://dl.acm.org/doi/10.1145/3503222.3507778},
	doi = {10.1145/3503222.3507778},
	eventtitle = {{ASPLOS} '22: 27th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {402--416},
	booktitle = {Proceedings of the 27th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
	urldate = {2023-06-10},
	date = {2022-02-28},
	langid = {english},
	file = {已提交版本:files/208230/Jangda 等 - 2022 - Breaking the computation and communication abstrac.pdf:application/pdf},
}

@inproceedings{izadi_codefill_2022,
	location = {Pittsburgh Pennsylvania},
	title = {{CodeFill}: multi-token code completion by jointly learning from structure and naming sequences},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510172},
	doi = {10.1145/3510003.3510172},
	shorttitle = {{CodeFill}},
	abstract = {Code completion is an essential feature of {IDEs}, yet current autocompleters are restricted to either grammar-based or {NLP}-based single token completions. Both approaches have significant drawbacks: grammar-based autocompletion is restricted in dynamically-typed language environments, whereas {NLP}-based autocompleters struggle to understand the semantics of the programming language and the developer's code context. In this work, we present {CodeFill}, a language model for autocompletion that combines learned structure and naming information. Using a parallel Transformer architecture and multi-task learning, {CodeFill} consumes sequences of source code token names and their equivalent {AST} token types. Uniquely, {CodeFill} is trained both for single-token and multi-token (statement) prediction, which enables it to learn long-range dependencies among grammatical and naming elements. We train {CodeFill} on two datasets, consisting of 29M and 425M lines of code, respectively. To make the evaluation more realistic, we develop a method to automatically infer points in the source code at which completion matters. We compare {CodeFill} against four baselines and two state-of-the-art models, {GPT}-C and {TravTrans}+.{CodeFill} surpasses all baselines in single token prediction ({MRR}: 70.9\% vs. 66.2\% and 67.8\%) and outperforms the state of the art for multi-token prediction ({ROUGE}-L: 63.7\% vs. 52.4\% and 59.2\%, for n=4 tokens). We publicly release our source code and datasets.},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {401--412},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Izadi, Maliheh and Gismondi, Roberta and Gousios, Georgios},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.06689 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {全文:files/208207/Izadi 等 - 2022 - CodeFill multi-token code completion by jointly l.pdf:application/pdf;已提交版本:files/209588/Izadi 等 - 2022 - CodeFill multi-token code completion by jointly l.pdf:application/pdf;arXiv Fulltext PDF:files/208519/Izadi 等 - 2022 - CodeFill Multi-token Code Completion by Jointly L.pdf:application/pdf;arXiv.org Snapshot:files/209064/2202.html:text/html},
}

@inproceedings{chen_transferability_2022,
	location = {Virtual Event},
	title = {On the transferability of pre-trained language models for low-resource programming languages},
	isbn = {978-1-4503-9298-3},
	url = {https://dl.acm.org/doi/10.1145/3524610.3527917},
	doi = {10.1145/3524610.3527917},
	eventtitle = {{ICPC} '22: 30th International Conference on Program Comprehension},
	pages = {401--412},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} International Conference on Program Comprehension},
	publisher = {{ACM}},
	author = {Chen, Fuxiang and Fard, Fatemeh H. and Lo, David and Bryksin, Timofey},
	urldate = {2023-06-10},
	date = {2022-05-16},
	langid = {english},
	file = {全文:files/208206/Chen 等 - 2022 - On the transferability of pre-trained language mod.pdf:application/pdf},
}

@incollection{strauss_bert-based_2021,
	location = {Cham},
	title = {{BERT}-Based Sentiment Analysis: A Software Engineering Perspective},
	volume = {12923},
	isbn = {978-3-030-86471-2 978-3-030-86472-9},
	url = {https://link.springer.com/10.1007/978-3-030-86472-9_13},
	shorttitle = {{BERT}-Based Sentiment Analysis},
	abstract = {Sentiment analysis can provide a suitable lead for the tools used in software engineering along with the {API} recommendation systems and relevant libraries to be used. In this context, the existing tools like {SentiCR}, {SentiStrength}-{SE}, etc. exhibited low f1-scores that completely defeats the purpose of deployment of such strategies, thereby there is enough scope for performance improvement. Recent advancements show that transformer based pre-trained models (e.g., {BERT}, {RoBERTa}, {ALBERT}, etc.) have displayed better results in the text classification task. Following this context, the present research explores different {BERT}-based models to analyze the sentences in {GitHub} comments, Jira comments, and Stack Overflow posts. The paper presents three different strategies to analyse {BERT} based model for sentiment analysis, where in the first strategy the {BERT} based pre-trained models are fine-tuned; in the second strategy an ensemble model is developed from {BERT} variants, and in the third strategy a compressed model (Distil {BERT}) is used. The experimental results show that the {BERT} based ensemble approach and the compressed {BERT} model attain improvements by 6-12\% over prevailing tools for the F1 measure on all three datasets.},
	pages = {138--148},
	booktitle = {Database and Expert Systems Applications},
	publisher = {Springer International Publishing},
	author = {Batra, Himanshu and Punn, Narinder Singh and Sonbhadra, Sanjay Kumar and Agarwal, Sonali},
	editor = {Strauss, Christine and Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail},
	urldate = {2023-06-10},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-86472-9_13},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {已提交版本:files/208195/Batra 等 - 2021 - BERT-Based Sentiment Analysis A Software Engineer.pdf:application/pdf;arXiv.org Snapshot:files/209793/2106.html:text/html},
}

@incollection{gervasi_fine-tuning_2022,
	location = {Cham},
	title = {Fine-Tuning {GPT}-2 to Patch Programs, Is It Worth It?},
	volume = {13380},
	isbn = {978-3-031-10541-8 978-3-031-10542-5},
	url = {https://link.springer.com/10.1007/978-3-031-10542-5_6},
	pages = {79--91},
	booktitle = {Computational Science and Its Applications – {ICCSA} 2022 Workshops},
	publisher = {Springer International Publishing},
	author = {Lajkó, Márk and Horváth, Dániel and Csuvik, Viktor and Vidács, László},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Rocha, Ana Maria A. C. and Garau, Chiara},
	urldate = {2023-06-10},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-10542-5_6},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {已接受版本:files/208220/Lajkó 等 - 2022 - Fine-Tuning GPT-2 to Patch Programs, Is It Worth I.pdf:application/pdf},
}

@inproceedings{ciborowska_fast_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Fast changeset-based bug localization with {BERT}},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510042},
	doi = {10.1145/3510003.3510042},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {946--957},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Ciborowska, Agnieszka and Damevski, Kostadin},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {全文:files/208183/Ciborowska 和 Damevski - 2022 - Fast changeset-based bug localization with BERT.pdf:application/pdf},
}

@inproceedings{niu_spt-code_2022,
	location = {Pittsburgh Pennsylvania},
	title = {{SPT}-code: sequence-to-sequence pre-training for learning source code representations},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510096},
	doi = {10.1145/3510003.3510096},
	shorttitle = {{SPT}-code},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2006--2018},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Ge, Jidong and Huang, Liguo and Luo, Bin},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
}

@inproceedings{jain_jigsaw_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Jigsaw: large language models meet program synthesis},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510203},
	doi = {10.1145/3510003.3510203},
	shorttitle = {Jigsaw},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {1219--1231},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
}

@inproceedings{jiang_promptmaker_2022,
	location = {New Orleans {LA} {USA}},
	title = {{PromptMaker}: Prompt-based Prototyping with Large Language Models},
	isbn = {978-1-4503-9156-6},
	url = {https://dl.acm.org/doi/10.1145/3491101.3503564},
	doi = {10.1145/3491101.3503564},
	shorttitle = {{PromptMaker}},
	eventtitle = {{CHI} '22: {CHI} Conference on Human Factors in Computing Systems},
	pages = {1--8},
	booktitle = {{CHI} Conference on Human Factors in Computing Systems Extended Abstracts},
	publisher = {{ACM}},
	author = {Jiang, Ellen and Olson, Kristen and Toh, Edwin and Molina, Alejandra and Donsbach, Aaron and Terry, Michael and Cai, Carrie J},
	urldate = {2023-06-10},
	date = {2022-04-27},
	langid = {english},
}

@incollection{jia_ai-based_2021,
	location = {Cham},
	title = {{AI}-Based Language Chatbot 2.0 – The Design and Implementation of English Language Concept Learning Agent App},
	volume = {13089},
	isbn = {978-3-030-92835-3 978-3-030-92836-0},
	url = {https://link.springer.com/10.1007/978-3-030-92836-0_3},
	pages = {25--35},
	booktitle = {Emerging Technologies for Education},
	publisher = {Springer International Publishing},
	author = {Liu, Rui and Shu, Xin and Li, Peishan and Xu, Yinong and Yeung, Philip and Lee, Raymond},
	editor = {Jia, Weijia and Tang, Yong and Lee, Raymond S. T. and Herzog, Michael and Zhang, Hui and Hao, Tianyong and Wang, Tian},
	urldate = {2023-06-10},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-92836-0_3},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{gundogan_deep_2022,
	title = {Deep learning based conference program organization system from determining articles in session to scheduling},
	volume = {59},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457322002084},
	doi = {10.1016/j.ipm.2022.103107},
	pages = {103107},
	number = {6},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {Gündoğan, Esra and Kaya, Mehmet},
	urldate = {2023-06-10},
	date = {2022-11},
	langid = {english},
}

@inproceedings{shi_ispy_2021,
	location = {Melbourne, Australia},
	title = {{ISPY}: Automatic Issue-Solution Pair Extraction from Community Live Chats},
	isbn = {978-1-66540-337-5},
	url = {https://ieeexplore.ieee.org/document/9678894/},
	doi = {10.1109/ASE51524.2021.9678894},
	shorttitle = {{ISPY}},
	eventtitle = {2021 36th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {142--154},
	booktitle = {2021 36th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Shi, Lin and Jiang, Ziyou and Yang, Ye and Chen, Xiao and Zhang, Yumin and Mu, Fangwen and Jiang, Hanzhi and Wang, Qing},
	urldate = {2023-06-10},
	date = {2021-11},
	file = {已提交版本:files/208221/Shi 等 - 2021 - ISPY Automatic Issue-Solution Pair Extraction fro.pdf:application/pdf},
}

@article{gomes_bert-_2023,
	title = {{BERT}- and {TF}-{IDF}-based feature extraction for long-lived bug prediction in {FLOSS}: A comparative study},
	volume = {160},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095058492300071X},
	doi = {10.1016/j.infsof.2023.107217},
	shorttitle = {{BERT}- and {TF}-{IDF}-based feature extraction for long-lived bug prediction in {FLOSS}},
	pages = {107217},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Gomes, Luiz and Da Silva Torres, Ricardo and Côrtes, Mario Lúcio},
	urldate = {2023-06-10},
	date = {2023-08},
	langid = {english},
}

@incollection{taibi_predicting_2022,
	location = {Cham},
	title = {Predicting Bug-Fixing Time: {DistilBERT} Versus Google {BERT}},
	volume = {13709},
	isbn = {978-3-031-21387-8 978-3-031-21388-5},
	url = {https://link.springer.com/10.1007/978-3-031-21388-5_46},
	shorttitle = {Predicting Bug-Fixing Time},
	pages = {610--620},
	booktitle = {Product-Focused Software Process Improvement},
	publisher = {Springer International Publishing},
	author = {Ardimento, Pasquale},
	editor = {Taibi, Davide and Kuhrmann, Marco and Mikkonen, Tommi and Klünder, Jil and Abrahamsson, Pekka},
	urldate = {2023-06-10},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-21388-5_46},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{fatima_flakify_2023,
	title = {Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9866550/},
	doi = {10.1109/TSE.2022.3201209},
	shorttitle = {Flakify},
	pages = {1912--1927},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Fatima, Sakina and Ghaleb, Taher A. and Briand, Lionel},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/208252/Fatima 等 - 2023 - Flakify A Black-Box, Language Model-Based Predict.pdf:application/pdf},
}

@inproceedings{sharma_exploratory_2022,
	location = {Virtual Event},
	title = {An exploratory study on code attention in {BERT}},
	isbn = {978-1-4503-9298-3},
	url = {https://dl.acm.org/doi/10.1145/3524610.3527921},
	doi = {10.1145/3524610.3527921},
	eventtitle = {{ICPC} '22: 30th International Conference on Program Comprehension},
	pages = {437--448},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} International Conference on Program Comprehension},
	publisher = {{ACM}},
	author = {Sharma, Rishab and Chen, Fuxiang and Fard, Fatemeh and Lo, David},
	urldate = {2023-06-10},
	date = {2022-05-16},
	langid = {english},
	file = {全文:files/208196/Sharma 等 - 2022 - An exploratory study on code attention in BERT.pdf:application/pdf},
}

@article{rahmani_multi-modal_2021,
	title = {Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3485535},
	doi = {10.1145/3485535},
	shorttitle = {Multi-modal program inference},
	abstract = {Multi-modal program synthesis refers to the task of synthesizing programs (code) from their specification given in different forms, such as a combination of natural language and examples. Examples provide a precise but incomplete specification, and natural language provides an ambiguous but more "complete" task description. Machine-learned pre-trained models ({PTMs}) are adept at handling ambiguous natural language, but struggle with generating syntactically and semantically precise code. Program synthesis techniques can generate correct code, often even from incomplete but precise specifications, such as examples, but they are unable to work with the ambiguity of natural languages. We present an approach that combines {PTMs} with component-based synthesis ({CBS}): {PTMs} are used to generate candidates programs from the natural language description of the task, which are then used to guide the {CBS} procedure to find the program that matches the precise examples-based specification. We use our combination approach to instantiate multi-modal synthesis systems for two programming domains: the domain of regular expressions and the domain of {CSS} selectors. Our evaluation demonstrates the effectiveness of our domain-agnostic approach in comparison to a state-of-the-art specialized system, and the generality of our approach in providing multi-modal program synthesis from natural language and examples in different programming domains.},
	pages = {1--29},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Rahmani, Kia and Raza, Mohammad and Gulwani, Sumit and Le, Vu and Morris, Daniel and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish},
	urldate = {2023-06-10},
	date = {2021-10-20},
	langid = {english},
	file = {全文:files/208197/Rahmani 等 - 2021 - Multi-modal program inference a marriage of pre-t.pdf:application/pdf},
}

@article{tang_csgvd_2023,
	title = {{CSGVD}: A deep learning approach combining sequence and graph embedding for source code vulnerability detection},
	volume = {199},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121223000183},
	doi = {10.1016/j.jss.2023.111623},
	shorttitle = {{CSGVD}},
	pages = {111623},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Tang, Wei and Tang, Mingwei and Ban, Minchao and Zhao, Ziguo and Feng, Mingjun},
	urldate = {2023-06-10},
	date = {2023-05},
	langid = {english},
}

@article{danjou_manufacturing_2017,
	title = {Manufacturing knowledge management based on {STEP}-{NC} standard: a Closed-Loop Manufacturing approach},
	volume = {30},
	issn = {0951-192X, 1362-3052},
	url = {https://www.tandfonline.com/doi/full/10.1080/0951192X.2016.1268718},
	doi = {10.1080/0951192X.2016.1268718},
	shorttitle = {Manufacturing knowledge management based on {STEP}-{NC} standard},
	pages = {995--1009},
	number = {9},
	journaltitle = {International Journal of Computer Integrated Manufacturing},
	shortjournal = {International Journal of Computer Integrated Manufacturing},
	author = {Danjou, Christophe and Le Duigou, Julien and Eynard, Benoît},
	urldate = {2023-06-10},
	date = {2017-09-02},
	langid = {english},
}

@article{li_deep_2022,
	title = {Deep semantic mining of big multimedia data advertisements based on needs ontology construction},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-021-11892-y},
	doi = {10.1007/s11042-021-11892-y},
	pages = {28079--28102},
	number = {20},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Li, Zhiyi and Shen, Zhirui},
	urldate = {2023-06-10},
	date = {2022-08},
	langid = {english},
}

@inproceedings{he_ptm4tag_2022,
	location = {Virtual Event},
	title = {{PTM}4Tag: sharpening tag recommendation of stack overflow posts with pre-trained models},
	isbn = {978-1-4503-9298-3},
	url = {https://dl.acm.org/doi/10.1145/3524610.3527897},
	doi = {10.1145/3524610.3527897},
	shorttitle = {{PTM}4Tag},
	abstract = {Stack Overflow is often viewed as the most influential Software Question Answer ({SQA}) website with millions of programming-related questions and answers. Tags play a critical role in efficiently structuring the contents in Stack Overflow and are vital to support a range of site operations, e.g., querying relevant contents. Poorly selected tags often introduce extra noise and redundancy, which leads to tag synonym and tag explosion problems. Thus, an automated tag recommendation technique that can accurately recommend high-quality tags is desired to alleviate the problems mentioned above. Inspired by the recent success of pre-trained language models ({PTMs}) in natural language processing ({NLP}), we present {PTM}4Tag, a tag recommendation framework for Stack Overflow posts that utilize {PTMs} with a triplet architecture, which models the components of a post, i.e., Title, Description, and Code with independent language models. To the best of our knowledge, this is the first work that leverages {PTMs} in the tag recommendation task of {SQA} sites. We comparatively evaluate the performance of {PTM}4Tag based on five popular pre-trained models: {BERT}, {RoBERTa}, {ALBERT}, {CodeBERT}, and {BERTOverflow}. Our results show that leveraging the software engineering ({SE}) domain-specific {PTM} {CodeBERT} in {PTM}4Tag achieves the best performance among the five considered {PTMs} and outperforms the state-of-the-art deep learning (Convolutional Neural Network-based) approach by a large margin in terms of average \$Precision@k\$, \$Recall@k\$, and \$F1\$-\$score@k\$. We conduct an ablation study to quantify the contribution of a post's constituent components (Title, Description, and Code Snippets) to the performance of {PTM}4Tag. Our results show that Title is the most important in predicting the most relevant tags, and utilizing all the components achieves the best performance.},
	eventtitle = {{ICPC} '22: 30th International Conference on Program Comprehension},
	pages = {1--11},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} International Conference on Program Comprehension},
	publisher = {{ACM}},
	author = {He, Junda and Xu, Bowen and Yang, Zhou and Han, {DongGyun} and Yang, Chengran and Lo, David},
	urldate = {2023-06-10},
	date = {2022-05-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.10965 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {全文:files/208203/He 等 - 2022 - PTM4Tag sharpening tag recommendation of stack ov.pdf:application/pdf;arXiv Fulltext PDF:files/209058/He 等 - 2022 - PTM4Tag Sharpening Tag Recommendation of Stack Ov.pdf:application/pdf;arXiv.org Snapshot:files/209756/2203.html:text/html},
}

@incollection{wang_bert-based_2022,
	location = {Cham},
	title = {{BERT}-Based Vulnerability Type Identification with Effective Program Representation},
	volume = {13471},
	isbn = {978-3-031-19207-4 978-3-031-19208-1},
	url = {https://link.springer.com/10.1007/978-3-031-19208-1_23},
	pages = {271--282},
	booktitle = {Wireless Algorithms, Systems, and Applications},
	publisher = {Springer Nature Switzerland},
	author = {Zhu, Chenguang and Du, Gewangzi and Wu, Tongshuai and Cui, Ningning and Chen, Liwei and Shi, Gang},
	editor = {Wang, Lei and Segal, Michael and Chen, Jenhui and Qiu, Tie},
	urldate = {2023-06-10},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-19208-1_23},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{tufano_using_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Using pre-trained models to boost code review automation},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510621},
	doi = {10.1145/3510003.3510621},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2291--2302},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Tufano, Rosalia and Masiero, Simone and Mastropaolo, Antonio and Pascarella, Luca and Poshyvanyk, Denys and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {已提交版本:files/208227/Tufano 等 - 2022 - Using pre-trained models to boost code review auto.pdf:application/pdf},
}

@article{mastropaolo_using_2023,
	title = {Using Transfer Learning for Code-Related Tasks},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9797060/},
	doi = {10.1109/TSE.2022.3183297},
	pages = {1580--1598},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/208289/Mastropaolo 等 - 2023 - Using Transfer Learning for Code-Related Tasks.pdf:application/pdf},
}

@incollection{ait-ameur_automatic_2022,
	location = {Cham},
	title = {Automatic Classification of Bug Reports Based on Multiple Text Information and Reports’ Intention},
	volume = {13299},
	isbn = {978-3-031-10362-9 978-3-031-10363-6},
	url = {https://link.springer.com/10.1007/978-3-031-10363-6_9},
	pages = {131--147},
	booktitle = {Theoretical Aspects of Software Engineering},
	publisher = {Springer International Publishing},
	author = {Meng, Fanqi and Wang, Xuesong and Wang, Jingdong and Wang, Peifang},
	editor = {Aït-Ameur, Yamine and Crăciun, Florin},
	urldate = {2023-06-10},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-031-10363-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {已提交版本:files/208253/Meng 等 - 2022 - Automatic Classification of Bug Reports Based on M.pdf:application/pdf},
}

@article{verbruggen_semantic_2021,
	title = {Semantic programming by example with pre-trained models},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3485477},
	doi = {10.1145/3485477},
	abstract = {The ability to learn programs from few examples is a powerful technology with disruptive applications in many domains, as it allows users to automate repetitive tasks in an intuitive way. Existing frameworks on inductive synthesis only perform syntactic manipulations, where they rely on the syntactic structure of the given examples and not their meaning. Any semantic manipulations, such as transforming dates, have to be manually encoded by the designer of the inductive programming framework. Recent advances in large language models have shown these models to be very adept at performing semantic transformations of its input by simply providing a few examples of the task at hand. When it comes to syntactic transformations, however, these models are limited in their expressive power. In this paper, we propose a novel framework for integrating inductive synthesis with few-shot learning language models to combine the strength of these two popular technologies. In particular, the inductive synthesis is tasked with breaking down the problem in smaller subproblems, among which those that cannot be solved syntactically are passed to the language model. We formalize three semantic operators that can be integrated with inductive synthesizers. To minimize invoking expensive semantic operators during learning, we introduce a novel deferred query execution algorithm that considers the operators to be oracles during learning. We evaluate our approach in the domain of string transformations: the combination methodology can automate tasks that cannot be handled using either technologies by themselves. Finally, we demonstrate the generality of our approach via a case study in the domain of string profiling.},
	pages = {1--25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Verbruggen, Gust and Le, Vu and Gulwani, Sumit},
	urldate = {2023-06-10},
	date = {2021-10-20},
	langid = {english},
	file = {全文:files/208213/Verbruggen 等 - 2021 - Semantic programming by example with pre-trained m.pdf:application/pdf},
}

@incollection{zhu_anaphora_2020,
	location = {Cham},
	title = {Anaphora Resolution in Chinese for Analysis of Medical Q\&A Platforms},
	volume = {12431},
	isbn = {978-3-030-60456-1 978-3-030-60457-8},
	url = {http://link.springer.com/10.1007/978-3-030-60457-8_40},
	pages = {490--497},
	booktitle = {Natural Language Processing and Chinese Computing},
	publisher = {Springer International Publishing},
	author = {Tsvetkova, Alena},
	editor = {Zhu, Xiaodan and Zhang, Min and Hong, Yu and He, Ruifang},
	urldate = {2023-06-10},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-60457-8_40},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{lajko_towards_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Towards {JavaScript} program repair with generative pre-trained transformer ({GPT}-2)},
	isbn = {978-1-4503-9285-3},
	url = {https://dl.acm.org/doi/10.1145/3524459.3527350},
	doi = {10.1145/3524459.3527350},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {61--68},
	booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
	publisher = {{ACM}},
	author = {Lajkó, Márk and Csuvik, Viktor and Vidács, László},
	urldate = {2023-06-10},
	date = {2022-05-19},
	langid = {english},
	file = {全文:files/208216/Lajkó 等 - 2022 - Towards JavaScript program repair with generative .pdf:application/pdf},
}

@incollection{siuly_research_2021,
	location = {Cham},
	title = {Research on the Fluctuation Characteristics of Social Media Message Sentiment with Time Before and During the {COVID}-19 Epidemic},
	volume = {13079},
	isbn = {978-3-030-90884-3 978-3-030-90885-0},
	url = {https://link.springer.com/10.1007/978-3-030-90885-0_1},
	pages = {3--14},
	booktitle = {Health Information Science},
	publisher = {Springer International Publishing},
	author = {Guo, Chaohui and Lin, Shaofu and Huang, Zhisheng and Shi, Chengyu and Yao, Yahong},
	editor = {Siuly, Siuly and Wang, Hua and Chen, Lu and Guo, Yanhui and Xing, Chunxiao},
	urldate = {2023-06-10},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-90885-0_1},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {全文:files/208225/Guo 等 - 2021 - Research on the Fluctuation Characteristics of Soc.pdf:application/pdf},
}

@article{salza_effectiveness_2023,
	title = {On the Effectiveness of Transfer Learning for Code Search},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9835142/},
	doi = {10.1109/TSE.2022.3192755},
	pages = {1804--1822},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C.},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/208228/Salza 等 - 2023 - On the Effectiveness of Transfer Learning for Code.pdf:application/pdf},
}

@inproceedings{finnie-ansley_robots_2022,
	location = {Virtual Event Australia},
	title = {The Robots Are Coming: Exploring the Implications of {OpenAI} Codex on Introductory Programming},
	isbn = {978-1-4503-9643-1},
	url = {https://dl.acm.org/doi/10.1145/3511861.3511863},
	doi = {10.1145/3511861.3511863},
	shorttitle = {The Robots Are Coming},
	eventtitle = {{ACE} '22: Australasian Computing Education Conference},
	pages = {10--19},
	booktitle = {Australasian Computing Education Conference},
	publisher = {{ACM}},
	author = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A. and Luxton-Reilly, Andrew and Prather, James},
	urldate = {2023-06-10},
	date = {2022-02-14},
	langid = {english},
	file = {全文:files/208224/Finnie-Ansley 等 - 2022 - The Robots Are Coming Exploring the Implications .pdf:application/pdf},
}

@inproceedings{mastropaolo_using_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Using deep learning to generate complete log statements},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3511561},
	doi = {10.1145/3510003.3511561},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2279--2290},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Mastropaolo, Antonio and Pascarella, Luca and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {已提交版本:files/208236/Mastropaolo 等 - 2022 - Using deep learning to generate complete log state.pdf:application/pdf},
}

@inproceedings{bhatia_sector_2021,
	location = {Virtual Event Republic of Korea},
	title = {Sector classification for crowd-based software requirements},
	isbn = {978-1-4503-8104-8},
	url = {https://dl.acm.org/doi/10.1145/3412841.3442005},
	doi = {10.1145/3412841.3442005},
	eventtitle = {{SAC} '21: The 36th {ACM}/{SIGAPP} Symposium on Applied Computing},
	pages = {1312--1320},
	booktitle = {Proceedings of the 36th Annual {ACM} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {Bhatia, Kushagra and Sharma, Arpit},
	urldate = {2023-06-10},
	date = {2021-03-22},
	langid = {english},
}

@article{sun_assbert_2023,
	title = {{ASSBert}: Active and semi-supervised bert for smart contract vulnerability detection},
	volume = {73},
	issn = {22142126},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S221421262300008X},
	doi = {10.1016/j.jisa.2023.103423},
	shorttitle = {{ASSBert}},
	pages = {103423},
	journaltitle = {Journal of Information Security and Applications},
	shortjournal = {Journal of Information Security and Applications},
	author = {Sun, Xiaobing and Tu, Liangqiong and Zhang, Jiale and Cai, Jie and Li, Bin and Wang, Yu},
	urldate = {2023-06-10},
	date = {2023-03},
	langid = {english},
}

@incollection{ferrari_requirements_2023,
	location = {Cham},
	title = {Requirements Classification Using {FastText} and {BETO} in Spanish Documents},
	volume = {13975},
	isbn = {978-3-031-29785-4 978-3-031-29786-1},
	url = {https://link.springer.com/10.1007/978-3-031-29786-1_11},
	pages = {159--176},
	booktitle = {Requirements Engineering: Foundation for Software Quality},
	publisher = {Springer Nature Switzerland},
	author = {Limaylla-Lunarejo, María-Isabel and Condori-Fernandez, Nelly and Luaces, Miguel R.},
	editor = {Ferrari, Alessio and Penzenstadler, Birgit},
	urldate = {2023-06-10},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-29786-1_11},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{chen_varclr_2022,
	location = {Pittsburgh Pennsylvania},
	title = {{VarCLR}: variable semantic representation pre-training via contrastive learning},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510162},
	doi = {10.1145/3510003.3510162},
	shorttitle = {{VarCLR}},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2327--2339},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J. and Neubig, Graham and Vasilescu, Bogdan and Goues, Claire Le},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {全文:files/208226/Chen 等 - 2022 - VarCLR variable semantic representation pre-train.pdf:application/pdf},
}

@inproceedings{tian_evaluating_2020,
	location = {Virtual Event Australia},
	title = {Evaluating representation learning of code changes for predicting patch correctness in program repair},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416532},
	doi = {10.1145/3324884.3416532},
	eventtitle = {{ASE} '20: 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {981--992},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Tian, Haoye and Liu, Kui and Kaboré, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyandé, Tegawendé F.},
	urldate = {2023-06-10},
	date = {2020-12-21},
	langid = {english},
	file = {已提交版本:files/208265/Tian 等 - 2020 - Evaluating representation learning of code changes.pdf:application/pdf},
}

@article{von_der_mosel_validity_2023,
	title = {On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9785808/},
	doi = {10.1109/TSE.2022.3178469},
	pages = {1487--1507},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Von Der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/208237/Von Der Mosel 等 - 2023 - On the Validity of Pre-Trained Transformers for Na.pdf:application/pdf},
}

@inproceedings{moharil_identification_2022,
	location = {Pittsburgh Pennsylvania},
	title = {Identification of intra-domain ambiguity using transformer-based machine learning},
	isbn = {978-1-4503-9343-0},
	url = {https://dl.acm.org/doi/10.1145/3528588.3528651},
	doi = {10.1145/3528588.3528651},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {51--58},
	booktitle = {Proceedings of the 1st International Workshop on Natural Language-based Software Engineering},
	publisher = {{ACM}},
	author = {Moharil, Ambarish and Sharma, Arpit},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {全文:files/208268/Moharil 和 Sharma - 2022 - Identification of intra-domain ambiguity using tra.pdf:application/pdf},
}

@inproceedings{robe_pair_2022,
	location = {Singapore Singapore},
	title = {Pair programming conversations with agents vs. developers: challenges and opportunities for {SE} community},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549127},
	doi = {10.1145/3540250.3549127},
	shorttitle = {Pair programming conversations with agents vs. developers},
	eventtitle = {{ESEC}/{FSE} '22: 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages = {319--331},
	booktitle = {Proceedings of the 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Robe, Peter and Kuttal, Sandeep K. and {AuBuchon}, Jake and Hart, Jacob},
	urldate = {2023-06-10},
	date = {2022-11-07},
	langid = {english},
}

@inproceedings{zhang_using_2022,
	location = {Virtual South Korea},
	title = {Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534396},
	doi = {10.1145/3533767.3534396},
	eventtitle = {{ISSTA} '22: 31st {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	pages = {77--88},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	publisher = {{ACM}},
	author = {Zhang, Jialu and Mytkowicz, Todd and Kaufman, Mike and Piskac, Ruzica and Lahiri, Shuvendu K.},
	urldate = {2023-06-10},
	date = {2022-07-18},
	langid = {english},
}

@inproceedings{parthasarathy_measuring_2022,
	location = {Singapore Singapore},
	title = {Measuring design compliance using neural language models: an automotive case study},
	isbn = {978-1-4503-9860-2},
	url = {https://dl.acm.org/doi/10.1145/3558489.3559067},
	doi = {10.1145/3558489.3559067},
	shorttitle = {Measuring design compliance using neural language models},
	eventtitle = {{PROMISE} '22: 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
	pages = {12--21},
	booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
	publisher = {{ACM}},
	author = {Parthasarathy, Dhasarathy and Ekelin, Cecilia and Karri, Anjali and Sun, Jiapeng and Moraitis, Panagiotis},
	urldate = {2023-06-10},
	date = {2022-11-07},
	langid = {english},
	file = {全文:files/208281/Parthasarathy 等 - 2022 - Measuring design compliance using neural language .pdf:application/pdf},
}

@article{gong_what_2023,
	title = {What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3569934},
	doi = {10.1145/3569934},
	shorttitle = {What Is the Intended Usage Context of This Model?},
	abstract = {There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering ({SE}) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the {SE} field, reuse {SE} could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and {FactSheet}’s guidance for suppliers of pre-trained models on what information they should be published, we propose
              model contracts
              including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the {TensorFlow} Hub, {PyTorch} Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.},
	pages = {1--57},
	number = {3},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Gong, Lina and Zhang, Jingxuan and Wei, Mingqiang and Zhang, Haoxiang and Huang, Zhiqiu},
	urldate = {2023-06-10},
	date = {2023-07-31},
	langid = {english},
	file = {全文:files/208245/Gong 等 - 2023 - What Is the Intended Usage Context of This Model .pdf:application/pdf},
}

@inproceedings{jell_towards_2023,
	location = {Seeon/Bavaria Germany},
	title = {Towards Automated Interactive Tutoring - Focussing on Misconceptions and Adaptive Level-Specific Feedback},
	isbn = {978-1-4503-9956-2},
	url = {https://dl.acm.org/doi/10.1145/3593663.3593692},
	doi = {10.1145/3593663.3593692},
	eventtitle = {{ECSEE} 2023: European Conference on Software Engineering Education},
	pages = {226--235},
	booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
	publisher = {{ACM}},
	author = {Jell, Lea and List, Corinna and Kipp, Michael},
	urldate = {2023-06-10},
	date = {2023-06-19},
	langid = {english},
}

@article{yuan_deep_2022,
	title = {Deep Neural Embedding for Software Vulnerability Discovery: Comparison and Optimization},
	volume = {2022},
	issn = {1939-0122, 1939-0114},
	url = {https://www.hindawi.com/journals/scn/2022/5203217/},
	doi = {10.1155/2022/5203217},
	shorttitle = {Deep Neural Embedding for Software Vulnerability Discovery},
	abstract = {Due to multitudinous vulnerabilities in sophisticated software programs, the detection performance of existing approaches requires further improvement. Multiple vulnerability detection approaches have been proposed to aid code inspection. Among them, there is a line of approaches that apply deep learning ({DL}) techniques and achieve promising results. This paper attempts to utilize {CodeBERT} which is a deep contextualized model as an embedding solution to facilitate the detection of vulnerabilities in C open-source projects. The application of {CodeBERT} for code analysis allows the rich and latent patterns within software code to be revealed, having the potential to facilitate various downstream tasks such as the detection of software vulnerability. {CodeBERT} inherits the architecture of {BERT}, providing a stacked encoder of transformer in a bidirectional structure. This facilitates the learning of vulnerable code patterns which requires long-range dependency analysis. Additionally, the multihead attention mechanism of transformer enables multiple key variables of a data flow to be focused, which is crucial for analyzing and tracing potentially vulnerable data flaws, eventually, resulting in optimized detection performance. To evaluate the effectiveness of the proposed {CodeBERT}-based embedding solution, four mainstream-embedding methods are compared for generating software code embeddings, including Word2Vec, {GloVe}, and {FastText}. Experimental results show that {CodeBERT}-based embedding outperforms other embedding models on the downstream vulnerability detection tasks. To further boost performance, we proposed to include synthetic vulnerable functions and perform synthetic and real-world data fine tuning to facilitate the model learning of C-related vulnerable code patterns. Meanwhile, we explored the suitable configuration of {CodeBERT}. The evaluation results show that the model with new parameters outperform some state-of-the-art detection methods in our dataset.},
	pages = {1--12},
	journaltitle = {Security and Communication Networks},
	shortjournal = {Security and Communication Networks},
	author = {Yuan, Xue and Lin, Guanjun and Tai, Yonghang and Zhang, Jun},
	editor = {Meng, Weizhi},
	urldate = {2023-06-10},
	date = {2022-01-18},
	langid = {english},
	file = {全文:files/208258/Yuan 等 - 2022 - Deep Neural Embedding for Software Vulnerability D.pdf:application/pdf},
}

@inproceedings{pei_neudep_2022,
	location = {Singapore Singapore},
	title = {{NeuDep}: neural binary memory dependence analysis},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549147},
	doi = {10.1145/3540250.3549147},
	shorttitle = {{NeuDep}},
	eventtitle = {{ESEC}/{FSE} '22: 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages = {747--759},
	booktitle = {Proceedings of the 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Pei, Kexin and She, Dongdong and Wang, Michael and Geng, Scott and Xuan, Zhou and David, Yaniv and Yang, Junfeng and Jana, Suman and Ray, Baishakhi},
	urldate = {2023-06-10},
	date = {2022-11-07},
	langid = {english},
	file = {全文:files/208272/Pei 等 - 2022 - NeuDep neural binary memory dependence analysis.pdf:application/pdf},
}

@inproceedings{ross_programmers_2023,
	location = {Sydney {NSW} Australia},
	title = {The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development},
	isbn = {9798400701061},
	url = {https://dl.acm.org/doi/10.1145/3581641.3584037},
	doi = {10.1145/3581641.3584037},
	shorttitle = {The Programmer’s Assistant},
	abstract = {Large language models ({LLMs}) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model's responses. We developed a prototype system -- the Programmer's Assistant -- in order to explore the utility of conversational interactions grounded in code, as well as software engineers' receptiveness to the idea of conversing with, rather than invoking, a code-fluent {LLM}. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the {LLM}. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant's capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with {LLMs} for co-creative processes like software development.},
	eventtitle = {{IUI} '23: 28th International Conference on Intelligent User Interfaces},
	pages = {491--514},
	booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
	publisher = {{ACM}},
	author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
	urldate = {2023-06-10},
	date = {2023-03-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.07080 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {全文:files/208248/Ross 等 - 2023 - The Programmer’s Assistant Conversational Interac.pdf:application/pdf;arXiv Fulltext PDF:files/209397/Ross 等 - 2023 - The Programmer's Assistant Conversational Interac.pdf:application/pdf;arXiv.org Snapshot:files/209401/2302.html:text/html},
}

@article{li_ptr4bert_2022,
	title = {Ptr4BERT: Automatic Semisupervised Chinese Government Message Text Classification Method Based on Transformer-Based Pointer Generator Network},
	volume = {2022},
	issn = {1687-5699, 1687-5680},
	url = {https://www.hindawi.com/journals/am/2022/6540696/},
	doi = {10.1155/2022/6540696},
	shorttitle = {Ptr4BERT},
	abstract = {With the development of Internet technology, government affairs can be handled online. More and more citizens are using online platforms to report to government departments, which is generating a lot of textual data. Among them, the basic but important problem is to automatically classify the different categories of messages, so that staff from different departments can process relevant information quickly. However, government messages have problems such as fast update rate, a large amount of information, long texts, and difficulty in capturing key points, which make supervised learning methods unsuitable for processing such texts. To address these problems, we propose a semisupervised text classification method based on a transformer-based pointer generator network named Ptr4BERT, which uses the pointer generator network with {BERT}(bidirectional encoder representation from transformers) embedding as a preprocessor for feature extraction. In this method, text classification can achieve very good results with a small set of labeled data, by extracting features exclusively from the message text. In order to verify the effect of our proposed model, we performed some experiments. Besides, we designed a crawler program and obtained two datasets from different websites, which are named {HNMes} and {QDMes}. Experimental results have shown that the proposed method outperforms the state-of-the-art methods significantly.},
	pages = {1--11},
	journaltitle = {Advances in Multimedia},
	shortjournal = {Advances in Multimedia},
	author = {Li, Mingxin and Yin, Kaiqian and Wang, Minghao},
	editor = {Khosravi, Mohammad R.},
	urldate = {2023-06-10},
	date = {2022-08-27},
	langid = {english},
	file = {全文:files/208259/Li 等 - 2022 - Ptr4BERT Automatic Semisupervised Chinese Governm.pdf:application/pdf},
}

@article{liu_rise_2023,
	title = {Rise of Distributed Deep Learning Training in the Big Model Era: From A Software Engineering Perspective},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3597204},
	doi = {10.1145/3597204},
	shorttitle = {Rise of Distributed Deep Learning Training in the Big Model Era},
	abstract = {Deep learning ({DL}) has become a key component of modern software. In the “
              big model
              ” era, the rich features of {DL}-based software (i.e., {DL} software) substantially rely on powerful {DL} models, e.g., {BERT}, {GPT}-3, and the recently emerging {GPT}-4, which are trained on the powerful cloud with large datasets. Hence, training effective {DL} models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of {GPUs}) in the training process, which is known as
              distributed deep learning training
              , or
              distributed training
              for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current {DL}-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers’ issues in distributed training. To this end, we focus on popular {DL} frameworks that support distributed training (including {TensorFlow}, {PyTorch}, Keras, and Horovod) and analyze 1,131 real-world developers’ issues about using these frameworks reported on Stack Overflow and {GitHub}. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that : (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop {DL}-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless {APIs} for cloud platforms.},
	pages = {3597204},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Liu, Xuanzhe and Gu, Diandian and Chen, Zhenpeng and Wen, Jinfeng and Zhang, Zili and Ma, Yun and Wang, Haoyu and Jin, Xin},
	urldate = {2023-06-10},
	date = {2023-05-13},
	langid = {english},
	file = {全文:files/208255/Liu 等 - 2023 - Rise of Distributed Deep Learning Training in the .pdf:application/pdf},
}

@inproceedings{zhang_coditt5_2022,
	location = {Rochester {MI} {USA}},
	title = {{CoditT}5: Pretraining for Source Code and Natural Language Editing},
	isbn = {978-1-4503-9475-8},
	url = {https://dl.acm.org/doi/10.1145/3551349.3556955},
	doi = {10.1145/3551349.3556955},
	shorttitle = {{CoditT}5},
	eventtitle = {{ASE} '22: 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {1--12},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Zhang, Jiyang and Panthaplackel, Sheena and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos},
	urldate = {2023-06-10},
	date = {2022-10-10},
	langid = {english},
	file = {全文:files/208254/Zhang 等 - 2022 - CoditT5 Pretraining for Source Code and Natural L.pdf:application/pdf},
}

@article{kamal_automated_2022,
	title = {An Automated Approach for the Prediction of the Severity Level of Bug Reports Using {GPT}-2},
	volume = {2022},
	issn = {1939-0122, 1939-0114},
	url = {https://www.hindawi.com/journals/scn/2022/2892401/},
	doi = {10.1155/2022/2892401},
	abstract = {Manual investigation is warranted in traditional approaches for estimating the bug severity level, which adds to the effort and time required. For bug severity report prediction, numerous automated strategies have been proposed in addition to manual ones. However, the current bug report predictors by facing several issues, such as overfitting and weight computation, and therefore, their efficiency for specific levels of data noise needs to improve. As a result, a bug report predictor is required to solve these concerns (e.g., overfitting and avoiding weight calculation, which increases computing complexity) and perform better in the situation of data noise. We use {GPT}-2’s features (limiting overfitting and supplying sequential predictors rather than weight computation) to develop a new approach for predicting the severity level of bug reports in this study. The proposed approach is divided into four stages. First, the bug reports are subjected to text preprocessing. Second, we assess each bug report’s emotional score. Third, each report is presented in vector format. Finally, an emotion score is assigned to each bug report, and a vector of each bug report is produced and sent to {GPT}-2. We employ statistical indicators like recall, precision, and F1-score to evaluate the suggested method’s effectiveness and efficacy. A comparison was also made using state-of-the-art bug report predictors such as Random Forest ({RF}), Convolutional Neural Network ({CNN}), Long Short-Term Memory ({LSTM}) Network, Support Vector Machine ({SVM}), {XGBoost}, and Naive Bayes Multinomial ({NBM}). The proposed method’s promising result indicates its efficacy in bug information retrieval.},
	pages = {1--11},
	journaltitle = {Security and Communication Networks},
	shortjournal = {Security and Communication Networks},
	author = {Kamal, Mohsin and Ali, Sikandar and Nasir, Anam and Samad, Ali and Basser, Samad and Irshad, Azeem},
	editor = {Arif, Muhammad},
	urldate = {2023-06-10},
	date = {2022-05-29},
	langid = {english},
	file = {全文:files/208285/Kamal 等 - 2022 - An Automated Approach for the Prediction of the Se.pdf:application/pdf},
}

@inproceedings{finnie-ansley_my_2023,
	location = {Melbourne {VIC} Australia},
	title = {My {AI} Wants to Know if This Will Be on the Exam: Testing {OpenAI}’s Codex on {CS}2 Programming Exercises},
	isbn = {978-1-4503-9941-8},
	url = {https://dl.acm.org/doi/10.1145/3576123.3576134},
	doi = {10.1145/3576123.3576134},
	shorttitle = {My {AI} Wants to Know if This Will Be on the Exam},
	eventtitle = {{ACE} '23: Australasian Computing Education Conference},
	pages = {97--104},
	booktitle = {Australasian Computing Education Conference},
	publisher = {{ACM}},
	author = {Finnie-Ansley, James and Denny, Paul and Luxton-Reilly, Andrew and Santos, Eddie Antonio and Prather, James and Becker, Brett A.},
	urldate = {2023-06-10},
	date = {2023-01-30},
	langid = {english},
	file = {全文:files/208262/Finnie-Ansley 等 - 2023 - My AI Wants to Know if This Will Be on the Exam T.pdf:application/pdf},
}

@inproceedings{jain_code_2023,
	location = {Allahabad India},
	title = {A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques},
	isbn = {9798400700644},
	url = {https://dl.acm.org/doi/10.1145/3578527.3578530},
	doi = {10.1145/3578527.3578530},
	eventtitle = {{ISEC} 2023: 16th Innovations in Software Engineering Conference},
	pages = {1--10},
	booktitle = {16th Innovations in Software Engineering Conference},
	publisher = {{ACM}},
	author = {Jain, Ridhi and Gervasoni, Nicole and Ndhlovu, Mthandazo and Rawat, Sanjay},
	urldate = {2023-06-10},
	date = {2023-02-23},
	langid = {english},
}

@inproceedings{sun_incorporating_2022,
	location = {Hohhot China},
	title = {Incorporating Pre-trained Transformer Models into {TextCNN} for Sentiment Analysis on Software Engineering Texts},
	isbn = {978-1-4503-9780-3},
	url = {https://dl.acm.org/doi/10.1145/3545258.3545273},
	doi = {10.1145/3545258.3545273},
	eventtitle = {Internetware 2022: 13th Asia-Pacific Symposium on Internetware},
	pages = {127--136},
	booktitle = {13th Asia-Pacific Symposium on Internetware},
	publisher = {{ACM}},
	author = {Sun, Kexin and Shi, Xiaobo and Gao, Hui and Kuang, Hongyu and Ma, Xiaoxing and Rong, Guoping and Shao, Dong and Zhao, Zheng and Zhang, He},
	urldate = {2023-06-10},
	date = {2022-06-11},
	langid = {english},
}

@article{wang_incident_2022,
	title = {Incident detection and classification in renewable energy news using pre-trained language models on deep neural networks},
	volume = {22},
	issn = {14727978, 18758983},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/JCM-215594},
	doi = {10.3233/JCM-215594},
	abstract = {The surge of renewable energy systems can lead to increasing incidents that negatively impact economics and society, rendering incident detection paramount to understand the mechanism and range of those impacts. In this paper, a deep learning framework is proposed to detect renewable energy incidents from news articles containing accidents in various renewable energy systems. The pre-trained language models like Bidirectional Encoder Representations from Transformers ({BERT}) and word2vec are utilized to represent textual inputs, which are trained by the Text Convolutional Neural Networks ({TCNNs}) and Text Recurrent Neural Networks. Two types of classifiers for incident detection are trained and tested in this paper, one is a binary classifier for detecting the existence of an incident, the other is a multi-label classifier for identifying different incident attributes such as causal-effects and consequences, etc. The proposed incident detection framework is implemented on a hand-annotated dataset with 5 190 records. The results show that the proposed framework performs well on both the incident existence detection task (F1-score 91.4\%) and the incident attributes identification task (micro F1-score 81.7\%). It is also shown that the {BERT}-based {TCNNs} are effective and robust in detecting renewable energy incidents from large-scale textual materials.},
	pages = {57--76},
	number = {1},
	journaltitle = {Journal of Computational Methods in Sciences and Engineering},
	shortjournal = {{JCM}},
	author = {Wang, Qiqing and Li, Cunbin},
	urldate = {2023-06-10},
	date = {2022-01-26},
}

@article{tian_best_2023,
	title = {The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3576039},
	doi = {10.1145/3576039},
	shorttitle = {The Best of Both Worlds},
	abstract = {A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower
              Leopard
              (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with {BERT} transformer-based learned embeddings associated with {XGBoost} achieves an {AUC} value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, {PATCH}-{SIM}, which relies on dynamic information. By combining deep learned embeddings and engineered features,
              Panther
              (the upgraded version of
              Leopard
              implemented in this work) outperforms
              Leopard
              with higher scores in terms of {AUC}, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable {ML} technique, {SHAP}, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.},
	pages = {1--34},
	number = {4},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Tian, Haoye and Liu, Kui and Li, Yinghua and Kaboré, Abdoul Kader and Koyuncu, Anil and Habib, Andrew and Li, Li and Wen, Junhao and Klein, Jacques and Bissyandé, Tegawendé F.},
	urldate = {2023-06-10},
	date = {2023-10-31},
	langid = {english},
	file = {全文:files/208274/Tian 等 - 2023 - The Best of Both Worlds Combining Learned Embeddi.pdf:application/pdf},
}

@inproceedings{savage_objectify_2023,
	location = {Hamburg Germany},
	title = {Objectify: Better Living Through Anticipatory, Just-for-you 3D Printing!},
	isbn = {978-1-4503-9422-2},
	url = {https://dl.acm.org/doi/10.1145/3544549.3582748},
	doi = {10.1145/3544549.3582748},
	shorttitle = {Objectify},
	eventtitle = {{CHI} '23: {CHI} Conference on Human Factors in Computing Systems},
	pages = {1--8},
	booktitle = {Extended Abstracts of the 2023 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Savage, Valkyrie and Homewood, Sarah and Shklovski, Irina},
	urldate = {2023-06-10},
	date = {2023-04-19},
	langid = {english},
	file = {全文:files/208278/Savage 等 - 2023 - Objectify Better Living Through Anticipatory, Jus.pdf:application/pdf},
}

@inproceedings{li_auger_2022,
	location = {Singapore Singapore},
	title = {{AUGER}: automatically generating review comments with pre-training models},
	isbn = {978-1-4503-9413-0},
	url = {https://dl.acm.org/doi/10.1145/3540250.3549099},
	doi = {10.1145/3540250.3549099},
	shorttitle = {{AUGER}},
	eventtitle = {{ESEC}/{FSE} '22: 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	pages = {1009--1021},
	booktitle = {Proceedings of the 30th {ACM} Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Li, Lingwei and Yang, Li and Jiang, Huaxi and Yan, Jun and Luo, Tiejian and Hua, Zihan and Liang, Geng and Zuo, Chun},
	urldate = {2023-06-10},
	date = {2022-11-07},
	langid = {english},
	file = {全文:files/208273/Li 等 - 2022 - AUGER automatically generating review comments wi.pdf:application/pdf},
}

@inproceedings{jiang_represent_2022,
	location = {Hohhot China},
	title = {Represent Code as Action Sequence for Predicting Next Method Call},
	isbn = {978-1-4503-9780-3},
	url = {https://dl.acm.org/doi/10.1145/3545258.3545263},
	doi = {10.1145/3545258.3545263},
	eventtitle = {Internetware 2022: 13th Asia-Pacific Symposium on Internetware},
	pages = {45--54},
	booktitle = {13th Asia-Pacific Symposium on Internetware},
	publisher = {{ACM}},
	author = {Jiang, Yu and Wang, Liang and Hu, Hao and Tao, Xianping},
	urldate = {2023-06-10},
	date = {2022-06-11},
	langid = {english},
}

@inproceedings{zeng_extensive_2022,
	location = {Virtual South Korea},
	title = {An extensive study on pre-trained models for program understanding and generation},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534390},
	doi = {10.1145/3533767.3534390},
	eventtitle = {{ISSTA} '22: 31st {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	pages = {39--51},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	publisher = {{ACM}},
	author = {Zeng, Zhengran and Tan, Hanzhuo and Zhang, Haotian and Li, Jing and Zhang, Yuqun and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2022-07-18},
	langid = {english},
}

@inproceedings{lee_light_2022,
	location = {Rochester {MI} {USA}},
	title = {A Light Bug Triage Framework for Applying Large Pre-trained Language Model},
	isbn = {978-1-4503-9475-8},
	url = {https://dl.acm.org/doi/10.1145/3551349.3556898},
	doi = {10.1145/3551349.3556898},
	eventtitle = {{ASE} '22: 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {1--11},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Lee, Jaehyung and Han, Kisun and Yu, Hwanjo},
	urldate = {2023-06-10},
	date = {2022-10-10},
	langid = {english},
	file = {全文:files/208277/Lee 等 - 2022 - A Light Bug Triage Framework for Applying Large Pr.pdf:application/pdf},
}

@inproceedings{thapa_transformer-based_2022,
	location = {Austin {TX} {USA}},
	title = {Transformer-Based Language Models for Software Vulnerability Detection},
	isbn = {978-1-4503-9759-9},
	url = {https://dl.acm.org/doi/10.1145/3564625.3567985},
	doi = {10.1145/3564625.3567985},
	eventtitle = {{ACSAC}: Annual Computer Security Applications Conference},
	pages = {481--496},
	booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
	publisher = {{ACM}},
	author = {Thapa, Chandra and Jang, Seung Ick and Ahmed, Muhammad Ejaz and Camtepe, Seyit and Pieprzyk, Josef and Nepal, Surya},
	urldate = {2023-06-10},
	date = {2022-12-05},
	langid = {english},
	file = {已提交版本:files/208287/Thapa 等 - 2022 - Transformer-Based Language Models for Software Vul.pdf:application/pdf},
}

@article{chang_applying_2021,
	title = {Applying Code Transform Model to Newly Generated Program for Improving Execution Performance},
	volume = {2021},
	issn = {1875-919X, 1058-9244},
	url = {https://www.hindawi.com/journals/sp/2021/6691010/},
	doi = {10.1155/2021/6691010},
	abstract = {The existing programs inside the voice assistant machine prompt human-machine interaction in response to a request from a user. However, the crucial problem is that the machine often may not give a proper answer to the user or cannot work out the existing program execution efficiently. Therefore, this study proposes a novel transform method to replace the existing programs (called sample programs in this paper) inside the machine with newly generated programs through code transform model {GPT}-2 that can reasonably solve the problem mentioned above. In essence, this paper introduces a theoretical estimation in statistics to infer at least a number of generated programs as required so as to guarantee that the best one can be found within them. In addition, the proposed approach not only imitates a voice assistant system with filtering redundant keywords or adding new keywords to complete keyword retrieval in semantic database but also checks code similarity and verifies the conformity of the executive outputs between sample programs and newly generated programs. According to code checking and program output verification, the processes can expedite transform operations efficiently by removing the redundant generated programs and finding the best-performing generated program. As a result, the newly generated programs outperform the sample programs because the proposed approach reduces the number of code lines by 32.71\% and lowers the program execution time by 24.34\%, which is of great significance.},
	pages = {1--21},
	journaltitle = {Scientific Programming},
	shortjournal = {Scientific Programming},
	author = {Chang, Bao Rong and Tsai, Hsiu-Fen and Su, Po-Wen},
	editor = {Ali, Sikandar},
	urldate = {2023-06-10},
	date = {2021-02-01},
	langid = {english},
	file = {全文:files/208283/Chang 等 - 2021 - Applying Code Transform Model to Newly Generated P.pdf:application/pdf},
}

@inproceedings{liu_what_2023,
	location = {Hamburg Germany},
	title = {“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580817},
	doi = {10.1145/3544548.3580817},
	shorttitle = {“What It Wants Me To Say”},
	abstract = {Code-generating large language models translate natural language into code. However, only a small portion of the infinite space of naturalistic utterances is effective at guiding code generation. For non-expert end-user programmers, learning this is the challenge of abstraction matching. We examine this challenge in the specific context of data analysis in spreadsheets, in a system that maps the users natural language query to Python code using the Codex generator, executes the code, and shows the result. We propose grounded abstraction matching, which bridges the abstraction gap by translating the code back into a systematic and predictable naturalistic utterance. In a between-subjects, think-aloud study (n=24), we compare grounded abstraction matching to an ungrounded alternative based on previously established query framing principles. We find that the grounded approach improves end-users' understanding of the scope and capabilities of the code-generating model, and the kind of language needed to use it effectively.},
	eventtitle = {{CHI} '23: {CHI} Conference on Human Factors in Computing Systems},
	pages = {1--31},
	booktitle = {Proceedings of the 2023 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Liu, Michael Xieyang and Sarkar, Advait and Negreanu, Carina and Zorn, Benjamin and Williams, Jack and Toronto, Neil and Gordon, Andrew D.},
	urldate = {2023-06-10},
	date = {2023-04-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.06597 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {全文:files/208280/Liu 等 - 2023 - “What It Wants Me To Say” Bridging the Abstractio.pdf:application/pdf;arXiv.org Snapshot:files/209851/2304.html:text/html},
}

@inproceedings{he_binprov_2022,
	location = {Limassol Cyprus},
	title = {{BinProv}: Binary Code Provenance Identification without Disassembly},
	isbn = {978-1-4503-9704-9},
	url = {https://dl.acm.org/doi/10.1145/3545948.3545956},
	doi = {10.1145/3545948.3545956},
	shorttitle = {{BinProv}},
	eventtitle = {{RAID} 2022: 25th International Symposium on Research in Attacks, Intrusions and Defenses},
	pages = {350--363},
	booktitle = {25th International Symposium on Research in Attacks, Intrusions and Defenses},
	publisher = {{ACM}},
	author = {He, Xu and Wang, Shu and Xing, Yunlong and Feng, Pengbin and Wang, Haining and Li, Qi and Chen, Songqing and Sun, Kun},
	urldate = {2023-06-10},
	date = {2022-10-26},
	langid = {english},
	file = {全文:files/208279/He 等 - 2022 - BinProv Binary Code Provenance Identification wit.pdf:application/pdf},
}

@inproceedings{yuan_circle_2022,
	location = {Virtual South Korea},
	title = {{CIRCLE}: continual repair across programming languages},
	isbn = {978-1-4503-9379-9},
	url = {https://dl.acm.org/doi/10.1145/3533767.3534219},
	doi = {10.1145/3533767.3534219},
	shorttitle = {{CIRCLE}},
	eventtitle = {{ISSTA} '22: 31st {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	pages = {678--690},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	publisher = {{ACM}},
	author = {Yuan, Wei and Zhang, Quanjun and He, Tieke and Fang, Chunrong and Hung, Nguyen Quoc Viet and Hao, Xiaodong and Yin, Hongzhi},
	urldate = {2023-06-10},
	date = {2022-07-18},
	langid = {english},
	file = {已提交版本:files/208286/Yuan 等 - 2022 - CIRCLE continual repair across programming langua.pdf:application/pdf},
}

@article{bird_taking_2022,
	title = {Taking Flight with Copilot: Early insights and opportunities of {AI}-powered pair-programming tools},
	volume = {20},
	issn = {1542-7730, 1542-7749},
	url = {https://dl.acm.org/doi/10.1145/3582083},
	doi = {10.1145/3582083},
	shorttitle = {Taking Flight with Copilot},
	abstract = {Over the next five years, {AI}-powered tools likely will be helping developers in many diverse tasks. For example, such models may be used to improve code review, directing reviewers to parts of a change where review is most needed or even directly providing feedback on changes. Models such as Codex may suggest fixes for defects in code, build failures, or failing tests. These models are able to write tests automatically, helping to improve code quality and downstream reliability of distributed systems. This study of Copilot shows that developers spend more time reviewing code than actually writing code. As {AI}-powered tools are integrated into more software development tasks, developer roles will shift so that more time is spent assessing suggestions related to the task than doing the task itself.},
	pages = {35--57},
	number = {6},
	journaltitle = {Queue},
	shortjournal = {Queue},
	author = {Bird, Christian and Ford, Denae and Zimmermann, Thomas and Forsgren, Nicole and Kalliamvakou, Eirini and Lowdermilk, Travis and Gazit, Idan},
	urldate = {2023-06-10},
	date = {2022-12-31},
	langid = {english},
	file = {全文:files/208282/Bird 等 - 2022 - Taking Flight with Copilot Early insights and opp.pdf:application/pdf},
}

@article{shi_chinese_2023,
	title = {A Chinese Short Text Classification Method for Tax Audit Reports based on Word Importance and Syntactic Enhancement {BERT}},
	issn = {2375-4699, 2375-4702},
	url = {https://dl.acm.org/doi/10.1145/3594635},
	doi = {10.1145/3594635},
	abstract = {Tax audit is an important part of the tax collection and management system, which directly affects the economic interests of the country and taxpayers. Therefore, reducing the enforcement risk in tax audit is crucial to continuously improve the tax collection and management system. Recently, the research of using deep learning to classify Chinese tax audit data to achieve this goal has attracted much attention. Inspired by {BERT}, this paper proposes a syntactic enhancement {BERT} ({SE}-{BERT}). It can improve {BERT}’s text understanding ability by learning input features and grammatical structure of text from text content and location embeddings. In addition, we weight the word importance calculated by {TF}-{IDF} with {SE}-{BERT} to improve the ability of recognizing local salient features. Through comparative experiments on our Chinese tax audit dataset, our method achieves better performance.},
	pages = {3594635},
	journaltitle = {{ACM} Transactions on Asian and Low-Resource Language Information Processing},
	shortjournal = {{ACM} Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Shi, Yaning and Wang, Lukun and Tian, Chunpeng and Wang, Rujia and Pei, Jiaming and Hussian, Amir and Bashir, Ali Kashif},
	urldate = {2023-06-10},
	date = {2023-04-25},
	langid = {english},
	file = {全文:files/208284/Shi 等 - 2023 - A Chinese Short Text Classification Method for Tax.pdf:application/pdf},
}

@book{noauthor_nlbse_2022,
	location = {New York, {NY}, {USA}},
	title = {{NLBSE} '22: Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
	isbn = {978-1-4503-9343-0},
	abstract = {Welcome to the 1st edition of the International Workshop on Natural Language-Based Software Engineering ({NLBSE}). The potential of Natural Language Processing ({NLP}) and Natural Language Generation ({NLG}) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., {NLTK}, {CoreNLP}, and fasttext) and models (e.g., {BERT}) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
	publisher = {Association for Computing Machinery},
	date = {2022},
}

@incollection{wu_stochastic_2019,
	location = {Red Hook, {NY}, {USA}},
	title = {Stochastic Shared Embeddings: Data-Driven Regularization of Embedding Layers},
	abstract = {In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastic shared embeddings ({SSE}), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent ({SGD}). Because {SSE} integrates seamlessly with existing {SGD} algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of {SSE}: {SSE}-Graph using knowledge graphs of embeddings; {SSE}-{SE} using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and {BERT} in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed {SSE} can further reduce overfitting, which often leads to more favorable generalization results.},
	booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Wu, Liwei and Li, Shuqing and Hsieh, Cho-Jui and Sharpnack, James},
	date = {2019},
}

@inproceedings{kici_text_2021,
	location = {{USA}},
	title = {Text Classification on Software Requirements Specifications Using Transformer Models},
	series = {{CASCON} '21},
	abstract = {Text classification in Software Requirements Specifications ({SRS}) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over {SRS} documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing ({NLP}) and text classification. One of the main challenges in using deep learning models for various {NLP} tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over {SRS} documents using different pre-trained transformer models including {BERT}, {DistilBERT}, Roberta, {AlBERT}, and {XLNet}, and compare their performance. We test the performance of these models using three {SRS} datasets: {DOORS}, {NFR}-{PROMISE}, and {PURE}. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80\% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60\%.},
	pages = {163--172},
	booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
	publisher = {{IBM} Corp.},
	author = {Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and Parikh, Devang and Başar, Ayşe},
	date = {2021},
	note = {event-place: Toronto, Canada},
	keywords = {{BERT}, transfer learning, {NLP}, text classification, software requirement specifications},
}

@inproceedings{kanade_learning_2020,
	title = {Learning and Evaluating Contextual Embedding of Source Code},
	series = {{ICML}'20},
	abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as {BERT}, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from {GitHub}, which we use to pre-train {CuBERT}, an open-sourced code-understanding {BERT} model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune {CuBERT} on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, {BiLSTM} and Transformer models, as well as published state-of-the-art models, showing that {CuBERT} outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against {CuBERT} models as a strong baseline.},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{JMLR}.org},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	date = {2020},
}

@inproceedings{helmeczi_prompt-based_2022,
	location = {{USA}},
	title = {A Prompt-Based Few-Shot Learning Approach to Software Conflict Detection},
	series = {{CASCON} '22},
	abstract = {A software requirement specification ({SRS}) document is an essen­tial part of the software development life cycle which outlines the requirements that a software program in development must satisfy. This document is often specified by a diverse group of stakeholders and is subject to continual change, making the process of maintain­ing the document and detecting conflicts between requirements an essential task in software development. Notably, projects that do not address conflicts in the {SRS} document early on face consider­able problems later in the development life cycle. These problems incur substantial costs in terms of time and money, and these costs often become insurmountable barriers that ultimately result in the termination of a software project altogether. As a result, early detec­tion of {SRS} conflicts is critical to project sustainability. The conflict detection task is approached in numerous ways, many of which require a significant amount of manual intervention from devel­opers, or require access to a large amount of labeled, task-specific training data. In this work, we propose using a prompt-based learn­ing approach to perform few-shot learning for conflict detection. We compare our results to supervised learning approaches that use pretrained language models, such as {BERT} and its variants. Our results show that prompting with just 32 labeled examples can achieve a similar level of performance in many key metrics to that of supervised learning on training sets that are magnitudes larger in size. In contrast to many other conflict detection approaches, we make no assumptions about the type of underlying requirements, al­lowing us to analyze pairings of both functional and non-functional requirements. This allows us to omit the potentially expensive task of filtering out non-functional requirements from our dataset.},
	pages = {101--109},
	booktitle = {Proceedings of the 32nd Annual International Conference on Computer Science and Software Engineering},
	publisher = {{IBM} Corp.},
	author = {Helmeczi, Robert K. and Cevik, Mucahit and Y1ldmm, Savas},
	date = {2022},
	note = {event-place: Toronto, Canada},
	keywords = {{PET}, few-shot learning, conflict detection, Prompt-based learning, Prompting, soft­ware requirement specification ({SRS}), trans­former models},
}

@thesis{kessler_teaching_2020,
	location = {{USA}},
	title = {Teaching and Learning in a Content-Based Classroom: Understanding Pedagogy and the Development of L2 Writers' Metacognitive Genre Awareness},
	abstract = {The origins of second language (L2) writing as a field can be traced back to the early 1990s, when it first began to gain momentum as an area of interdisciplinary inquiry (Matsuda \&amp; De Pew, 2002). Since then, the field has evolved to become rich and diverse in scope, as scholars have examined numerous phenomena by drawing on an array of theoretical and methodological approaches. In particular, two areas that have received significant attention are investigations involving (1) genre-based teaching and learning, and, (2) the linguistic development of L2 writers, respectively. Though much has been learned, research involving genre-based pedagogies has often been insular and limited to applied linguistics-related contexts, and little is known about the pedagogical practices of L2 writing instructors in contexts such as those involved in Content-Based Instruction ({CBI}). Relatedly, as Polio (2017) has noted, few studies have addressed how L2 learners' genre knowledge actually develops over time in specific contexts, as development has tended to be assessed using timed-writing tasks. Thus, the current dissertation examines the nexus of these two areas, exploring phenomena related {CBI} pedagogical practices and L2 learners' subsequent genre learning and performance.This study takes place in a Master of Laws ({LLM}) program in a Midwestern law school. Adopting a classroom-based ethnographic approach (e.g., Bloome, 2012), I explored the beliefs and pedagogical practices of one writing instructor, as he taught a professional legal genre called the office memorandum during a semester-long {CBI} legal research and writing course. Data consisted of: in-class observations, field notes, semi-structured interviews, artifacts, and a visualization reflection. In addition to observing the instructor, I tracked the developing genre awareness among six (N = 6) L2 English learners/{LLMs} throughout the semester using a case study design. Students' development was examined using metacognition theory (Flavell, 1979; Schraw \&amp; Dennison, 1994), and data included: a survey, semi-structured interviews, and modified stimulated recalls with students' office memoranda. Finally, the relationship between students' genre awareness and their perceived writing performance was explored. This was addressed through eliciting holistic rankings and evaluative comments from the course instructor and by comparing them with/against students' self-reported metacognitive genre awareness.The findings highlight the instructor's rich, multifaceted philosophy of teaching L2 writing and the office memorandum, which in turn, subsequently influenced both his curriculum design and in-classroom teaching practices. The impact of this pedagogy was also apparent in its resulting influence on {LLM} students' development of metacognitive genre awareness, as students' self-reports often reflected those themes addressed by the instructor in the areas such as understanding the genre's audience. Apart from the influence of pedagogy, the impact of students' prior experiences (academic and professional) with related genres can also be seen in influencing their developing genre awareness. In terms of performance, it appears that an increased ability to self-report one's metacognitive genre awareness (both a general awareness plus how one uses that awareness while writing) is connected to perceived writing quality.In closing, I discuss both the strengths and limitations of the current study, including characteristics related to the study's design. Finally, I expound on the implications of this dissertation's findings, and I address their significance and potential to influence future teaching and research in the field of L2 writing, and, in second language acquisition more broadly.},
	institution = {Michigan State University},
	type = {phdthesis},
	author = {Kessler, Matthew Jeffrey and Loewen, Shawn and Francis, Jeremy},
	date = {2020},
	note = {{ISBN}: 9798662410145},
}

@thesis{alanazi_software_2021,
	title = {Software Analytics for Improving Program Comprehension},
	abstract = {Program comprehension is an essential part of software development and maintenance. Traditional methods of program comprehension, such as reviewing the codebase and documentation, are still challenging for understanding the software's overall structure and implementation. In recent years, software static analysis studies have emerged to facilitate program comprehensions, such as call graphs, which represent the system's structure and its implementation as a directed graph. Furthermore, some studies focused on semantic enrichment of the software system problems using systematic learning analytics, including machine learning and {NLP}. While call graphs can enhance the program comprehension process, they still face three main challenges: (1) complex call graphs can become very difficult to understand making call graphs much harder to visualize and interpret by a developer and thus increases the overhead in program comprehension; (2) they are often limited to a single level of granularity, such as function calls; and (3) there is a lack of the interpretation semantics about the graphs.In this dissertation, we propose a novel framework, called {CodEx}, to facilitate and accelerate program comprehension. {CodEx} enables top-down and bottom-up analysis of the system's call graph and its execution paths for an enhanced program comprehension experience. Specifically, the proposed framework is designed to cope with the following techniques: multi-level graph abstraction using a coarsening technique, hierarchical clustering to represent the call graph into subgraphs (i.e., multi-levels of granularity), and interactive visual exploration of the graphs at different levels of abstraction. Moreover, we are also worked on building semantics of software systems using {NLP} and machine learning, including topic modeling, to interpret the meaning of the abstraction levels of the call graph.},
	institution = {University of Missouri - Kansas City},
	type = {phdthesis},
	author = {Alanazi, Rakan and Sejun, Song,  and Raveen, Rao,  and Baek-Young, Choi,  and Zhu, Li, },
	date = {2021},
	note = {{ISBN}: 9798516944291},
}

@thesis{abraham_hazard_2022,
	title = {Hazard Classification of Federal Aviation Administration ({FAA}) Unmanned Aircraft Systems ({UAS}) Sightings Reports Using Machine Learning},
	abstract = {The current rapid progression of Unmanned Aircraft Systems ({UASs}) introduces a new class of safety risks into the National Airspace System ({NAS}). Recently, the Federal Aviation Administration ({FAA}) has witnessed a growing number of {UAS} sighting reports submitted by pilots, which are written in natural language—unstructured text that requires a human to manually explore the data. Exclusively relying on a human is time-consuming and likely subjective, as humans are inherently biased. This research introduces a new hazard classification scheme and demonstrates an automatic {UAS} sighting classification by means of Machine Learning ({ML}) and Natural Language Processing ({NLP}) techniques. This classification model aims to decrease the time and effort required by aviation analysts to identify and validate {UAS} safety risks and enhance the efficacy of the risk analysis process. Additionally, this model enables {FAA} executives to make risk-informed decisions about the potential hazards of {UAS} integration in the {NAS}, specifically its impact on the air traffic around airports in the United States. In this research, three {ML} models and five Deep Learning ({DL}) architectures were trained to classify 3,132 reports, including Random Forest ({RF}), Gradient Boosting ({GB}), Extreme Gradient Boosting ({XGB}), Artificial Neural Network ({ANN}), Sequential Long Short-Term Memory (Se-{LSTM}), Bi-directional Long Short-Term Memory (Bi-{LSTM}), Convolutional Neural Network ({CNN}), and Bidirectional Encoder Representations from Transformers ({BERT}). The results show that {BERT} scored the highest classification performance with accuracy, precision, recall, and F1-score of 96\%, 96\%, 96\%, and 96\%, respectively.},
	institution = {The George Washington University},
	type = {phdthesis},
	author = {abraham, Noran and Amir, Etemadi, },
	date = {2022},
	note = {{ISBN}: 9798759968740},
}

@thesis{almeida_cinematica_2019,
	title = {{CinemáTica} Do Membro Superior No Gesto de Alcance-Comparação Entre Jovens Adultos Pré-Termo e de Termo},
	abstract = {Introdução:Uma percentagem significativa de crianças pré-termo semlesão neurológica aparente, exibe disfunções do movimento que se relacionam com um desenvolvimento neuromotor e comportamental atípico, verificando-se que os défices iniciais têm um efeito cascata no neurodesenvolvimento.Contudo, apesar da evidência de queasalterações docontrolo postural ({CP}) se mantèm ao longo da vida da criança, não foi encontrada bibliografia que explorasse a manutenção dessas alterações até à idade adulta, especificamente entre os 18 e os 25 anos. Objetivo: Avaliar a cinemática domembro superior ({MS}) etronco, bem comoo comportamento do centro de pressão ({CoP}) durante o gesto de alcance ({GA}) em pé, comparando o membro superior dominante ({MSD}) como não dominante ({MSND}), em jovens adultos pré-termo e de termo. Métodos: Estudo observacional analitico transversal comuma amostra de 36indivíduos, entre os 18 e os 25 anos, divididosem dois grupos: o grupo pré-termo ({GPT}) indivíduos com idadegestacional inferior a 37 semanas, eo grupo termo ({GT}), participantes com idade gestacionaligual ou superior a 37 semanas. Recorreu-se ao sistema de aquisição de imagem Qualisys e respetivo software para avaliar a cinemática do {MS} e tronco, e utilizaram-se duas plataformas de forças para avaliar o comportamento do {CoPdurante} o {GA} em pé. A tarefa consistiu em realizar o {GAde} uma garrafade 0,5L, partindo com o {MS} ao longo do corpo, colocada no plano da omoplata, a uma altura correspondente ao ponto médio do esterno,à distância do comprimento funcional do {MS}. Para a análise estatística, recorreu-se ao Software de análise {SPSS} - Statistical Package for the Social Sciences({IBM}), versão 23.0eutilizou-se a estatística descritiva para caracterizar a amostra e para analisar os dados. Resultados 0 {GT} apresentou um valor de média significativamente superior ao {GPT} do ângulo final de flexão do ombro no movimento realizado pelo {MSD} (p-0,041)e pelo {MSND} (p-0,017);do ângulo inicial de abdução do ombrocom o {MSD} (p-0,041); e uma maior variação do ângulo de flexão do ombro\&amp;.com o {MSD} (p-0,014)e como {MSND}(p-0,024).0GPT apresentou uma maior variação do ángulo de abdução do ombro {comoMSD}(p-0,014)e {MSND}(p-0,001)umângulo final de abdução do ombro superiorcom o {MSND} (p-0,002) e uma maior variação daextensãodo cotovelo com o {MSND} (p-0,044). Na comparação intragrupo, as principais diferenças entre membros verificam-se no {GT} ao nível das variáveisespácio-temporais, de cinemática angular e de comportamento do {CoP}. Conclusão: Os resultados parecem apontar que as alterações de cinemática angular decorrentes da prematuridade se podem manter até à idade adulta (dos 18 aos 25 anos). No entanto, as conclusões não são claras no que diz respeito às variáveis de cinemática espácio-temporal e de deslocamento do tranco e comportamento do {CoP}.},
	institution = {Instituto Politecnico do Porto (Portugal)},
	type = {phdthesis},
	author = {Almeida, Bruna Rafaela Ferreira},
	date = {2019},
	note = {{ISBN}: 9798790621055},
}

@thesis{bavishi_tools_2022,
	title = {Tools and Techniques for Building Programming Assistants for Data Analysis},
	abstract = {We live in the data age. Today, data analytics drives much of business decision-making, logistics, advertising, and recommendations. Data wrangling, profiling, and visualization are some of the key tasks in a data analytics workflow. These tasks also account for a majority of the time spent in data analysis. Academics and industry leaders have long attributed the disparity to the inherent domain-specific nature of data, which necessitates highly custom treatment for every new source of data. Specifying these custom analysis steps using low-level tools such as Excel can be prohibitively cumbersome. In response, much research has focused on smarter interactive and graphical tools for data processing and visualization tasks. Successful commercialization of this research has contributed to a \$3 billion self-service analytics industry.However, analysts with a programming background have not adopted such tools as widely as their non-programmer colleagues have. The desire to avoid shuffling between tools and work in a single environment, as well as a need for the full, unbounded expressivity of programming-based analysis tools, are a few major reasons. This does not mean that programmers are immune to the specification burden; the expressivity of programming comes at the cost of complexity and steep learning curves. Novices have to spend much time learning these tools from books or fragmented resources online. Even experts report a loss in productivity from having to constantly look up documentation to get uninteresting details such as function names and argument values right.Thus, there is a need for programming assistants that reconcile the need to reduce the specification burden for programmer analysts with their desire to work with code in their preferred development environments. These assistants should help programmer-analysts write code more efficiently by automatically generating human-readable and readily-integrable code from high-level specifications.This dissertation introduces techniques and corresponding prototypical assistants that accept input-output examples, demonstrations, or natural language specifications and automatically generate suitable data processing and visualization code utilizing popular data science libraries such as pandas, matplotlib, seaborn, and scikit-learn. Automatic code generation has long faced the tradeoff barrier between expressivity and performance/accuracy: supporting a large number of analysis tasks makes the problem of generating the right code quickly that much more difficult. Accordingly, prior research in program synthesis and semantic parsing has largely sacrificed full expressivity to support efficient code generation for a small but useful subset of tasks. The code-as-text approach of modern natural language processing systems, including the use of large language models, promises unbounded expressivity, but their sub-optimal accuracy remains a concern.This dissertation tries to push the boundaries in terms of breaking this tradeoff barrier — can we build programming systems that are fully expressive while remaining fast and accurate? Specifically, this dissertation builds upon prior work and introduces novel code generation techniques that combine insights from synthesis, automated testing, program analysis, and machine learning. It contributes four core techniques and corresponding assistants, namely {AutoPandas}, Gauss, {VizSmith}, and Datana. {AutoPandas} and Gauss constitute core advances in search space and algorithm design for example-based synthesis. {VizSmith} and Datana introduce novel mining and auto-summarization techniques to automatically build aligned code and natural language corpora, which Datana uses to greatly improve the code-generation capabilities of modern large language models. Compared to prior work, these assistants improve the expressivity of synthesis-based systems and the accuracy of machine-learning-based systems.},
	institution = {University of California, Berkeley},
	type = {phdthesis},
	author = {Bavishi, Rohan Jayesh and Ion, Stoica,  and Joseph, Hellerstein,  and Mukul, Prasad, },
	date = {2022},
	note = {{ISBN}: 9798352951378},
}

@thesis{moura_user_2021,
	title = {User Chat Clustering Using Deep Learning Representations and Unsupervised Methods for Dialog System Applications},
	abstract = {Os sistemas automáticos de conversação, conhecidos normalmente como chat bots, estão a tornar-se cada vez mais populares e devem ser capazes de interpretar a linguagem humana para compreender e comunicar com os seres humanos. A deteção de intenções desempenha uma tarefa crucial para desenvolver conversas inteligentes nestes sistemas de conversa. As implementações existentes destes sistemas requerem muitos dados etiquetados e a sua aquisição pode ser dispendiosa e demorada. Esta tese visa avaliar representações de texto existentes, utilizando abordagens clássicas, tais como Word2Vec, {GloVe} e modelos de Transformer pré-treinados ({BERT}, {RoBERTa}, {GPT}2 e outros), para possível automatização de dados de diálogo não etiquetados através de algoritmos de agrupamento. Os algoritmos de agrupamento testados, vão desde o clássico K-Means até abordagens mais sofisticadas, tais como {HDBSCAN}, com a ajuda de técnicas de redução de dimensão (t-{SNE}, {UMAP}). Um conjunto de dados é utilizado para avaliação das técnicas utilizadas, que contêm diálogo de intents de utilizadores em múltiplos domínios e taxonomia de intents variada que se encontram no mesmo domínio.Os resultados mostram que os Transformers apresentam um desempenho de representação de texto superior às representações clássicas. No entanto, um modelo ensemble com múltiplos algoritmos de agrupamento e de múltiplas representações de fontes diferentes apresenta uma melhoria drástica na solução final. A aplicação do {UMAP} e t-{SNE} em dimensões mais baixas pode também apresentar um desempenho tão bom ou mesmo melhor do que as representações originais.},
	institution = {Universidade da Madeira (Portugal)},
	type = {phdthesis},
	author = {Moura, André Filipe Nóbrega and Karolina, Karolina,  and Lucas, de Sousa Pereira, Amâncio},
	date = {2021},
	note = {{ISBN}: 9798358432031},
}

@inproceedings{ciborowska_fast_2022-1,
	location = {Pittsburgh Pennsylvania},
	title = {Fast changeset-based bug localization with {BERT}},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510042},
	doi = {10.1145/3510003.3510042},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {946--957},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Ciborowska, Agnieszka and Damevski, Kostadin},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {全文:files/208313/Ciborowska 和 Damevski - 2022 - Fast changeset-based bug localization with BERT.pdf:application/pdf},
}

@inproceedings{macneil_experiences_2023,
	location = {Toronto {ON} Canada},
	title = {Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book},
	isbn = {978-1-4503-9431-4},
	url = {https://dl.acm.org/doi/10.1145/3545945.3569785},
	doi = {10.1145/3545945.3569785},
	eventtitle = {{SIGCSE} 2023: The 54th {ACM} Technical Symposium on Computer Science Education},
	pages = {931--937},
	booktitle = {Proceedings of the 54th {ACM} Technical Symposium on Computer Science Education V. 1},
	publisher = {{ACM}},
	author = {{MacNeil}, Stephen and Tran, Andrew and Hellas, Arto and Kim, Joanne and Sarsa, Sami and Denny, Paul and Bernstein, Seth and Leinonen, Juho},
	urldate = {2023-06-10},
	date = {2023-03-02},
	langid = {english},
	file = {全文:files/208318/MacNeil 等 - 2023 - Experiences from Using Code Explanations Generated.pdf:application/pdf},
}

@article{kim_predicting_2022,
	title = {Predicting Duplicate in Bug Report Using Topic-Based Duplicate Learning With Fine Tuning-Based {BERT} Algorithm},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9968242/},
	doi = {10.1109/ACCESS.2022.3226238},
	pages = {129666--129675},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Kim, Taemin and Yang, Geunseok},
	urldate = {2023-06-10},
	date = {2022},
}

@article{rahmani_multi-modal_2021-1,
	title = {Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3485535},
	doi = {10.1145/3485535},
	shorttitle = {Multi-modal program inference},
	abstract = {Multi-modal program synthesis refers to the task of synthesizing programs (code) from their specification given in different forms, such as a combination of natural language and examples. Examples provide a precise but incomplete specification, and natural language provides an ambiguous but more "complete" task description. Machine-learned pre-trained models ({PTMs}) are adept at handling ambiguous natural language, but struggle with generating syntactically and semantically precise code. Program synthesis techniques can generate correct code, often even from incomplete but precise specifications, such as examples, but they are unable to work with the ambiguity of natural languages. We present an approach that combines {PTMs} with component-based synthesis ({CBS}): {PTMs} are used to generate candidates programs from the natural language description of the task, which are then used to guide the {CBS} procedure to find the program that matches the precise examples-based specification. We use our combination approach to instantiate multi-modal synthesis systems for two programming domains: the domain of regular expressions and the domain of {CSS} selectors. Our evaluation demonstrates the effectiveness of our domain-agnostic approach in comparison to a state-of-the-art specialized system, and the generality of our approach in providing multi-modal program synthesis from natural language and examples in different programming domains.},
	pages = {1--29},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Rahmani, Kia and Raza, Mohammad and Gulwani, Sumit and Le, Vu and Morris, Daniel and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish},
	urldate = {2023-06-10},
	date = {2021-10-20},
	langid = {english},
	file = {全文:files/208317/Rahmani 等 - 2021 - Multi-modal program inference a marriage of pre-t.pdf:application/pdf},
}

@inproceedings{wu_bert_2021,
	location = {Xi'an, China},
	title = {{BERT} for Sentiment Classification in Software Engineering},
	isbn = {978-1-66544-045-5},
	url = {https://ieeexplore.ieee.org/document/9492202/},
	doi = {10.1109/ICSS53362.2021.00026},
	eventtitle = {2021 International Conference on Service Science ({ICSS})},
	pages = {115--121},
	booktitle = {2021 International Conference on Service Science ({ICSS})},
	publisher = {{IEEE}},
	author = {Wu, Junfang and Ye, Chunyang and Zhou, Hui},
	urldate = {2023-06-10},
	date = {2021-05},
}

@article{uddin_software_2022-1,
	title = {Software defect prediction employing {BiLSTM} and {BERT}-based semantic feature},
	volume = {26},
	issn = {1432-7643, 1433-7479},
	url = {https://link.springer.com/10.1007/s00500-022-06830-5},
	doi = {10.1007/s00500-022-06830-5},
	pages = {7877--7891},
	number = {16},
	journaltitle = {Soft Computing},
	shortjournal = {Soft Comput},
	author = {Uddin, Md Nasir and Li, Bixin and Ali, Zafar and Kefalas, Pavlos and Khan, Inayat and Zada, Islam},
	urldate = {2023-06-10},
	date = {2022-08},
	langid = {english},
}

@inproceedings{pena_siambert_2022,
	location = {Stockholm, Sweden},
	title = {Siambert: Siamese Bert-based Code Search},
	isbn = {978-1-66547-126-8},
	url = {https://ieeexplore.ieee.org/document/9833051/},
	doi = {10.1109/SAIS55783.2022.9833051},
	shorttitle = {Siambert},
	eventtitle = {2022 Swedish Artificial Intelligence Society Workshop ({SAIS})},
	pages = {1--7},
	booktitle = {2022 Swedish Artificial Intelligence Society Workshop ({SAIS})},
	publisher = {{IEEE}},
	author = {Pena, Francisco J. and Gonzalez, Angel Luis and Pashami, Sepideh and Al-Shishtawy, Ahmad and Payberah, Amir H.},
	urldate = {2023-06-10},
	date = {2022-06-13},
}

@inproceedings{ahmed_few-shot_2022,
	location = {Rochester {MI} {USA}},
	title = {Few-shot training {LLMs} for project-specific code-summarization},
	isbn = {978-1-4503-9475-8},
	url = {https://dl.acm.org/doi/10.1145/3551349.3559555},
	doi = {10.1145/3551349.3559555},
	eventtitle = {{ASE} '22: 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {1--5},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Ahmed, Toufique and Devanbu, Premkumar},
	urldate = {2023-06-10},
	date = {2022-10-10},
	langid = {english},
	file = {全文:files/208326/Ahmed 和 Devanbu - 2022 - Few-shot training LLMs for project-specific code-s.pdf:application/pdf},
}

@inproceedings{lajko_towards_2022-1,
	location = {Pittsburgh Pennsylvania},
	title = {Towards {JavaScript} program repair with generative pre-trained transformer ({GPT}-2)},
	isbn = {978-1-4503-9285-3},
	url = {https://dl.acm.org/doi/10.1145/3524459.3527350},
	doi = {10.1145/3524459.3527350},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {61--68},
	booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
	publisher = {{ACM}},
	author = {Lajkó, Márk and Csuvik, Viktor and Vidács, László},
	urldate = {2023-06-10},
	date = {2022-05-19},
	langid = {english},
	file = {全文:files/208330/Lajkó 等 - 2022 - Towards JavaScript program repair with generative .pdf:application/pdf},
}

@inproceedings{cao_sbrpbert_2022,
	location = {Baltimore, {MD}, {USA}},
	title = {{SbrPBert}: A {BERT}-Based Model for Accurate Security Bug Report Prediction},
	isbn = {978-1-66540-262-0},
	url = {https://ieeexplore.ieee.org/document/9833825/},
	doi = {10.1109/DSN-W54100.2022.00030},
	shorttitle = {{SbrPBert}},
	eventtitle = {2022 52nd Annual {IEEE}/{IFIP} International Conference on Dependable Systems and Networks Workshops ({DSN}-W)},
	pages = {129--134},
	booktitle = {2022 52nd Annual {IEEE}/{IFIP} International Conference on Dependable Systems and Networks Workshops ({DSN}-W)},
	publisher = {{IEEE}},
	author = {Cao, Xudong and Liu, Tianwei and Zhang, Jiayuan and Feng, Mengyue and Zhang, Xin and Cao, Wanying and Sun, Hongyu and Zhang, Yuqing},
	urldate = {2023-06-10},
	date = {2022-06},
}

@inproceedings{de_araujo_re-bert_2021,
	location = {Virtual Event Republic of Korea},
	title = {{RE}-{BERT}: automatic extraction of software requirements from app reviews using {BERT} language model},
	isbn = {978-1-4503-8104-8},
	url = {https://dl.acm.org/doi/10.1145/3412841.3442006},
	doi = {10.1145/3412841.3442006},
	shorttitle = {{RE}-{BERT}},
	eventtitle = {{SAC} '21: The 36th {ACM}/{SIGAPP} Symposium on Applied Computing},
	pages = {1321--1327},
	booktitle = {Proceedings of the 36th Annual {ACM} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {De Araújo, Adailton Ferreira and Marcacini, Ricardo Marcondes},
	urldate = {2023-06-10},
	date = {2021-03-22},
	langid = {english},
}

@inproceedings{biswas_achieving_2020,
	location = {Adelaide, Australia},
	title = {Achieving Reliable Sentiment Analysis in the Software Engineering Domain using {BERT}},
	isbn = {978-1-72815-619-4},
	url = {https://ieeexplore.ieee.org/document/9240599/},
	doi = {10.1109/ICSME46990.2020.00025},
	eventtitle = {2020 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {162--173},
	booktitle = {2020 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Biswas, Eeshita and Karabulut, Mehmet Efruz and Pollock, Lori and Vijay-Shanker, K.},
	urldate = {2023-06-10},
	date = {2020-09},
}

@article{ciniselli_empirical_2021,
	title = {An Empirical Study on the Usage of Transformer Models for Code Completion},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9616462/},
	doi = {10.1109/TSE.2021.3128234},
	abstract = {Code completion aims at speeding up code writing by predicting the next code token(s) the developer is likely to write. Works in this field focused on improving the accuracy of the generated predictions, with substantial leaps forward made possible by deep learning ({DL}) models. However, code completion techniques are mostly evaluated in the scenario of predicting the next token to type, with few exceptions pushing the boundaries to the prediction of an entire code statement. Thus, little is known about the performance of state-of-the-art code completion approaches in more challenging scenarios in which, for example, an entire code block must be generated. We present a large-scale study exploring the capabilities of state-of-the-art Transformer-based models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). We experimented with several variants of two recently proposed Transformer-based models, namely {RoBERTa} and the Text-To-Text Transfer Transformer (T5), for the task of code completion. The achieved results show that Transformer-based models, and in particular the T5, represent a viable solution for code completion, with perfect predictions ranging from {\textasciitilde}29\%, obtained when asking the model to guess entire blocks, up to {\textasciitilde}69\%, reached in the simpler scenario of few tokens masked from the same code statement.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Mastropaolo, Antonio and Aghajani, Emad and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2108.01585 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {已接受版本:files/208354/Ciniselli 等 - 2021 - An Empirical Study on the Usage of Transformer Mod.pdf:application/pdf;已提交版本:files/209973/Ciniselli 等 - 2021 - An Empirical Study on the Usage of Transformer Mod.pdf:application/pdf;arXiv Fulltext PDF:files/209339/Ciniselli 等 - 2021 - An Empirical Study on the Usage of Transformer Mod.pdf:application/pdf;arXiv.org Snapshot:files/209340/2108.html:text/html},
}

@inproceedings{majumdar_effective_2022,
	location = {Guangzhou, China},
	title = {An Effective Low-Dimensional Software Code Representation using {BERT} and {ELMo}},
	isbn = {978-1-66547-704-8},
	url = {https://ieeexplore.ieee.org/document/10062388/},
	doi = {10.1109/QRS57517.2022.00082},
	eventtitle = {2022 {IEEE} 22nd International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {763--774},
	booktitle = {2022 {IEEE} 22nd International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Majumdar, Srijoni and Varshney, Ashutosh and Pratim Das, Partha and Clough, Paul D and Chattopadhyay, Samiran},
	urldate = {2023-06-10},
	date = {2022-12},
}

@inproceedings{jain_jigsaw_2022-1,
	location = {Pittsburgh Pennsylvania},
	title = {Jigsaw: large language models meet program synthesis},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510203},
	doi = {10.1145/3510003.3510203},
	shorttitle = {Jigsaw},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {1219--1231},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
}

@inproceedings{zhang_sentiment_2020,
	location = {Adelaide, Australia},
	title = {Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?},
	isbn = {978-1-72815-619-4},
	url = {https://ieeexplore.ieee.org/document/9240704/},
	doi = {10.1109/ICSME46990.2020.00017},
	shorttitle = {Sentiment Analysis for Software Engineering},
	eventtitle = {2020 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {70--80},
	booktitle = {2020 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Zhang, Ting and Xu, Bowen and Thung, Ferdian and Haryono, Stefanus Agus and Lo, David and Jiang, Lingxiao},
	urldate = {2023-06-10},
	date = {2020-09},
}

@article{dawson_introducing_1992,
	title = {Introducing new software engineering graduates to the ‘real world’ at the {GPT} company},
	volume = {7},
	issn = {02686961},
	url = {https://digital-library.theiet.org/content/journals/10.1049/sej.1992.0018},
	doi = {10.1049/sej.1992.0018},
	pages = {171},
	number = {3},
	journaltitle = {Software Engineering Journal},
	shortjournal = {Softw. Eng. J. {UK}},
	author = {Dawson, R.J. and Newsham, R.W. and Kerridge, R.S.},
	urldate = {2023-06-10},
	date = {1992},
	langid = {english},
}

@inproceedings{li_arb-bert_2021,
	location = {Yinchuan, China},
	title = {{ARB}-{BERT}: An Automatic Aging-Related Bug Report Classification Method based on {BERT}},
	isbn = {978-1-66544-391-3},
	url = {https://ieeexplore.ieee.org/document/9623011/},
	doi = {10.1109/DSA52907.2021.00071},
	shorttitle = {{ARB}-{BERT}},
	eventtitle = {2021 8th International Conference on Dependable Systems and Their Applications ({DSA})},
	pages = {474--483},
	booktitle = {2021 8th International Conference on Dependable Systems and Their Applications ({DSA})},
	publisher = {{IEEE}},
	author = {Li, Mingxi and Yin, Bei-Bei},
	urldate = {2023-06-10},
	date = {2021-08},
}

@inproceedings{li_cross-lingual_2021,
	location = {Melbourne, Australia},
	title = {Cross-Lingual Transfer Learning Framework for Program Analysis},
	isbn = {978-1-66540-337-5},
	url = {https://ieeexplore.ieee.org/document/9678848/},
	doi = {10.1109/ASE51524.2021.9678848},
	eventtitle = {2021 36th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {1074--1078},
	booktitle = {2021 36th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Li, Zhiming},
	urldate = {2023-06-10},
	date = {2021-11},
}

@inproceedings{zan_cert_2022,
	location = {Vienna, Austria},
	title = {{CERT}: Continual Pre-training on Sketches for Library-oriented Code Generation},
	isbn = {978-1-956792-00-3},
	url = {https://www.ijcai.org/proceedings/2022/329},
	doi = {10.24963/ijcai.2022/329},
	shorttitle = {{CERT}},
	abstract = {Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large unlabelled code corpora and perform well in generating code. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present {CERT} with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and generator are continually pre-trained upon a base model using unlabelled data. Also, we carefully craft two benchmarks to evaluate library-oriented code generation named {PandasEval} and {NumpyEval}. Experimental results have shown the impressive performance of {CERT}. For example, it surpasses the base model by an absolute 15.67\% improvement in terms of pass@1 on {PandasEval}. Our work is available at https://github.com/microsoft/{PyCodeGPT}.},
	eventtitle = {Thirty-First International Joint Conference on Artificial Intelligence \{{IJCAI}-22\}},
	pages = {2369--2375},
	booktitle = {Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
	urldate = {2023-06-10},
	date = {2022-07},
	langid = {english},
	file = {全文:files/208353/Zan 等 - 2022 - CERT Continual Pre-training on Sketches for Libra.pdf:application/pdf},
}

@inproceedings{kim_vuldebert_2022,
	location = {Charlotte, {NC}, {USA}},
	title = {{VulDeBERT}: A Vulnerability Detection System Using {BERT}},
	isbn = {978-1-66547-679-9},
	url = {https://ieeexplore.ieee.org/document/9985089/},
	doi = {10.1109/ISSREW55968.2022.00042},
	shorttitle = {{VulDeBERT}},
	eventtitle = {2022 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	pages = {69--74},
	booktitle = {2022 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	publisher = {{IEEE}},
	author = {Kim, Soolin and Choi, Jusop and Ahmed, Muhammad Ejaz and Nepal, Surya and Kim, Hyoungshick},
	urldate = {2023-06-10},
	date = {2022-10},
}

@incollection{rios_preliminary_2017,
	location = {Cham},
	title = {Preliminary Study on Workshop Facilitation for {IoT} Innovation as Industry-University Collaboration {PLM} Program for Small and Medium Sized Enterprises},
	volume = {517},
	isbn = {978-3-319-72904-6 978-3-319-72905-3},
	url = {https://link.springer.com/10.1007/978-3-319-72905-3_26},
	pages = {285--296},
	booktitle = {Product Lifecycle Management and the Industry of the Future},
	publisher = {Springer International Publishing},
	author = {Goto, Satoshi and Yoshie, Osamu and Fujimura, Shigeru and Tamaki, Kin’ya},
	editor = {Ríos, José and Bernard, Alain and Bouras, Abdelaziz and Foufou, Sebti},
	urldate = {2023-06-10},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-72905-3_26},
	note = {Series Title: {IFIP} Advances in Information and Communication Technology},
	file = {已提交版本:files/208362/Goto 等 - 2017 - Preliminary Study on Workshop Facilitation for IoT.pdf:application/pdf},
}

@inproceedings{vaithilingam_expectation_2022,
	location = {New Orleans {LA} {USA}},
	title = {Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
	isbn = {978-1-4503-9156-6},
	url = {https://dl.acm.org/doi/10.1145/3491101.3519665},
	doi = {10.1145/3491101.3519665},
	shorttitle = {Expectation vs. Experience},
	eventtitle = {{CHI} '22: {CHI} Conference on Human Factors in Computing Systems},
	pages = {1--7},
	booktitle = {{CHI} Conference on Human Factors in Computing Systems Extended Abstracts},
	publisher = {{ACM}},
	author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
	urldate = {2023-06-10},
	date = {2022-04-27},
	langid = {english},
}

@article{ozkaya_application_2023,
	title = {Application of Large Language Models to Software Engineering Tasks: Opportunities, Risks, and Implications},
	volume = {40},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/10109345/},
	doi = {10.1109/MS.2023.3248401},
	shorttitle = {Application of Large Language Models to Software Engineering Tasks},
	pages = {4--8},
	number = {3},
	journaltitle = {{IEEE} Software},
	shortjournal = {{IEEE} Softw.},
	author = {Ozkaya, Ipek},
	urldate = {2023-06-10},
	date = {2023-05},
	file = {全文:files/208361/Ozkaya - 2023 - Application of Large Language Models to Software E.pdf:application/pdf},
}

@article{verbruggen_semantic_2021-1,
	title = {Semantic programming by example with pre-trained models},
	volume = {5},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3485477},
	doi = {10.1145/3485477},
	abstract = {The ability to learn programs from few examples is a powerful technology with disruptive applications in many domains, as it allows users to automate repetitive tasks in an intuitive way. Existing frameworks on inductive synthesis only perform syntactic manipulations, where they rely on the syntactic structure of the given examples and not their meaning. Any semantic manipulations, such as transforming dates, have to be manually encoded by the designer of the inductive programming framework. Recent advances in large language models have shown these models to be very adept at performing semantic transformations of its input by simply providing a few examples of the task at hand. When it comes to syntactic transformations, however, these models are limited in their expressive power. In this paper, we propose a novel framework for integrating inductive synthesis with few-shot learning language models to combine the strength of these two popular technologies. In particular, the inductive synthesis is tasked with breaking down the problem in smaller subproblems, among which those that cannot be solved syntactically are passed to the language model. We formalize three semantic operators that can be integrated with inductive synthesizers. To minimize invoking expensive semantic operators during learning, we introduce a novel deferred query execution algorithm that considers the operators to be oracles during learning. We evaluate our approach in the domain of string transformations: the combination methodology can automate tasks that cannot be handled using either technologies by themselves. Finally, we demonstrate the generality of our approach via a case study in the domain of string profiling.},
	pages = {1--25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Verbruggen, Gust and Le, Vu and Gulwani, Sumit},
	urldate = {2023-06-10},
	date = {2021-10-20},
	langid = {english},
	file = {全文:files/208346/Verbruggen 等 - 2021 - Semantic programming by example with pre-trained m.pdf:application/pdf},
}

@inproceedings{ciniselli_empirical_2021-1,
	location = {Madrid, Spain},
	title = {An Empirical Study on the Usage of {BERT} Models for Code Completion},
	isbn = {978-1-72818-710-5},
	url = {https://ieeexplore.ieee.org/document/9463129/},
	doi = {10.1109/MSR52588.2021.00024},
	eventtitle = {2021 {IEEE}/{ACM} 18th International Conference on Mining Software Repositories ({MSR})},
	pages = {108--119},
	booktitle = {2021 {IEEE}/{ACM} 18th International Conference on Mining Software Repositories ({MSR})},
	publisher = {{IEEE}},
	author = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021-05},
	file = {已提交版本:files/208360/Ciniselli 等 - 2021 - An Empirical Study on the Usage of BERT Models for.pdf:application/pdf},
}

@article{von_der_mosel_validity_2023-1,
	title = {On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9785808/},
	doi = {10.1109/TSE.2022.3178469},
	pages = {1487--1507},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Von Der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/208359/Von Der Mosel 等 - 2023 - On the Validity of Pre-Trained Transformers for Na.pdf:application/pdf},
}

@article{alqarni_low_2022,
	title = {Low Level Source Code Vulnerability Detection Using Advanced {BERT} Language Model},
	url = {https://caiac.pubpub.org/pub/gdhb8oq4},
	doi = {10.21428/594757db.b85e6625},
	journaltitle = {Proceedings of the Canadian Conference on Artificial Intelligence},
	author = {Alqarni, Mansour and Azim, Akramul},
	urldate = {2023-06-10},
	date = {2022-05-27},
	langid = {english},
	file = {全文:files/208355/Alqarni 和 Azim - 2022 - Low Level Source Code Vulnerability Detection Usin.pdf:application/pdf},
}

@incollection{fanmuy_mbse_2017,
	location = {Cham},
	title = {{MBSE}, {PLM}, {MIP} and Robust Optimization for System of Systems Management, Application to {SCCOA} French Air Defense Program},
	isbn = {978-3-319-49102-8 978-3-319-49103-5},
	url = {http://link.springer.com/10.1007/978-3-319-49103-5_3},
	pages = {29--40},
	booktitle = {Complex Systems Design \& Management},
	publisher = {Springer International Publishing},
	author = {Peugeot, Thomas and Dupin, Nicolas and Sembely, Marie-Joëlle and Dubecq, Catherine},
	editor = {Fanmuy, Gauthier and Goubault, Eric and Krob, Daniel and Stephan, François},
	urldate = {2023-06-10},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-49103-5_3},
}

@article{wang_fret_2020,
	title = {Fret: Functional Reinforced Transformer With {BERT} for Code Summarization},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9146834/},
	doi = {10.1109/ACCESS.2020.3011744},
	shorttitle = {Fret},
	pages = {135591--135604},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Wang, Ruyun and Zhang, Hanwen and Lu, Guoliang and Lyu, Lei and Lyu, Chen},
	urldate = {2023-06-10},
	date = {2020},
	file = {全文:files/208365/Wang 等 - 2020 - Fret Functional Reinforced Transformer With BERT .pdf:application/pdf},
}

@inproceedings{liu_multi-task_2020-1,
	location = {Virtual Event Australia},
	title = {Multi-task learning based pre-trained language model for code completion},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416591},
	doi = {10.1145/3324884.3416591},
	eventtitle = {{ASE} '20: 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {473--485},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2020-12-21},
	langid = {english},
	file = {已提交版本:files/208364/Liu 等 - 2020 - Multi-task learning based pre-trained language mod.pdf:application/pdf},
}

@inproceedings{khan_automatic_2022,
	location = {Rochester {MI} {USA}},
	title = {Automatic Code Documentation Generation Using {GPT}-3},
	isbn = {978-1-4503-9475-8},
	url = {https://dl.acm.org/doi/10.1145/3551349.3559548},
	doi = {10.1145/3551349.3559548},
	eventtitle = {{ASE} '22: 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {1--6},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Khan, Junaed Younus and Uddin, Gias},
	urldate = {2023-06-10},
	date = {2022-10-10},
	langid = {english},
	file = {全文:files/208356/Khan 和 Uddin - 2022 - Automatic Code Documentation Generation Using GPT-.pdf:application/pdf},
}

@incollection{samsonovich_independent_2020,
	location = {Cham},
	title = {Independent Core Observer Model Research Program Assumption Codex},
	volume = {948},
	isbn = {978-3-030-25718-7 978-3-030-25719-4},
	url = {http://link.springer.com/10.1007/978-3-030-25719-4_24},
	pages = {187--192},
	booktitle = {Biologically Inspired Cognitive Architectures 2019},
	publisher = {Springer International Publishing},
	author = {Kelley, David J.},
	editor = {Samsonovich, Alexei V.},
	urldate = {2023-06-10},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-25719-4_24},
	note = {Series Title: Advances in Intelligent Systems and Computing},
}

@article{wang_incident_2022-1,
	title = {Incident detection and classification in renewable energy news using pre-trained language models on deep neural networks},
	volume = {22},
	issn = {14727978, 18758983},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/JCM-215594},
	doi = {10.3233/JCM-215594},
	abstract = {The surge of renewable energy systems can lead to increasing incidents that negatively impact economics and society, rendering incident detection paramount to understand the mechanism and range of those impacts. In this paper, a deep learning framework is proposed to detect renewable energy incidents from news articles containing accidents in various renewable energy systems. The pre-trained language models like Bidirectional Encoder Representations from Transformers ({BERT}) and word2vec are utilized to represent textual inputs, which are trained by the Text Convolutional Neural Networks ({TCNNs}) and Text Recurrent Neural Networks. Two types of classifiers for incident detection are trained and tested in this paper, one is a binary classifier for detecting the existence of an incident, the other is a multi-label classifier for identifying different incident attributes such as causal-effects and consequences, etc. The proposed incident detection framework is implemented on a hand-annotated dataset with 5 190 records. The results show that the proposed framework performs well on both the incident existence detection task (F1-score 91.4\%) and the incident attributes identification task (micro F1-score 81.7\%). It is also shown that the {BERT}-based {TCNNs} are effective and robust in detecting renewable energy incidents from large-scale textual materials.},
	pages = {57--76},
	number = {1},
	journaltitle = {Journal of Computational Methods in Sciences and Engineering},
	shortjournal = {{JCM}},
	author = {Wang, Qiqing and Li, Cunbin},
	urldate = {2023-06-10},
	date = {2022-01-26},
}

@article{salza_effectiveness_2023-1,
	title = {On the Effectiveness of Transfer Learning for Code Search},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9835142/},
	doi = {10.1109/TSE.2022.3192755},
	pages = {1804--1822},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C.},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/208363/Salza 等 - 2023 - On the Effectiveness of Transfer Learning for Code.pdf:application/pdf},
}

@misc{shang_pre-training_2019,
	title = {Pre-training of Graph Augmented Transformers for Medication Recommendation},
	url = {http://arxiv.org/abs/1906.00346},
	abstract = {Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records ({EHRs}) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-{BERT}, a new model to combine the power of Graph Neural Networks ({GNNs}) and {BERT} (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use {GNNs} to represent the internal hierarchical structures of medical codes. Then we integrate the {GNN} representation into a transformer-based visit encoder and pre-train it on {EHR} data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal {EHRs} from patients with multiple visits. G-{BERT} is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.},
	number = {{arXiv}:1906.00346},
	publisher = {{arXiv}},
	author = {Shang, Junyuan and Ma, Tengfei and Xiao, Cao and Sun, Jimeng},
	urldate = {2023-06-10},
	date = {2019-11-26},
	eprinttype = {arxiv},
	eprint = {1906.00346 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208392/Shang 等 - 2019 - Pre-training of Graph Augmented Transformers for M.pdf:application/pdf;arXiv.org Snapshot:files/209685/1906.html:text/html},
}

@misc{wu_stochastic_2020,
	title = {Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers},
	url = {http://arxiv.org/abs/1905.10630},
	shorttitle = {Stochastic Shared Embeddings},
	abstract = {In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastically shared embeddings ({SSE}), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent ({SGD}). Because {SSE} integrates seamlessly with existing {SGD} algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of {SSE}: {SSE}-Graph using knowledge graphs of embeddings; {SSE}-{SE} using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and {BERT} in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed {SSE} can further reduce overfitting, which often leads to more favorable generalization results.},
	number = {{arXiv}:1905.10630},
	publisher = {{arXiv}},
	author = {Wu, Liwei and Li, Shuqing and Hsieh, Cho-Jui and Sharpnack, James},
	urldate = {2023-06-10},
	date = {2020-10-03},
	eprinttype = {arxiv},
	eprint = {1905.10630 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:files/208967/Wu 等 - 2020 - Stochastic Shared Embeddings Data-driven Regulari.pdf:application/pdf;arXiv.org Snapshot:files/209218/1905.html:text/html},
}

@misc{andor_giving_2019,
	title = {Giving {BERT} a Calculator: Finding Operations and Arguments with Reading Comprehension},
	url = {http://arxiv.org/abs/1909.00109},
	shorttitle = {Giving {BERT} a Calculator},
	abstract = {Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a {BERT}-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages ({DROP}) dataset, designed to challenge reading comprehension models, we show a 33\% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.},
	number = {{arXiv}:1909.00109},
	publisher = {{arXiv}},
	author = {Andor, Daniel and He, Luheng and Lee, Kenton and Pitler, Emily},
	urldate = {2023-06-10},
	date = {2019-09-12},
	eprinttype = {arxiv},
	eprint = {1909.00109 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208393/Andor 等 - 2019 - Giving BERT a Calculator Finding Operations and A.pdf:application/pdf;arXiv.org Snapshot:files/208939/1909.html:text/html},
}

@misc{liao_gpt-based_2019,
	title = {{GPT}-based Generation for Classical Chinese Poetry},
	url = {http://arxiv.org/abs/1907.00151},
	abstract = {We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model ({GPT}). The method adopts a simple {GPT} model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju, L{\textbackslash}"\{u\}shi, various Cipai and Couples, the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ {GPT} in developing a poetry generation system. We have released an online mini demonstration program on Wechat to show the generation capability of the proposed method for classical Chinese poetry.},
	number = {{arXiv}:1907.00151},
	publisher = {{arXiv}},
	author = {Liao, Yi and Wang, Yasheng and Liu, Qun and Jiang, Xin},
	urldate = {2023-06-10},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1907.00151 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208927/Liao 等 - 2019 - GPT-based Generation for Classical Chinese Poetry.pdf:application/pdf;arXiv.org Snapshot:files/209215/1907.html:text/html},
}

@misc{wang_evalutation_2019,
	title = {An Evalutation of Programming Language Models' performance on Software Defect Detection},
	url = {http://arxiv.org/abs/1909.10309},
	abstract = {This dissertation presents an evaluation of several language models on software defect datasets. A language Model ({LM}) "can provide word representation and probability indication of word sequences as the core component of an {NLP} system." Language models for source code are specified for tasks in the software engineering field. While some models are directly the {NLP} ones, others contain structural information that is uniquely owned by source code. Software defects are defects in the source code that lead to unexpected behaviours and malfunctions at all levels. This study provides an original attempt to detect these defects at three different levels (syntactical, algorithmic and general) We also provide a tool chain that researchers can use to reproduce the experiments. We have tested the different models against different datasets, and performed an analysis over the results. Our original attempt to deploy bert, the state-of-the-art model for multitasks, leveled or outscored all other models compared.},
	number = {{arXiv}:1909.10309},
	publisher = {{arXiv}},
	author = {Wang, Kailun},
	urldate = {2023-06-10},
	date = {2019-09-10},
	eprinttype = {arxiv},
	eprint = {1909.10309 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208583/Wang - 2019 - An Evalutation of Programming Language Models' per.pdf:application/pdf;arXiv.org Snapshot:files/208609/1909.html:text/html},
}

@misc{kanade_learning_2020-1,
	title = {Learning and Evaluating Contextual Embedding of Source Code},
	url = {http://arxiv.org/abs/2001.00059},
	abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as {BERT}, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from {GitHub}, which we use to pre-train {CuBERT}, an open-sourced code-understanding {BERT} model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune {CuBERT} on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, {BiLSTM} and Transformer models, as well as published state-of-the-art models, showing that {CuBERT} outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against {CuBERT} models as a strong baseline.},
	number = {{arXiv}:2001.00059},
	publisher = {{arXiv}},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	urldate = {2023-06-10},
	date = {2020-08-17},
	eprinttype = {arxiv},
	eprint = {2001.00059 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208401/Kanade 等 - 2020 - Learning and Evaluating Contextual Embedding of So.pdf:application/pdf;arXiv.org Snapshot:files/208940/2001.html:text/html},
}

@misc{tian_evaluating_2020-1,
	title = {Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair},
	url = {http://arxiv.org/abs/2008.02944},
	abstract = {A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations to learn deep features that may encode the properties of patch correctness. Our work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with {BERT} transformer-based embeddings associated with logistic regression yielded an {AUC} value of about 0.8 in predicting patch correctness on a deduplicated dataset of 1000 labeled patches. Our study shows that learned representations can lead to reasonable performance when comparing against the state-of-the-art, {PATCH}-{SIM}, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.},
	number = {{arXiv}:2008.02944},
	publisher = {{arXiv}},
	author = {Tian, Haoye and Liu, Kui and Kaboreé, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyandé, Tegawendé F.},
	urldate = {2023-06-10},
	date = {2020-08-06},
	eprinttype = {arxiv},
	eprint = {2008.02944 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208611/Tian 等 - 2020 - Evaluating Representation Learning of Code Changes.pdf:application/pdf;arXiv.org Snapshot:files/208636/2008.html:text/html},
}

@misc{wan_leveraging_2020,
	title = {Leveraging Personal Navigation Assistant Systems Using Automated Social Media Traffic Reporting},
	url = {http://arxiv.org/abs/2004.13823},
	abstract = {Modern urbanization is demanding smarter technologies to improve a variety of applications in intelligent transportation systems to relieve the increasing amount of vehicular traffic congestion and incidents. Existing incident detection techniques are limited to the use of sensors in the transportation network and hang on human-inputs. Despite of its data abundance, social media is not well-exploited in such context. In this paper, we develop an automated traffic alert system based on Natural Language Processing ({NLP}) that filters this flood of information and extract important traffic-related bullets. To this end, we employ the fine-tuning Bidirectional Encoder Representations from Transformers ({BERT}) language embedding model to filter the related traffic information from social media. Then, we apply a question-answering model to extract necessary information characterizing the report event such as its exact location, occurrence time, and nature of the events. We demonstrate the adopted {NLP} approaches outperform other existing approach and, after effectively training them, we focus on real-world situation and show how the developed approach can, in real-time, extract traffic-related information and automatically convert them into alerts for navigation assistance applications such as navigation apps.},
	number = {{arXiv}:2004.13823},
	publisher = {{arXiv}},
	author = {Wan, Xiangpeng and Ghazzai, Hakim and Massoud, Yehia},
	urldate = {2023-06-10},
	date = {2020-04-20},
	eprinttype = {arxiv},
	eprint = {2004.13823 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208605/Wan 等 - 2020 - Leveraging Personal Navigation Assistant Systems U.pdf:application/pdf;arXiv.org Snapshot:files/208834/2004.html:text/html},
}

@misc{chen_tabfact_2020,
	title = {{TabFact}: A Large-scale Dataset for Table-based Fact Verification},
	url = {http://arxiv.org/abs/1909.02164},
	shorttitle = {{TabFact}},
	abstract = {The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called {TabFact} with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either {ENTAILED} or {REFUTED}. {TabFact} is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-{BERT} and Latent Program Algorithm ({LPA}). Table-{BERT} leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. {LPA} parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in {\textbackslash}url\{https://github.com/wenhuchen/Table-Fact-Checking\}.},
	number = {{arXiv}:1909.02164},
	publisher = {{arXiv}},
	author = {Chen, Wenhu and Wang, Hongmin and Chen, Jianshu and Zhang, Yunkai and Wang, Hong and Li, Shiyang and Zhou, Xiyou and Wang, William Yang},
	urldate = {2023-06-10},
	date = {2020-06-14},
	eprinttype = {arxiv},
	eprint = {1909.02164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209234/Chen 等 - 2020 - TabFact A Large-scale Dataset for Table-based Fac.pdf:application/pdf;arXiv.org Snapshot:files/209277/1909.html:text/html},
}

@misc{tol_fastspec_2021,
	title = {{FastSpec}: Scalable Generation and Detection of Spectre Gadgets Using Neural Embeddings},
	url = {http://arxiv.org/abs/2006.14147},
	shorttitle = {{FastSpec}},
	abstract = {Several techniques have been proposed to detect vulnerable Spectre gadgets in widely deployed commercial software. Unfortunately, detection techniques proposed so far rely on hand-written rules which fall short in covering subtle variations of known Spectre gadgets as well as demand a huge amount of time to analyze each conditional branch in software. Moreover, detection tool evaluations are based only on a handful of these gadgets, as it requires arduous effort to craft new gadgets manually. In this work, we employ both fuzzing and deep learning techniques to automate the generation and detection of Spectre gadgets. We first create a diverse set of Spectre-V1 gadgets by introducing perturbations to the known gadgets. Using mutational fuzzing, we produce a data set with more than 1 million Spectre-V1 gadgets which is the largest Spectre gadget data set built to date. Next, we conduct the first empirical usability study of Generative Adversarial Networks ({GANs}) in the context of assembly code generation without any human interaction. We introduce {SpectreGAN} which leverages masking implementation of {GANs} for both learning the gadget structures and generating new gadgets. This provides the first scalable solution to extend the variety of Spectre gadgets. Finally, we propose {FastSpec} which builds a classifier with the generated Spectre gadgets based on a novel high dimensional Neural Embeddings technique ({BERT}). For the case studies, we demonstrate that {FastSpec} discovers potential gadgets with a high success rate in {OpenSSL} libraries and Phoronix benchmarks. Further, {FastSpec} offers much greater flexibility and time-related performance gain compared to the existing tools and therefore can be used for gadget detection in large-scale software.},
	number = {{arXiv}:2006.14147},
	publisher = {{arXiv}},
	author = {Tol, M. Caner and Gulmezoglu, Berk and Yurtseven, Koray and Sunar, Berk},
	urldate = {2023-06-10},
	date = {2021-03-26},
	eprinttype = {arxiv},
	eprint = {2006.14147 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209270/Tol 等 - 2021 - FastSpec Scalable Generation and Detection of Spe.pdf:application/pdf;arXiv.org Snapshot:files/209289/2006.html:text/html},
}

@misc{enayet_transfer_2020,
	title = {A Transfer Learning Approach for Dialogue Act Classification of {GitHub} Issue Comments},
	url = {http://arxiv.org/abs/2011.04867},
	abstract = {Social coding platforms, such as {GitHub}, serve as laboratories for studying collaborative problem solving in open source software development; a key feature is their ability to support issue reporting which is used by teams to discuss tasks and ideas. Analyzing the dialogue between team members, as expressed in issue comments, can yield important insights about the performance of virtual teams. This paper presents a transfer learning approach for performing dialogue act classification on issue comments. Since no large labeled corpus of {GitHub} issue comments exists, employing transfer learning enables us to leverage standard dialogue act datasets in combination with our own {GitHub} comment dataset. We compare the performance of several word and sentence level encoding models including Global Vectors for Word Representations ({GloVe}), Universal Sentence Encoder ({USE}), and Bidirectional Encoder Representations from Transformers ({BERT}). Being able to map the issue comments to dialogue acts is a useful stepping stone towards understanding cognitive team processes.},
	number = {{arXiv}:2011.04867},
	publisher = {{arXiv}},
	author = {Enayet, Ayesha and Sukthankar, Gita},
	urldate = {2023-06-10},
	date = {2020-11-09},
	eprinttype = {arxiv},
	eprint = {2011.04867 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:files/208410/Enayet 和 Sukthankar - 2020 - A Transfer Learning Approach for Dialogue Act Clas.pdf:application/pdf;arXiv.org Snapshot:files/209831/2011.html:text/html},
}

@misc{le_shield_2022,
	title = {{SHIELD}: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher},
	url = {http://arxiv.org/abs/2011.08908},
	shorttitle = {{SHIELD}},
	abstract = {Even though several methods have proposed to defend textual neural network ({NN}) models against black-box adversarial attacks, they often defend against a specific text perturbation strategy and/or require re-training the models from scratch. This leads to a lack of generalization in practice and redundant computation. In particular, the state-of-the-art transformer models (e.g., {BERT}, {RoBERTa}) require great time and computation resources. By borrowing an idea from software engineering, in order to address these limitations, we propose a novel algorithm, {SHIELD}, which modifies and re-trains only the last layer of a textual {NN}, and thus it "patches" and "transforms" the {NN} into a stochastic weighted ensemble of multi-expert prediction heads. Considering that most of current black-box attacks rely on iterative search mechanisms to optimize their adversarial perturbations, {SHIELD} confuses the attackers by automatically utilizing different weighted ensembles of predictors depending on the input. In other words, {SHIELD} breaks a fundamental assumption of the attack, which is a victim {NN} model remains constant during an attack. By conducting comprehensive experiments, we demonstrate that all of {CNN}, {RNN}, {BERT}, and {RoBERTa}-based textual {NNs}, once patched by {SHIELD}, exhibit a relative enhancement of 15\%--70\% in accuracy on average against 14 different black-box attacks, outperforming 6 defensive baselines across 3 public datasets. All codes are to be released.},
	number = {{arXiv}:2011.08908},
	publisher = {{arXiv}},
	author = {Le, Thai and Park, Noseong and Lee, Dongwon},
	urldate = {2023-06-10},
	date = {2022-03-16},
	eprinttype = {arxiv},
	eprint = {2011.08908 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208982/Le 等 - 2022 - SHIELD Defending Textual Neural Networks against .pdf:application/pdf;arXiv.org Snapshot:files/209718/2011.html:text/html},
}

@inproceedings{japa_question_2020,
	title = {Question Answering over Knowledge Base using Language Model Embeddings},
	url = {http://arxiv.org/abs/2010.08883},
	doi = {10.1109/IJCNN48605.2020.9206698},
	abstract = {Knowledge Base, represents facts about the world, often in some form of subsumption ontology, rather than implicitly, embedded in procedural code, the way a conventional computer program does. While there is a rapid growth in knowledge bases, it poses a challenge of retrieving information from them. Knowledge Base Question Answering is one of the promising approaches for extracting substantial knowledge from Knowledge Bases. Unlike web search, Question Answering over a knowledge base gives accurate and concise results, provided that natural language questions can be understood and mapped precisely to an answer in the knowledge base. However, some of the existing embedding-based methods for knowledge base question answering systems ignore the subtle correlation between the question and the Knowledge Base (e.g., entity types, relation paths, and context) and suffer from the Out Of Vocabulary problem. In this paper, we focused on using a pre-trained language model for the Knowledge Base Question Answering task. Firstly, we used Bert base uncased for the initial experiments. We further fine-tuned these embeddings with a two-way attention mechanism from the knowledge base to the asked question and from the asked question to the knowledge base answer aspects. Our method is based on a simple Convolutional Neural Network architecture with a Multi-Head Attention mechanism to represent the asked question dynamically in multiple aspects. Our experimental results show the effectiveness and the superiority of the Bert pre-trained language model embeddings for question answering systems on knowledge bases over other well-known embedding methods.},
	pages = {1--8},
	booktitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Japa, Sai Sharath and Banafsheh, Rekabdar},
	urldate = {2023-06-10},
	date = {2020-07},
	eprinttype = {arxiv},
	eprint = {2010.08883 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/208411/Japa 和 Banafsheh - 2020 - Question Answering over Knowledge Base using Langu.pdf:application/pdf;arXiv.org Snapshot:files/208984/2010.html:text/html},
}

@misc{tufano_unit_2021,
	title = {Unit Test Case Generation with Transformers and Focal Context},
	url = {http://arxiv.org/abs/2009.05617},
	abstract = {Automated unit test case generation tools facilitate test-driven development and support developers by suggesting tests intended to identify flaws in their code. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult for developers to read or understand. In this paper we propose {AthenaTest}, an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written testcases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure consisting of denoising pretraining on a large unsupervised Java corpus, and supervised finetuning for a downstream translation task of generating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information surrounding the focal method. Both techniques provide improvements in terms of validation loss, with pretraining yielding 25\% relative improvement and focal context providing additional 11.1\% improvement. We also introduce Methods2Test, the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K test cases mined from 91K open-source repositories from {GitHub}. We evaluate {AthenaTest} on five defects4j projects, generating 25K passing test cases covering 43.7\% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage information, and compare them with test cases generated by {EvoSuite} and {GPT}-3, finding that our approach outperforms {GPT}-3 and has comparable coverage w.r.t. {EvoSuite}. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated tests, showing overwhelmingly preference towards {AthenaTest}.},
	number = {{arXiv}:2009.05617},
	publisher = {{arXiv}},
	author = {Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Deng, Shao Kun and Sundaresan, Neel},
	urldate = {2023-06-10},
	date = {2021-05-20},
	eprinttype = {arxiv},
	eprint = {2009.05617 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208983/Tufano 等 - 2021 - Unit Test Case Generation with Transformers and Fo.pdf:application/pdf;arXiv.org Snapshot:files/208999/2009.html:text/html},
}

@misc{yang_feature_2022,
	title = {Feature Learning in Infinite-Width Neural Networks},
	url = {http://arxiv.org/abs/2011.14522},
	abstract = {As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel ({NTK})), if it is parametrized appropriately (e.g. the {NTK} parametrization). However, we show that the standard and {NTK} parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with {BERT}. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via {MAML}, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both {NTK} baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, {NTK}, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/{TP}4.},
	number = {{arXiv}:2011.14522},
	publisher = {{arXiv}},
	author = {Yang, Greg and Hu, Edward J.},
	urldate = {2023-06-10},
	date = {2022-07-15},
	eprinttype = {arxiv},
	eprint = {2011.14522 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
	file = {arXiv Fulltext PDF:files/209283/Yang 和 Hu - 2022 - Feature Learning in Infinite-Width Neural Networks.pdf:application/pdf;arXiv.org Snapshot:files/209293/2011.html:text/html},
}

@misc{brown_towards_2021,
	title = {Towards Neural Programming Interfaces},
	url = {http://arxiv.org/abs/2012.05983},
	abstract = {It is notoriously difficult to control the behavior of artificial neural networks such as generative neural language models. We recast the problem of controlling natural language generation as that of learning to interface with a pretrained language model, just as Application Programming Interfaces ({APIs}) control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network (called a Neural Programming Interface or {NPI}) learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model to produce desired outputs. Importantly, no permanent changes are made to the weights of the original model, allowing us to re-purpose pretrained models for new tasks without overwriting any aspect of the language model. We also contribute a new data set construction algorithm and {GAN}-inspired loss function that allows us to train {NPI} models to control outputs of autoregressive transformers. In experiments against other state-of-the-art approaches, we demonstrate the efficacy of our methods using {OpenAI}'s {GPT}-2 model, successfully controlling noun selection, topic aversion, offensive speech filtering, and other aspects of language while largely maintaining the controlled model's fluency under deterministic settings.},
	number = {{arXiv}:2012.05983},
	publisher = {{arXiv}},
	author = {Brown, Zachary C. and Robinson, Nathaniel and Wingate, David and Fulda, Nancy},
	urldate = {2023-06-10},
	date = {2021-02-17},
	eprinttype = {arxiv},
	eprint = {2012.05983 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208429/Brown 等 - 2021 - Towards Neural Programming Interfaces.pdf:application/pdf;arXiv.org Snapshot:files/209805/2012.html:text/html},
}

@misc{liu_multi-task_2020-2,
	title = {Multi-task Learning based Pre-trained Language Model for Code Completion},
	url = {http://arxiv.org/abs/2012.14631},
	abstract = {Code completion is one of the most useful features in the Integrated Development Environments ({IDEs}), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.},
	number = {{arXiv}:2012.14631},
	publisher = {{arXiv}},
	author = {Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2020-12-29},
	eprinttype = {arxiv},
	eprint = {2012.14631 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209294/Liu 等 - 2020 - Multi-task Learning based Pre-trained Language Mod.pdf:application/pdf;arXiv.org Snapshot:files/209298/2012.html:text/html},
}

@misc{hu_misspelling_2021,
	title = {Misspelling Correction with Pre-trained Contextual Language Model},
	url = {http://arxiv.org/abs/2101.03204},
	abstract = {Spelling irregularities, known now as spelling mistakes, have been found for several centuries. As humans, we are able to understand most of the misspelled words based on their location in the sentence, perceived pronunciation, and context. Unlike humans, computer systems do not possess the convenient auto complete functionality of which human brains are capable. While many programs provide spelling correction functionality, many systems do not take context into account. Moreover, Artificial Intelligence systems function in the way they are trained on. With many current Natural Language Processing ({NLP}) systems trained on grammatically correct text data, many are vulnerable against adversarial examples, yet correctly spelled text processing is crucial for learning. In this paper, we investigate how spelling errors can be corrected in context, with a pre-trained language model {BERT}. We present two experiments, based on {BERT} and the edit distance algorithm, for ranking and selecting candidate corrections. The results of our experiments demonstrated that when combined properly, contextual word embeddings of {BERT} and edit distance are capable of effectively correcting spelling errors.},
	number = {{arXiv}:2101.03204},
	publisher = {{arXiv}},
	author = {Hu, Yifei and Jing, Xiaonan and Ko, Youlim and Rayz, Julia Taylor},
	urldate = {2023-06-10},
	date = {2021-01-08},
	eprinttype = {arxiv},
	eprint = {2101.03204 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208412/Hu 等 - 2021 - Misspelling Correction with Pre-trained Contextual.pdf:application/pdf;arXiv.org Snapshot:files/208985/2101.html:text/html},
}

@misc{lin_traceability_2021-1,
	title = {Traceability Transformed: Generating more Accurate Links with Pre-Trained {BERT} Models},
	url = {http://arxiv.org/abs/2102.04411},
	shorttitle = {Traceability Transformed},
	abstract = {Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace {BERT} (T-{BERT}) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-{BERT} framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three {BERT} architectures. Results show that a Single-{BERT} architecture generated the most accurate links, while a Siamese-{BERT} architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical {IR} trace models. On the three evaluated real-word {OSS} projects, the best T-{BERT} stably outperformed the {VSM} model with average improvements of 60.31\% measured using Mean Average Precision ({MAP}). {RNN} severely underperformed on these projects due to insufficient training data, while T-{BERT} overcame this problem by using pretrained language models and transfer learning.},
	number = {{arXiv}:2102.04411},
	publisher = {{arXiv}},
	author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
	urldate = {2023-06-10},
	date = {2021-02-22},
	eprinttype = {arxiv},
	eprint = {2102.04411 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209274/Lin 等 - 2021 - Traceability Transformed Generating more Accurate.pdf:application/pdf;arXiv.org Snapshot:files/209760/2102.html:text/html},
}

@misc{lu_codexglue_2021,
	title = {{CodeXGLUE}: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
	url = {http://arxiv.org/abs/2102.04664},
	shorttitle = {{CodeXGLUE}},
	abstract = {Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce {CodeXGLUE}, a benchmark dataset to foster machine learning research for program understanding and generation. {CodeXGLUE} includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. {CodeXGLUE} also features three baseline systems, including the {BERT}-style, {GPT}-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.},
	number = {{arXiv}:2102.04664},
	publisher = {{arXiv}},
	author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and Gong, Ming and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and Liu, Shujie},
	urldate = {2023-06-10},
	date = {2021-03-16},
	eprinttype = {arxiv},
	eprint = {2102.04664 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208987/Lu 等 - 2021 - CodeXGLUE A Machine Learning Benchmark Dataset fo.pdf:application/pdf;arXiv.org Snapshot:files/209227/2102.html:text/html},
}

@misc{roziere_dobf_2021,
	title = {{DOBF}: A Deobfuscation Pre-Training Objective for Programming Languages},
	url = {http://arxiv.org/abs/2102.07492},
	shorttitle = {{DOBF}},
	abstract = {Recent advances in self-supervised learning have dramatically improved the state of the art on a wide variety of tasks. However, research in language model pre-training has mostly focused on natural languages, and it is unclear whether models like {BERT} and its variants provide the best pre-training when applied to other modalities, such as source code. In this paper, we introduce a new pre-training objective, {DOBF}, that leverages the structural aspect of programming languages and pre-trains a model to recover the original version of obfuscated source code. We show that models pre-trained with {DOBF} significantly outperform existing approaches on multiple downstream tasks, providing relative improvements of up to 13\% in unsupervised code translation, and 24\% in natural language code search. Incidentally, we found that our pre-trained model is able to de-obfuscate fully obfuscated source files, and to suggest descriptive variable names.},
	number = {{arXiv}:2102.07492},
	publisher = {{arXiv}},
	author = {Roziere, Baptiste and Lachaux, Marie-Anne and Szafraniec, Marc and Lample, Guillaume},
	urldate = {2023-06-10},
	date = {2021-10-27},
	eprinttype = {arxiv},
	eprint = {2102.07492 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209275/Roziere 等 - 2021 - DOBF A Deobfuscation Pre-Training Objective for P.pdf:application/pdf;arXiv.org Snapshot:files/209290/2102.html:text/html},
}

@misc{perez_automatic_2021,
	title = {Automatic Code Generation using Pre-Trained Language Models},
	url = {http://arxiv.org/abs/2102.10535},
	abstract = {Recent advancements in natural language processing {\textbackslash}cite\{gpt2\} {\textbackslash}cite\{{BERT}\} have led to near-human performance in multiple natural language tasks. In this paper, we seek to understand whether similar techniques can be applied to a highly structured environment with strict syntax rules. Specifically, we propose an end-to-end machine learning model for code generation in the Python language built on-top of pre-trained language models. We demonstrate that a fine-tuned model can perform well in code generation tasks, achieving a {BLEU} score of 0.22, an improvement of 46{\textbackslash}\% over a reasonable sequence-to-sequence baseline. All results and related code used for training and data processing are available on {GitHub}.},
	number = {{arXiv}:2102.10535},
	publisher = {{arXiv}},
	author = {Perez, Luis and Ottens, Lizi and Viswanathan, Sudharshan},
	urldate = {2023-06-10},
	date = {2021-02-21},
	eprinttype = {arxiv},
	eprint = {2102.10535 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209295/Perez 等 - 2021 - Automatic Code Generation using Pre-Trained Langua.pdf:application/pdf;arXiv.org Snapshot:files/209794/2102.html:text/html},
}

@misc{banerjee_variable_2021,
	title = {Variable Name Recovery in Decompiled Binary Code using Constrained Masked Language Modeling},
	url = {http://arxiv.org/abs/2103.12801},
	abstract = {Decompilation is the procedure of transforming binary programs into a high-level representation, such as source code, for human analysts to examine. While modern decompilers can reconstruct and recover much information that is discarded during compilation, inferring variable names is still extremely difficult. Inspired by recent advances in natural language processing, we propose a novel solution to infer variable names in decompiled code based on Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as Transformers and {BERT}. Our solution takes {\textbackslash}textit\{raw\} decompiler output, the less semantically meaningful code, as input, and enriches it using our proposed {\textbackslash}textit\{finetuning\} technique, Constrained Masked Language Modeling. Using Constrained Masked Language Modeling introduces the challenge of predicting the number of masked tokens for the original variable name. We address this {\textbackslash}textit\{count of token prediction\} challenge with our post-processing algorithm. Compared to the state-of-the-art approaches, our trained {VarBERT} model is simpler and of much better performance. We evaluated our model on an existing large-scale data set with 164,632 binaries and showed that it can predict variable names identical to the ones present in the original source code up to 84.15{\textbackslash}\% of the time.},
	number = {{arXiv}:2103.12801},
	publisher = {{arXiv}},
	author = {Banerjee, Pratyay and Pal, Kuntal Kumar and Wang, Fish and Baral, Chitta},
	urldate = {2023-06-10},
	date = {2021-03-23},
	eprinttype = {arxiv},
	eprint = {2103.12801 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208439/Banerjee 等 - 2021 - Variable Name Recovery in Decompiled Binary Code u.pdf:application/pdf;arXiv.org Snapshot:files/209683/2103.html:text/html},
}

@misc{hasan_text2app_2021,
	title = {Text2App: A Framework for Creating Android Apps from Text Descriptions},
	url = {http://arxiv.org/abs/2104.08301},
	shorttitle = {Text2App},
	abstract = {We present Text2App -- a framework that allows users to create functional Android applications from natural language specifications. The conventional method of source code generation tries to generate source code directly, which is impractical for creating complex software. We overcome this limitation by transforming natural language into an abstract intermediate formal language representing an application with a substantially smaller number of tokens. The intermediate formal representation is then compiled into target source codes. This abstraction of programming details allows seq2seq networks to learn complex application structures with less overhead. In order to train sequence models, we introduce a data synthesis method grounded in a human survey. We demonstrate that Text2App generalizes well to unseen combination of app components and it is capable of handling noisy natural language instructions. We explore the possibility of creating applications from highly abstract instructions by coupling our system with {GPT}-3 -- a large pretrained language model. We perform an extensive human evaluation and identify the capabilities and limitations of our system. The source code, a ready-to-run demo notebook, and a demo video are publicly available at {\textbackslash}url\{https://github.com/text2app/Text2App\}.},
	number = {{arXiv}:2104.08301},
	publisher = {{arXiv}},
	author = {Hasan, Masum and Mehrab, Kazi Sajeed and Ahmad, Wasi Uddin and Shahriyar, Rifat},
	urldate = {2023-06-10},
	date = {2021-07-07},
	eprinttype = {arxiv},
	eprint = {2104.08301 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209292/Hasan 等 - 2021 - Text2App A Framework for Creating Android Apps fr.pdf:application/pdf;arXiv.org Snapshot:files/209297/2104.html:text/html},
}

@misc{xie_sot_2021,
	title = {{SoT}: Delving Deeper into Classification Head for Transformer},
	url = {http://arxiv.org/abs/2104.10935},
	shorttitle = {{SoT}},
	abstract = {Transformer models are not only successful in natural language processing ({NLP}) but also demonstrate high potential in computer vision ({CV}). Despite great advance, most of works only focus on improvement of architectures but pay little attention to the classification head. For years transformer models base exclusively on classification token to construct the final classifier, without explicitly harnessing high-level word tokens. In this paper, we propose a novel transformer model called second-order transformer ({SoT}), exploiting simultaneously the classification token and word tokens for the classifier. Specifically, we empirically disclose that high-level word tokens contain rich information, which per se are very competent with the classifier and moreover, are complementary to the classification token. To effectively harness such rich information, we propose multi-headed global cross-covariance pooling with singular value power normalization, which shares similar philosophy and thus is compatible with the transformer block, better than commonly used pooling methods. Then, we study comprehensively how to explicitly combine word tokens with classification token for building the final classification head. For {CV} tasks, our {SoT} significantly improves state-of-the-art vision transformers on challenging benchmarks including {ImageNet} and {ImageNet}-A. For {NLP} tasks, through fine-tuning based on pretrained language transformers including {GPT} and {BERT}, our {SoT} greatly boosts the performance on widely used tasks such as {CoLA} and {RTE}. Code will be available at https://peihuali.org/{SoT}},
	number = {{arXiv}:2104.10935},
	publisher = {{arXiv}},
	author = {Xie, Jiangtao and Zeng, Ruiren and Wang, Qilong and Zhou, Ziqi and Li, Peihua},
	urldate = {2023-06-10},
	date = {2021-12-17},
	eprinttype = {arxiv},
	eprint = {2104.10935 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209005/Xie 等 - 2021 - SoT Delving Deeper into Classification Head for T.pdf:application/pdf;arXiv.org Snapshot:files/209682/2104.html:text/html},
}

@misc{jangda_breaking_2022-1,
	title = {Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads},
	url = {http://arxiv.org/abs/2105.05720},
	abstract = {Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone. Therefore, we present {CoCoNeT}, with a {DSL} to express a program with both computation and communication. {CoCoNeT} contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. {CoCoNeT} enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show {CoCoNeT} significantly outperforms state-of-the-art distributed machine learning implementations.},
	number = {{arXiv}:2105.05720},
	publisher = {{arXiv}},
	author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Sarikivi, Olli},
	urldate = {2023-06-10},
	date = {2022-03-26},
	eprinttype = {arxiv},
	eprint = {2105.05720 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:files/208893/Jangda 等 - 2022 - Breaking the Computation and Communication Abstrac.pdf:application/pdf;arXiv.org Snapshot:files/208911/2105.html:text/html},
}

@misc{li_toward_2021,
	title = {Toward Less Hidden Cost of Code Completion with Acceptance and Ranking Models},
	url = {http://arxiv.org/abs/2106.13928},
	abstract = {Code completion is widely used by software developers to provide coding suggestions given a partially written code snippet. Apart from the traditional code completion methods, which only support single token completion at minimal positions, recent studies show the ability to provide longer code completion at more flexible positions. However, such frequently triggered and longer completion results reduce the overall precision as they generate more invalid results. Moreover, different studies are mostly incompatible with each other. Thus, it is vital to develop an ensemble framework that can combine results from multiple models to draw merits and offset defects of each model. This paper conducts a coding simulation to collect data from code context and different code completion models and then apply the data in two tasks. First, we introduce an acceptance model which can dynamically control whether to display completion results to the developer. It uses simulation features to predict whether correct results exist in the output of these models. Our best model reduces the percentage of false-positive completion from 55.09\% to 17.44\%. Second, we design a fusion ranking scheme that can automatically identify the priority of the completion results and reorder the candidates from multiple code completion models. This scheme is flexible in dealing with various models, regardless of the type or the length of their completion results. We integrate this ranking scheme with two frequency models and a {GPT}-2 styled language model, along with the acceptance model to yield 27.80\% and 37.64\% increase in {TOP}1 and {TOP}5 accuracy, respectively. In addition, we propose a new code completion evaluation metric, Benefit-Cost Ratio({BCR}), taking into account the benefit of keystrokes saving and hidden cost of completion list browsing, which is closer to real coder experience scenario.},
	number = {{arXiv}:2106.13928},
	publisher = {{arXiv}},
	author = {Li, Jingxuan and Huang, Rui and Li, Wei and Yao, Kai and Tan, Weiguo},
	urldate = {2023-06-10},
	date = {2021-06-25},
	eprinttype = {arxiv},
	eprint = {2106.13928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208454/Li 等 - 2021 - Toward Less Hidden Cost of Code Completion with Ac.pdf:application/pdf;arXiv.org Snapshot:files/208482/2106.html:text/html},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring Coding Challenge Competence With {APPS}},
	url = {http://arxiv.org/abs/2105.09938},
	abstract = {While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce {APPS}, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both {GitHub} and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as {GPT}-Neo can pass approximately 20\% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.},
	number = {{arXiv}:2105.09938},
	publisher = {{arXiv}},
	author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
	urldate = {2023-06-10},
	date = {2021-11-08},
	eprinttype = {arxiv},
	eprint = {2105.09938 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208986/Hendrycks 等 - 2021 - Measuring Coding Challenge Competence With APPS.pdf:application/pdf;arXiv.org Snapshot:files/209709/2105.html:text/html},
}

@misc{chen_spreadsheetcoder_2021,
	title = {{SpreadsheetCoder}: Formula Prediction from Semi-structured Context},
	url = {http://arxiv.org/abs/2106.15339},
	shorttitle = {{SpreadsheetCoder}},
	abstract = {Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose {SpreadsheetCoder}, a {BERT}-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that {SpreadsheetCoder} achieves top-1 prediction accuracy of 42.51\%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, {SpreadsheetCoder} assists 82\% more users in composing formulas on Google Sheets.},
	number = {{arXiv}:2106.15339},
	publisher = {{arXiv}},
	author = {Chen, Xinyun and Maniatis, Petros and Singh, Rishabh and Sutton, Charles and Dai, Hanjun and Lin, Max and Zhou, Denny},
	urldate = {2023-06-10},
	date = {2021-06-26},
	eprinttype = {arxiv},
	eprint = {2106.15339 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209006/Chen 等 - 2021 - SpreadsheetCoder Formula Prediction from Semi-str.pdf:application/pdf;arXiv.org Snapshot:files/209231/2106.html:text/html},
}

@misc{bagheri_comparison_2021,
	title = {A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python},
	url = {http://arxiv.org/abs/2108.02044},
	abstract = {In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering ({SE}) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, {fastText}, and {BERT} applied to the {SE} task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, {fastText}, and {BERT} to vectors and used a Long Short-Term Memory network to train on them. Using the same {LSTM} architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the {BERT} model is the most promising as it is the least time consuming and the {LSTM} model based on it achieved the best overall accuracy(93.8\%) in predicting Python source code vulnerabilities.},
	number = {{arXiv}:2108.02044},
	publisher = {{arXiv}},
	author = {Bagheri, Amirreza and Hegedűs, Péter},
	urldate = {2023-06-10},
	date = {2021-08-04},
	eprinttype = {arxiv},
	eprint = {2108.02044 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209299/Bagheri 和 Hegedűs - 2021 - A Comparison of Different Source Code Representati.pdf:application/pdf;arXiv.org Snapshot:files/209774/2108.html:text/html},
}

@misc{chen_evaluating_2021,
	title = {Evaluating Large Language Models Trained on Code},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a {GPT} language model fine-tuned on publicly available code from {GitHub}, and study its Python code-writing capabilities. A distinct production version of Codex powers {GitHub} Copilot. On {HumanEval}, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while {GPT}-3 solves 0\% and {GPT}-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	number = {{arXiv}:2107.03374},
	publisher = {{arXiv}},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and {McGrew}, Bob and Amodei, Dario and {McCandlish}, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	urldate = {2023-06-10},
	date = {2021-07-14},
	eprinttype = {arxiv},
	eprint = {2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209306/Chen 等 - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf;arXiv.org Snapshot:files/209311/2107.html:text/html},
}

@misc{mastropaolo_empirical_2021,
	title = {An Empirical Study on Code Comment Completion},
	url = {http://arxiv.org/abs/2107.10544},
	abstract = {Code comments play a prominent role in program comprehension activities. However, source code is not always documented and code and comments not always co-evolve. To deal with these issues, researchers have proposed techniques to automatically generate comments documenting a given code at hand. The most recent works in the area applied deep learning ({DL}) techniques to support such a task. Despite the achieved advances, the empirical evaluations of these approaches show that they are still far from a performance level that would make them valuable for developers. We tackle a simpler and related problem: Code comment completion. Instead of generating a comment for a given code from scratch, we investigate the extent to which state-of-the-art techniques can help developers in writing comments faster. We present a large-scale study in which we empirically assess how a simple n-gram model and the recently proposed Text-To-Text Transfer Transformer (T5) architecture can perform in autocompleting a code comment the developer is typing. The achieved results show the superiority of the T5 model, despite the n-gram model being a competitive solution.},
	number = {{arXiv}:2107.10544},
	publisher = {{arXiv}},
	author = {Mastropaolo, Antonio and Aghajani, Emad and Pascarella, Luca and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021-07-22},
	eprinttype = {arxiv},
	eprint = {2107.10544 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209302/Mastropaolo 等 - 2021 - An Empirical Study on Code Comment Completion.pdf:application/pdf;arXiv.org Snapshot:files/209304/2107.html:text/html},
}

@misc{wang_syncobert_2021,
	title = {{SynCoBERT}: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation},
	url = {http://arxiv.org/abs/2108.04556},
	shorttitle = {{SynCoBERT}},
	abstract = {Code representation learning, which aims to encode the semantics of source code into distributed vectors, plays an important role in recent deep-learning-based models for code intelligence. Recently, many pre-trained language models for source code (e.g., {CuBERT} and {CodeBERT}) have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code search, code clone detection, and program translation. Current approaches typically consider the source code as a plain sequence of tokens, or inject the structure information (e.g., {AST} and data-flow) into the sequential model pre-training. To further explore the properties of programming languages, this paper proposes {SynCoBERT}, a syntax-guided multi-modal contrastive pre-training approach for better code representations. Specially, we design two novel pre-training objectives originating from the symbolic and syntactic properties of source code, i.e., Identifier Prediction ({IP}) and {AST} Edge Prediction ({TEP}), which are designed to predict identifiers, and edges between two nodes of {AST}, respectively. Meanwhile, to exploit the complementary information in semantically equivalent modalities (i.e., code, comment, {AST}) of the code, we propose a multi-modal contrastive learning strategy to maximize the mutual information among different modalities. Extensive experiments on four downstream tasks related to code intelligence show that {SynCoBERT} advances the state-of-the-art with the same pre-training corpus and model size.},
	number = {{arXiv}:2108.04556},
	publisher = {{arXiv}},
	author = {Wang, Xin and Wang, Yasheng and Mi, Fei and Zhou, Pingyi and Wan, Yao and Liu, Xiao and Li, Li and Wu, Hao and Liu, Jin and Jiang, Xin},
	urldate = {2023-06-10},
	date = {2021-09-09},
	eprinttype = {arxiv},
	eprint = {2108.04556 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209300/Wang 等 - 2021 - SynCoBERT Syntax-Guided Multi-Modal Contrastive P.pdf:application/pdf;arXiv.org Snapshot:files/209303/2108.html:text/html},
}

@misc{austin_program_2021,
	title = {Program Synthesis with Large Language Models},
	url = {http://arxiv.org/abs/2108.07732},
	abstract = {This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, {MBPP} and {MathQA}-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems ({MBPP}) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The {MathQA}-Python dataset, a Python version of the {MathQA} benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from {MBPP} using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the {MathQA}-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.},
	number = {{arXiv}:2108.07732},
	publisher = {{arXiv}},
	author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
	urldate = {2023-06-10},
	date = {2021-08-15},
	eprinttype = {arxiv},
	eprint = {2108.07732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209312/Austin 等 - 2021 - Program Synthesis with Large Language Models.pdf:application/pdf;arXiv.org Snapshot:files/209317/2108.html:text/html},
}

@misc{zou_bleser_2021,
	title = {{BLESER}: Bug Localization Based on Enhanced Semantic Retrieval},
	url = {http://arxiv.org/abs/2109.03555},
	shorttitle = {{BLESER}},
	abstract = {Static bug localization techniques that locate bugs at method granularity have gained much attention from both researchers and practitioners. For a static method-level bug localization technique, a key but challenging step is to fully retrieve the semantics of methods and bug reports. Currently, existing studies mainly use the same bag-of-word space to represent the semantics of methods and bug reports without considering structure information of methods and textual contexts of bug reports, which largely and negatively affects bug localization performance. To address this problem, we develop {BLESER}, a new bug localization technique based on enhanced semantic retrieval. Specifically, we use an {AST}-based code embedding model (capturing code structure better) to retrieve the semantics of methods, and word embedding models (capturing textual contexts better) to represent the semantics of bug reports. Then, a deep learning model is built on the enhanced semantic representations. During model building, we compare five typical word embedding models in representing bug reports and try to explore the usefulness of re-sampling strategies and cost-sensitive strategies in handling class imbalance problems. We evaluate our {BLESER} on five Java projects from the Defects4J dataset. We find that: (1) On the whole, the word embedding model {ELMo} outperformed the other four models (including word2vec, {BERT}, etc.) in facilitating bug localization techniques. (2) Among four strategies aiming at solving class imbalance problems, the strategy {ROS} (random over-sampling) performed much better than the other three strategies (including random under-sampling, Focal Loss, etc.). (3) By integrating {ELMo} and {ROS} into {BLESER}, at method-level bug localization, we could achieve {MAP} of 0.108-0.504, {MRR} of 0.134-0.510, and Accuracy@1 of 0.125-0.5 on five Defects4J projects.},
	number = {{arXiv}:2109.03555},
	publisher = {{arXiv}},
	author = {Zou, Weiqin and Li, Enming and Fang, Chunrong},
	urldate = {2023-06-10},
	date = {2021-09-08},
	eprinttype = {arxiv},
	eprint = {2109.03555 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209305/Zou 等 - 2021 - BLESER Bug Localization Based on Enhanced Semanti.pdf:application/pdf;arXiv.org Snapshot:files/209310/2109.html:text/html},
}

@misc{liang_lyra_2022,
	title = {Lyra: A Benchmark for Turducken-Style Code Generation},
	url = {http://arxiv.org/abs/2108.12144},
	shorttitle = {Lyra},
	abstract = {Recently, neural techniques have been used to generate source code automatically. While promising for declarative languages, these approaches achieve much poorer performance on datasets for imperative languages. Since a declarative language is typically embedded in an imperative language (i.e., the turducken-style programming) in real-world software development, the promising results on declarative languages can hardly lead to significant reduction of manual software development efforts. In this paper, we define a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative language with an embedded declarative language. To our knowledge, this is the first turducken-style code generation task. For this task, we present Lyra: a dataset in Python with embedded {SQL}. This dataset contains 2,000 carefully annotated database manipulation programs from real-world projects. Each program is paired with both a Chinese comment and an English comment. In our experiment, we adopted Transformer, {BERT}-style, and {GPT}-style models as baselines. In the best setting, the generation performance of {GPT}-style models is better than others, where the {AST} exact matching accuracy is 24\% and 25.5\% when using Chinese and English comments, respectively. Therefore, we believe that Lyra provides a new challenge for code generation. Yet, overcoming this challenge may significantly boost the applicability of code generation techniques for real-world software development.},
	number = {{arXiv}:2108.12144},
	publisher = {{arXiv}},
	author = {Liang, Qingyuan and Sun, Zeyu and Zhu, Qihao and Zhang, Wenjie and Yu, Lian and Xiong, Yingfei and Zhang, Lu},
	urldate = {2023-06-10},
	date = {2022-07-24},
	eprinttype = {arxiv},
	eprint = {2108.12144 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208473/Liang 等 - 2022 - Lyra A Benchmark for Turducken-Style Code Generat.pdf:application/pdf;arXiv.org Snapshot:files/209739/2108.html:text/html},
}

@misc{huang_solving_2021,
	title = {Solving the Families In the Wild Kinship Verification Challenge by Program Synthesis},
	url = {http://arxiv.org/abs/2110.07020},
	abstract = {Kinship verification is the task of determining whether a parent-child, sibling, or grandparent-grandchild relationship exists between two people and is important in social media applications, forensic investigations, finding missing children, and reuniting families. We demonstrate high quality kinship verification by participating in the 2021 Recognizing Families in the Wild challenge which provides the largest publicly available dataset in the field. Our approach is among the top 3 winning entries in the competition. We ensemble models written by both human experts and a foundation model, {OpenAI} Codex, trained on text and code. We use Codex to generate model variants, and also demonstrate its ability to generate entire running programs for kinship verification tasks of specific relationships.},
	number = {{arXiv}:2110.07020},
	publisher = {{arXiv}},
	author = {Huang, Junyi and Strome, Maxwell Benjamin and Jenkins, Ian and Williams, Parker and Feng, Bo and Wang, Yaning and Wang, Roman and Bagri, Vaibhav and Cheng, Newman and Drori, Iddo},
	urldate = {2023-06-10},
	date = {2021-12-02},
	eprinttype = {arxiv},
	eprint = {2110.07020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/208535/Huang 等 - 2021 - Solving the Families In the Wild Kinship Verificat.pdf:application/pdf;arXiv.org Snapshot:files/209829/2110.html:text/html},
}

@misc{wu_generalized_2022,
	title = {Generalized Polarization Transform: A Novel Coded Transmission Paradigm},
	url = {http://arxiv.org/abs/2110.12224},
	shorttitle = {Generalized Polarization Transform},
	abstract = {For the upcoming 6G wireless networks, a new wave of applications and services will demand ultra-high data rates and reliability. To this end, future wireless systems are expected to pave the way for entirely new fundamental air interface technologies to attain a breakthrough in spectrum efficiency ({SE}). This article discusses a new paradigm, named generalized polarization transform ({GPT}), to achieve an integrated design of coding, modulation, multi-antenna, multiple access, etc., in a real sense. The {GPT} enabled air interface develops far-reaching insights that the joint optimization of critical air interface ingredients can achieve remarkable gains on {SE} compared with the state-of-the-art module-stacking design.},
	number = {{arXiv}:2110.12224},
	publisher = {{arXiv}},
	author = {Wu, Bolin and Dai, Jincheng and Niu, Kai and Si, Zhongwei and Zhang, Ping and Wang, Sen and Yuan, Yifei and I, Chih-Lin},
	urldate = {2023-06-10},
	date = {2022-04-27},
	eprinttype = {arxiv},
	eprint = {2110.12224 [cs, eess, math]},
	keywords = {Computer Science - Information Theory, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:files/209007/Wu 等 - 2022 - Generalized Polarization Transform A Novel Coded .pdf:application/pdf;arXiv.org Snapshot:files/209232/2110.html:text/html},
}

@article{kuling_bi-rads_2022,
	title = {{BI}-{RADS} {BERT} \& Using Section Segmentation to Understand Radiology Reports},
	volume = {8},
	issn = {2313-433X},
	url = {http://arxiv.org/abs/2110.07552},
	doi = {10.3390/jimaging8050131},
	abstract = {Radiology reports are one of the main forms of communication between radiologists and other clinicians and contain important information for patient care. In order to use this information for research and automated patient care programs, it is necessary to convert the raw text into structured data suitable for analysis. State-of-the-art natural language processing ({NLP}) domain-specific contextual word embeddings have been shown to achieve impressive accuracy for these tasks in medicine, but have yet to be utilized for section structure segmentation. In this work, we pre-trained a contextual embedding {BERT} model using breast radiology reports and developed a classifier that incorporated the embedding with auxiliary global textual features in order to perform section segmentation. This model achieved a 98\% accuracy at segregating free text reports sentence by sentence into sections of information outlined in the Breast Imaging Reporting and Data System ({BI}-{RADS}) lexicon, a significant improvement over the Classic {BERT} model without auxiliary information. We then evaluated whether using section segmentation improved the downstream extraction of clinically relevant information such as modality/procedure, previous cancer, menopausal status, the purpose of the exam, breast density, and breast {MRI} background parenchymal enhancement. Using the {BERT} model pre-trained on breast radiology reports combined with section segmentation resulted in an overall accuracy of 95.9\% in the field extraction tasks. This is a 17\% improvement compared to an overall accuracy of 78.9\% for field extraction with models using Classic {BERT} embeddings and not using section segmentation. Our work shows the strength of using {BERT} in radiology report analysis and the advantages of section segmentation in identifying key features of patient factors recorded in breast radiology reports.},
	pages = {131},
	number = {5},
	journaltitle = {Journal of Imaging},
	shortjournal = {J. Imaging},
	author = {Kuling, Grey and Curpen, Dr Belinda and Martel, Anne L.},
	urldate = {2023-06-10},
	date = {2022-05-09},
	eprinttype = {arxiv},
	eprint = {2110.07552 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209004/Kuling 等 - 2022 - BI-RADS BERT & Using Section Segmentation to Under.pdf:application/pdf;arXiv.org Snapshot:files/209230/2110.html:text/html},
}

@misc{drori_solving_2021,
	title = {Solving Linear Algebra by Program Synthesis},
	url = {http://arxiv.org/abs/2111.08171},
	abstract = {We solve {MIT}'s Linear Algebra 18.06 course and Columbia University's Computational Linear Algebra {COMS}3251 courses with perfect accuracy by interactive program synthesis. This surprisingly strong result is achieved by turning the course questions into programming tasks and then running the programs to produce the correct answers. We use {OpenAI} Codex with zero-shot learning, without providing any examples in the prompts, to synthesize code from questions. We quantify the difference between the original question text and the transformed question text that yields a correct answer. Since all {COMS}3251 questions are not available online the model is not overfitting. We go beyond just generating code for questions with numerical answers by interactively generating code that also results visually pleasing plots as output. Finally, we automatically generate new questions given a few sample questions which may be used as new course content. This work is a significant step forward in solving quantitative math problems and opens the door for solving many university level {STEM} courses by machine.},
	number = {{arXiv}:2111.08171},
	publisher = {{arXiv}},
	author = {Drori, Iddo and Verma, Nakul},
	urldate = {2023-06-10},
	date = {2021-11-15},
	eprinttype = {arxiv},
	eprint = {2111.08171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208476/Drori 和 Verma - 2021 - Solving Linear Algebra by Program Synthesis.pdf:application/pdf;arXiv.org Snapshot:files/209821/2111.html:text/html},
}

@misc{zhang_can_2021,
	title = {Can Pre-trained Language Models be Used to Resolve Textual and Semantic Merge Conflicts?},
	url = {http://arxiv.org/abs/2111.11904},
	abstract = {Program merging is standard practice when developers integrate their individual changes to a common code base. When the merge algorithm fails, this is called a merge conflict. The conflict either manifests in textual merge conflicts where the merge fails to produce code, or semantic merge conflicts where the merged code results in compiler or test breaks. Resolving these conflicts for large code projects is expensive because it requires developers to manually identify the sources of conflict and correct them. In this paper, we explore the feasibility of automatically repairing merge conflicts (both textual and semantic) using k-shot learning with large neural language models ({LM}) such as {GPT}-3. One of the challenges in leveraging such language models is to fit the examples and the queries within a small prompt (2048 tokens). We evaluate {LMs} and k-shot learning for two broad applications: (a) textual and semantic merge conflicts for a divergent fork Microsoft Edge, and (b) textual merge conflicts for a large number of {JavaScript} projects in {GitHub}. Our results are mixed: one one-hand, {LMs} provide the state-of-the-art ({SOTA}) performance on semantic merge conflict resolution for Edge compared to earlier symbolic approaches; on the other hand, {LMs} do not yet obviate the benefits of fine-tuning neural models (when sufficient data is available) or the design of special purpose domain-specific languages ({DSL}) for restricted patterns for program synthesis.},
	number = {{arXiv}:2111.11904},
	publisher = {{arXiv}},
	author = {Zhang, Jialu and Mytkowicz, Todd and Kaufman, Mike and Piskac, Ruzica and Lahiri, Shuvendu K.},
	urldate = {2023-06-10},
	date = {2021-11-23},
	eprinttype = {arxiv},
	eprint = {2111.11904 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209308/Zhang 等 - 2021 - Can Pre-trained Language Models be Used to Resolve.pdf:application/pdf;arXiv.org Snapshot:files/209315/2111.html:text/html},
}

@misc{nye_show_2021,
	title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
	url = {http://arxiv.org/abs/2112.00114},
	shorttitle = {Show Your Work},
	abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
	number = {{arXiv}:2112.00114},
	publisher = {{arXiv}},
	author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
	urldate = {2023-06-10},
	date = {2021-11-30},
	eprinttype = {arxiv},
	eprint = {2112.00114 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/209309/Nye 等 - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf:application/pdf;arXiv.org Snapshot:files/209316/2112.html:text/html},
}

@misc{sobania_choose_2021,
	title = {Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of {GitHub} Copilot and Genetic Programming},
	url = {http://arxiv.org/abs/2111.07875},
	shorttitle = {Choose Your Programming Copilot},
	abstract = {{GitHub} Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate {GitHub} Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to {GitHub} Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability.},
	number = {{arXiv}:2111.07875},
	publisher = {{arXiv}},
	author = {Sobania, Dominik and Briesch, Martin and Rothlauf, Franz},
	urldate = {2023-06-10},
	date = {2021-11-15},
	eprinttype = {arxiv},
	eprint = {2111.07875 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/209313/Sobania 等 - 2021 - Choose Your Programming Copilot A Comparison of t.pdf:application/pdf;arXiv.org Snapshot:files/209762/2111.html:text/html},
}

@misc{korbak_controlling_2022,
	title = {Controlling Conditional Language Models without Catastrophic Forgetting},
	url = {http://arxiv.org/abs/2112.00791},
	abstract = {Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities ("catastrophic forgetting"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models ({EBMs}) and approximating these {EBMs} using distributional policy gradients ({DPG}). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend {DPG} to conditional tasks by proposing Conditional {DPG} ({CDPG}). We evaluate {CDPG} on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and {GPT}-Neo). Our results show that fine-tuning using {CDPG} robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.},
	number = {{arXiv}:2112.00791},
	publisher = {{arXiv}},
	author = {Korbak, Tomasz and Elsahar, Hady and Kruszewski, German and Dymetman, Marc},
	urldate = {2023-06-10},
	date = {2022-06-20},
	eprinttype = {arxiv},
	eprint = {2112.00791 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209325/Korbak 等 - 2022 - Controlling Conditional Language Models without Ca.pdf:application/pdf;arXiv.org Snapshot:files/209750/2112.html:text/html},
}

@misc{scholak_towards_2021,
	title = {Towards Neural Functional Program Evaluation},
	url = {http://arxiv.org/abs/2112.04630},
	abstract = {This paper explores the capabilities of current transformer-based language models for program evaluation of simple functional programming languages. We introduce a new program generation mechanism that allows control over syntactic sugar for semantically equivalent programs. T5 experiments reveal that neural functional program evaluation performs surprisingly well, achieving high 90\% exact program match scores for most in-distribution and out-of-distribution tests. Using pretrained T5 weights has significant advantages over random initialization. We present and evaluate on three datasets to study generalization abilities that are specific to functional programs based on: type, function composition, and reduction steps. Code and data are publicly available at https://github.com/{ElementAI}/neural-interpreters.},
	number = {{arXiv}:2112.04630},
	publisher = {{arXiv}},
	author = {Scholak, Torsten and Pilault, Jonathan and Velez-Ginorio, Joey},
	urldate = {2023-06-10},
	date = {2021-12-08},
	eprinttype = {arxiv},
	eprint = {2112.04630 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209314/Scholak 等 - 2021 - Towards Neural Functional Program Evaluation.pdf:application/pdf;arXiv.org Snapshot:files/209687/2112.html:text/html},
}

@misc{liu_rise_2023-1,
	title = {Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective},
	url = {http://arxiv.org/abs/2112.06222},
	shorttitle = {Rise of Distributed Deep Learning Training in the Big Model Era},
	abstract = {Deep learning ({DL}) has become a key component of modern software. In the "big model" era, the rich features of {DL}-based software substantially rely on powerful {DL} models, e.g., {BERT}, {GPT}-3, and the recently emerging {GPT}-4, which are trained on the powerful cloud with large datasets. Hence, training effective {DL} models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current {DL}-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers' issues in distributed training. To this end, we analyze 1,131 real-world developers' issues about using these frameworks reported on Stack Overflow and {GitHub}. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop {DL}-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless {APIs} for cloud platforms.},
	number = {{arXiv}:2112.06222},
	publisher = {{arXiv}},
	author = {Liu, Xuanzhe and Gu, Diandian and Chen, Zhenpeng and Wen, Jinfeng and Zhang, Zili and Ma, Yun and Wang, Haoyu and Jin, Xin},
	urldate = {2023-06-10},
	date = {2023-04-28},
	eprinttype = {arxiv},
	eprint = {2112.06222 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209321/Liu 等 - 2023 - Rise of Distributed Deep Learning Training in the .pdf:application/pdf;arXiv.org Snapshot:files/209326/2112.html:text/html},
}

@misc{pearce_examining_2022,
	title = {Examining Zero-Shot Vulnerability Repair with Large Language Models},
	url = {http://arxiv.org/abs/2112.02125},
	abstract = {Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models ({LLMs}) for code (such as {OpenAI}'s Codex and {AI}21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax {LLMs} into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, "off-the-shelf" {LLMs}, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the {LLMs} could collectively repair 100\% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.},
	number = {{arXiv}:2112.02125},
	publisher = {{arXiv}},
	author = {Pearce, Hammond and Tan, Benjamin and Ahmad, Baleegh and Karri, Ramesh and Dolan-Gavitt, Brendan},
	urldate = {2023-06-10},
	date = {2022-08-15},
	eprinttype = {arxiv},
	eprint = {2112.02125 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209318/Pearce 等 - 2022 - Examining Zero-Shot Vulnerability Repair with Larg.pdf:application/pdf;arXiv.org Snapshot:files/209320/2112.html:text/html},
}

@misc{liu_improving_2021,
	title = {Improving Logical-Level Natural Language Generation with Topic-Conditioned Data Augmentation and Logical Form Generation},
	url = {http://arxiv.org/abs/2112.06240},
	abstract = {Logical Natural Language Generation, i.e., generating textual descriptions that can be logically entailed by a structured table, has been a challenge due to the low fidelity of the generation. {\textbackslash}citet\{chen2020logic2text\} have addressed this problem by annotating interim logical programs to control the generation contents and semantics, and presented the task of table-aware logical form to text (Logic2text) generation. However, although table instances are abundant in the real world, logical forms paired with textual descriptions require costly human annotation work, which limits the performance of neural models. To mitigate this, we propose topic-conditioned data augmentation ({TopicDA}), which utilizes {GPT}-2 to generate unpaired logical forms and textual descriptions directly from tables. We further introduce logical form generation ({LG}), a dual task of Logic2text that requires generating a valid logical form based on a text description of a table. We also propose a semi-supervised learning approach to jointly train a Logic2text and an {LG} model with both labeled and augmented data. The two models benefit from each other by providing extra supervision signals through back-translation. Experimental results on the Logic2text dataset and the {LG} task demonstrate that our approach can effectively utilize the augmented data and outperform supervised baselines by a substantial margin.},
	number = {{arXiv}:2112.06240},
	publisher = {{arXiv}},
	author = {Liu, Ao and Luo, Congjian and Okazaki, Naoaki},
	urldate = {2023-06-10},
	date = {2021-12-12},
	eprinttype = {arxiv},
	eprint = {2112.06240 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209327/Liu 等 - 2021 - Improving Logical-Level Natural Language Generatio.pdf:application/pdf;arXiv.org Snapshot:files/209332/2112.html:text/html},
}

@misc{khan_automatic_2022-1,
	title = {Automatic Detection and Analysis of Technical Debts in Peer-Review Documentation of R Packages},
	url = {http://arxiv.org/abs/2201.04241},
	abstract = {Technical debt ({TD}) is a metaphor for code-related problems that arise as a result of prioritizing speedy delivery over perfect code. Given that the reduction of {TDs} can have long-term positive impact in the software engineering life-cycle ({SDLC}), {TDs} are studied extensively in the literature. However, very few of the existing research focused on the technical debts of R programming language despite its popularity and usage. Recent research by Codabux et al. [21] finds that R packages can have 10 diverse {TD} types analyzing peer-review documentation. However, the findings are based on the manual analysis of a small sample of R package review comments. In this paper, we develop a suite of Machine Learning ({ML}) classifiers to detect the 10 {TDs} automatically. The best performing classifier is based on the deep {ML} model {BERT}, which achieves F1-scores of 0.71 - 0.91. We then apply the trained {BERT} models on all available peer-review issue comments from two platforms, {rOpenSci} and {BioConductor} (13.5K review comments coming from a total of 1297 R packages). We conduct an empirical study on the prevalence and evolution of 10 {TDs} in the two R platforms. We discovered documentation debt is the most prevalent among all types of {TD}, and it is also expanding rapidly. We also find that R packages of generic platform (i.e. {rOpenSci}) are more prone to {TD} compared to domain-specific platform (i.e. {BioConductor}). Our empirical study findings can guide future improvements opportunities in R package documentation. Our {ML} models can be used to automatically monitor the prevalence and evolution of {TDs} in R package documentation.},
	number = {{arXiv}:2201.04241},
	publisher = {{arXiv}},
	author = {Khan, Junaed Younus and Uddin, Gias},
	urldate = {2023-06-10},
	date = {2022-01-11},
	eprinttype = {arxiv},
	eprint = {2201.04241 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209319/Khan 和 Uddin - 2022 - Automatic Detection and Analysis of Technical Debt.pdf:application/pdf;arXiv.org Snapshot:files/209322/2201.html:text/html},
}

@article{fatima_flakify_2023-1,
	title = {Flakify: A Black-Box, Language Model-based Predictor for Flaky Tests},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {http://arxiv.org/abs/2112.12331},
	doi = {10.1109/TSE.2022.3201209},
	shorttitle = {Flakify},
	abstract = {Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. The state-of-the-art {ML}-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, require access to production code, which is not always available to software test engineers. Therefore, in this paper, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed {CodeBERT}, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets ({FlakeFlagger} and {IDoFT}) for flaky test cases and compared our technique with the {FlakeFlagger} approach using two different evaluation procedures: cross-validation and per-project validation. Flakify achieved high F1-scores on both datasets using cross-validation and per-project validation, and surpassed {FlakeFlagger} by 10 and 18 percentage points in terms of precision and recall, respectively, when evaluated on the {FlakeFlagger} dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages. Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over {FlakeFlagger}. Our results further show that a black-box version of {FlakeFlagger} is not a viable option for predicting flaky test cases.},
	pages = {1912--1927},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Fatima, Sakina and Ghaleb, Taher A. and Briand, Lionel},
	urldate = {2023-06-10},
	date = {2023-04-01},
	eprinttype = {arxiv},
	eprint = {2112.12331 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208564/Fatima 等 - 2023 - Flakify A Black-Box, Language Model-based Predict.pdf:application/pdf;arXiv.org Snapshot:files/208853/2112.html:text/html},
}

@misc{ciborowska_fast_2022-2,
	title = {Fast Changeset-based Bug Localization with {BERT}},
	url = {http://arxiv.org/abs/2112.14169},
	abstract = {Automatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as {BERT}, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, {BERT}-like models are computationally expensive, which precludes them from being used in an environment where response time is important. In this paper, we describe how {BERT} can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using {BERT} for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and {BERT}-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed {BERT} model compared to the baselines, especially for bug reports that lack any hints about related code elements.},
	number = {{arXiv}:2112.14169},
	publisher = {{arXiv}},
	author = {Ciborowska, Agnieszka and Damevski, Kostadin},
	urldate = {2023-06-10},
	date = {2022-04-11},
	eprinttype = {arxiv},
	eprint = {2112.14169 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209323/Ciborowska 和 Damevski - 2022 - Fast Changeset-based Bug Localization with BERT.pdf:application/pdf;arXiv.org Snapshot:files/209842/2112.html:text/html},
}

@misc{mastropaolo_using_2022-1,
	title = {Using Deep Learning to Generate Complete Log Statements},
	url = {http://arxiv.org/abs/2201.04837},
	abstract = {Logging is a practice widely adopted in several phases of the software lifecycle. For example, during software development log statements allow engineers to verify and debug the system by exposing fine-grained information of the running software. While the benefits of logging are undisputed, taking proper decisions about where to inject log statements, what information to log, and at which log level (e.g., error, warning) is crucial for the logging effectiveness. In this paper, we present {LANCE} (Log {stAtemeNt} {reCommEnder}), the first approach supporting developers in all these decisions. {LANCE} features a Text-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456 Java methods. {LANCE} takes as input a Java method and injects in it a full log statement, including a human-comprehensible logging message and properly choosing the needed log level and the statement location. Our results show that {LANCE} is able to (i) properly identify the location in the code where to inject the statement in 65.9\% of Java methods requiring it; (ii) selecting the proper log level in 66.2\% of cases; and (iii) generate a completely correct log statement including a meaningful logging message in 15.2\% of cases.},
	number = {{arXiv}:2201.04837},
	publisher = {{arXiv}},
	author = {Mastropaolo, Antonio and Pascarella, Luca and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2022-01-14},
	eprinttype = {arxiv},
	eprint = {2201.04837 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209324/Mastropaolo 等 - 2022 - Using Deep Learning to Generate Complete Log State.pdf:application/pdf;arXiv.org Snapshot:files/209329/2201.html:text/html},
}

@misc{gu_assemble_2022,
	title = {Assemble Foundation Models for Automatic Code Summarization},
	url = {http://arxiv.org/abs/2201.05222},
	abstract = {Automatic code summarization is beneficial to daily software development since it could help reduce the requirement of manual writing. Currently, artificial intelligence is undergoing a paradigm shift. The foundation models pretrained on massive data and finetuned to downstream tasks surpass specially customized models. This trend inspired us to consider reusing foundation models instead of learning from scratch. Thereby, we propose a flexible and robust approach for automatic code summarization, based on neural models. We assemble available foundation models, such as {CodeBERT} and {GPT}-2, into a single neural model named {AdaMo}. Moreover, we utilize Gaussian noise as the simulation of contextual information to optimize the latent representation. Furthermore, we introduce two adaptive schemes from the perspective of knowledge transfer, namely continuous pretraining and intermediate finetuning, and design intermediate stage tasks for general sequence-to-sequence learning. Finally, we evaluate {AdaMo} against a benchmark dataset for code summarization, by comparing it with state-of-the-art models.},
	number = {{arXiv}:2201.05222},
	publisher = {{arXiv}},
	author = {Gu, Jian and Salza, Pasquale and Gall, Harald C.},
	urldate = {2023-06-10},
	date = {2022-03-11},
	eprinttype = {arxiv},
	eprint = {2201.05222 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209328/Gu 等 - 2022 - Assemble Foundation Models for Automatic Code Summ.pdf:application/pdf;arXiv.org Snapshot:files/209333/2201.html:text/html},
}

@misc{tufano_using_2022-1,
	title = {Using Pre-Trained Models to Boost Code Review Automation},
	url = {http://arxiv.org/abs/2201.06850},
	abstract = {Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning ({DL}) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous {DL} models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.},
	number = {{arXiv}:2201.06850},
	publisher = {{arXiv}},
	author = {Tufano, Rosalia and Masiero, Simone and Mastropaolo, Antonio and Pascarella, Luca and Poshyvanyk, Denys and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2022-01-18},
	eprinttype = {arxiv},
	eprint = {2201.06850 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209331/Tufano 等 - 2022 - Using Pre-Trained Models to Boost Code Review Auto.pdf:application/pdf;arXiv.org Snapshot:files/209336/2201.html:text/html},
}

@misc{liang_astbert_2022,
	title = {{AstBERT}: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees},
	url = {http://arxiv.org/abs/2201.07984},
	shorttitle = {{AstBERT}},
	abstract = {Using the pre-trained language models to understand source codes has attracted increasing attention from financial institutions owing to the great potential to uncover financial risks. However, there are several challenges in applying these language models to solve programming language-related problems directly. For instance, the shift of domain knowledge between natural language ({NL}) and programming language ({PL}) requires understanding the semantic and syntactic information from the data from different perspectives. To this end, we propose the {AstBERT} model, a pre-trained {PL} model aiming to better understand the financial codes using the abstract syntax tree ({AST}). Specifically, we collect a sheer number of source codes (both Java and Python) from the Alipay code repository and incorporate both syntactic and semantic code knowledge into our model through the help of code parsers, in which {AST} information of the source codes can be interpreted and integrated. We evaluate the performance of the proposed model on three tasks, including code question answering, code clone detection and code refinement. Experiment results show that our {AstBERT} achieves promising performance on three different downstream tasks.},
	number = {{arXiv}:2201.07984},
	publisher = {{arXiv}},
	author = {Liang, Rong and Zhang, Tiehua and Lu, Yujie and Liu, Yuze and Huang, Zhen and Chen, Xin},
	urldate = {2023-06-10},
	date = {2022-10-11},
	eprinttype = {arxiv},
	eprint = {2201.07984 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209330/Liang 等 - 2022 - AstBERT Enabling Language Model for Financial Cod.pdf:application/pdf;arXiv.org Snapshot:files/209742/2201.html:text/html},
}

@misc{wan_what_2022-1,
	title = {What Do They Capture? -- A Structural Analysis of Pre-Trained Language Models for Source Code},
	url = {http://arxiv.org/abs/2202.06840},
	shorttitle = {What Do They Capture?},
	abstract = {Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., {CodeBERT}, and {GraphCodeBERT}) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.},
	number = {{arXiv}:2202.06840},
	publisher = {{arXiv}},
	author = {Wan, Yao and Zhao, Wei and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
	urldate = {2023-06-10},
	date = {2022-02-14},
	eprinttype = {arxiv},
	eprint = {2202.06840 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209356/Wan 等 - 2022 - What Do They Capture -- A Structural Analysis of .pdf:application/pdf;arXiv.org Snapshot:files/209780/2202.html:text/html},
}

@misc{jones_capturing_2022,
	title = {Capturing Failures of Large Language Models via Human Cognitive Biases},
	url = {http://arxiv.org/abs/2202.12299},
	abstract = {Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we find that {OpenAI}'s Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting files. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.},
	number = {{arXiv}:2202.12299},
	publisher = {{arXiv}},
	author = {Jones, Erik and Steinhardt, Jacob},
	urldate = {2023-06-10},
	date = {2022-11-23},
	eprinttype = {arxiv},
	eprint = {2202.12299 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208494/Jones 和 Steinhardt - 2022 - Capturing Failures of Large Language Models via Hu.pdf:application/pdf;arXiv.org Snapshot:files/209038/2202.html:text/html},
}

@misc{wang_compilable_2022,
	title = {Compilable Neural Code Generation with Compiler Feedback},
	url = {http://arxiv.org/abs/2203.05132},
	abstract = {Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., {CodeGPT}, {PLBART}, and {CodeT}5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes {COMPCODER}, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art {CodeGPT}.},
	number = {{arXiv}:2203.05132},
	publisher = {{arXiv}},
	author = {Wang, Xin and Wang, Yasheng and Wan, Yao and Mi, Fei and Li, Yitong and Zhou, Pingyi and Liu, Jin and Wu, Hao and Jiang, Xin and Liu, Qun},
	urldate = {2023-06-10},
	date = {2022-03-09},
	eprinttype = {arxiv},
	eprint = {2203.05132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208508/Wang 等 - 2022 - Compilable Neural Code Generation with Compiler Fe.pdf:application/pdf;arXiv.org Snapshot:files/208532/2203.html:text/html},
}

@misc{kuznia_less_2022,
	title = {Less is More: Summary of Long Instructions is Better for Program Synthesis},
	url = {http://arxiv.org/abs/2203.08597},
	shorttitle = {Less is More},
	abstract = {Despite the success of large pre-trained language models ({LMs}) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that {LMs} benefit from the summarized version of complicated questions. Our findings show that superfluous information often present in problem description such as human characters, background stories, and names (which are included to help humans in understanding a task) does not help models in understanding a task. To this extent, we create a meta-dataset from the frequently used {APPS} dataset and the newly created {CodeContests} dataset for the program synthesis task. Our meta-dataset consists of human and synthesized summaries of the long and complicated programming questions. Experimental results on Codex show that our proposed approach outperforms baseline by 8.13\% on the {APPS} dataset and 11.88\% on the {CodeContests} dataset on average in terms of strict accuracy. Our analysis shows that summaries significantly improve performance for introductory (9.86\%) and interview (11.48\%) programming questions. However, it shows improvement by a small margin ({\textasciitilde} 2\%) for competitive programming questions, implying scope for future research in this direction.},
	number = {{arXiv}:2203.08597},
	publisher = {{arXiv}},
	author = {Kuznia, Kirby and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
	urldate = {2023-06-10},
	date = {2022-10-22},
	eprinttype = {arxiv},
	eprint = {2203.08597 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208513/Kuznia 等 - 2022 - Less is More Summary of Long Instructions is Bett.pdf:application/pdf;arXiv.org Snapshot:files/208540/2203.html:text/html},
}

@misc{brusilovsky_neural_2022,
	title = {Neural Token Segmentation for High Token-Internal Complexity},
	url = {http://arxiv.org/abs/2203.10845},
	abstract = {Tokenizing raw texts into word units is an essential pre-processing step for critical tasks in the {NLP} pipeline such as tagging, parsing, named entity recognition, and more. For most languages, this tokenization step straightforward. However, for languages with high token-internal complexity, further token-to-word segmentation is required. Previous canonical segmentation studies were based on character-level frameworks, with no contextualised representation involved. Contextualized vectors a la {BERT} show remarkable results in many applications, but were not shown to improve performance on linguistic segmentation per se. Here we propose a novel neural segmentation model which combines the best of both worlds, contextualised token representation and char-level decoding, which is particularly effective for languages with high token-internal complexity and extreme morphological ambiguity. Our model shows substantial improvements in segmentation accuracy on Hebrew and Arabic compared to the state-of-the-art, and leads to further improvements on downstream tasks such as Part-of-Speech Tagging, Dependency Parsing and Named-Entity Recognition, over existing pipelines. When comparing our segmentation-first pipeline with joint segmentation and labeling in the same settings, we show that, contrary to pre-neural studies, the pipeline performance is superior.},
	number = {{arXiv}:2203.10845},
	publisher = {{arXiv}},
	author = {Brusilovsky, Idan and Tsarfaty, Reut},
	urldate = {2023-06-10},
	date = {2022-03-21},
	eprinttype = {arxiv},
	eprint = {2203.10845 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208489/Brusilovsky 和 Tsarfaty - 2022 - Neural Token Segmentation for High Token-Internal .pdf:application/pdf;arXiv.org Snapshot:files/209783/2203.html:text/html},
}

@misc{nijkamp_codegen_2023,
	title = {{CodeGen}: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
	url = {http://arxiv.org/abs/2203.13474},
	shorttitle = {{CodeGen}},
	abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called {CODEGEN}, on natural language and programming language data, and open source the training library {JAXFORMER}. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on {HumanEval}. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark ({MTPB}), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on {MTPB} shows that the same intent provided to {CODEGEN} in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library {JAXFORMER} and model checkpoints available as open source contribution: https://github.com/salesforce/{CodeGen}.},
	number = {{arXiv}:2203.13474},
	publisher = {{arXiv}},
	author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
	urldate = {2023-06-10},
	date = {2023-02-27},
	eprinttype = {arxiv},
	eprint = {2203.13474 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209334/Nijkamp 等 - 2023 - CodeGen An Open Large Language Model for Code wit.pdf:application/pdf;arXiv.org Snapshot:files/209338/2203.html:text/html},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: Scaling Language Modeling with Pathways},
	url = {http://arxiv.org/abs/2204.02311},
	shorttitle = {{PaLM}},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model {PaLM}. We trained {PaLM} on 6144 {TPU} v4 chips using Pathways, a new {ML} system which enables highly efficient training across multiple {TPU} Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, {PaLM} 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released {BIG}-bench benchmark. A significant number of {BIG}-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. {PaLM} also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	number = {{arXiv}:2204.02311},
	publisher = {{arXiv}},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	urldate = {2023-06-10},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209342/Chowdhery 等 - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:files/209734/2204.html:text/html},
}

@misc{degiovanni_mubert_2022,
	title = {\${\textbackslash}mu\${BERT}: Mutation Testing using Pre-Trained Language Models},
	url = {http://arxiv.org/abs/2203.03289},
	shorttitle = {\${\textbackslash}mu\${BERT}},
	abstract = {We introduce \${\textbackslash}mu\${BERT}, a mutation testing tool that uses a pre-trained language model ({CodeBERT}) to generate mutants. This is done by masking a token from the expression given as input and using {CodeBERT} to predict it. Thus, the mutants are generated by replacing the masked tokens with the predicted ones. We evaluate \${\textbackslash}mu\${BERT} on 40 real faults from Defects4J and show that it can detect 27 out of the 40 faults, while the baseline ({PiTest}) detects 26 of them. We also show that \${\textbackslash}mu\${BERT} can be 2 times more cost-effective than {PiTest}, when the same number of mutants are analysed. Additionally, we evaluate the impact of \${\textbackslash}mu\${BERT}'s mutants when used by program assertion inference techniques, and show that they can help in producing better specifications. Finally, we discuss about the quality and naturalness of some interesting mutants produced by \${\textbackslash}mu\${BERT} during our experimental evaluation.},
	number = {{arXiv}:2203.03289},
	publisher = {{arXiv}},
	author = {Degiovanni, Renzo and Papadakis, Mike},
	urldate = {2023-06-10},
	date = {2022-03-07},
	eprinttype = {arxiv},
	eprint = {2203.03289 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209337/Degiovanni 和 Papadakis - 2022 - \$mu\$BERT Mutation Testing using Pre-Trained Lang.pdf:application/pdf;arXiv.org Snapshot:files/209757/2203.html:text/html},
}

@misc{yuan_circle_2022-1,
	title = {{CIRCLE}: Continual Repair across Programming Languages},
	url = {http://arxiv.org/abs/2205.10956},
	shorttitle = {{CIRCLE}},
	abstract = {Automatic Program Repair ({APR}) aims at fixing buggy source code with less manual debugging efforts, which plays a vital role in improving software reliability and development productivity. Recent {APR} works have achieved remarkable progress via applying deep learning ({DL}), particularly neural machine translation ({NMT}) techniques. However, we observe that existing {DL}-based {APR} models suffer from at least two severe drawbacks: (1) Most of them can only generate patches for a single programming language, as a result, to repair multiple languages, we have to build and train many repairing models. (2) Most of them are developed in an offline manner. Therefore, they won't function when there are new-coming requirements. To address the above problems, a T5-based {APR} framework equipped with continual learning ability across multiple programming languages is proposed, namely {\textbackslash}emph\{C\}ont{\textbackslash}emph\{I\}nual {\textbackslash}emph\{R\}epair a{\textbackslash}emph\{C\}ross Programming {\textbackslash}emph\{L\}anguag{\textbackslash}emph\{E\}s ({\textbackslash}emph\{{CIRCLE}\}). Specifically, (1) {CIRCLE} utilizes a prompting function to narrow the gap between natural language processing ({NLP}) pre-trained tasks and {APR}. (2) {CIRCLE} adopts a difficulty-based rehearsal strategy to achieve lifelong learning for {APR} without access to the full historical data. (3) An elastic regularization method is employed to strengthen {CIRCLE}'s continual learning ability further, preventing it from catastrophic forgetting. (4) {CIRCLE} applies a simple but effective re-repairing method to revise generated errors caused by crossing multiple programming languages. We train {CIRCLE} for four languages (i.e., C, {JAVA}, {JavaScript}, and Python) and evaluate it on five commonly used benchmarks. The experimental results demonstrate that {CIRCLE} not only effectively and efficiently repairs multiple programming languages in continual learning settings, but also achieves state-of-the-art performance with a single repair model.},
	number = {{arXiv}:2205.10956},
	publisher = {{arXiv}},
	author = {Yuan, Wei and Zhang, Quanjun and He, Tieke and Fang, Chunrong and Hung, Nguyen Quoc Viet and Hao, Xiaodong and Yin, Hongzhi},
	urldate = {2023-06-10},
	date = {2022-12-03},
	eprinttype = {arxiv},
	eprint = {2205.10956 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209044/Yuan 等 - 2022 - CIRCLE Continual Repair across Programming Langua.pdf:application/pdf;arXiv.org Snapshot:files/209688/2205.html:text/html},
}

@misc{wu_autoformalization_2022,
	title = {Autoformalization with Large Language Models},
	url = {http://arxiv.org/abs/2205.12615},
	abstract = {Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that {LLMs} can correctly translate a significant portion (\$25.3{\textbackslash}\%\$) of mathematical competition problems perfectly to formal specifications in Isabelle/{HOL}. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the {MiniF}2F theorem proving benchmark, improving the proof rate from \$29.6{\textbackslash}\%\$ to \$35.2{\textbackslash}\%\$.},
	number = {{arXiv}:2205.12615},
	publisher = {{arXiv}},
	author = {Wu, Yuhuai and Jiang, Albert Q. and Li, Wenda and Rabe, Markus N. and Staats, Charles and Jamnik, Mateja and Szegedy, Christian},
	urldate = {2023-06-10},
	date = {2022-05-25},
	eprinttype = {arxiv},
	eprint = {2205.12615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:files/209335/Wu 等 - 2022 - Autoformalization with Large Language Models.pdf:application/pdf;arXiv.org Snapshot:files/209801/2205.html:text/html},
}

@misc{zhong_active_2022,
	title = {Active Programming by Example with a Natural Language Prior},
	url = {http://arxiv.org/abs/2205.12422},
	abstract = {We introduce {APEL}, a new framework that enables non-programmers to indirectly annotate natural language utterances with executable meaning representations, such as {SQL} programs. Based on a natural language utterance, we first run a seed semantic parser to generate a prior over a list of candidate programs. To obtain information about which candidate is correct, we synthesize an input on which the more likely programs tend to produce different outputs, and ask an annotator which output is appropriate for the utterance. Hence, the annotator does not have to directly inspect the programs. To further reduce effort required from annotators, we aim to synthesize simple input databases that nonetheless have high information gain. With human annotators and Bayesian inference to handle annotation errors, we outperform Codex's top-1 performance (59\%) and achieve the same accuracy as the original expert annotators (75\%), by soliciting answers for each utterance on only 2 databases with an average of 9 records each. In contrast, it would be impractical to solicit outputs on the original 30K-record databases provided by {SPIDER}},
	number = {{arXiv}:2205.12422},
	publisher = {{arXiv}},
	author = {Zhong, Ruiqi and Snell, Charlie and Klein, Dan and Eisner, Jason},
	urldate = {2023-06-10},
	date = {2022-05-24},
	eprinttype = {arxiv},
	eprint = {2205.12422 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209345/Zhong 等 - 2022 - Active Programming by Example with a Natural Langu.pdf:application/pdf;arXiv.org Snapshot:files/209700/2205.html:text/html},
}

@misc{zhang_automatically_2022,
	title = {Automatically Answering and Generating Machine Learning Final Exams},
	url = {http://arxiv.org/abs/2206.05442},
	abstract = {Can a machine learn machine learning? We propose to answer this question using the same criteria we use to answer a similar question: can a human learn machine learning? We automatically answer final exams in {MIT}'s, Harvard's and Cornell's large machine learning courses and generate new questions at a human level. Recently, program synthesis and few-shot learning solved university-level problem set questions in mathematics and {STEM} courses at a human level. In this work, we solve questions from final exams that differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We provide a new dataset and benchmark of questions from machine learning final exams and code for automatically answering these questions and generating new questions. To make our dataset a reproducible benchmark, we use automatic checkers for multiple choice questions, questions with numeric answers, and questions with expression answers, and evaluate a large free language model, Meta's {OPT}, and compare the results with Open {AI}'s {GPT}-3, {ChatGPT}, and Codex. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning, chain-of-thought prompting, {GPT}-3, {ChatGPT}, and {OPT} pre-trained on text and Codex fine-tuned on code on a range of machine learning topics and find that few-shot learning methods perform best. We make our data and code publicly available for the machine learning community.},
	number = {{arXiv}:2206.05442},
	publisher = {{arXiv}},
	author = {Zhang, Sarah and Shuttleworth, Reece and Chin, Zad and Lantigua, Pedro and Surbehera, Saisamrit and Hunter, Gregory and Austin, Derek and Hicke, Yann and Tang, Leonard and Karnik, Sathwik and Granberry, Darnell and Drori, Iddo},
	urldate = {2023-06-10},
	date = {2022-12-23},
	eprinttype = {arxiv},
	eprint = {2206.05442 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209349/Zhang 等 - 2022 - Automatically Answering and Generating Machine Lea.pdf:application/pdf;arXiv.org Snapshot:files/209735/2206.html:text/html},
}

@misc{lehman_evolution_2022,
	title = {Evolution through Large Models},
	url = {http://arxiv.org/abs/2206.08896},
	abstract = {This paper pursues the insight that large language models ({LLMs}) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming ({GP}). Because such {LLMs} benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models ({ELM}), in the main experiment {ELM} combined with {MAP}-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original {LLM} had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by {ELM}.},
	number = {{arXiv}:2206.08896},
	publisher = {{arXiv}},
	author = {Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O.},
	urldate = {2023-06-10},
	date = {2022-06-17},
	eprinttype = {arxiv},
	eprint = {2206.08896 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/209343/Lehman 等 - 2022 - Evolution through Large Models.pdf:application/pdf;arXiv.org Snapshot:files/209347/2206.html:text/html},
}

@misc{lima_looking_2022,
	title = {Looking for related discussions on {GitHub} Discussions},
	url = {http://arxiv.org/abs/2206.11971},
	abstract = {Software teams are increasingly adopting different tools and communication channels to aid the software collaborative development model and coordinate tasks. Among such resources, Programming Community-based Question Answering ({PCQA}) forums have become widely used by developers. Such environments enable developers to get and share technical information. Interested in supporting the development and management of Open Source Software ({OSS}) projects, {GitHub} announced {GitHub} Discussions - a native forum to facilitate collaborative discussions between users and members of communities hosted on the platform. As {GitHub} Discussions resembles {PCQA} forums, it faces challenges similar to those faced by such environments, which include the occurrence of related discussions (duplicates or near-duplicated posts). While duplicate posts have the same content - and may be exact copies - near-duplicates share similar topics and information. Both can introduce noise to the platform and compromise project knowledge sharing. In this paper, we address the problem of detecting related posts in {GitHub} Discussions. To do so, we propose an approach based on a Sentence-{BERT} pre-trained model: the {RD}-Detector. We evaluated {RD}-Detector using data from different {OSS} communities. {OSS} maintainers and Software Engineering ({SE}) researchers manually evaluated the {RD}-Detector results, which achieved 75\% to 100\% in terms of precision. In addition, maintainers pointed out practical applications of the approach, such as merging the discussions' threads and making discussions as comments on one another. {OSS} maintainers can benefit from {RD}-Detector to address the labor-intensive task of manually detecting related discussions and answering the same question multiple times.},
	number = {{arXiv}:2206.11971},
	publisher = {{arXiv}},
	author = {Lima, Marcia and Steinmacher, Igor and Ford, Denae and Liu, Evangeline and Vorreuter, Grace and Conte, Tayana and Gadelha, Bruno},
	urldate = {2023-06-10},
	date = {2022-06-23},
	eprinttype = {arxiv},
	eprint = {2206.11971 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209043/Lima 等 - 2022 - Looking for related discussions on GitHub Discussi.pdf:application/pdf;arXiv.org Snapshot:files/209727/2206.html:text/html},
}

@misc{zan_cert_2022-1,
	title = {{CERT}: Continual Pre-Training on Sketches for Library-Oriented Code Generation},
	url = {http://arxiv.org/abs/2206.06888},
	shorttitle = {{CERT}},
	abstract = {Code generation is a longstanding challenge, aiming to generate a code snippet based on a natural language description. Usually, expensive text-code paired data is essential for training a code generation model. Recently, thanks to the success of pre-training techniques, large language models are trained on large-scale unlabelled code corpora and perform well in code generation. In this paper, we investigate how to leverage an unlabelled code corpus to train a model for library-oriented code generation. Since it is a common practice for programmers to reuse third-party libraries, in which case the text-code paired data are harder to obtain due to the huge number of libraries. We observe that library-oriented code snippets are more likely to share similar code sketches. Hence, we present {CERT} with two steps: a sketcher generates the sketch, then a generator fills the details in the sketch. Both the sketcher and the generator are continually pre-trained upon a base model using unlabelled data. Furthermore, we craft two benchmarks named {PandasEval} and {NumpyEval} to evaluate library-oriented code generation. Experimental results demonstrate the impressive performance of {CERT}. For example, it surpasses the base model by an absolute 15.67\% improvement in terms of pass@1 on {PandasEval}. Our work is available at https://github.com/microsoft/{PyCodeGPT}.},
	number = {{arXiv}:2206.06888},
	publisher = {{arXiv}},
	author = {Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
	urldate = {2023-06-10},
	date = {2022-06-14},
	eprinttype = {arxiv},
	eprint = {2206.06888 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209341/Zan 等 - 2022 - CERT Continual Pre-Training on Sketches for Libra.pdf:application/pdf;arXiv.org Snapshot:files/209725/2206.html:text/html},
}

@misc{ahmed_few-shot_2022-1,
	title = {Few-shot training {LLMs} for project-specific code-summarization},
	url = {http://arxiv.org/abs/2207.04237},
	abstract = {Very large language models ({LLMs}), such as {GPT}-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of {LLMs} is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, {APIs}, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of {LLMs} might be very relevant. In this paper, we investigate the use few-shot training with the very large {GPT} (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.},
	number = {{arXiv}:2207.04237},
	publisher = {{arXiv}},
	author = {Ahmed, Toufique and Devanbu, Premkumar},
	urldate = {2023-06-10},
	date = {2022-09-08},
	eprinttype = {arxiv},
	eprint = {2207.04237 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209344/Ahmed 和 Devanbu - 2022 - Few-shot training LLMs for project-specific code-s.pdf:application/pdf;arXiv.org Snapshot:files/209350/2207.html:text/html},
}

@misc{ferreira_incivility_2022,
	title = {Incivility Detection in Open Source Code Review and Issue Discussions},
	url = {http://arxiv.org/abs/2206.13429},
	abstract = {Given the democratic nature of open source development, code review and issue discussions may be uncivil. Incivility, defined as features of discussion that convey an unnecessarily disrespectful tone, can have negative consequences to open source communities. To prevent or minimize these negative consequences, open source platforms have included mechanisms for removing uncivil language from the discussions. However, such approaches require manual inspection, which can be overwhelming given the large number of discussions. To help open source communities deal with this problem, in this paper, we aim to compare six classical machine learning models with {BERT} to detect incivility in open source code review and issue discussions. Furthermore, we assess if adding contextual information improves the models' performance and how well the models perform in a cross-platform setting. We found that {BERT} performs better than classical machine learning models, with a best F1-score of 0.95. Furthermore, classical machine learning models tend to underperform to detect non-technical and civil discussions. Our results show that adding the contextual information to {BERT} did not improve its performance and that none of the analyzed classifiers had an outstanding performance in a cross-platform setting. Finally, we provide insights into the tones that the classifiers misclassify.},
	number = {{arXiv}:2206.13429},
	publisher = {{arXiv}},
	author = {Ferreira, Isabella and Rafiq, Ahlaam and Cheng, Jinghui},
	urldate = {2023-06-10},
	date = {2022-06-27},
	eprinttype = {arxiv},
	eprint = {2206.13429 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209358/Ferreira 等 - 2022 - Incivility Detection in Open Source Code Review an.pdf:application/pdf;arXiv.org Snapshot:files/209362/2206.html:text/html},
}

@misc{wang_novel_2022,
	title = {A Novel {DeBERTa}-based Model for Financial Question Answering Task},
	url = {http://arxiv.org/abs/2207.05875},
	abstract = {As a rising star in the field of natural language processing, question answering systems (Q\&A Systems) are widely used in all walks of life. Compared with other scenarios, the applicationin financial scenario has strong requirements in the traceability and interpretability of the Q\&A systems. In addition, since the demand for artificial intelligence technology has gradually shifted from the initial computational intelligence to cognitive intelligence, this research mainly focuses on the financial numerical reasoning dataset - {FinQA}. In the shared task, the objective is to generate the reasoning program and the final answer according to the given financial report containing text and tables. We use the method based on {DeBERTa} pre-trained language model, with additional optimization methods including multi-model fusion, training set combination on this basis. We finally obtain an execution accuracy of 68.99 and a program accuracy of 64.53, ranking No. 4 in the 2022 {FinQA} Challenge.},
	number = {{arXiv}:2207.05875},
	publisher = {{arXiv}},
	author = {Wang, Yanbo J. and Li, Yuming and Qin, Hui and Guan, Yuhang and Chen, Sheng},
	urldate = {2023-06-10},
	date = {2022-07-12},
	eprinttype = {arxiv},
	eprint = {2207.05875 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208524/Wang 等 - 2022 - A Novel DeBERTa-based Model for Financial Question.pdf:application/pdf;arXiv.org Snapshot:files/208542/2207.html:text/html},
}

@misc{christopoulou_pangu-coder_2022,
	title = {{PanGu}-Coder: Program Synthesis with Function-Level Language Modeling},
	url = {http://arxiv.org/abs/2207.11280},
	shorttitle = {{PanGu}-Coder},
	abstract = {We present {PanGu}-Coder, a pretrained decoder-only language model adopting the {PanGu}-Alpha architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description. We train {PanGu}-Coder using a two-stage strategy: the first stage employs Causal Language Modelling ({CLM}) to pre-train on raw programming language data, while the second stage uses a combination of Causal Language Modelling and Masked Language Modelling ({MLM}) training objectives that focus on the downstream task of text-to-code generation and train on loosely curated pairs of natural language program definitions and code functions. Finally, we discuss {PanGu}-Coder-{FT}, which is fine-tuned on a combination of competitive programming problems and code with continuous integration tests. We evaluate {PanGu}-Coder with a focus on whether it generates functionally correct programs and demonstrate that it achieves equivalent or better performance than similarly sized models, such as {CodeX}, while attending a smaller context window and training on less data.},
	number = {{arXiv}:2207.11280},
	publisher = {{arXiv}},
	author = {Christopoulou, Fenia and Lampouras, Gerasimos and Gritta, Milan and Zhang, Guchun and Guo, Yinpeng and Li, Zhongqi and Zhang, Qi and Xiao, Meng and Shen, Bo and Li, Lin and Yu, Hao and Yan, Li and Zhou, Pingyi and Wang, Xin and Ma, Yuchi and Iacobacci, Ignacio and Wang, Yasheng and Liang, Guangtai and Wei, Jiansheng and Jiang, Xin and Wang, Qianxiang and Liu, Qun},
	urldate = {2023-06-10},
	date = {2022-07-22},
	eprinttype = {arxiv},
	eprint = {2207.11280 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208548/Christopoulou 等 - 2022 - PanGu-Coder Program Synthesis with Function-Level.pdf:application/pdf;arXiv.org Snapshot:files/209084/2207.html:text/html},
}

@misc{zhang_coditt5_2022-1,
	title = {{CoditT}5: Pretraining for Source Code and Natural Language Editing},
	url = {http://arxiv.org/abs/2208.05446},
	shorttitle = {{CoditT}5},
	abstract = {Pretrained language models have been shown to be effective in many software-related generation tasks; however, they are not well-suited for editing tasks as they are not designed to reason about edits. To address this, we propose a novel pretraining objective which explicitly models edits and use it to build {CoditT}5, a large language model for software-related editing tasks that is pretrained on large amounts of source code and natural language comments. We fine-tune it on various downstream editing tasks, including comment updating, bug fixing, and automated code review. By outperforming standard generation-based models, we demonstrate the generalizability of our approach and its suitability for editing tasks. We also show how a standard generation model and our edit-based model can complement one another through simple reranking strategies, with which we achieve state-of-the-art performance for the three downstream editing tasks.},
	number = {{arXiv}:2208.05446},
	publisher = {{arXiv}},
	author = {Zhang, Jiyang and Panthaplackel, Sheena and Nie, Pengyu and Li, Junyi Jessy and Gligoric, Milos},
	urldate = {2023-06-10},
	date = {2022-09-14},
	eprinttype = {arxiv},
	eprint = {2208.05446 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209346/Zhang 等 - 2022 - CoditT5 Pretraining for Source Code and Natural L.pdf:application/pdf;arXiv.org Snapshot:files/209759/2208.html:text/html},
}

@misc{khlaaf_hazard_2022,
	title = {A Hazard Analysis Framework for Code Synthesis Large Language Models},
	url = {http://arxiv.org/abs/2207.14157},
	abstract = {Codex, a large language model ({LLM}) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at {OpenAI} to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.},
	number = {{arXiv}:2207.14157},
	publisher = {{arXiv}},
	author = {Khlaaf, Heidy and Mishkin, Pamela and Achiam, Joshua and Krueger, Gretchen and Brundage, Miles},
	urldate = {2023-06-10},
	date = {2022-07-25},
	eprinttype = {arxiv},
	eprint = {2207.14157 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209348/Khlaaf 等 - 2022 - A Hazard Analysis Framework for Code Synthesis Lar.pdf:application/pdf;arXiv.org Snapshot:files/209351/2207.html:text/html},
}

@incollection{meng_automatic_2022,
	title = {Automatic Classification of Bug Reports Based on Multiple Text Information and Reports' Intention},
	volume = {13299},
	url = {http://arxiv.org/abs/2208.01274},
	abstract = {With the rapid growth of software scale and complexity, a large number of bug reports are submitted to the bug tracking system. In order to speed up defect repair, these reports need to be accurately classified so that they can be sent to the appropriate developers. However, the existing classification methods only use the text information of the bug report, which leads to their low performance. To solve the above problems, this paper proposes a new automatic classification method for bug reports. The innovation is that when categorizing bug reports, in addition to using the text information of the report, the intention of the report (i.e. suggestion or explanation) is also considered, thereby improving the performance of the classification. First, we collect bug reports from four ecosystems (Apache, Eclipse, Gentoo, Mozilla) and manually annotate them to construct an experimental data set. Then, we use Natural Language Processing technology to preprocess the data. On this basis, {BERT} and {TF}-{IDF} are used to extract the features of the intention and the multiple text information. Finally, the features are used to train the classifiers. The experimental result on five classifiers (including K-Nearest Neighbor, Naive Bayes, Logistic Regression, Support Vector Machine, and Random Forest) show that our proposed method achieves better performance and its F-Measure achieves from 87.3\% to 95.5\%.},
	pages = {131--147},
	author = {Meng, Fanqi and Wang, Xuesong and Wang, Jingdong and Wang, Peifang},
	urldate = {2023-06-10},
	date = {2022},
	doi = {10.1007/978-3-031-10363-6_9},
	eprinttype = {arxiv},
	eprint = {2208.01274 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208537/Meng 等 - 2022 - Automatic Classification of Bug Reports Based on M.pdf:application/pdf;arXiv.org Snapshot:files/209072/2208.html:text/html},
}

@misc{su_selective_2022,
	title = {Selective Annotation Makes Language Models Better Few-Shot Learners},
	url = {http://arxiv.org/abs/2209.01975},
	abstract = {Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9\%/11.4\% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/{HKUNLP}/icl-selective-annotation.},
	number = {{arXiv}:2209.01975},
	publisher = {{arXiv}},
	author = {Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
	urldate = {2023-06-10},
	date = {2022-09-05},
	eprinttype = {arxiv},
	eprint = {2209.01975 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209353/Su 等 - 2022 - Selective Annotation Makes Language Models Better .pdf:application/pdf;arXiv.org Snapshot:files/209797/2209.html:text/html},
}

@inproceedings{parthasarathy_measuring_2022-1,
	title = {Measuring design compliance using neural language models -- an automotive case study},
	url = {http://arxiv.org/abs/2208.13215},
	doi = {10.1145/3558489.3559067},
	abstract = {As the modern vehicle becomes more software-defined, it is beginning to take significant effort to avoid serious regression in software design. This is because automotive software architects rely largely upon manual review of code to spot deviations from specified design principles. Such an approach is both inefficient and prone to error. In recent days, neural language models pre-trained on source code are beginning to be used for automating a variety of programming tasks. In this work, we extend the application of such a Programming Language Model ({PLM}) to automate the assessment of design compliance. Using a {PLM}, we construct a system that assesses whether a set of query programs comply with Controller-Handler, a design pattern specified to ensure hardware abstraction in automotive control software. The assessment is based upon measuring whether the geometrical arrangement of query program embeddings, extracted from the {PLM}, aligns with that of a set of known implementations of the pattern. The level of alignment is then transformed into an interpretable measure of compliance. Using a controlled experiment, we demonstrate that our technique determines compliance with a precision of 92\%. Also, using expert review to calibrate the automated assessment, we introduce a protocol to determine the nature of the violation, helping eventual refactoring. Results from this work indicate that neural language models can provide valuable assistance to human architects in assessing and fixing violations in automotive software design.},
	pages = {12--21},
	booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
	author = {Parthasarathy, Dhasarathy and Ekelin, Cecilia and Karri, Anjali and Sun, Jiapeng and Moraitis, Panagiotis},
	urldate = {2023-06-10},
	date = {2022-11-07},
	eprinttype = {arxiv},
	eprint = {2208.13215 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209355/Parthasarathy 等 - 2022 - Measuring design compliance using neural language .pdf:application/pdf;arXiv.org Snapshot:files/209360/2208.html:text/html},
}

@misc{cassano_multipl-e_2022,
	title = {{MultiPL}-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation},
	url = {http://arxiv.org/abs/2208.08227},
	shorttitle = {{MultiPL}-E},
	abstract = {Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose {MultiPL}-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using {MultiPL}-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use {MultiPL}-E to extend the {HumanEval} benchmark and {MBPP} benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, {CodeGen}, and {InCoder}. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in {MultiPL}-E allow us to explore the impact of language frequency and language features on model performance. Finally, the {MultiPL}-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.},
	number = {{arXiv}:2208.08227},
	publisher = {{arXiv}},
	author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q. and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
	urldate = {2023-06-10},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2208.08227 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209352/Cassano 等 - 2022 - MultiPL-E A Scalable and Extensible Approach to B.pdf:application/pdf;arXiv.org Snapshot:files/209357/2208.html:text/html},
}

@misc{khan_automatic_2022-2,
	title = {Automatic Code Documentation Generation Using {GPT}-3},
	url = {http://arxiv.org/abs/2209.02235},
	abstract = {Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a {GPT}-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall {BLEU} score of 20.6 for six different programming languages (11.2\% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.},
	number = {{arXiv}:2209.02235},
	publisher = {{arXiv}},
	author = {Khan, Junaed Younus and Uddin, Gias},
	urldate = {2023-06-10},
	date = {2022-09-06},
	eprinttype = {arxiv},
	eprint = {2209.02235 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209354/Khan 和 Uddin - 2022 - Automatic Code Documentation Generation Using GPT-.pdf:application/pdf;arXiv.org Snapshot:files/209832/2209.html:text/html},
}

@misc{singh_progprompt_2022,
	title = {{ProgPrompt}: Generating Situated Robot Task Plans using Large Language Models},
	url = {http://arxiv.org/abs/2209.11302},
	shorttitle = {{ProgPrompt}},
	abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models ({LLMs}) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic {LLM} prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the {LLM} with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in {VirtualHome} household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
	number = {{arXiv}:2209.11302},
	publisher = {{arXiv}},
	author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	urldate = {2023-06-10},
	date = {2022-09-22},
	eprinttype = {arxiv},
	eprint = {2209.11302 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/208849/Singh 等 - 2022 - ProgPrompt Generating Situated Robot Task Plans u.pdf:application/pdf;arXiv.org Snapshot:files/208860/2209.html:text/html},
}

@misc{liu_autoupdate_2023,
	title = {{AutoUpdate}: Automatically Recommend Code Updates for Android Apps},
	url = {http://arxiv.org/abs/2209.07048},
	shorttitle = {{AutoUpdate}},
	abstract = {Android has become the predominant smartphone operating system, with a rapidly evolving ecosystem that requires app developers to frequently update their apps to maintain quality, security, and compatibility. While deep learning has made significant strides in various software engineering tasks, including automated code updates, existing methods are not specifically tailored for Android apps, and the potential of pre-trained Language Models of Code ({CodeLMs}) for updating Android app code remains unexplored. In this paper, we present the first comprehensive evaluation of state-of-the-art {CodeLMs}, including {CodeT}5, {CodeBERT}, {CodeGPT}, and {UniXcoder}, for recommending code updates in Android applications. To facilitate this evaluation, we curate a unique dataset of paired updated methods from 3,195 Android apps published on Google Play and hosted on {GitHub} between 2008 and 2022. Our findings demonstrate that pre-trained {CodeLMs} outperform traditional approaches, achieving a higher accuracy ranging from 190\% to 385\% under a realistic time-wise evaluation scenario. Among the {CodeLMs}, {CodeT}5 consistently exhibits superior performance across most code update types. Furthermore, we examine the impact of update types, evaluation scenarios, method size, and update size on the performance of {CodeLMs}, revealing areas for future research to improve temporal adaptability and generalization capabilities.},
	number = {{arXiv}:2209.07048},
	publisher = {{arXiv}},
	author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Thongtanunam, Patanamon and Li, Li},
	urldate = {2023-06-10},
	date = {2023-05-10},
	eprinttype = {arxiv},
	eprint = {2209.07048 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209369/Liu 等 - 2023 - AutoUpdate Automatically Recommend Code Updates f.pdf:application/pdf;arXiv.org Snapshot:files/209374/2209.html:text/html},
}

@misc{kang_large_2022,
	title = {Large Language Models are Few-shot Testers: Exploring {LLM}-based General Bug Reproduction},
	url = {http://arxiv.org/abs/2209.11515},
	shorttitle = {Large Language Models are Few-shot Testers},
	abstract = {Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28\% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose {LIBRO}, a framework that uses Large Language Models ({LLMs}), which have been shown to be capable of performing code-related tasks. Since {LLMs} themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when {LLMs} are effective, and rank the produced tests according to their validity. Our evaluation of {LIBRO} shows that, on the widely studied Defects4J benchmark, {LIBRO} can generate failure reproducing test cases for 33\% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination, we also evaluate {LIBRO} against 31 bug reports submitted after the collection of the {LLM} training data terminated: {LIBRO} produces bug reproducing tests for 32\% of the studied bug reports. Overall, our results show {LIBRO} has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.},
	number = {{arXiv}:2209.11515},
	publisher = {{arXiv}},
	author = {Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
	urldate = {2023-06-10},
	date = {2022-12-22},
	eprinttype = {arxiv},
	eprint = {2209.11515 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209359/Kang 等 - 2022 - Large Language Models are Few-shot Testers Explor.pdf:application/pdf;arXiv.org Snapshot:files/209364/2209.html:text/html},
}

@misc{zhang_repairing_2022,
	title = {Repairing Bugs in Python Assignments Using Large Language Models},
	url = {http://arxiv.org/abs/2209.14876},
	abstract = {Students often make mistakes on their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair ({APR}) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for {APR} in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex, to build an {APR} system -- {MMAPR} -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate {MMAPR} on 286 real student programs and compare to a baseline built by combining a state-of-the-art Python syntax repair engine, {BIFI}, and state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that {MMAPR} can fix more programs and produce smaller patches on average.},
	number = {{arXiv}:2209.14876},
	publisher = {{arXiv}},
	author = {Zhang, Jialu and Cambronero, José and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust},
	urldate = {2023-06-10},
	date = {2022-09-29},
	eprinttype = {arxiv},
	eprint = {2209.14876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209361/Zhang 等 - 2022 - Repairing Bugs in Python Assignments Using Large L.pdf:application/pdf;arXiv.org Snapshot:files/209365/2209.html:text/html},
}

@misc{key_i_2022,
	title = {I Speak, You Verify: Toward Trustworthy Neural Program Synthesis},
	url = {http://arxiv.org/abs/2210.00848},
	shorttitle = {I Speak, You Verify},
	abstract = {We develop an approach for improving the trustworthiness and overall accuracy of program synthesizers based on large language models for source code. Given a natural language description of a programming problem, our method samples both candidate programs as well as candidate predicates specifying how the program should behave. We learn to analyze the agreement between programs and predicates to judge both which program is most likely to be correct, and also judge whether the language model is able to solve the programming problem in the first place. This latter capacity allows favoring high precision over broad recall: fostering trust by only proposing a program when the system is certain that it is correct.},
	number = {{arXiv}:2210.00848},
	publisher = {{arXiv}},
	author = {Key, Darren and Li, Wen-Ding and Ellis, Kevin},
	urldate = {2023-06-10},
	date = {2022-09-29},
	eprinttype = {arxiv},
	eprint = {2210.00848 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209366/Key 等 - 2022 - I Speak, You Verify Toward Trustworthy Neural Pro.pdf:application/pdf;arXiv.org Snapshot:files/209371/2210.html:text/html},
}

@misc{malladi_kernel-based_2023,
	title = {A Kernel-Based View of Language Model Fine-Tuning},
	url = {http://arxiv.org/abs/2210.05643},
	abstract = {It has become standard to solve {NLP} tasks by fine-tuning pre-trained language models ({LMs}), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with \$10{\textasciicircum}8\$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel ({NTK}) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained {LMs}. This study was inspired by the decent performance of {NTK} for computer vision tasks (Wei et al., 2022). We extend the {NTK} formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the {NTK} lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 {NLP} tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting often induces kernel-based dynamics during fine-tuning. Finally, we use this kernel view to propose an explanation for the success of parameter-efficient subspace-based fine-tuning methods.},
	number = {{arXiv}:2210.05643},
	publisher = {{arXiv}},
	author = {Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2210.05643 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208560/Malladi 等 - 2023 - A Kernel-Based View of Language Model Fine-Tuning.pdf:application/pdf;arXiv.org Snapshot:files/208578/2210.html:text/html},
}

@misc{madaan_language_2022,
	title = {Language Models of Code are Few-Shot Commonsense Learners},
	url = {http://arxiv.org/abs/2210.07128},
	abstract = {We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event -- or a reasoning-graph. To employ large language models ({LMs}) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that {LMs} were pre-trained on, hindering {LMs} from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained {LMs} of code are better structured commonsense reasoners than {LMs} of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation {LM} ({CODEX}) outperforms natural-{LMs} that are fine-tuned on the target task (e.g., T5) and other strong {LMs} such as {GPT}-3 in the few-shot setting.},
	number = {{arXiv}:2210.07128},
	publisher = {{arXiv}},
	author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
	urldate = {2023-06-10},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2210.07128 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208558/Madaan 等 - 2022 - Language Models of Code are Few-Shot Commonsense L.pdf:application/pdf;arXiv.org Snapshot:files/209713/2210.html:text/html},
}

@misc{yang_testaug_2022,
	title = {{TestAug}: A Framework for Augmenting Capability-based {NLP} Tests},
	url = {http://arxiv.org/abs/2210.08097},
	shorttitle = {{TestAug}},
	abstract = {The recently proposed capability-based {NLP} testing allows model developers to test the functional capabilities of {NLP} models, revealing functional failures that cannot be detected by the traditional heldout mechanism. However, existing work on capability-based testing requires extensive manual efforts and domain expertise in creating the test cases. In this paper, we investigate a low-cost approach for the test case generation by leveraging the {GPT}-3 engine. We further propose to use a classifier to remove the invalid outputs from {GPT}-3 and expand the outputs into templates to generate more test cases. Our experiments show that {TestAug} has three advantages over the existing work on behavioral testing: (1) {TestAug} can find more bugs than existing work; (2) The test cases in {TestAug} are more diverse; and (3) {TestAug} largely saves the manual efforts in creating the test suites. The code and data for {TestAug} can be found at our project website (https://guanqun-yang.github.io/testaug/) and {GitHub} (https://github.com/guanqun-yang/testaug).},
	number = {{arXiv}:2210.08097},
	publisher = {{arXiv}},
	author = {Yang, Guanqun and Haque, Mirazul and Song, Qiaochu and Yang, Wei and Liu, Xueqing},
	urldate = {2023-06-10},
	date = {2022-10-14},
	eprinttype = {arxiv},
	eprint = {2210.08097 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208549/Yang 等 - 2022 - TestAug A Framework for Augmenting Capability-bas.pdf:application/pdf;arXiv.org Snapshot:files/208858/2210.html:text/html},
}

@misc{shen_benchmarking_2022,
	title = {Benchmarking Language Models for Code Syntax Understanding},
	url = {http://arxiv.org/abs/2210.14473},
	abstract = {Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the syntactic rules of natural languages without finetuning on syntax understanding tasks. However, there is limited understanding of how well pre-trained models understand the code structure so far. In this work, we perform the first thorough benchmarking of the state-of-the-art pre-trained models for identifying the syntactic structures of programs. Specifically, we introduce {CodeSyntax}, a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. Our key observation is that existing language models pretrained on code still lack the understanding of code syntax. In fact, these pre-trained programming language models fail to match the performance of simple baselines based on positional offsets and keywords. We also present a natural language benchmark to highlight the differences between natural languages and programming languages in terms of syntactic structure understanding. Our findings point out key limitations of existing pre-training methods for programming languages, and suggest the importance of modeling code syntactic structures.},
	number = {{arXiv}:2210.14473},
	publisher = {{arXiv}},
	author = {Shen, Da and Chen, Xinyun and Wang, Chenguang and Sen, Koushik and Song, Dawn},
	urldate = {2023-06-10},
	date = {2022-10-26},
	eprinttype = {arxiv},
	eprint = {2210.14473 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208557/Shen 等 - 2022 - Benchmarking Language Models for Code Syntax Under.pdf:application/pdf;arXiv.org Snapshot:files/209707/2210.html:text/html},
}

@misc{dibia_aligning_2022,
	title = {Aligning Offline Metrics and Human Judgments of Value of {AI}-Pair Programmers},
	url = {http://arxiv.org/abs/2210.16494},
	abstract = {Large language models trained on massive amounts of natural language data and code have shown impressive capabilities in automatic code generation scenarios. Development and evaluation of these models has largely been driven by offline functional correctness metrics, which consider a task to be solved if the generated code passes corresponding unit tests. While functional correctness is clearly an important property of a code generation model, we argue that it may not fully capture what programmers value when collaborating with their {AI} pair programmers. For example, while a nearly correct suggestion that does not consider edge cases may fail a unit test, it may still provide a substantial starting point or hint to the programmer, thereby reducing total needed effort to complete a coding task. To investigate this, we conduct a user study with (N=49) experienced programmers, and find that while both correctness and effort correlate with value, the association is strongest for effort. We argue that effort should be considered as an important dimension of evaluation in code generation scenarios. We also find that functional correctness remains better at identifying the highest-value generations; but participants still saw considerable value in code that failed unit tests. Conversely, similarity-based metrics are very good at identifying the lowest-value generations among those that fail unit tests. Based on these findings, we propose a simple hybrid metric, which combines functional correctness and similarity-based metrics to capture different dimensions of what programmers might value and show that this hybrid metric more strongly correlates with both value and effort. Our findings emphasize the importance of designing human-centered metrics that capture what programmers need from and value in their {AI} pair programmers.},
	number = {{arXiv}:2210.16494},
	publisher = {{arXiv}},
	author = {Dibia, Victor and Fourney, Adam and Bansal, Gagan and Poursabzi-Sangdeh, Forough and Liu, Han and Amershi, Saleema},
	urldate = {2023-06-10},
	date = {2022-10-29},
	eprinttype = {arxiv},
	eprint = {2210.16494 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Programming Languages, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209376/Dibia 等 - 2022 - Aligning Offline Metrics and Human Judgments of Va.pdf:application/pdf;arXiv.org Snapshot:files/209377/2210.html:text/html},
}

@misc{doderlein_piloting_2023,
	title = {Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?},
	url = {http://arxiv.org/abs/2210.14699},
	shorttitle = {Piloting Copilot and Codex},
	abstract = {Language models are promising solutions for tackling increasing complex problems. In software engineering, they recently attracted attention in code assistants, with programs automatically written in a given programming language from a programming task description in natural language. They have the potential to save time and effort when writing code. However, these systems are currently poorly understood, preventing them from being used optimally. In this paper, we investigate the various input parameters of two language models, and conduct a study to understand if variations of these input parameters (e.g. programming task description and the surrounding context, creativity of the language model, number of generated solutions) can have a significant impact on the quality of the generated programs. We design specific operators for varying input parameters and apply them over two code assistants (Copilot and Codex) and two benchmarks representing algorithmic problems ({HumanEval} and {LeetCode}). Our results showed that varying the input parameters can significantly improve the performance of language models. However, there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard for developers to properly control the parameters to obtain an optimal result. This work opens opportunities to propose (automated) strategies for improving performance.},
	number = {{arXiv}:2210.14699},
	publisher = {{arXiv}},
	author = {Döderlein, Jean-Baptiste and Acher, Mathieu and Khelladi, Djamel Eddine and Combemale, Benoit},
	urldate = {2023-06-10},
	date = {2023-02-15},
	eprinttype = {arxiv},
	eprint = {2210.14699 [cs]},
	keywords = {68T50, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208865/Döderlein 等 - 2023 - Piloting Copilot and Codex Hot Temperature, Cold .pdf:application/pdf;arXiv.org Snapshot:files/209204/2210.html:text/html},
}

@misc{macneil_experiences_2022,
	title = {Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book},
	url = {http://arxiv.org/abs/2211.02265},
	abstract = {Advances in natural language processing have resulted in large language models ({LLMs}) that are capable of generating understandable and sensible written text. Recent versions of these models, such as {OpenAI} Codex and {GPT}-3, can generate code and code explanations. However, it is unclear whether and how students might engage with such explanations. In this paper, we report on our experiences generating multiple code explanation types using {LLMs} and integrating them into an interactive e-book on web software development. We modified the e-book to make {LLM}-generated code explanations accessible through buttons next to code snippets in the materials, which allowed us to track the use of the explanations as well as to ask for feedback on their utility. Three different types of explanations were available for students for each explainable code snippet; a line-by-line explanation, a list of important concepts, and a high-level summary of the code. Our preliminary results show that all varieties of explanations were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement appeared to vary by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by {LLMs} into existing computer science classrooms.},
	number = {{arXiv}:2211.02265},
	publisher = {{arXiv}},
	author = {{MacNeil}, Stephen and Tran, Andrew and Hellas, Arto and Kim, Joanne and Sarsa, Sami and Denny, Paul and Bernstein, Seth and Leinonen, Juho},
	urldate = {2023-06-10},
	date = {2022-11-04},
	eprinttype = {arxiv},
	eprint = {2211.02265 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209363/MacNeil 等 - 2022 - Experiences from Using Code Explanations Generated.pdf:application/pdf;arXiv.org Snapshot:files/209368/2211.html:text/html},
}

@misc{helmeczi_prompt-based_2022-1,
	title = {A Prompt-based Few-shot Learning Approach to Software Conflict Detection},
	url = {http://arxiv.org/abs/2211.02709},
	abstract = {A software requirement specification ({SRS}) document is an essential part of the software development life cycle which outlines the requirements that a software program in development must satisfy. This document is often specified by a diverse group of stakeholders and is subject to continual change, making the process of maintaining the document and detecting conflicts between requirements an essential task in software development. Notably, projects that do not address conflicts in the {SRS} document early on face considerable problems later in the development life cycle. These problems incur substantial costs in terms of time and money, and these costs often become insurmountable barriers that ultimately result in the termination of a software project altogether. As a result, early detection of {SRS} conflicts is critical to project sustainability. The conflict detection task is approached in numerous ways, many of which require a significant amount of manual intervention from developers, or require access to a large amount of labeled, task-specific training data. In this work, we propose using a prompt-based learning approach to perform few-shot learning for conflict detection. We compare our results to supervised learning approaches that use pretrained language models, such as {BERT} and its variants. Our results show that prompting with just 32 labeled examples can achieve a similar level of performance in many key metrics to that of supervised learning on training sets that are magnitudes larger in size. In contrast to many other conflict detection approaches, we make no assumptions about the type of underlying requirements, allowing us to analyze pairings of both functional and non-functional requirements. This allows us to omit the potentially expensive task of filtering out non-functional requirements from our dataset.},
	number = {{arXiv}:2211.02709},
	publisher = {{arXiv}},
	author = {Helmeczi, Robert K. and Cevik, Mucahit and Yıldırım, Savas},
	urldate = {2023-06-10},
	date = {2022-11-04},
	eprinttype = {arxiv},
	eprint = {2211.02709 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208559/Helmeczi 等 - 2022 - A Prompt-based Few-shot Learning Approach to Softw.pdf:application/pdf;arXiv.org Snapshot:files/209090/2211.html:text/html},
}

@misc{lai_ds-1000_2022,
	title = {{DS}-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
	url = {http://arxiv.org/abs/2211.11501},
	shorttitle = {{DS}-1000},
	abstract = {We introduce {DS}-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as {NumPy} and Pandas. Compared to prior works, {DS}-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from {StackOverflow}. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting {API} usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original {StackOverflow} source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
	number = {{arXiv}:2211.11501},
	publisher = {{arXiv}},
	author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Scott Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
	urldate = {2023-06-10},
	date = {2022-11-18},
	eprinttype = {arxiv},
	eprint = {2211.11501 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209388/Lai 等 - 2022 - DS-1000 A Natural and Reliable Benchmark for Data.pdf:application/pdf;arXiv.org Snapshot:files/209692/2211.html:text/html},
}

@misc{agrawal_towards_2022,
	title = {Towards a Mathematics Formalisation Assistant using Large Language Models},
	url = {http://arxiv.org/abs/2211.07524},
	abstract = {Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful input-dependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75{\textbackslash}\% accuracy for \$120\$ theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of \$13\$ theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.},
	number = {{arXiv}:2211.07524},
	publisher = {{arXiv}},
	author = {Agrawal, Ayush and Gadgil, Siddhartha and Goyal, Navin and Narayanan, Ashvni and Tadipatri, Anand},
	urldate = {2023-06-10},
	date = {2022-11-14},
	eprinttype = {arxiv},
	eprint = {2211.07524 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209367/Agrawal 等 - 2022 - Towards a Mathematics Formalisation Assistant usin.pdf:application/pdf;arXiv.org Snapshot:files/209373/2211.html:text/html},
}

@misc{roberts_steps_2022,
	title = {Steps towards prompt-based creation of virtual worlds},
	url = {http://arxiv.org/abs/2211.05875},
	abstract = {Large language models trained for code generation can be applied to speaking virtual worlds into existence (creating virtual worlds). In this work we show that prompt-based methods can both accelerate in-{VR} level editing, as well as can become part of gameplay rather than just part of game development. As an example, we present Codex {VR} Pong which shows non-deterministic game mechanics using generative processes to not only create static content but also non-trivial interactions between 3D objects. This demonstration naturally leads to an integral discussion on how one would evaluate and benchmark experiences created by generative models - as there are no qualitative or quantitative metrics that apply in these scenarios. We conclude by discussing impending challenges of {AI}-assisted co-creation in {VR}.},
	number = {{arXiv}:2211.05875},
	publisher = {{arXiv}},
	author = {Roberts, Jasmine and Banburski-Fahey, Andrzej and Lanier, Jaron},
	urldate = {2023-06-10},
	date = {2022-11-10},
	eprinttype = {arxiv},
	eprint = {2211.05875 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209393/Roberts 等 - 2022 - Steps towards prompt-based creation of virtual wor.pdf:application/pdf;arXiv.org Snapshot:files/209398/2211.html:text/html},
}

@misc{wang_clebpi_2022,
	title = {{CLeBPI}: Contrastive Learning for Bug Priority Inference},
	url = {http://arxiv.org/abs/2212.01011},
	shorttitle = {{CLeBPI}},
	abstract = {Automated bug priority inference can reduce the time overhead of bug triagers for priority assignments, improving the efficiency of software maintenance. Currently, there are two orthogonal lines for this task, i.e., traditional machine learning based ({TML}-based) and neural network based ({NN}-based) approaches. Although these approaches achieve competitive performance, our observation finds that existing approaches face the following two issues: 1) {TML}-based approaches require much manual feature engineering and cannot learn the semantic information of bug reports; 2) Both {TML}-based and {NN}-based approaches cannot effectively address the label imbalance problem because they are difficult to distinguish the semantic difference between bug reports with different priorities. In this paper, we propose {CLeBPI} (Contrastive Learning for Bug Priority Inference), which leverages pre-trained language model and contrastive learning to tackle the above-mentioned two issues. Specifically, {CLeBPI} is first pre-trained on a large-scale bug report corpus in a self-supervised way, thus it can automatically learn contextual representations of bug reports without manual feature engineering. Afterward, it is further pre-trained by a contrastive learning objective, which enables it to distinguish semantic differences between bug reports, learning more precise contextual representations for each bug report. When finishing pre-training, we can connect a classification layer to {CLeBPI} and fine-tune it for bug priority inference in a supervised way. To verify the effectiveness of {CLeBPI}, we choose four baseline approaches and conduct comparison experiments on a public dataset. The experimental results show that {CLeBPI} outperforms all baseline approaches by 23.86\%-77.80\% in terms of weighted average F1-score, showing its effectiveness.},
	number = {{arXiv}:2212.01011},
	publisher = {{arXiv}},
	author = {Wang, Wenyao and Wu, Chenhao and He, Jie},
	urldate = {2023-06-10},
	date = {2022-12-02},
	eprinttype = {arxiv},
	eprint = {2212.01011 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208864/Wang 等 - 2022 - CLeBPI Contrastive Learning for Bug Priority Infe.pdf:application/pdf;arXiv.org Snapshot:files/208884/2212.html:text/html},
}

@misc{amin_detecting_2022,
	title = {Detecting Conspiracy Theory Against {COVID}-19 Vaccines},
	url = {http://arxiv.org/abs/2211.13003},
	doi = {10.13140/RG.2.2.32561.04960/2},
	abstract = {Since the beginning of the vaccination trial, social media has been flooded with anti-vaccination comments and conspiracy beliefs. As the day passes, the number of {COVID}- 19 cases increases, and online platforms and a few news portals entertain sharing different conspiracy theories. The most popular conspiracy belief was the link between the 5G network spreading {COVID}-19 and the Chinese government spreading the virus as a bioweapon, which initially created racial hatred. Although some disbelief has less impact on society, others create massive destruction. For example, the 5G conspiracy led to the burn of the 5G Tower, and belief in the Chinese bioweapon story promoted an attack on the Asian-Americans. Another popular conspiracy belief was that Bill Gates spread this Coronavirus disease ({COVID}-19) by launching a mass vaccination program to track everyone. This Conspiracy belief creates distrust issues among laypeople and creates vaccine hesitancy. This study aims to discover the conspiracy theory against the vaccine on social platforms. We performed a sentiment analysis on the 598 unique sample comments related to {COVID}-19 vaccines. We used two different models, {BERT} and Perspective {API}, to find out the sentiment and toxicity of the sentence toward the {COVID}-19 vaccine.},
	author = {Amin, Md Hasibul and Madanu, Harika and Lavu, Sahithi and Mansourifar, Hadi and Alsagheer, Dana and Shi, Weidong},
	urldate = {2023-06-10},
	date = {2022-11-19},
	eprinttype = {arxiv},
	eprint = {2211.13003 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Social and Information Networks, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209372/Amin 等 - 2022 - Detecting Conspiracy Theory Against COVID-19 Vacci.pdf:application/pdf;arXiv.org Snapshot:files/209732/2211.html:text/html},
}

@misc{zhang_entity_2022,
	title = {Entity Set Co-Expansion in {StackOverflow}},
	url = {http://arxiv.org/abs/2212.02271},
	abstract = {Given a few seed entities of a certain type (e.g., Software or Programming Language), entity set expansion aims to discover an extensive set of entities that share the same type as the seeds. Entity set expansion in software-related domains such as {StackOverflow} can benefit many downstream tasks (e.g., software knowledge graph construction) and facilitate better {IT} operations and service management. Meanwhile, existing approaches are less concerned with two problems: (1) How to deal with multiple types of seed entities simultaneously? (2) How to leverage the power of pre-trained language models ({PLMs})? Being aware of these two problems, in this paper, we study the entity set co-expansion task in {StackOverflow}, which extracts Library, {OS}, Application, and Language entities from {StackOverflow} question-answer threads. During the co-expansion process, we use {PLMs} to derive embeddings of candidate entities for calculating similarities between entities. Experimental results show that our proposed {SECoExpan} framework outperforms previous approaches significantly.},
	number = {{arXiv}:2212.02271},
	publisher = {{arXiv}},
	author = {Zhang, Yu and Zhang, Yunyi and Jiang, Yucheng and Michalski, Martin and Deng, Yu and Popa, Lucian and Zhai, {ChengXiang} and Han, Jiawei},
	urldate = {2023-06-10},
	date = {2022-12-05},
	eprinttype = {arxiv},
	eprint = {2212.02271 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209370/Zhang 等 - 2022 - Entity Set Co-Expansion in StackOverflow.pdf:application/pdf;arXiv.org Snapshot:files/209375/2212.html:text/html},
}

@misc{ding_cocomic_2023,
	title = {{CoCoMIC}: Code Completion By Jointly Modeling In-file and Cross-file Context},
	url = {http://arxiv.org/abs/2212.10007},
	shorttitle = {{CoCoMIC}},
	abstract = {While pre-trained language models ({LM}) for code have achieved great success in code completion, they generate code conditioned only on the contents within the file, i.e., in-file context, but ignore the rich semantics in other files within the same project, i.e., cross-file context, a critical source of information that is especially useful in modern modular software development. Such overlooking constrains code language models' capacity in code completion, leading to unexpected behaviors such as generating hallucinated class member functions or function calls with unexpected arguments. In this work, we develop a cross-file context finder tool, {CCFINDER}, that effectively locates and retrieves the most relevant cross-file context. We propose {CoCoMIC}, a framework that incorporates cross-file context to learn the in-file and cross-file context jointly on top of pretrained code {LMs}. {CoCoMIC} successfully improves the existing code {LM} with a 33.94\% relative increase in exact match and a 28.69\% relative increase in identifier matching for code completion when the cross-file context is provided.},
	number = {{arXiv}:2212.10007},
	publisher = {{arXiv}},
	author = {Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2212.10007 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209383/Ding 等 - 2023 - CoCoMIC Code Completion By Jointly Modeling In-fi.pdf:application/pdf;arXiv.org Snapshot:files/209386/2212.html:text/html},
}

@misc{chai_ernie-code_2023,
	title = {{ERNIE}-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages},
	url = {http://arxiv.org/abs/2212.06742},
	shorttitle = {{ERNIE}-Code},
	abstract = {Software engineers working with the same programming language ({PL}) may speak different natural languages ({NLs}) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual {NLs} and multilingual {PLs} for large language models ({LLMs}). We release {ERNIE}-Code, a unified pre-trained language model for 116 {NLs} and 6 {PLs}. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual {NL} or {PL}; and pivot-based translation language modeling that relies on parallel data of many {NLs} and {PLs}. Extensive results show that {ERNIE}-Code outperforms previous multilingual {LLMs} for {PL} or {NL} across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints.},
	number = {{arXiv}:2212.06742},
	publisher = {{arXiv}},
	author = {Chai, Yekun and Wang, Shuohuan and Pang, Chao and Sun, Yu and Tian, Hao and Wu, Hua},
	urldate = {2023-06-10},
	date = {2023-05-19},
	eprinttype = {arxiv},
	eprint = {2212.06742 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208601/Chai 等 - 2023 - ERNIE-Code Beyond English-Centric Cross-lingual P.pdf:application/pdf;arXiv.org Snapshot:files/208622/2212.html:text/html},
}

@misc{li_generation-augmented_2022,
	title = {Generation-Augmented Query Expansion For Code Retrieval},
	url = {http://arxiv.org/abs/2212.10692},
	abstract = {Pre-trained language models have achieved promising success in code retrieval tasks, where a natural language documentation query is given to find the most relevant existing code snippet. However, existing models focus only on optimizing the documentation code pairs by embedding them into latent space, without the association of external knowledge. In this paper, we propose a generation-augmented query expansion framework. Inspired by the human retrieval process - sketching an answer before searching, in this work, we utilize the powerful code generation model to benefit the code retrieval task. Specifically, we demonstrate that rather than merely retrieving the target code snippet according to the documentation query, it would be helpful to augment the documentation query with its generation counterpart - generated code snippets from the code generation model. To the best of our knowledge, this is the first attempt that leverages the code generation model to enhance the code retrieval task. We achieve new state-of-the-art results on the {CodeSearchNet} benchmark and surpass the baselines significantly.},
	number = {{arXiv}:2212.10692},
	publisher = {{arXiv}},
	author = {Li, Dong and Shen, Yelong and Jin, Ruoming and Mao, Yi and Wang, Kuan and Chen, Weizhu},
	urldate = {2023-06-10},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10692 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208593/Li 等 - 2022 - Generation-Augmented Query Expansion For Code Retr.pdf:application/pdf;arXiv.org Snapshot:files/209690/2212.html:text/html},
}

@misc{wang_recode_2022,
	title = {{ReCode}: Robustness Evaluation of Code Generation Models},
	url = {http://arxiv.org/abs/2212.10264},
	shorttitle = {{ReCode}},
	abstract = {Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose {ReCode}, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model's robustness performance. With human annotators, we verified that over 90\% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate {ReCode} on {SOTA} models using {HumanEval}, {MBPP}, as well as function completion tasks derived from them. Interesting observations include: better robustness for {CodeGen} over {InCoder} and {GPT}-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on {MBPP} over {HumanEval}.},
	number = {{arXiv}:2212.10264},
	publisher = {{arXiv}},
	author = {Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and Nallapati, Ramesh and Ramanathan, Murali Krishna and Roth, Dan and Xiang, Bing},
	urldate = {2023-06-10},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10264 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208694/Wang 等 - 2022 - ReCode Robustness Evaluation of Code Generation M.pdf:application/pdf;arXiv.org Snapshot:files/209827/2212.html:text/html},
}

@misc{wang_execution-based_2023,
	title = {Execution-Based Evaluation for Open-Domain Code Generation},
	url = {http://arxiv.org/abs/2212.10481},
	abstract = {To extend the scope of coding queries to more realistic settings, we propose {ODEX}, the first Open-Domain {EXecution}-based natural language ({NL}) to Python code generation dataset. {ODEX} has 945 {NL}-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our {NL}-Code pairs are harvested from {StackOverflow} forums to encourage natural and practical coding queries. Moreover, {ODEX} supports four natural languages as intents, in English, Spanish, Japanese, and Russian. {ODEX} unveils intriguing behavioral differences among top-performing code language models ({LM}). While {CODEX} achieves better overall results, {CODEGEN} improves effectively via scaling -- {CODEGEN} 6.1B performs comparably with {CODEX} 12B. Both models show substantial gaps between open and closed domains, but {CODEGEN} gaps tend to decrease with model size while {CODEX} gaps increase. We release {ODEX} to facilitate research into open-domain problems for the code generation community.},
	number = {{arXiv}:2212.10481},
	publisher = {{arXiv}},
	author = {Wang, Zhiruo and Zhou, Shuyan and Fried, Daniel and Neubig, Graham},
	urldate = {2023-06-10},
	date = {2023-05-19},
	eprinttype = {arxiv},
	eprint = {2212.10481 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209390/Wang 等 - 2023 - Execution-Based Evaluation for Open-Domain Code Ge.pdf:application/pdf;arXiv.org Snapshot:files/209396/2212.html:text/html},
}

@misc{al-kaswan_extending_2023,
	title = {Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries},
	url = {http://arxiv.org/abs/2301.01701},
	abstract = {Reverse engineering binaries is required to understand and analyse programs for which the source code is unavailable. Decompilers can transform the largely unreadable binaries into a more readable source code-like representation. However, reverse engineering is time-consuming, much of which is taken up by labelling the functions with semantic information. While the automated summarisation of decompiled code can help Reverse Engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise decompiled binary functions. Furthermore, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build {CAPYBARA}, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend {CAPYBARA} further by generating synthetic datasets and deduplicating the data. Next, we fine-tune the {CodeT}5 base model with {CAPYBARA} to create {BinT}5. {BinT}5 achieves the state-of-the-art {BLEU}-4 score of 60.83, 58.82, and 44.21 for summarising source, decompiled, and synthetically stripped decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of {BinT}5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
	number = {{arXiv}:2301.01701},
	publisher = {{arXiv}},
	author = {Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie},
	urldate = {2023-06-10},
	date = {2023-01-13},
	eprinttype = {arxiv},
	eprint = {2301.01701 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208841/Al-Kaswan 等 - 2023 - Extending Source Code Pre-Trained Language Models .pdf:application/pdf;arXiv.org Snapshot:files/209813/2301.html:text/html},
}

@misc{aghakhani_trojanpuzzle_2023,
	title = {{TrojanPuzzle}: Covertly Poisoning Code-Suggestion Models},
	url = {http://arxiv.org/abs/2301.02344},
	shorttitle = {{TrojanPuzzle}},
	abstract = {With tools like {GitHub} Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training or fine-tuning phases by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior poisoning attacks explicitly inject the insecure code payload into the training data, making the poisoning data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel data poisoning attacks, {COVERT} and {TROJANPUZZLE}, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings. Our most novel attack, {TROJANPUZZLE}, goes one step further in generating less suspicious poisoning data by never including certain (suspicious) parts of the payload in the poisoned data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes {TROJANPUZZLE} robust against signature-based dataset-cleansing methods that identify and filter out suspicious sequences from the training data. Our evaluation against two model sizes demonstrates that both {COVERT} and {TROJANPUZZLE} have significant implications for how practitioners should select code used to train or tune code-suggestion models.},
	number = {{arXiv}:2301.02344},
	publisher = {{arXiv}},
	author = {Aghakhani, Hojjat and Dai, Wei and Manoel, Andre and Fernandes, Xavier and Kharkar, Anant and Kruegel, Christopher and Vigna, Giovanni and Evans, David and Zorn, Ben and Sim, Robert},
	urldate = {2023-06-10},
	date = {2023-01-05},
	eprinttype = {arxiv},
	eprint = {2301.02344 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209384/Aghakhani 等 - 2023 - TrojanPuzzle Covertly Poisoning Code-Suggestion M.pdf:application/pdf;arXiv.org Snapshot:files/209389/2301.html:text/html},
}

@misc{khanfir_efficient_2023,
	title = {Efficient Mutation Testing via Pre-Trained Language Models},
	url = {http://arxiv.org/abs/2301.03543},
	abstract = {Mutation testing is an established fault-based testing technique. It operates by seeding faults into the programs under test and asking developers to write tests that reveal these faults. These tests have the potential to reveal a large number of faults -- those that couple with the seeded ones -- and thus are deemed important. To this end, mutation testing should seed faults that are both "natural" in a sense easily understood by developers and strong (have high chances to reveal faults). To achieve this we propose using pre-trained generative language models (i.e. {CodeBERT}) that have the ability to produce developer-like code that operates similarly, but not exactly, as the target code. This means that the models have the ability to seed natural faults, thereby offering opportunities to perform mutation testing. We realise this idea by implementing \${\textbackslash}mu\${BERT}, a mutation testing technique that performs mutation testing using {CodeBert} and empirically evaluated it using 689 faulty program versions. Our results show that the fault revelation ability of \${\textbackslash}mu\${BERT} is higher than that of a state-of-the-art mutation testing ({PiTest}), yielding tests that have up to 17\% higher fault detection potential than that of {PiTest}. Moreover, we observe that \${\textbackslash}mu\${BERT} can complement {PiTest}, being able to detect 47 bugs missed by {PiTest}, while at the same time, {PiTest} can find 13 bugs missed by \${\textbackslash}mu\${BERT}.},
	number = {{arXiv}:2301.03543},
	publisher = {{arXiv}},
	author = {Khanfir, Ahmed and Degiovanni, Renzo and Papadakis, Mike and Traon, Yves Le},
	urldate = {2023-06-10},
	date = {2023-01-09},
	eprinttype = {arxiv},
	eprint = {2301.03543 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209378/Khanfir 等 - 2023 - Efficient Mutation Testing via Pre-Trained Languag.pdf:application/pdf;arXiv.org Snapshot:files/209380/2301.html:text/html},
}

@misc{malik_transfer_2023,
	title = {Transfer learning for conflict and duplicate detection in software requirement pairs},
	url = {http://arxiv.org/abs/2301.03709},
	abstract = {Consistent and holistic expression of software requirements is important for the success of software projects. In this study, we aim to enhance the efficiency of the software development processes by automatically identifying conflicting and duplicate software requirement specifications. We formulate the conflict and duplicate detection problem as a requirement pair classification task. We design a novel transformers-based architecture, {SR}-{BERT}, which incorporates Sentence-{BERT} and Bi-encoders for the conflict and duplicate identification task. Furthermore, we apply supervised multi-stage fine-tuning to the pre-trained transformer models. We test the performance of different transfer models using four different datasets. We find that sequentially trained and fine-tuned transformer models perform well across the datasets with {SR}-{BERT} achieving the best performance for larger datasets. We also explore the cross-domain performance of conflict detection models and adopt a rule-based filtering approach to validate the model classifications. Our analysis indicates that the sentence pair classification approach and the proposed transformer-based natural language processing strategies can contribute significantly to achieving automation in conflict and duplicate detection},
	number = {{arXiv}:2301.03709},
	publisher = {{arXiv}},
	author = {Malik, Garima and Yildirim, Savas and Cevik, Mucahit and Bener, Ayse and Parikh, Devang},
	urldate = {2023-06-10},
	date = {2023-01-09},
	eprinttype = {arxiv},
	eprint = {2301.03709 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208594/Malik 等 - 2023 - Transfer learning for conflict and duplicate detec.pdf:application/pdf;arXiv.org Snapshot:files/208614/2301.html:text/html},
}

@misc{lehnert_ai_2023,
	title = {{AI} Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with {ChatGPT}},
	url = {http://arxiv.org/abs/2301.08155},
	shorttitle = {{AI} Insights into Theoretical Physics and the Swampland Program},
	abstract = {In this case study, we explore the capabilities and limitations of {ChatGPT}, a natural language processing model developed by {OpenAI}, in the field of string theoretical swampland conjectures. We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary. However, its ingenious use of language can be fruitful for identifying analogies and describing visual representations of abstract concepts.},
	number = {{arXiv}:2301.08155},
	publisher = {{arXiv}},
	author = {Lehnert, Kay},
	urldate = {2023-06-10},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2301.08155 [physics]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Physics - Popular Physics},
	file = {arXiv Fulltext PDF:files/208832/Lehnert - 2023 - AI Insights into Theoretical Physics and the Swamp.pdf:application/pdf;arXiv.org Snapshot:files/209818/2301.html:text/html},
}

@misc{zhang_which_2023,
	title = {Which Features are Learned by {CodeBert}: An Empirical Study of the {BERT}-based Source Code Representation Learning},
	url = {http://arxiv.org/abs/2301.08427},
	shorttitle = {Which Features are Learned by {CodeBert}},
	abstract = {The Bidirectional Encoder Representations from Transformers ({BERT}) were proposed in the natural language process ({NLP}) and shows promising results. Recently researchers applied the {BERT} to source-code representation learning and reported some good news on several downstream tasks. However, in this paper, we illustrated that current methods cannot effectively understand the logic of source codes. The representation of source code heavily relies on the programmer-defined variable and function names. We design and implement a set of experiments to demonstrate our conjecture and provide some insights for future works.},
	number = {{arXiv}:2301.08427},
	publisher = {{arXiv}},
	author = {Zhang, Lan and Cao, Chen and Wang, Zhilong and Liu, Peng},
	urldate = {2023-06-10},
	date = {2023-01-20},
	eprinttype = {arxiv},
	eprint = {2301.08427 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Programming Languages, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209379/Zhang 等 - 2023 - Which Features are Learned by CodeBert An Empiric.pdf:application/pdf;arXiv.org Snapshot:files/209381/2301.html:text/html},
}

@misc{zhang_boosting_2023,
	title = {Boosting Automated Patch Correctness Prediction via Pre-trained Language Model},
	url = {http://arxiv.org/abs/2301.12453},
	abstract = {Automated program repair ({APR}) aims to fix software bugs automatically without human debugging efforts and plays a crucial role in software development and maintenance. Despite the recent significant progress, {APR} is still challenged by a long-standing overfitting problem (i.e., the generated patch is plausible but overfitting). Various techniques have thus been proposed to address the overfitting problem. Among them, leveraging deep learning approaches to predict patch correctness is emerging along with the available large-scale patch benchmarks recently. However, existing learning-based techniques mainly rely on manually-designed code features, which can be extremely costly and challenging to construct in practice. In this paper, we propose {APPT}, a pre-trained model-based automated patch correctness assessment technique, which treats the source code as token sequences without extra overhead to design hand-crafted features. In particular, {APPT} adopts a pre-trained model as the encoder stack, followed by an {LSTM} stack and a deep learning classifier. Although our idea is general and can be built on various pre-trained models, we implemente {APPT} based on the {BERT} model. We conduct an extensive experiment on 1,183 Defects4J patches and the results show that {APPT} achieves prediction accuracy of 79.0\% and recall of 81.3\%, outperforming the state-of-the-art technique {CACHE} by 3.6\% and 4.8\%. Our additional investigation on 49,694 real-world patches shows that {APPT} achieves the optimum performance (exceeding 99\% in five common metrics for assessing patch classification techniques) compared with existing representation learning techniques. We also prove that adopting code pre-trained models can further provide substantial advancement (e.g., {GraphCodeBERT}-based {APPT} improves {BERT}-based {APPT} by 3.0\% and 2.6\% in precision and recall, respectively), highlighting the generalizability of {APPT}.},
	number = {{arXiv}:2301.12453},
	publisher = {{arXiv}},
	author = {Zhang, Quanjun and Fang, Chunrong and Sun, Weisong and Liu, Yan and He, Tieke and Hao, Xiaodong and Chen, Zhenyu},
	urldate = {2023-06-10},
	date = {2023-01-29},
	eprinttype = {arxiv},
	eprint = {2301.12453 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209382/Zhang 等 - 2023 - Boosting Automated Patch Correctness Prediction vi.pdf:application/pdf;arXiv.org Snapshot:files/209830/2301.html:text/html},
}

@inproceedings{jalil_chatgpt_2023,
	location = {Dublin, Ireland},
	title = {{ChatGPT} and Software Testing Education: Promises \& Perils},
	isbn = {9798350333350},
	url = {https://ieeexplore.ieee.org/document/10132255/},
	doi = {10.1109/ICSTW58534.2023.00078},
	shorttitle = {{ChatGPT} and Software Testing Education},
	abstract = {Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the advent of general purpose "large language models", based on neural transformer architectures, that have been trained on massive datasets of human written text spanning code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of {ChatGPT}, a language model created by {OpenAI} and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end users. The introduction of models, such as {ChatGPT}, has already spurred fervent discussion from educators, ranging from fear that students could use these {AI} tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well {ChatGPT} performs when tasked with answering common questions in a popular software testing curriculum. Our findings indicate that {ChatGPT} can provide correct or partially correct answers in 55.6\% of cases, provide correct or partially correct explanations of answers in 53.0\% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct responses. Based on these findings, we discuss the potential promises and perils related to the use of {ChatGPT} by students and instructors.},
	eventtitle = {2023 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	pages = {4130--4137},
	booktitle = {2023 {IEEE} International Conference on Software Testing, Verification and Validation Workshops ({ICSTW})},
	publisher = {{IEEE}},
	author = {Jalil, Sajed and Rafi, Suzzana and {LaToza}, Thomas D. and Moran, Kevin and Lam, Wing},
	urldate = {2023-06-10},
	date = {2023-04},
	eprinttype = {arxiv},
	eprint = {2302.03287 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Software Engineering, D.2.5},
	file = {arXiv Fulltext PDF:files/209385/Jalil 等 - 2023 - ChatGPT and Software Testing Education Promises &.pdf:application/pdf;arXiv.org Snapshot:files/209391/2302.html:text/html},
}

@misc{phung_generating_2023,
	title = {Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models},
	url = {http://arxiv.org/abs/2302.04662},
	abstract = {Large language models ({LLMs}), such as Codex, hold great promise in enhancing programming education by automatically generating feedback for students. We investigate using {LLMs} to generate feedback for fixing syntax errors in Python programs, a key scenario in introductory programming. More concretely, given a student's buggy program, our goal is to generate feedback comprising a fixed program along with a natural language explanation describing the errors/fixes, inspired by how a human tutor would give feedback. While using {LLMs} is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms. The main research question we study is: Can we develop {LLMs}-based feedback generation techniques with a tunable precision parameter, giving educators quality control over the feedback that students receive? To this end, we introduce {PyFiXV}, our technique to generate high-precision feedback powered by Codex. The key idea behind {PyFiXV} is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student; notably, this validation mechanism also provides a precision knob to educators. We perform an extensive evaluation using two real-world datasets of Python programs with syntax errors and show the efficacy of {PyFiXV} in generating high-precision feedback.},
	number = {{arXiv}:2302.04662},
	publisher = {{arXiv}},
	author = {Phung, Tung and Cambronero, José and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
	urldate = {2023-06-10},
	date = {2023-04-28},
	eprinttype = {arxiv},
	eprint = {2302.04662 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209387/Phung 等 - 2023 - Generating High-Precision Feedback for Programming.pdf:application/pdf;arXiv.org Snapshot:files/209394/2302.html:text/html},
}

@misc{madaan_learning_2023,
	title = {Learning Performance-Improving Code Edits},
	url = {http://arxiv.org/abs/2302.07867},
	abstract = {The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models ({LLMs}) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, {PIE}. {PIE} contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use {PIE} to evaluate and improve the capacity of large language models. Specifically, use examples from {PIE} to fine-tune multiple variants of {CODEGEN}, a billion-scale Transformer-decoder model. Additionally, we use examples from {PIE} to prompt {OpenAI}'s {CODEX} using a few-shot prompting. By leveraging {PIE}, we find that both {CODEX} and {CODEGEN} can generate performance-improving edits, with speedups of more than 2.5x for over 25\% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that {PIE} allows {CODEGEN}, an open-sourced and 10x smaller model than {CODEX}, to match the performance of {CODEX} on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.},
	number = {{arXiv}:2302.07867},
	publisher = {{arXiv}},
	author = {Madaan, Aman and Shypula, Alexander and Alon, Uri and Hashemi, Milad and Ranganathan, Parthasarathy and Yang, Yiming and Neubig, Graham and Yazdanbakhsh, Amir},
	urldate = {2023-06-10},
	date = {2023-02-21},
	eprinttype = {arxiv},
	eprint = {2302.07867 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Performance},
	file = {arXiv Fulltext PDF:files/208610/Madaan 等 - 2023 - Learning Performance-Improving Code Edits.pdf:application/pdf;arXiv.org Snapshot:files/209736/2302.html:text/html},
}

@misc{paas_foundation_2023,
	title = {Foundation Models for Natural Language Processing -- Pre-trained Language Models Integrating Media},
	url = {http://arxiv.org/abs/2302.08575},
	abstract = {This open access book provides a comprehensive overview of the state of the art in research and applications of Foundation Models and is intended for readers familiar with basic Natural Language Processing ({NLP}) concepts. Over the recent years, a revolutionary new paradigm has been developed for training models for {NLP}. These models are first pre-trained on large collections of text documents to acquire general syntactic knowledge and semantic information. Then, they are fine-tuned for specific tasks, which they can often solve with superhuman accuracy. When the models are large enough, they can be instructed by prompts to solve new tasks without any fine-tuning. Moreover, they can be applied to a wide range of different media and problem domains, ranging from image and video processing to robot control learning. Because they provide a blueprint for solving many tasks in artificial intelligence, they have been called Foundation Models. After a brief introduction to basic {NLP} models the main pre-trained language models {BERT}, {GPT} and sequence-to-sequence transformer are described, as well as the concepts of self-attention and context-sensitive embedding. Then, different approaches to improving these models are discussed, such as expanding the pre-training criteria, increasing the length of input texts, or including extra knowledge. An overview of the best-performing models for about twenty application areas is then presented, e.g., question answering, translation, story generation, dialog systems, generating images from text, etc. For each application area, the strengths and weaknesses of current models are discussed, and an outlook on further developments is given. In addition, links are provided to freely available program code. A concluding chapter summarizes the economic opportunities, mitigation of risks, and potential developments of {AI}.},
	number = {{arXiv}:2302.08575},
	publisher = {{arXiv}},
	author = {Paaß, Gerhard and Giesselbach, Sven},
	urldate = {2023-06-10},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2302.08575 [cs]},
	keywords = {I.2.6, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Computation and Language, 68W20, 68W25, I.2.10, I.2.7, I.2.8, I.4.10, I.4.8, I.5.2, I.5.4, I.7.0, J.1, J.3, K.4.1, K.4.2, K.5.0},
	file = {arXiv Fulltext PDF:files/209470/Paaß 和 Giesselbach - 2023 - Foundation Models for Natural Language Processing .pdf:application/pdf;arXiv.org Snapshot:files/209680/2302.html:text/html},
}

@misc{liu_lang2ltl_2023,
	title = {Lang2LTL: Translating Natural Language Commands to Temporal Robot Task Specification},
	url = {http://arxiv.org/abs/2302.11649},
	shorttitle = {Lang2LTL},
	abstract = {Natural language provides a powerful modality to program robots to perform temporal tasks. Linear temporal logic ({LTL}) provides unambiguous semantics for formal descriptions of temporal tasks. However, existing approaches cannot accurately and robustly translate English sentences to their equivalent {LTL} formulas in unseen environments. To address this problem, we propose Lang2LTL, a novel modular system that leverages pretrained large language models to first extract referring expressions from a natural language command, then ground the expressions to real-world landmarks and objects, and finally translate the command into an {LTL} task specification for the robot. It enables any robotic system to interpret natural language navigation commands without additional training, provided that it tracks its position and has a semantic map with landmarks labeled with free-form text. We demonstrate the state-of-the-art ability to generalize to multi-scale navigation domains such as {OpenStreetMap} ({OSM}) and {CleanUp} World (a simulated household environment). Lang2LTL achieves an average accuracy of 88.4\% in translating challenging {LTL} formulas in 22 unseen {OSM} environments as evaluated on a new corpus of over 10,000 commands, 22 times better than the previous {SoTA}. Without modification, the best performing Lang2LTL model on the {OSM} dataset can translate commands in {CleanUp} World with 82.8\% accuracy. As a part of our proposed comprehensive evaluation procedures, we collected a new labeled dataset of English commands representing 2,125 unique {LTL} formulas, the largest ever dataset of natural language commands to {LTL} specifications for robotic tasks with the most diverse {LTL} formulas, 40 times more than previous largest dataset. Finally, we integrated Lang2LTL with a planner to command a quadruped mobile robot to perform multi-step navigational tasks in an analog real-world environment created in the lab.},
	number = {{arXiv}:2302.11649},
	publisher = {{arXiv}},
	author = {Liu, Jason Xinyu and Yang, Ziyi and Idrees, Ifrah and Liang, Sam and Schornstein, Benjamin and Tellex, Stefanie and Shah, Ankit},
	urldate = {2023-06-10},
	date = {2023-02-22},
	eprinttype = {arxiv},
	eprint = {2302.11649 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Robotics, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv Fulltext PDF:files/209413/Liu 等 - 2023 - Lang2LTL Translating Natural Language Commands to.pdf:application/pdf;arXiv.org Snapshot:files/209416/2302.html:text/html},
}

@misc{omar_detecting_2023,
	title = {Detecting software vulnerabilities using Language Models},
	url = {http://arxiv.org/abs/2302.11773},
	abstract = {Recently, deep learning techniques have garnered substantial attention for their ability to identify vulnerable code patterns accurately. However, current state-of-the-art deep learning models, such as Convolutional Neural Networks ({CNN}), and Long Short-Term Memories ({LSTMs}) require substantial computational resources. This results in a level of overhead that makes their implementation unfeasible for deployment in realtime settings. This study presents a novel transformer-based vulnerability detection framework, referred to as {VulDetect}, which is achieved through the fine-tuning of a pre-trained large language model, ({GPT}) on various benchmark datasets of vulnerable code. Our empirical findings indicate that our framework is capable of identifying vulnerable software code with an accuracy of up to 92.65\%. Our proposed technique outperforms {SyseVR} and {VulDeBERT}, two state-of-the-art vulnerability detection techniques},
	number = {{arXiv}:2302.11773},
	publisher = {{arXiv}},
	author = {Omar, Marwan},
	urldate = {2023-06-10},
	date = {2023-02-22},
	eprinttype = {arxiv},
	eprint = {2302.11773 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209395/Omar - 2023 - Detecting software vulnerabilities using Language .pdf:application/pdf;arXiv.org Snapshot:files/209749/2302.html:text/html},
}

@misc{al-kaswan_abuse_2023,
	title = {The (ab)use of Open Source Code to Train Large Language Models},
	url = {http://arxiv.org/abs/2302.13681},
	abstract = {In recent years, Large Language Models ({LLMs}) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. {LLMs} for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyleft code to train {LLMs} is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue.},
	number = {{arXiv}:2302.13681},
	publisher = {{arXiv}},
	author = {Al-Kaswan, Ali and Izadi, Maliheh},
	urldate = {2023-06-10},
	date = {2023-02-28},
	eprinttype = {arxiv},
	eprint = {2302.13681 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209392/Al-Kaswan 和 Izadi - 2023 - The (ab)use of Open Source Code to Train Large Lan.pdf:application/pdf;arXiv.org Snapshot:files/209784/2302.html:text/html},
}

@misc{tang_does_2023,
	title = {Does Synthetic Data Generation of {LLMs} Help Clinical Text Mining?},
	url = {http://arxiv.org/abs/2303.04360},
	abstract = {Recent advancements in large language models ({LLMs}) have led to the development of highly potent models like {OpenAI}'s {ChatGPT}. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of {ChatGPT} to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing {ChatGPT} directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the {ChatGPT} {API}. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing {ChatGPT} and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37\% to 63.99\% for the named entity recognition task and from 75.86\% to 83.59\% for the relation extraction task. Furthermore, generating data using {ChatGPT} can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of {LLM} models to clinical text mining.},
	number = {{arXiv}:2303.04360},
	publisher = {{arXiv}},
	author = {Tang, Ruixiang and Han, Xiaotian and Jiang, Xiaoqian and Hu, Xia},
	urldate = {2023-06-10},
	date = {2023-04-10},
	eprinttype = {arxiv},
	eprint = {2303.04360 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208826/Tang 等 - 2023 - Does Synthetic Data Generation of LLMs Help Clinic.pdf:application/pdf;arXiv.org Snapshot:files/208844/2303.html:text/html},
}

@misc{yang_syntax-guided_2023,
	title = {A Syntax-Guided Multi-Task Learning Approach for Turducken-Style Code Generation},
	url = {http://arxiv.org/abs/2303.05061},
	abstract = {Due to the development of pre-trained language models, automated code generation techniques have shown great promise in recent years. However, the generated code is difficult to meet the syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize the lack of syntactic constraints into three significant challenges: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach {TurduckenGen}. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.},
	number = {{arXiv}:2303.05061},
	publisher = {{arXiv}},
	author = {Yang, Guang and Zhou, Yu and Chen, Xiang and Zhang, Xiangyu and Xu, Yiran and Han, Tingting and Chen, Taolue},
	urldate = {2023-06-10},
	date = {2023-03-09},
	eprinttype = {arxiv},
	eprint = {2303.05061 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208709/Yang 等 - 2023 - A Syntax-Guided Multi-Task Learning Approach for T.pdf:application/pdf;arXiv.org Snapshot:files/209776/2303.html:text/html},
}

@misc{zhang_planning_2023,
	title = {Planning with Large Language Models for Code Generation},
	url = {http://arxiv.org/abs/2303.05510},
	abstract = {Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding ({PG}-{TD}), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner to generate candidate programs and test them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.},
	number = {{arXiv}:2303.05510},
	publisher = {{arXiv}},
	author = {Zhang, Shun and Chen, Zhenfang and Shen, Yikang and Ding, Mingyu and Tenenbaum, Joshua B. and Gan, Chuang},
	urldate = {2023-06-10},
	date = {2023-03-09},
	eprinttype = {arxiv},
	eprint = {2303.05510 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208831/Zhang 等 - 2023 - Planning with Large Language Models for Code Gener.pdf:application/pdf;arXiv.org Snapshot:files/209190/2303.html:text/html},
}

@misc{zhu_automating_2023,
	title = {Automating Method Naming with Context-Aware Prompt-Tuning},
	url = {http://arxiv.org/abs/2303.05771},
	abstract = {Method names are crucial to program comprehension and maintenance. Recently, many approaches have been proposed to automatically recommend method names and detect inconsistent names. Despite promising, their results are still sub-optimal considering the three following drawbacks: 1) These models are mostly trained from scratch, learning two different objectives simultaneously. The misalignment between two objectives will negatively affect training efficiency and model performance. 2) The enclosing class context is not fully exploited, making it difficult to learn the abstract function of the method. 3) Current method name consistency checking methods follow a generate-then-compare process, which restricts the accuracy as they highly rely on the quality of generated names and face difficulty measuring the semantic consistency. In this paper, we propose an approach named {AUMENA} to {AUtomate} {MEthod} {NAming} tasks with context-aware prompt-tuning. Unlike existing deep learning based approaches, our model first learns the contextualized representation(i.e., class attributes) of {PL} and {NL} through the pre-training model, then fully exploits the capacity and knowledge of large language model with prompt-tuning to precisely detect inconsistent method names and recommend more accurate names. To better identify semantically consistent names, we model the method name consistency checking task as a two-class classification problem, avoiding the limitation of previous similarity-based consistency checking approaches. The experimental results reflect that {AUMENA} scores 68.6\%, 72.0\%, 73.6\%, 84.7\% on four datasets of method name recommendation, surpassing the state-of-the-art baseline by 8.5\%, 18.4\%, 11.0\%, 12.0\%, respectively. And our approach scores 80.8\% accuracy on method name consistency checking, reaching an 5.5\% outperformance. All data and trained models are publicly available.},
	number = {{arXiv}:2303.05771},
	publisher = {{arXiv}},
	author = {Zhu, Jie and Li, Lingwei and Yang, Li and Ma, Xiaoxiao and Zuo, Chun},
	urldate = {2023-06-10},
	date = {2023-03-10},
	eprinttype = {arxiv},
	eprint = {2303.05771 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208833/Zhu 等 - 2023 - Automating Method Naming with Context-Aware Prompt.pdf:application/pdf;arXiv.org Snapshot:files/209191/2303.html:text/html},
}

@misc{white_chatgpt_2023,
	title = {{ChatGPT} Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design},
	url = {http://arxiv.org/abs/2303.07839},
	abstract = {This paper presents prompt design techniques for software engineering, in the form of patterns, to solve common problems when using large language models ({LLMs}), such as {ChatGPT} to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and simulating a web application {API} before it is implemented. This paper provides two contributions to research on using {LLMs} for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, refactoring, and system design.},
	number = {{arXiv}:2303.07839},
	publisher = {{arXiv}},
	author = {White, Jules and Hays, Sam and Fu, Quchen and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	urldate = {2023-06-10},
	date = {2023-03-11},
	eprinttype = {arxiv},
	eprint = {2303.07839 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209400/White 等 - 2023 - ChatGPT Prompt Patterns for Improving Code Quality.pdf:application/pdf;arXiv.org Snapshot:files/209404/2303.html:text/html},
}

@misc{wei_typet5_2023,
	title = {{TypeT}5: Seq2seq Type Inference using Static Analysis},
	url = {http://arxiv.org/abs/2303.09564},
	shorttitle = {{TypeT}5},
	abstract = {There has been growing interest in automatically predicting missing type annotations in programs written in Python and {JavaScript}. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging {CodeT}5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model. We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, {TypeT}5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors -- while enabling easy user intervention.},
	number = {{arXiv}:2303.09564},
	publisher = {{arXiv}},
	author = {Wei, Jiayi and Durrett, Greg and Dillig, Isil},
	urldate = {2023-06-10},
	date = {2023-03-16},
	eprinttype = {arxiv},
	eprint = {2303.09564 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208777/Wei 等 - 2023 - TypeT5 Seq2seq Type Inference using Static Analys.pdf:application/pdf;arXiv.org Snapshot:files/209182/2303.html:text/html},
}

@misc{bordt_chatgpt_2023,
	title = {{ChatGPT} Participates in a Computer Science Exam},
	url = {http://arxiv.org/abs/2303.09461},
	abstract = {We asked {ChatGPT} to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. The program was evaluated on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that {ChatGPT} narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that {ChatGPT} can indeed succeed in challenging tasks like university exams. At the same time, the questions in our exam are structurally similar to those of other exams, solved homework problems, and teaching materials that can be found online and might have been part of {ChatGPT}'s training data. Therefore, it would be inadequate to conclude from this experiment that {ChatGPT} has any understanding of computer science. We also assess the improvements brought by {GPT}-4. We find that {GPT}-4 would have obtained about 17{\textbackslash}\% more exam points than {GPT}-3.5, reaching the performance of the average student. The transcripts of our conversations with {ChatGPT} are available at {\textbackslash}url\{https://github.com/tml-tuebingen/chatgpt-algorithm-exam\}, and the entire graded exam is in the appendix of this paper.},
	number = {{arXiv}:2303.09461},
	publisher = {{arXiv}},
	author = {Bordt, Sebastian and von Luxburg, Ulrike},
	urldate = {2023-06-10},
	date = {2023-03-22},
	eprinttype = {arxiv},
	eprint = {2303.09461 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209418/Bordt 和 von Luxburg - 2023 - ChatGPT Participates in a Computer Science Exam.pdf:application/pdf;arXiv.org Snapshot:files/209421/2303.html:text/html},
}

@misc{jiang_self-planning_2023,
	title = {Self-planning Code Generation with Large Language Model},
	url = {http://arxiv.org/abs/2303.06689},
	abstract = {Although large language models have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule the solution steps prior to implementation. Thus we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem solving. This paper proposes a self-planning code generation method with large language model, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, the language model plans out the solution steps from the intent combined with in-context learning. Then it enters the implementation phase, where the model generates code step by step, guided by the solution steps. The effectiveness of self-planning code generation has been rigorously evaluated on multiple code generation datasets and the results have demonstrated a marked superiority over naive direct generation approaches with language model. The improvement in performance is substantial, highlighting the significance of self-planning in code generation tasks.},
	number = {{arXiv}:2303.06689},
	publisher = {{arXiv}},
	author = {Jiang, Xue and Dong, Yihong and Wang, Lecheng and Shang, Qiwei and Li, Ge},
	urldate = {2023-06-10},
	date = {2023-03-12},
	eprinttype = {arxiv},
	eprint = {2303.06689 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208618/Jiang 等 - 2023 - Self-planning Code Generation with Large Language .pdf:application/pdf;arXiv.org Snapshot:files/209740/2303.html:text/html},
}

@misc{aiken_measuring_2023,
	title = {Measuring Improvement of F\$\_1\$-Scores in Detection of Self-Admitted Technical Debt},
	url = {http://arxiv.org/abs/2303.09617},
	abstract = {Artificial Intelligence and Machine Learning have witnessed rapid, significant improvements in Natural Language Processing ({NLP}) tasks. Utilizing Deep Learning, researchers have taken advantage of repository comments in Software Engineering to produce accurate methods for detecting Self-Admitted Technical Debt ({SATD}) from 20 open-source Java projects' code. In this work, we improve {SATD} detection with a novel approach that leverages the Bidirectional Encoder Representations from Transformers ({BERT}) architecture. For comparison, we re-evaluated previous deep learning methods and applied stratified 10-fold cross-validation to report reliable F\$\_1\$-scores. We examine our model in both cross-project and intra-project contexts. For each context, we use re-sampling and duplication as augmentation strategies to account for data imbalance. We find that our trained {BERT} model improves over the best performance of all previous methods in 19 of the 20 projects in cross-project scenarios. However, the data augmentation techniques were not sufficient to overcome the lack of data present in the intra-project scenarios, and existing methods still perform better. Future research will look into ways to diversify {SATD} datasets in order to maximize the latent power in large {BERT} models.},
	number = {{arXiv}:2303.09617},
	publisher = {{arXiv}},
	author = {Aiken, William and Mvula, Paul K. and Branco, Paula and Jourdan, Guy-Vincent and Sabetzadeh, Mehrdad and Viktor, Herna},
	urldate = {2023-06-10},
	date = {2023-03-16},
	eprinttype = {arxiv},
	eprint = {2303.09617 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209115/Aiken 等 - 2023 - Measuring Improvement of F\$_1\$-Scores in Detection.pdf:application/pdf;arXiv.org Snapshot:files/209766/2303.html:text/html},
}

@misc{treude_she_2023,
	title = {She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models},
	url = {http://arxiv.org/abs/2303.10131},
	shorttitle = {She Elicits Requirements and He Tests},
	abstract = {Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning {GitHub} issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, requirements elicitation was associated with the pronoun "he" in only 6\% of cases, while testing was associated with "he" in 100\% of cases. Additionally, tasks related to helping others had a 91\% association with "he" while the same association for tasks related to asking coworkers was only 52\%. These findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society.},
	number = {{arXiv}:2303.10131},
	publisher = {{arXiv}},
	author = {Treude, Christoph and Hata, Hideaki},
	urldate = {2023-06-10},
	date = {2023-03-17},
	eprinttype = {arxiv},
	eprint = {2303.10131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209399/Treude 和 Hata - 2023 - She Elicits Requirements and He Tests Software En.pdf:application/pdf;arXiv.org Snapshot:files/209403/2303.html:text/html},
}

@misc{jojic_gpt_2023,
	title = {{GPT} is becoming a Turing machine: Here are some ways to program it},
	url = {http://arxiv.org/abs/2303.14310},
	shorttitle = {{GPT} is becoming a Turing machine},
	abstract = {We demonstrate that, through appropriate prompting, {GPT}-3 family of models can be triggered to perform iterative behaviours necessary to execute (rather than just write or recall) programs that involve loops, including several popular algorithms found in computer science curricula or software developer interviews. We trigger execution and description of Iterations by Regimenting Self-Attention ({IRSA}) in one (or a combination) of three ways: 1) Using strong repetitive structure in an example of an execution path of a target program for one particular input, 2) Prompting with fragments of execution paths, and 3) Explicitly forbidding (skipping) self-attention to parts of the generated text. On a dynamic program execution, {IRSA} leads to larger accuracy gains than replacing the model with the much more powerful {GPT}-4. {IRSA} has promising applications in education, as the prompts and responses resemble student assignments in data structures and algorithms classes. Our findings hold implications for evaluating {LLMs}, which typically target the in-context learning: We show that prompts that may not even cover one full task example can trigger algorithmic behaviour, allowing solving problems previously thought of as hard for {LLMs}, such as logical puzzles. Consequently, prompt design plays an even more critical role in {LLM} performance than previously recognized.},
	number = {{arXiv}:2303.14310},
	publisher = {{arXiv}},
	author = {Jojic, Ana and Wang, Zhen and Jojic, Nebojsa},
	urldate = {2023-06-10},
	date = {2023-03-24},
	eprinttype = {arxiv},
	eprint = {2303.14310 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209402/Jojic 等 - 2023 - GPT is becoming a Turing machine Here are some wa.pdf:application/pdf;arXiv.org Snapshot:files/209406/2303.html:text/html},
}

@misc{diemert_can_2023,
	title = {Can Large Language Models assist in Hazard Analysis?},
	url = {http://arxiv.org/abs/2303.15473},
	abstract = {Large Language Models ({LLMs}), such as {GPT}-3, have demonstrated remarkable natural language processing and generation capabilities and have been applied to a variety tasks, such as source code generation. This paper explores the potential of integrating {LLMs} in the hazard analysis for safety-critical systems, a process which we refer to as co-hazard analysis ({CoHA}). In {CoHA}, a human analyst interacts with an {LLM} via a context-aware chat session and uses the responses to support elicitation of possible hazard causes. In this experiment, we explore {CoHA} with three increasingly complex versions of a simple system, using Open {AI}'s {ChatGPT} service. The quality of {ChatGPT}'s responses were systematically assessed to determine the feasibility of {CoHA} given the current state of {LLM} technology. The results suggest that {LLMs} may be useful for supporting human analysts performing hazard analysis.},
	number = {{arXiv}:2303.15473},
	publisher = {{arXiv}},
	author = {Diemert, Simon and Weber, Jens H.},
	urldate = {2023-06-10},
	date = {2023-03-25},
	eprinttype = {arxiv},
	eprint = {2303.15473 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:files/209405/Diemert 和 Weber - 2023 - Can Large Language Models assist in Hazard Analysi.pdf:application/pdf;arXiv.org Snapshot:files/209795/2303.html:text/html},
}

@misc{xia_revisiting_2023,
	title = {Revisiting the Plastic Surgery Hypothesis via Large Language Models},
	url = {http://arxiv.org/abs/2303.10494},
	abstract = {Automated Program Repair ({APR}) aspires to automatically generate patches for an input buggy program. Traditional {APR} tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based {APR} tools with recent work focused on directly using Large Language Models ({LLMs}) for {APR}. While {LLM}-based {APR} tools are able to achieve state-of-the-art performance on many repair datasets, the {LLMs} used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for {APR}, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional {APR} tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent {APR} research starts focusing on {LLM}-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of {LLMs}? Interestingly, {LLM}-based {APR} presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning and prompting. To this end, we propose {FitRepair}, which combines the direct usage of {LLMs} with two domain-specific fine-tuning strategies and one prompting strategy for more powerful {APR}. Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that {FitRepair} fixes 89 and 44 bugs (substantially outperforming the best-performing baseline by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of {LLMs}.},
	number = {{arXiv}:2303.10494},
	publisher = {{arXiv}},
	author = {Xia, Chunqiu Steven and Ding, Yifeng and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2023-03-18},
	eprinttype = {arxiv},
	eprint = {2303.10494 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209407/Xia 等 - 2023 - Revisiting the Plastic Surgery Hypothesis via Larg.pdf:application/pdf;arXiv.org Snapshot:files/209764/2303.html:text/html},
}

@misc{huang_survey_2023,
	title = {A Survey on Automated Program Repair Techniques},
	url = {http://arxiv.org/abs/2303.18184},
	abstract = {With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. Software defect has become an important factor troubling developers. In this context, Automated Program Repair ({APR}) techniques have emerged, aiming to automatically fix software defect problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based {APR} techniques have emerged in recent years, which also bring new opportunities for {APR} research. To give researchers a quick overview of {APR} techniques' complete development and future opportunities, we revisit the evolution of {APR} techniques and discuss in depth the latest advances in {APR} research. In this paper, the development of {APR} techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each {APR} tool, summarize the advantages and disadvantages of {APR} techniques, and discuss the current state of {APR} development. Furthermore, we introduce the research on the related technical areas of {APR} that have also provided a strong motivation to advance {APR} development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to {APR} research.},
	number = {{arXiv}:2303.18184},
	publisher = {{arXiv}},
	author = {Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing},
	urldate = {2023-06-10},
	date = {2023-05-13},
	eprinttype = {arxiv},
	eprint = {2303.18184 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209131/Huang 等 - 2023 - A Survey on Automated Program Repair Techniques.pdf:application/pdf;arXiv.org Snapshot:files/209697/2303.html:text/html},
}

@misc{chen_diversevul_2023,
	title = {{DiverseVul}: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection},
	url = {http://arxiv.org/abs/2304.00409},
	shorttitle = {{DiverseVul}},
	abstract = {We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 {CWEs}, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection. Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard {CWEs}. In particular, we demonstrate an important generalization challenge for the deployment of deep learning-based models. However, we also identify hopeful future research directions. We demonstrate that large language models ({LLMs}) are the future for vulnerability detection, outperforming Graph Neural Networks ({GNNs}) with manual feature engineering. Moreover, developing source code specific pre-training objectives is a promising research direction to improve the vulnerability detection performance.},
	number = {{arXiv}:2304.00409},
	publisher = {{arXiv}},
	author = {Chen, Yizheng and Ding, Zhoujie and Chen, Xinyun and Wagner, David},
	urldate = {2023-06-10},
	date = {2023-04-01},
	eprinttype = {arxiv},
	eprint = {2304.00409 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208827/Chen 等 - 2023 - DiverseVul A New Vulnerable Source Code Dataset f.pdf:application/pdf;arXiv.org Snapshot:files/208845/2304.html:text/html},
}

@misc{to_better_2023,
	title = {Better Language Models of Code through Self-Improvement},
	url = {http://arxiv.org/abs/2304.01228},
	abstract = {Pre-trained language models for code ({PLMCs}) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as {CodeT}5, {CodeBERT}, and {UnixCoder}. The results show that our framework significantly improves {PLMCs}' performance in code-related sequence generation tasks, such as code summarization and code generation in the {CodeXGLUE} benchmark.},
	number = {{arXiv}:2304.01228},
	publisher = {{arXiv}},
	author = {To, Hung Quoc and Bui, Nghi D. Q. and Guo, Jin and Nguyen, Tien N.},
	urldate = {2023-06-10},
	date = {2023-05-09},
	eprinttype = {arxiv},
	eprint = {2304.01228 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208830/To 等 - 2023 - Better Language Models of Code through Self-Improv.pdf:application/pdf;arXiv.org Snapshot:files/209189/2304.html:text/html},
}

@misc{kang_explainable_2023,
	title = {Explainable Automated Debugging via Large Language Model-driven Scientific Debugging},
	url = {http://arxiv.org/abs/2304.02195},
	abstract = {Automated debugging techniques have the potential to reduce developer effort in debugging, and have matured enough to be adopted by industry. However, one critical issue with existing techniques is that, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly from that of human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging ({AutoSD}), a technique that given buggy code and a bug-revealing test, prompts large language models to automatically generate hypotheses, uses debuggers to actively interact with buggy code, and thus automatically reach conclusions prior to patch generation. By aligning the reasoning of automated debugging more closely with that of human developers, we aim to produce intelligible explanations of how a specific patch has been generated, with the hope that the explanation will lead to more efficient and accurate developer decisions. Our empirical analysis on three program repair benchmarks shows that {AutoSD} performs competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants, including six professional developers, to evaluate the utility of explanations from {AutoSD}. Participants with access to explanations could judge patch correctness in roughly the same time as those without, but their accuracy improved for five out of six real-world bugs studied: 70\% of participants answered that they wanted explanations when using repair tools, while 55\% answered that they were satisfied with the Scientific Debugging presentation.},
	number = {{arXiv}:2304.02195},
	publisher = {{arXiv}},
	author = {Kang, Sungmin and Chen, Bei and Yoo, Shin and Lou, Jian-Guang},
	urldate = {2023-06-10},
	date = {2023-04-04},
	eprinttype = {arxiv},
	eprint = {2304.02195 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209132/Kang 等 - 2023 - Explainable Automated Debugging via Large Language.pdf:application/pdf;arXiv.org Snapshot:files/209814/2304.html:text/html},
}

@misc{prasath_synthesis_2023,
	title = {Synthesis of Mathematical programs from Natural Language Specifications},
	url = {http://arxiv.org/abs/2304.03287},
	abstract = {Several decision problems that are encountered in various business domains can be modeled as mathematical programs, i.e. optimization problems. The process of conducting such modeling often requires the involvement of experts trained in operations research and advanced algorithms. Surprisingly, despite the significant advances in the methods for program and code synthesis, {AutoML}, learning to optimize etc., there has been little or no attention paid to automating the task of synthesizing mathematical programs. We imagine a scenario where the specifications for modeling, i.e. the objective and constraints are expressed in an unstructured form in natural language ({NL}) and the mathematical program has to be synthesized from such an {NL} specification. In this work we evaluate the efficacy of employing {CodeT}5 with data augmentation and post-processing of beams. We utilize {GPT}-3 with back translation for generation of synthetic examples. Further we apply rules of linear programming to score beams and correct beams based on common error patterns. We observe that with these enhancements {CodeT}5 base gives an execution accuracy of 0.73 which is significantly better than zero-shot execution accuracy of 0.41 by {ChatGPT} and 0.36 by Codex.},
	number = {{arXiv}:2304.03287},
	publisher = {{arXiv}},
	author = {Prasath, Ganesh and Karande, Shirish},
	urldate = {2023-06-10},
	date = {2023-03-30},
	eprinttype = {arxiv},
	eprint = {2304.03287 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208653/Prasath 和 Karande - 2023 - Synthesis of Mathematical programs from Natural La.pdf:application/pdf;arXiv.org Snapshot:files/208682/2304.html:text/html},
}

@misc{chen_chatpipe_2023,
	title = {{ChatPipe}: Orchestrating Data Preparation Program by Optimizing Human-{ChatGPT} Interactions},
	url = {http://arxiv.org/abs/2304.03540},
	shorttitle = {{ChatPipe}},
	abstract = {Orchestrating a high-quality data preparation program is essential for successful machine learning ({ML}), but it is known to be time and effort consuming. Despite the impressive capabilities of large language models like {ChatGPT} in generating programs by interacting with users through natural language prompts, there are still limitations. Specifically, a user must provide specific prompts to iteratively guide {ChatGPT} in improving data preparation programs, which requires a certain level of expertise in programming, the dataset used and the {ML} task. Moreover, once a program has been generated, it is non-trivial to revisit a previous version or make changes to the program without starting the process over again. In this paper, we present {ChatPipe}, a novel system designed to facilitate seamless interaction between users and {ChatGPT}. {ChatPipe} provides users with effective recommendation on next data preparation operations, and guides {ChatGPT} to generate program for the operations. Also, {ChatPipe} enables users to easily roll back to previous versions of the program, which facilitates more efficient experimentation and testing. We have developed a web application for {ChatPipe} and prepared several real-world {ML} tasks from Kaggle. These tasks can showcase the capabilities of {ChatPipe} and enable {VLDB} attendees to easily experiment with our novel features to rapidly orchestrate a high-quality data preparation program.},
	number = {{arXiv}:2304.03540},
	publisher = {{arXiv}},
	author = {Chen, Sibei and Liu, Hanbing and Jin, Weiting and Sun, Xiangyu and Feng, Xiaoyao and Fan, Ju and Du, Xiaoyong and Tang, Nan},
	urldate = {2023-06-10},
	date = {2023-04-07},
	eprinttype = {arxiv},
	eprint = {2304.03540 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Human-Computer Interaction, Computer Science - Databases},
	file = {arXiv Fulltext PDF:files/209408/Chen 等 - 2023 - ChatPipe Orchestrating Data Preparation Program b.pdf:application/pdf;arXiv.org Snapshot:files/209410/2304.html:text/html},
}

@misc{pan_ltm_2023,
	title = {{LTM}: Scalable and Black-box Similarity-based Test Suite Minimization based on Language Models},
	url = {http://arxiv.org/abs/2304.01397},
	shorttitle = {{LTM}},
	abstract = {Test suite minimization ({TSM}) is typically used to improve the efficiency of software testing by removing redundant test cases, thus reducing testing time and resources, while maintaining the fault detection capability of the test suite. Though many {TSM} approaches exist, most of them rely on code coverage (white-box) or model-based features, which are not always available for test engineers. Recent {TSM} approaches that rely only on test code (black-box) have been proposed, such as {ATM} and {FAST}-R. Though {ATM} achieves a better trade-off between effectiveness and efficiency than {FAST}-R, it suffers from scalability issues for large software systems as its execution time increases rapidly with test suite size. To address scalability, we propose {LTM}, a scalable and black-box similarity-based {TSM} approach based on language models. To support similarity measurement, we investigated three different pre-trained language models: {CodeBERT}, {GraphCodeBERT}, and {UniXcoder}, to extract embeddings of test code (Java test methods), on which we computed two similarity measures: Cosine Similarity and Euclidean Distance. Our goal is to find similarity measures that are not only computationally more efficient but can also better guide a Genetic Algorithm ({GA}), which is used for minimizing test suites, thus reducing minimization time. Experimental results showed that the best configuration of {LTM} (using {UniXcoder} with Cosine similarity) outperformed the best two configurations of {ATM} by achieving significantly higher fault detection rates (0.84 versus 0.81, on average) and, more importantly, running much faster (26.73 minutes versus 72.75 minutes, on average) than {ATM}, in terms of both preparation time (up to two orders of magnitude faster) and minimization time (one order of magnitude faster).},
	number = {{arXiv}:2304.01397},
	publisher = {{arXiv}},
	author = {Pan, Rongqi and Ghaleb, Taher A. and Briand, Lionel},
	urldate = {2023-06-10},
	date = {2023-04-03},
	eprinttype = {arxiv},
	eprint = {2304.01397 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209412/Pan 等 - 2023 - LTM Scalable and Black-box Similarity-based Test .pdf:application/pdf;arXiv.org Snapshot:files/209415/2304.html:text/html},
}

@misc{deng_large_2023,
	title = {Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via {FuzzGPT}},
	url = {http://arxiv.org/abs/2304.02014},
	shorttitle = {Large Language Models are Edge-Case Fuzzers},
	abstract = {Deep Learning ({DL}) library bugs affect downstream {DL} applications, emphasizing the need for reliable systems. Generating valid input programs for fuzzing {DL} libraries is challenging due to the need for satisfying both language syntax/semantics and constraints for constructing valid computational graphs. Recently, the {TitanFuzz} work demonstrates that modern Large Language Models ({LLMs}) can be directly leveraged to implicitly learn all the constraints to generate valid {DL} programs for fuzzing. However, {LLMs} tend to generate ordinary programs following similar patterns seen in their massive training corpora, while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced. To fill this gap, this paper proposes {FuzzGPT}, the first technique to prime {LLMs} to synthesize unusual programs for fuzzing. {FuzzGPT} is built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Traditional techniques leveraging such historical information require intensive human efforts to design dedicated generators and ensure the validity of generated programs. {FuzzGPT} demonstrates that this process can be fully automated via the intrinsic capabilities of {LLMs} (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While {FuzzGPT} can be applied with different {LLMs}, this paper focuses on the powerful {GPT}-style models: Codex and {CodeGen}. Moreover, {FuzzGPT} also shows the potential of directly leveraging the instruct-following capability of the recent {ChatGPT} for effective fuzzing. Evaluation on two popular {DL} libraries ({PyTorch} and {TensorFlow}) shows that {FuzzGPT} can substantially outperform {TitanFuzz}, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.},
	number = {{arXiv}:2304.02014},
	publisher = {{arXiv}},
	author = {Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2023-04-04},
	eprinttype = {arxiv},
	eprint = {2304.02014 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209414/Deng 等 - 2023 - Large Language Models are Edge-Case Fuzzers Testi.pdf:application/pdf;arXiv.org Snapshot:files/209726/2304.html:text/html},
}

@misc{cheshkov_evaluation_2023,
	title = {Evaluation of {ChatGPT} Model for Vulnerability Detection},
	url = {http://arxiv.org/abs/2304.07232},
	abstract = {In this technical report, we evaluated the performance of the {ChatGPT} and {GPT}-3 models for the task of vulnerability detection in code. Our evaluation was conducted on our real-world dataset, using binary and multi-label classification tasks on {CWE} vulnerabilities. We decided to evaluate the model because it has shown good performance on other code-based tasks, such as solving programming challenges and understanding code at a high level. However, we found that the {ChatGPT} model performed no better than a dummy classifier for both binary and multi-label classification tasks for code vulnerability detection.},
	number = {{arXiv}:2304.07232},
	publisher = {{arXiv}},
	author = {Cheshkov, Anton and Zadorozhny, Pavel and Levichev, Rodion},
	urldate = {2023-06-10},
	date = {2023-04-12},
	eprinttype = {arxiv},
	eprint = {2304.07232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209409/Cheshkov 等 - 2023 - Evaluation of ChatGPT Model for Vulnerability Dete.pdf:application/pdf;arXiv.org Snapshot:files/209828/2304.html:text/html},
}

@misc{paul_automated_2023,
	title = {Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?},
	url = {http://arxiv.org/abs/2304.07840},
	shorttitle = {Automated Program Repair Based on Code Review},
	abstract = {Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review (natural language instruction about suggestive changes in code) can improve the program repair further. Large language models, trained with Natural Language ({NL}) and computer program corpora, have the capacity to contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of code and {NL} can be utilized to improve automated program repair. We applied {PLBART} and {CodeT}5, two state-of-the-art language models that are pre-trained with both Programming Language ({PL}) and Natural Language ({NL}), on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperform each of the previous models. We observed that the pre-trained models improve the previously best-reported results by 9.91\% on the Review4Repair dataset and by 24.72\% on the dataset by Tufano et al. This suggests that a pre-trained sequential model has a better understanding of natural language and can utilize it much better. We performed an ablation study to assess the contribution of the pre-training mechanism and the model architecture. We found that pre-training was significantly more important in the performance gain than the model architecture. The practical application of using pre-trained transformer models in the context of automated program repair is still a long way off. However, our study demonstrates the substantial value of employing pre-trained models, paving the path for future studies to use more of these.},
	number = {{arXiv}:2304.07840},
	publisher = {{arXiv}},
	author = {Paul, Rishov and Hossain, Md Mohib and Hasan, Masum and Iqbal, Anindya},
	urldate = {2023-06-10},
	date = {2023-04-16},
	eprinttype = {arxiv},
	eprint = {2304.07840 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209411/Paul 等 - 2023 - Automated Program Repair Based on Code Review How.pdf:application/pdf;arXiv.org Snapshot:files/209809/2304.html:text/html},
}

@misc{gao_constructing_2023,
	title = {Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study},
	url = {http://arxiv.org/abs/2304.07575},
	shorttitle = {Constructing Effective In-Context Demonstration for Code Intelligence Tasks},
	abstract = {Pre-trained models of code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning. These models employ task instructions and a few demonstration examples as prompts to learn the semantics of the task and make predictions for test samples. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of in-context learning heavily relies on the quality of demonstration, and there has been no systematic investigation into how to construct a good demonstration for code-related tasks with in-context learning. In this paper, by analyzing the design space of in-context demonstration, we empirically explore the impact of three key factors on the performance of in-context learning in code intelligence tasks: the selection of demonstration examples, the order of demonstration examples, and the number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including bug fixing, code summarization, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of in-context learning in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We show that a well-constructed demonstration can lead to significant improvements over simple demonstrations and previous fine-tuned state-of-the-art models, e.g., improving {EM}, {BLEU}-4, and {EM} by at least 11.91\%, 36.88\%, and 37.18\% on code summarization, bug fixing and program synthesis, respectively.},
	number = {{arXiv}:2304.07575},
	publisher = {{arXiv}},
	author = {Gao, Shuzheng and Wen, Xin-Cheng and Gao, Cuiyun and Wang, Wenxuan and Lyu, Michael R.},
	urldate = {2023-06-10},
	date = {2023-04-15},
	eprinttype = {arxiv},
	eprint = {2304.07575 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209419/Gao 等 - 2023 - Constructing Effective In-Context Demonstration fo.pdf:application/pdf;arXiv.org Snapshot:files/209745/2304.html:text/html},
}

@misc{bi_codekgc_2023,
	title = {{CodeKGC}: Code Language Model for Generative Knowledge Graph Construction},
	url = {http://arxiv.org/abs/2304.09048},
	shorttitle = {{CodeKGC}},
	abstract = {Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines. Code and datasets are available in https://github.com/zjunlp/{DeepKE}/tree/main/example/llm.},
	number = {{arXiv}:2304.09048},
	publisher = {{arXiv}},
	author = {Bi, Zhen and Chen, Jing and Jiang, Yinuo and Xiong, Feiyu and Guo, Wei and Chen, Huajun and Zhang, Ningyu},
	urldate = {2023-06-10},
	date = {2023-04-18},
	eprinttype = {arxiv},
	eprint = {2304.09048 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/209143/Bi 等 - 2023 - CodeKGC Code Language Model for Generative Knowle.pdf:application/pdf;arXiv.org Snapshot:files/209158/2304.html:text/html},
}

@misc{he-yueya_solving_2023,
	title = {Solving Math Word Problems by Combining Language Models With Symbolic Solvers},
	url = {http://arxiv.org/abs/2304.09102},
	abstract = {Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models ({LLMs}) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model ({PAL}) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an {LLM} that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original {PAL} on the {GSM}8K benchmark of math word problems and outperforms {PAL} by an absolute 20\% on {ALGEBRA}, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when interfacing with an external tool for solving complex math word problems. Our data and prompts are publicly available at https://github.com/joyheyueya/declarative-math-word-problem.},
	number = {{arXiv}:2304.09102},
	publisher = {{arXiv}},
	author = {He-Yueya, Joy and Poesia, Gabriel and Wang, Rose E. and Goodman, Noah D.},
	urldate = {2023-06-10},
	date = {2023-04-16},
	eprinttype = {arxiv},
	eprint = {2304.09102 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209420/He-Yueya 等 - 2023 - Solving Math Word Problems by Combining Language M.pdf:application/pdf;arXiv.org Snapshot:files/209423/2304.html:text/html},
}

@misc{lu_chameleon_2023,
	title = {Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
	url = {http://arxiv.org/abs/2304.09842},
	shorttitle = {Chameleon},
	abstract = {Large language models ({LLMs}) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, {LLMs} have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an {AI} system that mitigates these limitations by augmenting {LLMs} with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., {LLMs}, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an {LLM}-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: {ScienceQA} and {TabMWP}. Chameleon, powered by {GPT}-4, achieves an 86.54\% overall accuracy on {ScienceQA}, improving the best published few-shot result by 11.37\%. On {TabMWP}, {GPT}-4-powered Chameleon improves the accuracy by 17.0\%, lifting the state of the art to 98.78\%. Our analysis also shows that the {GPT}-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a {ChatGPT}-powered planner.},
	number = {{arXiv}:2304.09842},
	publisher = {{arXiv}},
	author = {Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2304.09842 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208798/Lu 等 - 2023 - Chameleon Plug-and-Play Compositional Reasoning w.pdf:application/pdf;arXiv.org Snapshot:files/209701/2304.html:text/html},
}

@misc{liventsev_fully_2023,
	title = {Fully Autonomous Programming with Large Language Models},
	url = {http://arxiv.org/abs/2304.10423},
	doi = {10.1145/3583131.3590481},
	abstract = {Current approaches to program synthesis with Large Language Models ({LLMs}) exhibit a "near miss syndrome": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format. This calls for an approach known as Synthesize, Execute, Debug ({SED}), whereby a draft of the solution is generated first, followed by a program repair phase addressing the failed tests. To effectively apply this approach to instruction-driven {LLMs}, one needs to determine which prompts perform best as instructions for {LLMs}, as well as strike a balance between repairing unsuccessful programs and replacing them with newly generated ones. We explore these trade-offs empirically, comparing replace-focused, repair-focused, and hybrid debug strategies, as well as different template-based and model-based prompt-generation techniques. We use {OpenAI} Codex as the {LLM} and Program Synthesis Benchmark 2 as a database of problem descriptions and tests for evaluation. The resulting framework outperforms both conventional usage of Codex without the repair phase and traditional genetic programming approaches.},
	author = {Liventsev, Vadim and Grishina, Anastasiia and Härmä, Aki and Moonen, Leon},
	urldate = {2023-06-10},
	date = {2023-04-20},
	eprinttype = {arxiv},
	eprint = {2304.10423 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/208683/Liventsev 等 - 2023 - Fully Autonomous Programming with Large Language M.pdf:application/pdf;arXiv.org Snapshot:files/209815/2304.html:text/html},
}

@misc{guo_learning_2023,
	title = {Learning to Program with Natural Language},
	url = {http://arxiv.org/abs/2304.10464},
	abstract = {Large Language Models ({LLMs}) have shown remarkable performance in various basic natural language tasks, which raises hope for achieving Artificial General Intelligence. For completing the complex task, we still need a program for the task first and then ask {LLMs} to follow the program to generate the specific solution. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and {LLMs}. {\textasciitilde}The {LLM} is capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program ({\textbackslash}text\{{LP}\}) method to ask {LLMs} themselves to learn the natural language program based on the training dataset of the complex task first and then use the learned program to guide the inference. Our experiments on the reasoning tasks of five different reasoning types (8 datasets) demonstrate the effectiveness of our approach. Further, our analysis experiment shows that the learned program can be directly used to guide another {LLM} to improve its performance, which reveals a new transfer learning paradigm.},
	number = {{arXiv}:2304.10464},
	publisher = {{arXiv}},
	author = {Guo, Yiduo and Liang, Yaobo and Wu, Chenfei and Wu, Wenshan and Zhao, Dongyan and Duan, Nan},
	urldate = {2023-06-10},
	date = {2023-05-28},
	eprinttype = {arxiv},
	eprint = {2304.10464 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209417/Guo 等 - 2023 - Learning to Program with Natural Language.pdf:application/pdf;arXiv.org Snapshot:files/209747/2304.html:text/html},
}

@misc{yetistiren_evaluating_2023,
	title = {Evaluating the Code Quality of {AI}-Assisted Code Generation Tools: An Empirical Study on {GitHub} Copilot, Amazon {CodeWhisperer}, and {ChatGPT}},
	url = {http://arxiv.org/abs/2304.10778},
	shorttitle = {Evaluating the Code Quality of {AI}-Assisted Code Generation Tools},
	abstract = {Context: {AI}-assisted code generation tools have become increasingly prevalent in software engineering, offering the ability to generate code from natural language prompts or partial code inputs. Notable examples of these tools include {GitHub} Copilot, Amazon {CodeWhisperer}, and {OpenAI}'s {ChatGPT}. Objective: This study aims to compare the performance of these prominent code generation tools in terms of code quality metrics, such as Code Validity, Code Correctness, Code Security, Code Reliability, and Code Maintainability, to identify their strengths and shortcomings. Method: We assess the code generation capabilities of {GitHub} Copilot, Amazon {CodeWhisperer}, and {ChatGPT} using the benchmark {HumanEval} Dataset. The generated code is then evaluated based on the proposed code quality metrics. Results: Our analysis reveals that the latest versions of {ChatGPT}, {GitHub} Copilot, and Amazon {CodeWhisperer} generate correct code 65.2\%, 46.3\%, and 31.1\% of the time, respectively. In comparison, the newer versions of {GitHub} {CoPilot} and Amazon {CodeWhisperer} showed improvement rates of 18\% for {GitHub} Copilot and 7\% for Amazon {CodeWhisperer}. The average technical debt, considering code smells, was found to be 8.9 minutes for {ChatGPT}, 9.1 minutes for {GitHub} Copilot, and 5.6 minutes for Amazon {CodeWhisperer}. Conclusions: This study highlights the strengths and weaknesses of some of the most popular code generation tools, providing valuable insights for practitioners. By comparing these generators, our results may assist practitioners in selecting the optimal tool for specific tasks, enhancing their decision-making process.},
	number = {{arXiv}:2304.10778},
	publisher = {{arXiv}},
	author = {Yetiştiren, Burak and Özsoy, Işık and Ayerdem, Miray and Tüzün, Eray},
	urldate = {2023-06-10},
	date = {2023-04-21},
	eprinttype = {arxiv},
	eprint = {2304.10778 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209422/Yetiştiren 等 - 2023 - Evaluating the Code Quality of AI-Assisted Code Ge.pdf:application/pdf;arXiv.org Snapshot:files/209723/2304.html:text/html},
}

@misc{geng_large_2023,
	title = {Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning},
	url = {http://arxiv.org/abs/2304.11384},
	shorttitle = {Large Language Models are Few-Shot Summarizers},
	abstract = {Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models ({LLMs}) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of {LLMs} to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the {LLMs} can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the {LLM} (e.g., providing it with ten or more examples), the {LLM} can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the {LLM}'s performances, which shed light on future research directions for using {LLMs} to achieve comment generation.},
	number = {{arXiv}:2304.11384},
	publisher = {{arXiv}},
	author = {Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke},
	urldate = {2023-06-10},
	date = {2023-06-08},
	eprinttype = {arxiv},
	eprint = {2304.11384 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209424/Geng 等 - 2023 - Large Language Models are Few-Shot Summarizers Mu.pdf:application/pdf;arXiv.org Snapshot:files/209843/2304.html:text/html},
}

@misc{li_finding_2023,
	title = {Finding Failure-Inducing Test Cases with {ChatGPT}},
	url = {http://arxiv.org/abs/2304.11686},
	abstract = {Automatically detecting software failures is an important task and a longstanding challenge. It requires finding failure-inducing test cases whose test input can trigger the software's fault, and constructing an automated oracle to detect the software's incorrect behaviors. Recent advancement of large language models ({LLMs}) motivates us to study how far this challenge can be addressed by {ChatGPT}, a state-of-the-art {LLM}. Unfortunately, our study shows that {ChatGPT} has a low probability (28.8\%) of finding correct failure-inducing test cases for buggy programs. A possible reason is that finding failure-inducing test cases requires analyzing the subtle code differences between a buggy program and its correct version. When these two versions have similar syntax, {ChatGPT} is weak at recognizing subtle code differences. Our insight is that {ChatGPT}'s performance can be substantially enhanced when {ChatGPT} is guided to focus on the subtle code difference. We have an interesting observation that {ChatGPT} is effective in inferring the intended behaviors of a buggy program. The intended behavior can be leveraged to synthesize programs, in order to make the subtle code difference between a buggy program and its correct version (i.e., the synthesized program) explicit. Driven by this observation, we propose a novel approach that synergistically combines {ChatGPT} and differential testing to find failure-inducing test cases. We evaluate our approach on Quixbugs (a benchmark of buggy programs), and compare it with state-of-the-art baselines, including direct use of {ChatGPT} and Pynguin. The experimental result shows that our approach has a much higher probability (77.8\%) of finding correct failure-inducing test cases, 2.7X as the best baseline.},
	number = {{arXiv}:2304.11686},
	publisher = {{arXiv}},
	author = {Li, Tsz-On and Zong, Wenxi and Wang, Yibo and Tian, Haoye and Wang, Ying and Cheung, Shing-Chi and Kramer, Jeff},
	urldate = {2023-06-10},
	date = {2023-04-30},
	eprinttype = {arxiv},
	eprint = {2304.11686 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209425/Li 等 - 2023 - Finding Failure-Inducing Test Cases with ChatGPT.pdf:application/pdf;arXiv.org Snapshot:files/209431/2304.html:text/html},
}

@misc{poldrack_ai-assisted_2023,
	title = {{AI}-assisted coding: Experiments with {GPT}-4},
	url = {http://arxiv.org/abs/2304.13187},
	shorttitle = {{AI}-assisted coding},
	abstract = {Artificial intelligence ({AI}) tools based on large language models have acheived human-level performance on some computer programming tasks. We report several experiments using {GPT}-4 to generate computer code. These experiments demonstrate that {AI} code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. We also demonstrate that {GPT}-4 refactoring of existing code can significantly improve that code along several established metrics for code quality, and we show that {GPT}-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. These findings suggest that while {AI} coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.},
	number = {{arXiv}:2304.13187},
	publisher = {{arXiv}},
	author = {Poldrack, Russell A. and Lu, Thomas and Beguš, Gašper},
	urldate = {2023-06-10},
	date = {2023-04-25},
	eprinttype = {arxiv},
	eprint = {2304.13187 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209430/Poldrack 等 - 2023 - AI-assisted coding Experiments with GPT-4.pdf:application/pdf;arXiv.org Snapshot:files/209711/2304.html:text/html},
}

@misc{siddiq_exploring_2023,
	title = {Exploring the Effectiveness of Large Language Models in Generating Unit Tests},
	url = {http://arxiv.org/abs/2305.00418},
	abstract = {A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., {GitHub} Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models ({CodeGen}, Codex, and {GPT}-3.5) can generate test cases. We used two benchmarks ({HumanEval} and Evosuite {SF}110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80\% coverage for the {HumanEval} dataset, but no model had more than 2\% coverage for the {EvoSuite} {SF}110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.},
	number = {{arXiv}:2305.00418},
	publisher = {{arXiv}},
	author = {Siddiq, Mohammed Latif and Santos, Joanna C. S. and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Rifat, Fahmid Al and Lopes, Vinicius Carvalho},
	urldate = {2023-06-10},
	date = {2023-04-30},
	eprinttype = {arxiv},
	eprint = {2305.00418 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209139/Siddiq 等 - 2023 - Exploring the Effectiveness of Large Language Mode.pdf:application/pdf;arXiv.org Snapshot:files/209246/2305.html:text/html},
}

@misc{zheng_outline_2023,
	title = {Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation},
	url = {http://arxiv.org/abs/2305.00909},
	shorttitle = {Outline, Then Details},
	abstract = {For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose {ChainCoder}, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to jointly encode the natural language descriptions and syntactically aligned I/O data samples. Extensive evaluations show that {ChainCoder} outperforms state-of-the-arts, demonstrating that our progressive generation eases the reasoning procedure and guides the language model to generate higher-quality solutions. Our codes are available at: https://github.com/{VITA}-Group/{ChainCoder}.},
	number = {{arXiv}:2305.00909},
	publisher = {{arXiv}},
	author = {Zheng, Wenqing and Sharan, S. P. and Jaiswal, Ajay Kumar and Wang, Kevin and Xi, Yihan and Xu, Dejia and Wang, Zhangyang},
	urldate = {2023-06-10},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2305.00909 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209446/Zheng 等 - 2023 - Outline, Then Details Syntactically Guided Coarse.pdf:application/pdf;arXiv.org Snapshot:files/209449/2305.html:text/html},
}

@misc{begus_large_2023,
	title = {Large Linguistic Models: Analyzing theoretical linguistic abilities of {LLMs}},
	url = {http://arxiv.org/abs/2305.00948},
	shorttitle = {Large Linguistic Models},
	abstract = {The performance of large language models ({LLMs}) has recently improved to the point where the models can generate valid and coherent meta-linguistic analyses of data. This paper illustrates a vast potential for analyses of the meta-linguistic abilities of large language models. {LLMs} are primarily trained on language data in the form of text; analyzing their meta-linguistic abilities is informative both for our understanding of the general capabilities of {LLMs} as well as for models of linguistics. In this paper, we propose several types of experiments and prompt designs that allow us to analyze the ability of {GPT}-4 to generate meta-linguistic analyses. We focus on three linguistics subfields with formalisms that allow for a detailed analysis of {GPT}-4's theoretical capabilities: theoretical syntax, phonology, and semantics. We identify types of experiments, provide general guidelines, discuss limitations, and offer future directions for this research program.},
	number = {{arXiv}:2305.00948},
	publisher = {{arXiv}},
	author = {Beguš, Gašper and Dąbkowski, Maksymilian and Rhodes, Ryan},
	urldate = {2023-06-10},
	date = {2023-05-01},
	eprinttype = {arxiv},
	eprint = {2305.00948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209426/Beguš 等 - 2023 - Large Linguistic Models Analyzing theoretical lin.pdf:application/pdf;arXiv.org Snapshot:files/209432/2305.html:text/html},
}

@misc{nijkamp_codegen2_2023,
	title = {{CodeGen}2: Lessons for Training {LLMs} on Programming and Natural Languages},
	url = {http://arxiv.org/abs/2305.02309},
	shorttitle = {{CodeGen}2},
	abstract = {Large language models ({LLMs}) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly. In this study, we attempt to render the training of {LLMs} for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-{LM}. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a "free lunch" hypothesis. For data distributions, the effect of a mixture distribution of programming and natural languages on model performance is explored. We conduct a comprehensive series of empirical experiments on 1B {LLMs}, for which failures and successes of this exploration are distilled into four lessons. We will provide a final recipe for training and release {CodeGen}2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/{CodeGen}2.},
	number = {{arXiv}:2305.02309},
	publisher = {{arXiv}},
	author = {Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
	urldate = {2023-06-10},
	date = {2023-05-03},
	eprinttype = {arxiv},
	eprint = {2305.02309 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209427/Nijkamp 等 - 2023 - CodeGen2 Lessons for Training LLMs on Programming.pdf:application/pdf;arXiv.org Snapshot:files/209433/2305.html:text/html},
}

@misc{dobslaw_experiences_2023,
	title = {Experiences with Remote Examination Formats in Light of {GPT}-4},
	url = {http://arxiv.org/abs/2305.02198},
	abstract = {Sudden access to the rapidly improving large language model {GPT} by open-ai forces educational institutions worldwide to revisit their exam procedures. In the pre-{GPT} era, we successfully applied oral and open-book home exams for two courses in the third year of our predominantly remote Software Engineering {BSc} program. We ask in this paper whether our current open-book exams are still viable or whether a move back to a legally compliant but less scalable oral exam is the only workable alternative. We further compare work-effort estimates between oral and open-book exams and report on differences in throughput and grade distribution over eight years to better understand the impact of examination format on the outcome. Examining {GPT} v4 on the most recent open-book exams showed that our current Artificial Intelligence and Reactive Programming exams are not {GPT} v4 proof. Three potential weaknesses of {GPT} are outlined. We also found that grade distributions have largely been unaffected by the examination format, opening up for a move to oral examinations only if needed. Throughput was higher for open-book exam course instances (73\% vs 64\%), while fail rates were too (12\% vs 7\%), with teacher workload increasing even for smaller classes. We also report on our experience regarding effort. Oral examinations are efficient for smaller groups but come with caveats regarding intensity and stress.},
	number = {{arXiv}:2305.02198},
	publisher = {{arXiv}},
	author = {Dobslaw, Felix and Bergh, Peter},
	urldate = {2023-06-10},
	date = {2023-03-27},
	eprinttype = {arxiv},
	eprint = {2305.02198 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209428/Dobslaw 和 Bergh - 2023 - Experiences with Remote Examination Formats in Lig.pdf:application/pdf;arXiv.org Snapshot:files/209434/2305.html:text/html},
}

@misc{zhang_should_2023,
	title = {Should {ChatGPT} and Bard Share Revenue with Their Data Providers? A New Business Model for the {AI} Era},
	url = {http://arxiv.org/abs/2305.02555},
	shorttitle = {Should {ChatGPT} and Bard Share Revenue with Their Data Providers?},
	abstract = {With various {AI} tools such as {ChatGPT} becoming increasingly popular, we are entering a true {AI} era. We can foresee that exceptional {AI} tools will soon reap considerable profits. A crucial question arise: should {AI} tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large {AI} tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between {AI} tools and their data providers could transform the current hostile zero-sum game relationship between {AI} tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among {AI} tools, their users and data providers that drives forward {AI} technology and builds a healthy {AI} ecosystem. However, current revenue-sharing business models do not work for {AI} tools in the forthcoming {AI} era, since the most widely used metrics for website-based traffic and action, such as clicks, will be replaced by new metrics such as prompts and cost per prompt for generative {AI} tools. A completely new revenue-sharing business model, which must be almost independent of {AI} tools and be easily explained to data providers, needs to establish a prompt-based scoring system to measure data engagement of each data provider. This paper systematically discusses how to build such a scoring system for all data providers for {AI} tools based on classification and content similarity models, and outlines the requirements for {AI} tools or third parties to build it. Sharing revenue with data providers using such a scoring system would encourage more data owners to participate in the revenue-sharing program. This will be a utilitarian {AI} era where all parties benefit.},
	number = {{arXiv}:2305.02555},
	publisher = {{arXiv}},
	author = {Zhang, Dong},
	urldate = {2023-06-10},
	date = {2023-05-04},
	eprinttype = {arxiv},
	eprint = {2305.02555 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209440/Zhang - 2023 - Should ChatGPT and Bard Share Revenue with Their D.pdf:application/pdf;arXiv.org Snapshot:files/209768/2305.html:text/html},
}

@misc{pujar_automated_2023,
	title = {Automated Code generation for Information Technology Tasks in {YAML} through Large Language Models},
	url = {http://arxiv.org/abs/2305.02783},
	abstract = {The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for {IT} Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-{YAML}, a widely used markup language for {IT} Automation. We present Ansible Wisdom, a natural-language to Ansible-{YAML} code generation tool, aimed at improving {IT} automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-{YAML}. We also develop two novel performance metrics for {YAML} and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models. In few-shot settings we asses the impact of training with Ansible, {YAML} data and compare with different baselines including Codex-Davinci-002. We also show that after finetuning, our Ansible specific model ({BLEU}: 66.67) can outperform a much larger Codex-Davinci-002 ({BLEU}: 50.4) model, which was evaluated in few shot settings.},
	number = {{arXiv}:2305.02783},
	publisher = {{arXiv}},
	author = {Pujar, Saurabh and Buratti, Luca and Guo, Xiaojie and Dupuis, Nicolas and Lewis, Burn and Suneja, Sahil and Sood, Atin and Nalawade, Ganesh and Jones, Matthew and Morari, Alessandro and Puri, Ruchir},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.02783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209429/Pujar 等 - 2023 - Automated Code generation for Information Technolo.pdf:application/pdf;arXiv.org Snapshot:files/209436/2305.html:text/html},
}

@misc{saieva_contrastive_2023,
	title = {On Contrastive Learning of Semantic Similarity {forCode} to Code Search},
	url = {http://arxiv.org/abs/2305.03843},
	abstract = {This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models ({LLMs}) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced {LLMs} to perform cross-language code-to-code search. Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7{\textbackslash}\%. Moreover, our ablation studies reveal that even a single positive and negative reference sample in the training process results in substantial performance improvements demonstrating both similar and dissimilar references are important parts of code search. Importantly, we show that enhanced well-crafted, fine-tuned models consistently outperform enhanced larger modern {LLMs} without fine tuning, even when enhancing the largest available {LLMs} highlighting the importance for open-sourced models. To ensure the reproducibility and extensibility of our research, we present an open-sourced implementation of our tool and training procedures called Cosco.},
	number = {{arXiv}:2305.03843},
	publisher = {{arXiv}},
	author = {Saieva, Anthony and Chakraborty, Saikat and Kaiser, Gail},
	urldate = {2023-06-10},
	date = {2023-05-05},
	eprinttype = {arxiv},
	eprint = {2305.03843 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209438/Saieva 等 - 2023 - On Contrastive Learning of Semantic Similarity for.pdf:application/pdf;arXiv.org Snapshot:files/209442/2305.html:text/html},
}

@misc{connor_large_2023,
	title = {Large Language Models in Sport Science \& Medicine: Opportunities, Risks and Considerations},
	url = {http://arxiv.org/abs/2305.03851},
	shorttitle = {Large Language Models in Sport Science \& Medicine},
	abstract = {This paper explores the potential opportunities, risks, and challenges associated with the use of large language models ({LLMs}) in sports science and medicine. {LLMs} are large neural networks with transformer style architectures trained on vast amounts of textual data, and typically refined with human feedback. {LLMs} can perform a large range of natural language processing tasks. In sports science and medicine, {LLMs} have the potential to support and augment the knowledge of sports medicine practitioners, make recommendations for personalised training programs, and potentially distribute high-quality information to practitioners in developing countries. However, there are also potential risks associated with the use and development of {LLMs}, including biases in the dataset used to create the model, the risk of exposing confidential data, the risk of generating harmful output, and the need to align these models with human preferences through feedback. Further research is needed to fully understand the potential applications of {LLMs} in sports science and medicine and to ensure that their use is ethical and beneficial to athletes, clients, patients, practitioners, and the general public.},
	number = {{arXiv}:2305.03851},
	publisher = {{arXiv}},
	author = {Connor, Mark and O'Neill, Michael},
	urldate = {2023-06-10},
	date = {2023-05-05},
	eprinttype = {arxiv},
	eprint = {2305.03851 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
	file = {arXiv Fulltext PDF:files/209435/Connor 和 O'Neill - 2023 - Large Language Models in Sport Science & Medicine.pdf:application/pdf;arXiv.org Snapshot:files/209439/2305.html:text/html},
}

@misc{zhang_toolcoder_2023,
	title = {{ToolCoder}: Teach Code Generation Models to use {API} search tools},
	url = {http://arxiv.org/abs/2305.04032},
	shorttitle = {{ToolCoder}},
	abstract = {Automatically generating source code from natural language descriptions has been a growing field of research in recent years. However, current large-scale code generation models often encounter difficulties when selecting appropriate {APIs} for specific contexts. These models may generate {APIs} that do not meet requirements or refer to non-existent {APIs} in third-party libraries, especially for lesser-known or private libraries. Inspired by the process of human developers using tools to search {APIs}, we propose {ToolCoder}, a novel approach that integrates {API} search tools with existing models to assist in code generation and {API} selection. To teach our model to use tools, we introduce an automated data annotation method using {ChatGPT} to add tool usage information into the source code data and fine-tune code generation models. During inference, we integrate {API} search tools into the generation process so that our model can automatically use the search tool to get suggestions when selecting an {API}. Our experimental results demonstrate that {ToolCoder} exhibits excellent performance and generalization across five public and private library code generation benchmarks, with at least 6.21{\textbackslash}\% improvement on average pass@1 metrics and 9.64{\textbackslash}\% improvement on average pass@10 metrics compared to state-of-the-art methods. Furthermore, we show that our relatively small {ToolCoder} model is comparable to one of the current best models, {GPT}-3.5, highlighting the potential of incorporating programming tools into the code generation process.},
	number = {{arXiv}:2305.04032},
	publisher = {{arXiv}},
	author = {Zhang, Kechi and Li, Ge and Li, Jia and Li, Zhuo and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2023-05-09},
	eprinttype = {arxiv},
	eprint = {2305.04032 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209437/Zhang 等 - 2023 - ToolCoder Teach Code Generation Models to use API.pdf:application/pdf;arXiv.org Snapshot:files/209441/2305.html:text/html},
}

@misc{zhang_self-edit_2023,
	title = {Self-Edit: Fault-Aware Code Editor for Code Generation},
	url = {http://arxiv.org/abs/2305.04087},
	shorttitle = {Self-Edit},
	abstract = {Large language models ({LLMs}) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, {LLMs} still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from {LLMs} to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different {LLMs}. Compared to directly generating from {LLMs}, our approach can improve the average of pass@1 by 89{\textbackslash}\% on {APPS}-dev, 31{\textbackslash}\% on {APPS}-test, and 48{\textbackslash}\% on {HumanEval} over nine popular code generation {LLMs} with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.},
	number = {{arXiv}:2305.04087},
	publisher = {{arXiv}},
	author = {Zhang, Kechi and Li, Zhuo and Li, Jia and Li, Ge and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2305.04087 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208699/Zhang 等 - 2023 - Self-Edit Fault-Aware Code Editor for Code Genera.pdf:application/pdf;arXiv.org Snapshot:files/208726/2305.html:text/html},
}

@misc{yuan_no_2023,
	title = {No More Manual Tests? Evaluating and Improving {ChatGPT} for Unit Test Generation},
	url = {http://arxiv.org/abs/2305.04207},
	shorttitle = {No More Manual Tests?},
	abstract = {Unit testing is essential in detecting bugs in functionally-discrete program units. Manually writing high-quality unit tests is time-consuming and laborious. Although traditional techniques can generate tests with reasonable coverage, they exhibit low readability and cannot be directly adopted by developers. Recent work has shown the large potential of large language models ({LLMs}) in unit test generation, which can generate more human-like and meaningful test code. {ChatGPT}, the latest {LLM} incorporating instruction tuning and reinforcement learning, has performed well in various domains. However, It remains unclear how effective {ChatGPT} is in unit test generation. In this work, we perform the first empirical study to evaluate {ChatGPT}'s capability of unit test generation. Specifically, we conduct a quantitative analysis and a user study to systematically investigate the quality of its generated tests regarding the correctness, sufficiency, readability, and usability. The tests generated by {ChatGPT} still suffer from correctness issues, including diverse compilation errors and execution failures. Still, the passing tests generated by {ChatGPT} resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers' preference. Our findings indicate that generating unit tests with {ChatGPT} could be very promising if the correctness of its generated tests could be further improved. Inspired by our findings above, we propose {ChatTESTER}, a novel {ChatGPT}-based unit test generation approach, which leverages {ChatGPT} itself to improve the quality of its generated tests. {ChatTESTER} incorporates an initial test generator and an iterative test refiner. Our evaluation demonstrates the effectiveness of {ChatTESTER} by generating 34.3\% more compilable tests and 18.7\% more tests with correct assertions than the default {ChatGPT}.},
	number = {{arXiv}:2305.04207},
	publisher = {{arXiv}},
	author = {Yuan, Zhiqiang and Lou, Yiling and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin},
	urldate = {2023-06-10},
	date = {2023-05-08},
	eprinttype = {arxiv},
	eprint = {2305.04207 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208803/Yuan 等 - 2023 - No More Manual Tests Evaluating and Improving Cha.pdf:application/pdf;arXiv.org Snapshot:files/209769/2305.html:text/html},
}

@misc{schlag_large_2023,
	title = {Large Language Model Programs},
	url = {http://arxiv.org/abs/2305.05364},
	abstract = {In recent years, large pre-trained language models ({LLMs}) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an {LLM} through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an {LLM} by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4{\textbackslash}\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.},
	number = {{arXiv}:2305.05364},
	publisher = {{arXiv}},
	author = {Schlag, Imanol and Sukhbaatar, Sainbayar and Celikyilmaz, Asli and Yih, Wen-tau and Weston, Jason and Schmidhuber, Jürgen and Li, Xian},
	urldate = {2023-06-10},
	date = {2023-05-09},
	eprinttype = {arxiv},
	eprint = {2305.05364 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208799/Schlag 等 - 2023 - Large Language Model Programs.pdf:application/pdf;arXiv.org Snapshot:files/209705/2305.html:text/html},
}

@misc{xie_chatunitest_2023,
	title = {{ChatUniTest}: a {ChatGPT}-based automated unit test generation tool},
	url = {http://arxiv.org/abs/2305.04764},
	shorttitle = {{ChatUniTest}},
	abstract = {Unit testing is a crucial, yet often tedious and time-consuming task. To relieve developers from this burden, automated unit test generation techniques are developed. Existing automated unit test generation tools, such as program-analysis-based tools like {EvoSuite} and Randoop, lack program comprehension, resulting in unit tests with poor readability and limited assertions. Language-model-based tools, such as {AthenaTest} and A3Test, have limitations in the generation of correct unit tests. In this paper, we introduce {ChatUniTest}, a {ChatGPT}-based automated unit test generation tool developed under the Generation-Validation-Repair framework. {ChatUniTest} generates tests by parsing the project, extracting essential information, and creating an adaptive focal context that includes the focal method and its dependencies within the pre-defined maximum prompt token limit. The context is incorporated into a prompt and subsequently submitted to {ChatGPT}. Once {ChatGPT}'s response is received, {ChatUniTest} proceeds to extract the raw test from the response. It then validates the test and employs rule-based repair to fix syntactic and simple compile errors, followed by {ChatGPT}-based repair to address challenging errors. Our rigorous evaluation demonstrates that {ChatUniTest} outperforms {EvoSuite} in branch and line coverage, surpasses {AthenaTest} and A3Test in focal method coverage, and effectively generates assertions while utilizing mock objects and reflection to achieve test objectives.},
	number = {{arXiv}:2305.04764},
	publisher = {{arXiv}},
	author = {Xie, Zhuokui and Chen, Yinghao and Zhi, Chen and Deng, Shuiguang and Yin, Jianwei},
	urldate = {2023-06-10},
	date = {2023-05-08},
	eprinttype = {arxiv},
	eprint = {2305.04764 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209448/Xie 等 - 2023 - ChatUniTest a ChatGPT-based automated unit test g.pdf:application/pdf;arXiv.org Snapshot:files/209837/2305.html:text/html},
}

@misc{weyssow_usage_2023,
	title = {On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code},
	url = {http://arxiv.org/abs/2305.04106},
	abstract = {Pre-trained language models ({PLMs}) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of {PLMs}. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the {PLM}'s performance on downstream tasks. In this paper, we stress the need for adapting {PLMs} of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the {PLM} in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model needs to learn from a stream of programs containing new, unseen {APIs} over time. We study two widely used {PLM} architectures, i.e., a {GPT}2 decoder and a {RoBERTa} encoder, on two downstream tasks, {API} call and {API} usage prediction. We demonstrate that the most commonly used fine-tuning technique from prior work is not robust enough to handle the dynamic nature of {APIs}, leading to the loss of previously acquired knowledge i.e., catastrophic forgetting. To address these issues, we implement five continual learning approaches, including replay-based and regularization-based methods. Our findings demonstrate that utilizing these straightforward methods effectively mitigates catastrophic forgetting in {PLMs} across both downstream tasks while achieving comparable or superior performance.},
	number = {{arXiv}:2305.04106},
	publisher = {{arXiv}},
	author = {Weyssow, Martin and Zhou, Xin and Kim, Kisub and Lo, David and Sahraoui, Houari},
	urldate = {2023-06-10},
	date = {2023-05-06},
	eprinttype = {arxiv},
	eprint = {2305.04106 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209447/Weyssow 等 - 2023 - On the Usage of Continual Learning for Out-of-Dist.pdf:application/pdf;arXiv.org Snapshot:files/209788/2305.html:text/html},
}

@misc{moskvichev_conceptarc_2023,
	title = {The {ConceptARC} Benchmark: Evaluating Understanding and Generalization in the {ARC} Domain},
	url = {http://arxiv.org/abs/2305.07141},
	shorttitle = {The {ConceptARC} Benchmark},
	abstract = {The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art {AI} systems. There has been substantial research on conceptual abstraction in {AI}, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when {AI} systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture. In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus ({ARC}), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe {ConceptARC}, a new, publicly available benchmark in the {ARC} domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. {ConceptARC} differs from the original {ARC} dataset in that it is specifically organized around "concept groups" -- sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 {ARC} competition and {OpenAI}'s {GPT}-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by {AI} systems. We believe that this benchmark will spur improvements in the development of {AI} systems for conceptual abstraction and in the effective evaluation of such systems.},
	number = {{arXiv}:2305.07141},
	publisher = {{arXiv}},
	author = {Moskvichev, Arseny and Odouard, Victor Vikram and Mitchell, Melanie},
	urldate = {2023-06-10},
	date = {2023-05-11},
	eprinttype = {arxiv},
	eprint = {2305.07141 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209450/Moskvichev 等 - 2023 - The ConceptARC Benchmark Evaluating Understanding.pdf:application/pdf;arXiv.org Snapshot:files/209451/2305.html:text/html},
}

@misc{feng_knowledge_2023,
	title = {Knowledge Refinement via Interaction Between Search Engines and Large Language Models},
	url = {http://arxiv.org/abs/2305.07402},
	abstract = {Information retrieval ({IR}) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines ({SEs}). The emergence of large language models ({LLMs}) has further revolutionized the {IR} field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of {LLMs} and {SEs}, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose {InteR}, a novel framework that facilitates knowledge refinement through interaction between {SEs} and {LLMs}. {InteR} allows {SEs} to expand knowledge in queries using {LLM}-generated knowledge collections and enables {LLMs} to enhance prompt formulation using {SE}-retrieved documents. This iterative refinement process augments the inputs of {SEs} and {LLMs}, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that {InteR} achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-{JZ}/{InteR}},
	number = {{arXiv}:2305.07402},
	publisher = {{arXiv}},
	author = {Feng, Jiazhan and Tao, Chongyang and Geng, Xiubo and Shen, Tao and Xu, Can and Long, Guodong and Zhao, Dongyan and Jiang, Daxin},
	urldate = {2023-06-10},
	date = {2023-05-21},
	eprinttype = {arxiv},
	eprint = {2305.07402 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/208797/Feng 等 - 2023 - Knowledge Refinement via Interaction Between Searc.pdf:application/pdf;arXiv.org Snapshot:files/209785/2305.html:text/html},
}

@misc{derner_beyond_2023,
	title = {Beyond the Safeguards: Exploring the Security Risks of {ChatGPT}},
	url = {http://arxiv.org/abs/2305.08005},
	shorttitle = {Beyond the Safeguards},
	abstract = {The increasing popularity of large language models ({LLMs}) such as {ChatGPT} has led to growing concerns about their safety, security risks, and ethical implications. This paper aims to provide an overview of the different types of security risks associated with {ChatGPT}, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. We present an empirical study examining the effectiveness of {ChatGPT}'s content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in {LLMs} even when protections are in place. Based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by {LLMs} like {ChatGPT}. This study contributes to the ongoing discussion on the ethical and security implications of {LLMs}, underscoring the need for continued research in this area.},
	number = {{arXiv}:2305.08005},
	publisher = {{arXiv}},
	author = {Derner, Erik and Batistič, Kristina},
	urldate = {2023-06-10},
	date = {2023-05-13},
	eprinttype = {arxiv},
	eprint = {2305.08005 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/208804/Derner 和 Batistič - 2023 - Beyond the Safeguards Exploring the Security Risk.pdf:application/pdf;arXiv.org Snapshot:files/209728/2305.html:text/html},
}

@misc{liu_improving_2023,
	title = {Improving {ChatGPT} Prompt for Code Generation},
	url = {http://arxiv.org/abs/2305.08360},
	abstract = {Automated code generation can be a powerful technique for software development, significantly reducing developers' efforts and time required to create new code by generating it automatically based on requirements. Recently, {OpenAI}'s language model {ChatGPT} has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation. However, the effectiveness of {ChatGPT} for code generation is not well understood, and the generation performance could be heavily influenced by the choice of prompt. To answer these questions, we conducted experiments using the {CodeXGlue} dataset to evaluate {ChatGPT}'s capabilities for two code generation tasks, including text-to-code and code-to-code generation. We designed prompts by leveraging the chain-of-thought strategy with multi-step optimizations. Our results showed that by carefully designing prompts to guide {ChatGPT}, the generation performance can be improved substantially. We also analyzed the factors that influenced the prompt design and provided insights that could guide future research.},
	number = {{arXiv}:2305.08360},
	publisher = {{arXiv}},
	author = {Liu, Chao and Bao, Xuanlin and Zhang, Hongyu and Zhang, Neng and Hu, Haibo and Zhang, Xiaohong and Yan, Meng},
	urldate = {2023-06-10},
	date = {2023-05-15},
	eprinttype = {arxiv},
	eprint = {2305.08360 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209443/Liu 等 - 2023 - Improving ChatGPT Prompt for Code Generation.pdf:application/pdf;arXiv.org Snapshot:files/209444/2305.html:text/html},
}

@misc{destefanis_preliminary_2023,
	title = {A Preliminary Analysis on the Code Generation Capabilities of {GPT}-3.5 and Bard {AI} Models for Java Functions},
	url = {http://arxiv.org/abs/2305.09402},
	abstract = {This paper evaluates the capability of two state-of-the-art artificial intelligence ({AI}) models, {GPT}-3.5 and Bard, in generating Java code given a function description. We sourced the descriptions from {CodingBat}.com, a popular online platform that provides practice problems to learn programming. We compared the Java code generated by both models based on correctness, verified through the platform's own test cases. The results indicate clear differences in the capabilities of the two models. {GPT}-3.5 demonstrated superior performance, generating correct code for approximately 90.6\% of the function descriptions, whereas Bard produced correct code for 53.1\% of the functions. While both models exhibited strengths and weaknesses, these findings suggest potential avenues for the development and refinement of more advanced {AI}-assisted code generation tools. The study underlines the potential of {AI} in automating and supporting aspects of software development, although further research is required to fully realize this potential.},
	number = {{arXiv}:2305.09402},
	publisher = {{arXiv}},
	author = {Destefanis, Giuseppe and Bartolucci, Silvia and Ortu, Marco},
	urldate = {2023-06-10},
	date = {2023-05-16},
	eprinttype = {arxiv},
	eprint = {2305.09402 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209445/Destefanis 等 - 2023 - A Preliminary Analysis on the Code Generation Capa.pdf:application/pdf;arXiv.org Snapshot:files/209706/2305.html:text/html},
}

@misc{wang_leti_2023,
	title = {{LeTI}: Learning to Generate from Textual Interactions},
	url = {http://arxiv.org/abs/2305.10314},
	shorttitle = {{LeTI}},
	abstract = {Finetuning pre-trained language models ({LMs}) enhances the models' capabilities. Prior techniques fine-tune a pre-trained {LM} on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore {LMs}' potential to learn from textual interactions ({LeTI}) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. {LeTI} iteratively fine-tunes the model, using the {LM} objective, on a concatenation of natural language instructions, {LM}-generated programs, and textual feedback, which is only provided when the generated program fails to solve the task. Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions. On {MBPP}, a code generation dataset, {LeTI} substantially improves the performance of two base {LMs} of different scales. {LeTI} requires no ground-truth outputs for training and even outperforms a fine-tuned baseline that does. {LeTI}'s strong performance generalizes to other datasets. Trained on {MBPP}, it achieves comparable or better performance than the base {LMs} on unseen problems in {HumanEval}. Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps. {LeTI} is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction.},
	number = {{arXiv}:2305.10314},
	publisher = {{arXiv}},
	author = {Wang, Xingyao and Peng, Hao and Jabbarvand, Reyhaneh and Ji, Heng},
	urldate = {2023-06-10},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2305.10314 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209464/Wang 等 - 2023 - LeTI Learning to Generate from Textual Interactio.pdf:application/pdf;arXiv.org Snapshot:files/209738/2305.html:text/html},
}

@misc{wang_codet5_2023,
	title = {{CodeT}5+: Open Code Large Language Models for Code Understanding and Generation},
	url = {http://arxiv.org/abs/2305.07922},
	shorttitle = {{CodeT}5+},
	abstract = {Large language models ({LLMs}) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code {LLMs} have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``{CodeT}5+'', a family of encoder-decoder {LLMs} for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal {LM} pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize {CodeT}5+ with frozen off-the-shelf {LLMs} without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate {CodeT}5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art ({SoTA}) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned {CodeT}5+ 16B achieves new {SoTA} results on {HumanEval} code generation task against other open code {LLMs}.},
	number = {{arXiv}:2305.07922},
	publisher = {{arXiv}},
	author = {Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi D. Q. and Li, Junnan and Hoi, Steven C. H.},
	urldate = {2023-06-10},
	date = {2023-05-20},
	eprinttype = {arxiv},
	eprint = {2305.07922 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209452/Wang 等 - 2023 - CodeT5+ Open Code Large Language Models for Code .pdf:application/pdf;arXiv.org Snapshot:files/209455/2305.html:text/html},
}

@misc{huang_instruct2act_2023,
	title = {Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model},
	url = {http://arxiv.org/abs/2305.11176},
	shorttitle = {Instruct2Act},
	abstract = {Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the {LLM} model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined {APIs} are used to access multiple foundation models where the Segment Anything Model ({SAM}) accurately locates candidate objects, and {CLIP} classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/{OpenGVLab}/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.},
	number = {{arXiv}:2305.11176},
	publisher = {{arXiv}},
	author = {Huang, Siyuan and Jiang, Zhengkai and Dong, Hao and Qiao, Yu and Gao, Peng and Li, Hongsheng},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.11176 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/209453/Huang 等 - 2023 - Instruct2Act Mapping Multi-modality Instructions .pdf:application/pdf;arXiv.org Snapshot:files/209847/2305.html:text/html},
}

@article{alhafi_vulnerability_nodate,
	title = {Vulnerability Detection Using Two-Stage Deep Learning Models},
	issn = {28368495},
	url = {http://arxiv.org/abs/2305.09673},
	doi = {10.33140/JCTCSR},
	abstract = {Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-{AI}-based such as Static Application Security Testing ({SAST}) and Dynamic Application Security Testing ({DAST}). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an {AI}-based vulnerability detection system employing deep learning models like {BERT}, {BLSTM}, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is {CNN} which detects if the source code contains any vulnerability (binary classification model) and the second stage is {CNN}-{LTSM} that classifies this vulnerability into a class of 50 different types of vulnerabilities (multiclass classification model). Experiments were done on {SySeVR} dataset. Results show an accuracy of 99\% for the first and 98\% for the second stage.},
	journaltitle = {Journal of Current Trends in Computer Science Research},
	shortjournal = {{JCTCSR}},
	author = {Alhafi, Mohamed Mjd and Hammade, Mohammad and Jallad, Khloud Al},
	urldate = {2023-06-10},
	eprinttype = {arxiv},
	eprint = {2305.09673 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208774/Alhafi 等 - Vulnerability Detection Using Two-Stage Deep Learn.pdf:application/pdf;arXiv.org Snapshot:files/209796/2305.html:text/html},
}

@misc{gou_critic_2023,
	title = {{CRITIC}: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
	url = {http://arxiv.org/abs/2305.11738},
	shorttitle = {{CRITIC}},
	abstract = {Recent developments in large language models ({LLMs}) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called {CRITIC} that allows {LLMs}, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, {CRITIC} interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that {CRITIC} consistently enhances the performance of {LLMs}. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of {LLMs}.},
	number = {{arXiv}:2305.11738},
	publisher = {{arXiv}},
	author = {Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
	urldate = {2023-06-10},
	date = {2023-05-19},
	eprinttype = {arxiv},
	eprint = {2305.11738 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209162/Gou 等 - 2023 - CRITIC Large Language Models Can Self-Correct wit.pdf:application/pdf;arXiv.org Snapshot:files/209252/2305.html:text/html},
}

@misc{murali_codecompose_2023,
	title = {{CodeCompose}: A Large-Scale Industrial Deployment of {AI}-assisted Code Authoring},
	url = {http://arxiv.org/abs/2305.12050},
	shorttitle = {{CodeCompose}},
	abstract = {The rise of large language models ({LLMs}) has unlocked various applications of this technology in software development. In particular, generative {LLMs} have been shown to effectively power {AI}-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present {CodeCompose}, an {AI}-assisted code authoring tool developed and deployed at Meta internally. {CodeCompose} is based on the {InCoder} {LLM} that merges generative capabilities with bi-directionality. We have scaled up {CodeCompose} to serve tens of thousands of developers at Meta, across 10+ programming languages and several coding surfaces. We discuss unique challenges in terms of user experience and metrics that arise when deploying such tools in large-scale industrial settings. We present our experience in making design decisions about the model and system architecture for {CodeCompose} that addresses these challenges. Finally, we present metrics from our large-scale deployment of {CodeCompose} that shows its impact on Meta's internal code authoring experience over a 15-day time window, where 4.5 million suggestions were made by {CodeCompose}. Quantitative metrics reveal that (i) {CodeCompose} has an acceptance rate of 22\% across several languages, and (ii) 8\% of the code typed by users of {CodeCompose} is through accepting code suggestions from {CodeCompose}. Qualitative feedback indicates an overwhelming 91.5\% positive reception for {CodeCompose}. In addition to assisting with code authoring, {CodeCompose} is also introducing other positive side effects such as encouraging developers to generate more in-code documentation, helping them with the discovery of new {APIs}, etc.},
	number = {{arXiv}:2305.12050},
	publisher = {{arXiv}},
	author = {Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan},
	urldate = {2023-06-10},
	date = {2023-05-19},
	eprinttype = {arxiv},
	eprint = {2305.12050 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208778/Murali 等 - 2023 - CodeCompose A Large-Scale Industrial Deployment o.pdf:application/pdf;arXiv.org Snapshot:files/209823/2305.html:text/html},
}

@misc{chen_theoremqa_2023,
	title = {{TheoremQA}: A Theorem-driven Question Answering dataset},
	url = {http://arxiv.org/abs/2305.12524},
	shorttitle = {{TheoremQA}},
	abstract = {The recent {LLMs} like {GPT}-4 and {PaLM}-2 have made tremendous progress in solving fundamental math problems like {GSM}8K by achieving over 90\% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce {TheoremQA}, the first theorem-driven question-answering dataset designed to evaluate {AI} models' capabilities to apply theorems to solve challenging science problems. {TheoremQA} is curated by domain experts containing 800 high-quality questions covering 350 theorems (e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem, Elasticity Theorem, etc) from Math, Physics, {EE}\&{CS}, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that {GPT}-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51\% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15\%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of {TheoremQA}, we believe it can be used as a better benchmark to evaluate {LLMs}' capabilities to solve challenging science problems. The data and code are released in https://github.com/wenhuchen/{TheoremQA}.},
	number = {{arXiv}:2305.12524},
	publisher = {{arXiv}},
	author = {Chen, Wenhu and Yin, Ming and Ku, Max and Lu, Pan and Wan, Yixin and Ma, Xueguang and Xu, Jianyu and Wang, Xinyi and Xia, Tony},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.12524 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208745/Chen 等 - 2023 - TheoremQA A Theorem-driven Question Answering dat.pdf:application/pdf;arXiv.org Snapshot:files/208765/2305.html:text/html},
}

@misc{pan_fact-checking_2023,
	title = {Fact-Checking Complex Claims with Program-Guided Reasoning},
	url = {http://arxiv.org/abs/2305.12744},
	abstract = {Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking ({ProgramFC}), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate {ProgramFC} on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/{ProgramFC}.},
	number = {{arXiv}:2305.12744},
	publisher = {{arXiv}},
	author = {Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav},
	urldate = {2023-06-10},
	date = {2023-05-22},
	eprinttype = {arxiv},
	eprint = {2305.12744 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209160/Pan 等 - 2023 - Fact-Checking Complex Claims with Program-Guided R.pdf:application/pdf;arXiv.org Snapshot:files/209752/2305.html:text/html},
}

@misc{zhao_understanding_2023,
	title = {Understanding Programs by Exploiting (Fuzzing) Test Cases},
	url = {http://arxiv.org/abs/2305.13592},
	abstract = {Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models ({LLMs}) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training {LLMs} on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of most part of the code, we resort to fuzz testing and propose fuzz tuning to boost the performance of program understanding and code representation learning, given a pre-trained {LLM}. The effectiveness of the proposed method is verified on two program understanding tasks including code clone detection and code classification, and it outperforms current state-of-the-arts by large margins. Code is available at https://github.com/rabbitjy/{FuzzTuning}.},
	number = {{arXiv}:2305.13592},
	publisher = {{arXiv}},
	author = {Zhao, Jianyu and Rong, Yuyang and Guo, Yiwen and He, Yifeng and Chen, Hao},
	urldate = {2023-06-10},
	date = {2023-05-22},
	eprinttype = {arxiv},
	eprint = {2305.13592 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208776/Zhao 等 - 2023 - Understanding Programs by Exploiting (Fuzzing) Tes.pdf:application/pdf;arXiv.org Snapshot:files/209180/2305.html:text/html},
}

@misc{zhu_pad_2023,
	title = {{PaD}: Program-aided Distillation Specializes Large Models in Reasoning},
	url = {http://arxiv.org/abs/2305.13888},
	shorttitle = {{PaD}},
	abstract = {While Large Language Models ({LLMs}) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on {LLMs}, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from {LLMs} contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation ({PaD}), which distills {LLMs} to obtain specialized small models in reasoning tasks. In {PaD}, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the {GSM}8K benchmark, a 0.06B model using {PaD} can not only outperform certain {LLMs} (e.g., {LLaMA}), but also achieves a 10\% improvement over baselines with a significantly smaller scale of parameters and data. Data pruning analysis reveals that {PaD} possesses higher training efficiency.},
	number = {{arXiv}:2305.13888},
	publisher = {{arXiv}},
	author = {Zhu, Xuekai and Qi, Biqing and Zhang, Kaiyan and Long, Xingwei and Zhou, Bowen},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.13888 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208840/Zhu 等 - 2023 - PaD Program-aided Distillation Specializes Large .pdf:application/pdf;arXiv.org Snapshot:files/208854/2305.html:text/html},
}

@misc{chang_chipgpt_2023,
	title = {{ChipGPT}: How far are we from natural language hardware design},
	url = {http://arxiv.org/abs/2305.14019},
	shorttitle = {{ChipGPT}},
	abstract = {As large language models ({LLMs}) like {ChatGPT} exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by {LLMs}, this work attempts to demonstrate an automated design environment that explores {LLMs} to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on {LLMs} without retraining or finetuning. At first, the demo, {ChipGPT}, begins by generating prompts for the {LLM}, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, {ChipGPT} will search through this space to select the optimal design under the target metrics. The evaluation sheds some light on whether {LLMs} can generate correct and complete hardware logic designs described by natural language for some specifications. It is shown that {ChipGPT} improves programmability, and controllability, and shows broader design optimization space compared to prior work and native {LLMs} alone.},
	number = {{arXiv}:2305.14019},
	publisher = {{arXiv}},
	author = {Chang, Kaiyan},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2305.14019 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:files/209454/Chang - 2023 - ChipGPT How far are we from natural language hard.pdf:application/pdf;arXiv.org Snapshot:files/209751/2305.html:text/html},
}

@misc{qian_creator_2023,
	title = {{CREATOR}: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation},
	url = {http://arxiv.org/abs/2305.14318},
	shorttitle = {{CREATOR}},
	abstract = {Large Language Models ({LLMs}) have demonstrated significant progress in utilizing external {APIs} as tools for various tasks. However, their tool-using ability is limited by the availability of suitable {APIs} and the instability of implicit reasoning, particularly when simultaneously engaging in reasoning about plans and actual calculations. To address these limitations, we propose {CREATOR}, a novel framework that empowers {LLMs} to create their own tools through documentation and code realization. {CREATOR} disentangles the {LLM}'s ability into two distinct phases: abstract tool creation and concrete decision execution, which results in improved {LLM} performance. We evaluate {CREATOR} on two established benchmarks: {MATH}, which consists of challenging math competition problems, and {TabMWP}, which includes diverse tabular contents for problem-solving. Remarkably, {CREATOR} significantly outperforms existing chain-of-thought ({CoT}), program-of-thought ({PoT}), and tool-using baselines on these two benchmarks. Additionally, we present a new dataset, Creation Challenge, comprising 2K diverse questions, to highlight the necessity and benefits of {LLMs}' tool creation ability in effectively addressing these problems. Furthermore, our research reveals that leveraging {LLMs} as tool creators facilitates knowledge transfer, and {LLMs} exhibit varying levels of tool creation abilities, enabling them to flexibly tackle diverse situations. Our study represents a promising avenue for maximizing the potential of {LLMs} and advancing toward truly intelligent and adaptable {AI} systems.},
	number = {{arXiv}:2305.14318},
	publisher = {{arXiv}},
	author = {Qian, Cheng and Han, Chi and Fung, Yi R. and Qin, Yujia and Liu, Zhiyuan and Ji, Heng},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14318 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209456/Qian 等 - 2023 - CREATOR Disentangling Abstract and Concrete Reaso.pdf:application/pdf;arXiv.org Snapshot:files/209457/2305.html:text/html},
}

@misc{zhang_algo_2023,
	title = {{ALGO}: Synthesizing Algorithmic Programs with Generated Oracle Verifiers},
	url = {http://arxiv.org/abs/2305.14591},
	shorttitle = {{ALGO}},
	abstract = {Large language models ({LLMs}) excel at implementing code from functionality descriptions, but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, {LLM}-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose {ALGO}, a framework that synthesizes Algorithmic programs with {LLM}-Generated Oracles to guide the creation and verify their correctness. {ALGO} first generates a probably correct but possibly slow reference oracle by prompting an {LLM} to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the algorithms synthesized. Our study shows that the {LLM}-generated oracles are correct for 88\% of the cases. With the oracles as verifiers, {ALGO} can be integrated with any existing code generation model in a model-agnostic manner to enhance its performance. Experiments show that when equipped with {ALGO}, we achieve an 8x better one-submission pass rate over the Codex model and a 2.6x better one-submission pass rate over {CodeT}, the current state-of-the-art model on {CodeContests}. We can also get 1.3x better pass rate over the {ChatGPT} Code Interpreter on unseen problems.},
	number = {{arXiv}:2305.14591},
	publisher = {{arXiv}},
	author = {Zhang, Kexun and Wang, Danqing and Xia, Jingtao and Wang, William Yang and Li, Lei},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14591 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209458/Zhang 等 - 2023 - ALGO Synthesizing Algorithmic Programs with Gener.pdf:application/pdf;arXiv.org Snapshot:files/209844/2305.html:text/html},
}

@misc{zhang_saga_2023,
	title = {{SAGA}: Summarization-Guided Assert Statement Generation},
	url = {http://arxiv.org/abs/2305.14808},
	shorttitle = {{SAGA}},
	abstract = {Generating meaningful assert statements is one of the key challenges in automated test case generation, which requires understanding the intended functionality of the tested code. Recently, deep learning-based models have shown promise in improving the performance of assert statement generation. However, existing models only rely on the test prefixes along with their corresponding focal methods, yet ignore the developer-written summarization. Based on our observations, the summarization contents usually express the intended program behavior or contain parameters that will appear directly in the assert statement. Such information will help existing models address their current inability to accurately predict assert statements. This paper presents a novel summarization-guided approach for automatically generating assert statements. To derive generic representations for natural language (i.e., summarization) and programming language (i.e., test prefixes and focal methods), we leverage a pre-trained language model as the reference architecture and fine-tune it on the task of assert statement generation. To the best of our knowledge, the proposed approach makes the first attempt to leverage the summarization of focal methods as the guidance for making the generated assert statements more accurate. We demonstrate the effectiveness of our approach on two real-world datasets when compared with state-of-the-art models.},
	number = {{arXiv}:2305.14808},
	publisher = {{arXiv}},
	author = {Zhang, Yuwei and Jin, Zhi and Wang, Zejun and Xing, Ying and Li, Ge},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.14808 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209459/Zhang 等 - 2023 - SAGA Summarization-Guided Assert Statement Genera.pdf:application/pdf;arXiv.org Snapshot:files/209461/2305.html:text/html},
}

@misc{jansen_words_2023,
	title = {From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions},
	url = {http://arxiv.org/abs/2305.14874},
	shorttitle = {From Words to Wires},
	abstract = {In this work, we show that contemporary language models have a previously unknown skill -- the capacity for electronic circuit design from high-level textual descriptions, akin to code generation. We introduce two benchmarks: Pins100, assessing model knowledge of electrical components, and Micro25, evaluating a model's capability to design common microcontroller circuits and code in the Arduino ecosystem that involve input, output, sensors, motors, protocols, and logic -- with models such as {GPT}-4 and Claude-V1 achieving between 60\% to 96\% Pass@1 on generating full devices. We include six case studies of using language models as a design assistant for moderately complex devices, such as a radiation-powered random number generator, an emoji keyboard, a visible spectrometer, and several assistive devices, while offering a qualitative analysis performance, outlining evaluation challenges, and suggesting areas of development to improve complex circuit design and practical utility. With this work, we aim to spur research at the juncture of natural language processing and electronic design.},
	number = {{arXiv}:2305.14874},
	publisher = {{arXiv}},
	author = {Jansen, Peter},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.14874 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209484/Jansen - 2023 - From Words to Wires Generating Functioning Electr.pdf:application/pdf;arXiv.org Snapshot:files/209488/2305.html:text/html},
}

@misc{zhong_self-evolution_2023,
	title = {Self-Evolution Learning for Discriminative Language Model Pretraining},
	url = {http://arxiv.org/abs/2305.15275},
	abstract = {Masked language modeling, widely used in discriminative language model (e.g., {BERT}) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning ({SE}), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. {SE} focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our {SE} brings consistent and significant improvements (+1.43{\textasciitilde}2.12 average scores) upon different {PLMs}. In-depth analyses demonstrate that {SE} improves linguistic knowledge learning and generalization.},
	number = {{arXiv}:2305.15275},
	publisher = {{arXiv}},
	author = {Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15275 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209173/Zhong 等 - 2023 - Self-Evolution Learning for Discriminative Languag.pdf:application/pdf;arXiv.org Snapshot:files/209257/2305.html:text/html},
}

@misc{cho_visual_2023,
	title = {Visual Programming for Text-to-Image Generation and Evaluation},
	url = {http://arxiv.org/abs/2305.15328},
	abstract = {As large language models have demonstrated impressive performance in many domains, recent works have adopted language models ({LMs}) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping {LMs} with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce {VPGen}, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an {LM} to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained {LMs}, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our {VPGen} has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce {VPEval}, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, {VPEval} produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows {VPEval} provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope our work encourages future progress on interpretable/explainable generation and evaluation for T2I models. Website: https://vp-t2i.github.io},
	number = {{arXiv}:2305.15328},
	publisher = {{arXiv}},
	author = {Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15328 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209491/Cho 等 - 2023 - Visual Programming for Text-to-Image Generation an.pdf:application/pdf;arXiv.org Snapshot:files/209497/2305.html:text/html},
}

@misc{cheng_is_2023,
	title = {Is {GPT}-4 a Good Data Analyst?},
	url = {http://arxiv.org/abs/2305.15038},
	abstract = {As large language models ({LLMs}) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by {AI}. This controversial topic has drawn a lot of attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of "is {GPT}-4 a good data analyst?" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard {GPT}-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for {GPT}-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and {GPT}-4. Experimental results show that {GPT}-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before we reach the conclusion that {GPT}-4 can replace data analysts.},
	number = {{arXiv}:2305.15038},
	publisher = {{arXiv}},
	author = {Cheng, Liying and Li, Xingxuan and Bing, Lidong},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15038 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209161/Cheng 等 - 2023 - Is GPT-4 a Good Data Analyst.pdf:application/pdf;arXiv.org Snapshot:files/209251/2305.html:text/html},
}

@misc{wang_measuring_2023,
	title = {Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-{API} Semantic Parsing},
	url = {http://arxiv.org/abs/2305.15338},
	abstract = {In executable task-oriented semantic parsing, the system aims to translate users' utterances in natural language to machine-interpretable programs ({API} calls) that can be executed according to pre-defined {API} specifications. With the popularity of Large Language Models ({LLMs}), in-context learning offers a strong baseline for such scenarios, especially in data-limited regimes. However, {LLMs} are known to hallucinate and therefore pose a formidable challenge in constraining generated content. Thus, it remains uncertain if {LLMs} can effectively perform task-oriented utterance-to-{API} generation where respecting {API}'s structural and task-specific constraints is crucial. In this work, we seek to measure, analyze and mitigate such constraints violations. First, we identify the categories of various constraints in obtaining {API}-semantics from task-oriented utterances, and define fine-grained metrics that complement traditional ones. Second, we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art {LLMs}, which motivates us to investigate two mitigation strategies: Semantic-Retrieval of Demonstrations ({SRD}) and {API}-aware Constrained Decoding ({API}-{CD}). Our experiments show that these strategies are effective at reducing constraints violations and improving the quality of the generated {API} calls, but require careful consideration given their implementation complexity and latency.},
	number = {{arXiv}:2305.15338},
	publisher = {{arXiv}},
	author = {Wang, Shufan and Jean, Sebastien and Sengupta, Sailik and Gung, James and Pappas, Nikolaos and Zhang, Yi},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15338 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208738/Wang 等 - 2023 - Measuring and Mitigating Constraint Violations of .pdf:application/pdf;arXiv.org Snapshot:files/208759/2305.html:text/html},
}

@misc{miceli-barone_larger_2023,
	title = {The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python},
	url = {http://arxiv.org/abs/2305.15507},
	shorttitle = {The Larger They Are, the Harder They Fail},
	abstract = {Large Language Models ({LLMs}) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that {LLMs} not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, {LLMs} still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.},
	number = {{arXiv}:2305.15507},
	publisher = {{arXiv}},
	author = {Miceli-Barone, Antonio Valerio and Barez, Fazl and Konstas, Ioannis and Cohen, Shay B.},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15507 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209163/Miceli-Barone 等 - 2023 - The Larger They Are, the Harder They Fail Languag.pdf:application/pdf;arXiv.org Snapshot:files/209846/2305.html:text/html},
}

@misc{ciborowska_too_2023,
	title = {Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization},
	url = {http://arxiv.org/abs/2305.16430},
	shorttitle = {Too Few Bug Reports?},
	abstract = {Modern Deep Learning ({DL}) architectures based on transformers (e.g., {BERT}, {RoBERTa}) are exhibiting performance improvements across a number of natural language tasks. While such {DL} models have shown tremendous potential for use in software engineering applications, they are often hampered by insufficient training data. Particularly constrained are applications that require project-specific data, such as bug localization, which aims at recommending code to fix a newly submitted bug report. Deep learning models for bug localization require a substantial training set of fixed bug reports, which are at a limited quantity even in popular and actively developed software projects. In this paper, we examine the effect of using synthetic training data on transformer-based {DL} models that perform a more complex variant of bug localization, which has the goal of retrieving bug-inducing changesets for each bug report. To generate high-quality synthetic data, we propose novel data augmentation operators that act on different constituent components of bug reports. We also describe a data balancing strategy that aims to create a corpus of augmented bug reports that better reflects the entire source code base, because existing bug reports used as training data usually reference a small part of the code base.},
	number = {{arXiv}:2305.16430},
	publisher = {{arXiv}},
	author = {Ciborowska, Agnieszka and Damevski, Kostadin},
	urldate = {2023-06-10},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2305.16430 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209172/Ciborowska 和 Damevski - 2023 - Too Few Bug Reports Exploring Data Augmentation f.pdf:application/pdf;arXiv.org Snapshot:files/209255/2305.html:text/html},
}

@misc{kannan_green_2023,
	title = {Green Runner: A tool for efficient model selection from model repositories},
	url = {http://arxiv.org/abs/2305.16849},
	shorttitle = {Green Runner},
	abstract = {Deep learning models have become essential in software engineering, enabling intelligent features like image captioning and document generation. However, their popularity raises concerns about environmental impact and inefficient model selection. This paper introduces {GreenRunnerGPT}, a novel tool for efficiently selecting deep learning models based on specific use cases. It employs a large language model to suggest weights for quality indicators, optimizing resource utilization. The tool utilizes a multi-armed bandit framework to evaluate models against target datasets, considering tradeoffs. We demonstrate that {GreenRunnerGPT} is able to identify a model suited to a target use case without wasteful computations that would occur under a brute-force approach to model selection.},
	number = {{arXiv}:2305.16849},
	publisher = {{arXiv}},
	author = {Kannan, Jai and Barnett, Scott and Simmons, Anj and Selvi, Taylan and Cruz, Luis},
	urldate = {2023-06-10},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2305.16849 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209463/Kannan 等 - 2023 - Green Runner A tool for efficient model selection.pdf:application/pdf;arXiv.org Snapshot:files/209466/2305.html:text/html},
}

@misc{perkins_game_2023,
	title = {Game of Tones: Faculty detection of {GPT}-4 generated content in university assessments},
	url = {http://arxiv.org/abs/2305.18081},
	shorttitle = {Game of Tones},
	abstract = {This study explores the robustness of university assessments against the use of Open {AI}'s Generative Pre-Trained Transformer 4 ({GPT}-4) generated content and evaluates the ability of academic staff to detect its use when supported by the Turnitin Artificial Intelligence ({AI}) detection tool. The research involved twenty-two {GPT}-4 generated submissions being created and included in the assessment process to be marked by fifteen different faculty members. The study reveals that although the detection tool identified 91\% of the experimental submissions as containing some {AI}-generated content, the total detected content was only 54.8\%. This suggests that the use of adversarial techniques regarding prompt engineering is an effective method in evading {AI} detection tools and highlights that improvements to {AI} detection software are needed. Using the Turnitin {AI} detect tool, faculty reported 54.5\% of the experimental submissions to the academic misconduct process, suggesting the need for increased awareness and training into these tools. Genuine submissions received a mean score of 54.4, whereas {AI}-generated content scored 52.3, indicating the comparable performance of {GPT}-4 in real-life situations. Recommendations include adjusting assessment strategies to make them more resistant to the use of {AI} tools, using {AI}-inclusive assessment where possible, and providing comprehensive training programs for faculty and students. This research contributes to understanding the relationship between {AI}-generated content and academic assessment, urging further investigation to preserve academic integrity.},
	number = {{arXiv}:2305.18081},
	publisher = {{arXiv}},
	author = {Perkins, Mike and Roe, Jasper and Postma, Darius and {McGaughran}, James and Hickerson, Don},
	urldate = {2023-06-10},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2305.18081 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, K.4},
	file = {arXiv Fulltext PDF:files/209462/Perkins 等 - 2023 - Game of Tones Faculty detection of GPT-4 generate.pdf:application/pdf;arXiv.org Snapshot:files/209465/2305.html:text/html},
}

@misc{jie_leveraging_2023,
	title = {Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning},
	url = {http://arxiv.org/abs/2305.18170},
	abstract = {Chain-of-thought ({CoT}) prompting with large language models has proven effective in numerous natural language processing tasks, but designing prompts that generalize well to diverse problem types can be challenging, especially in the context of math word problem ({MWP}) solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but {CoT} annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in a few-shot prompting scenario: dynamic program prompting and program distillation. Our approach is largely inspired by Gao et al., (2022), where they proposed to replace the {CoT} with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in {MWP} solving. Our dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data. Our experiments on three standard {MWP} datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning. Our results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned small models in {MWP} solving},
	number = {{arXiv}:2305.18170},
	publisher = {{arXiv}},
	author = {Jie, Zhanming and Lu, Wei},
	urldate = {2023-06-10},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2305.18170 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209460/Jie 和 Lu - 2023 - Leveraging Training Data in Few-Shot Prompting for.pdf:application/pdf;arXiv.org Snapshot:files/209746/2305.html:text/html},
}

@misc{jain_tuning_2023,
	title = {Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback},
	url = {http://arxiv.org/abs/2305.18341},
	abstract = {Large Language Models ({LLMs}) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called {RLCF}, that further trains a pre-trained {LLM} using feedback from a code compiler. {RLCF} views the {LLM} as an {RL} agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different {LLM} on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. {RLCF} is model- and language-agnostic. We empirically evaluate it on the {MBJP} and {MathQA} tasks for Java. Our experiments show that {RLCF} significantly raises the odds that an {LLM}-generated program compiles, is executable, and produces the right output on tests, often allowing {LLMs} to match the performance of 2x-8x larger {LLMs}.},
	number = {{arXiv}:2305.18341},
	publisher = {{arXiv}},
	author = {Jain, Abhinav and Adiole, Chima and Chaudhuri, Swarat and Reps, Thomas and Jermaine, Chris},
	urldate = {2023-06-10},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2305.18341 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209468/Jain 等 - 2023 - Tuning Models of Code with Compiler-Generated Rein.pdf:application/pdf;arXiv.org Snapshot:files/209472/2305.html:text/html},
}

@misc{laskar_systematic_2023,
	title = {A Systematic Study and Comprehensive Evaluation of {ChatGPT} on Benchmark Datasets},
	url = {http://arxiv.org/abs/2305.18486},
	abstract = {The development of large language models ({LLMs}) such as {ChatGPT} has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of {ChatGPT}'s performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate {ChatGPT} across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of {ChatGPT} in {NLP} benchmarks. In short, our study aims to validate the strengths and weaknesses of {ChatGPT} in various tasks and provide insights for future research using {LLMs}. We also report a new emergent ability to follow multi-query instructions that we mostly found in {ChatGPT} and other instruction-tuned models. Our extensive evaluation shows that even though {ChatGPT} is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of {ChatGPT}'s performance across diverse {NLP} tasks, this paper sets the stage for a targeted deployment of {ChatGPT}-like {LLMs} in real-world applications.},
	number = {{arXiv}:2305.18486},
	publisher = {{arXiv}},
	author = {Laskar, Md Tahmid Rahman and Bari, M. Saiful and Rahman, Mizanur and Bhuiyan, Md Amran Hossen and Joty, Shafiq and Huang, Jimmy Xiangji},
	urldate = {2023-06-10},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2305.18486 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209475/Laskar 等 - 2023 - A Systematic Study and Comprehensive Evaluation of.pdf:application/pdf;arXiv.org Snapshot:files/209681/2305.html:text/html},
}

@misc{wu_how_2023,
	title = {How Effective Are Neural Networks for Fixing Security Vulnerabilities},
	url = {http://arxiv.org/abs/2305.18607},
	doi = {10.1145/3597926.3598135},
	abstract = {Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models ({LLMs}) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair ({APR}) techniques that use deep learning ({DL}) models to automatically fix software bugs. This paper is the first to study and compare Java vulnerability repair capabilities of {LLMs} and {DL}-based {APR} models. The contributions include that we (1) apply and evaluate five {LLMs} (Codex, {CodeGen}, {CodeT}5, {PLBART} and {InCoder}), four fine-tuned {LLMs}, and four {DL}-based {APR} techniques on two real-world Java vulnerability benchmarks (Vul4J and {VJBench}), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark {VJBench}, and its transformed version {VJBench}-trans and (4) evaluate {LLMs} and {APR} techniques on the transformed vulnerabilities in {VJBench}-trans. Our findings include that (1) existing {LLMs} and {APR} models fix very few Java vulnerabilities. Codex fixes 10.2 (20.4\%), the most number of vulnerabilities. (2) Fine-tuning with general {APR} data improves {LLMs}' vulnerability-fixing capabilities. (3) Our new {VJBench} reveals that {LLMs} and {APR} models fail to fix many Common Weakness Enumeration ({CWE}) types, such as {CWE}-325 Missing cryptographic step and {CWE}-444 {HTTP} request smuggling. (4) Codex still fixes 8.3 transformed vulnerabilities, outperforming all the other {LLMs} and {APR} models on transformed vulnerabilities. The results call for innovations to enhance automated Java vulnerability repair such as creating larger vulnerability repair training data, tuning {LLMs} with such data, and applying code simplification transformation to facilitate vulnerability repair.},
	author = {Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
	urldate = {2023-06-10},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2305.18607 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/208770/Wu 等 - 2023 - How Effective Are Neural Networks for Fixing Secur.pdf:application/pdf;arXiv.org Snapshot:files/208788/2305.html:text/html},
}

@misc{rao_ai_2023,
	title = {{AI} for Low-Code for {AI}},
	url = {http://arxiv.org/abs/2305.20015},
	abstract = {Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In parallel, recent {AI}-powered tools such as Copilot and {ChatGPT} generate programs from natural language instructions. We argue that these modalities are complementary: tools like {ChatGPT} greatly reduce the need to memorize large {APIs} but still require their users to read (and modify) programs, whereas visual tools abstract away most or all programming but struggle to provide easy access to large {APIs}. At their intersection, we propose {LowCoder}, the first low-code tool for developing {AI} pipelines that supports both a visual programming interface ({LowCoder}\_VP) and an {AI}-powered natural language interface ({LowCoder}\_NL). We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of {AI} expertise with implementing four {ML} pipelines using {LowCoder}, replacing the {LowCoder}\_NL component with a simple keyword search in half the tasks. Overall, we find that {LowCoder} is especially useful for (i) Discoverability: using {LowCoder}\_NL, participants discovered new operators in 75\% of the tasks, compared to just 32.5\% and 27.5\% using web search or scrolling through options respectively in the keyword-search condition, and (ii) Iterative Composition: 82.5\% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that {AI} helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish. Overall, our work highlights the benefits of combining the power of {AI} with low-code programming.},
	number = {{arXiv}:2305.20015},
	publisher = {{arXiv}},
	author = {Rao, Nikitha and Tsay, Jason and Kate, Kiran and Hellendoorn, Vincent J. and Hirzel, Martin},
	urldate = {2023-06-10},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2305.20015 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209179/Rao 等 - 2023 - AI for Low-Code for AI.pdf:application/pdf;arXiv.org Snapshot:files/209772/2305.html:text/html},
}

@misc{narayanan_enhancing_2023,
	title = {Enhancing Programming {eTextbooks} with {ChatGPT} Generated Counterfactual-Thinking-Inspired Questions},
	url = {http://arxiv.org/abs/2306.00551},
	abstract = {Digital textbooks have become an integral part of everyday learning tasks. In this work, we consider the use of digital textbooks for programming classes. Generally, students struggle with utilizing textbooks on programming to the maximum, with a possible reason being that the example programs provided as illustration of concepts in these textbooks don't offer sufficient interactivity for students, and thereby not sufficiently motivating to explore or understand these programming examples better. In our work, we explore the idea of enhancing the navigability of intelligent textbooks with the use of ``counterfactual'' questions, to make students think critically about these programs and enhance possible program comprehension. Inspired from previous works on nudging students on counter factual thinking, we present the possibility to enhance digital textbooks with questions generated using {GPT}.},
	number = {{arXiv}:2306.00551},
	publisher = {{arXiv}},
	author = {Narayanan, Arun Balajiee Lekshmi and Hendrawan, Rully Agus and V, Venktesh},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2306.00551 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/208751/Narayanan 等 - 2023 - Enhancing Programming eTextbooks with ChatGPT Gene.pdf:application/pdf;arXiv.org Snapshot:files/208767/2306.html:text/html},
}

@misc{huang_ai_2023,
	title = {{AI} Chain on Large Language Model for Unsupervised Control Flow Graph Generation for Statically-Typed Partial Code},
	url = {http://arxiv.org/abs/2306.00757},
	abstract = {Control Flow Graphs ({CFGs}) are essential for visualizing, understanding and analyzing program behavior. For statically-typed programming language like Java, developers obtain {CFGs} by using bytecode-based methods for compilable code and Abstract Syntax Tree ({AST})-based methods for partially uncompilable code. However, explicit syntax errors during {AST} construction and implicit semantic errors caused by bad coding practices can lead to behavioral loss and deviation of {CFGs}.To address the issue, we propose a novel approach that leverages the error-tolerant and understanding ability of pre-trained Large Language Models ({LLMs}) to generate {CFGs}. Our approach involves a Chain of Thought ({CoT}) with four steps: structure hierarchy extraction, nested code block extraction, {CFG} generation of nested code blocks, and fusion of all nested code blocks' {CFGs}. To address the limitations of the original {CoT}'s single-prompt approach (i.e., completing all steps in a single generative pass), which can result in an ``epic'' prompt with hard-to-control behavior and error accumulation, we break down the {CoT} into an {AI} chain with explicit sub-steps. Each sub-step corresponds to a separate {AI}-unit, with an effective prompt assigned to each unit for interacting with {LLMs} to accomplish a specific purpose.Our experiments confirmed that our method outperforms existing {CFG} tools in terms of node and edge coverage, especially for incomplete or erroneous code. We also conducted an ablation experiment and confirmed the effectiveness of {AI} chain design principles: Hierarchical Task Breakdown, Unit Composition, and Mix of {AI} Units and Non-{AI} Units.Our work opens up new possibilities for building foundational software engineering tools based on {LLMs}, as opposed to traditional program analysis methods.},
	number = {{arXiv}:2306.00757},
	publisher = {{arXiv}},
	author = {Huang, Qing and Zou, Zhou and Xing, Zhenchang and Zuo, Zhenkang and Xu, Xiwei and Lu, Qinghua},
	urldate = {2023-06-10},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2306.00757 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209176/Huang 等 - 2023 - AI Chain on Large Language Model for Unsupervised .pdf:application/pdf;arXiv.org Snapshot:files/209260/2306.html:text/html},
}

@misc{sadik_analysis_2023,
	title = {Analysis of {ChatGPT} on Source Code},
	url = {http://arxiv.org/abs/2306.00597},
	abstract = {This paper explores the use of Large Language Models ({LLMs}) and in particular {ChatGPT} in programming, source code analysis, and code generation. {LLMs} and {ChatGPT} are built using machine learning and artificial intelligence techniques, and they offer several benefits to developers and programmers. While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely. The paper investigates the potential applications of {LLMs} and {ChatGPT} in various areas, such as code creation, code documentation, bug detection, refactoring, and more. The paper also suggests that the usage of {LLMs} and {ChatGPT} is expected to increase in the future as they offer unparalleled benefits to the programming community.},
	number = {{arXiv}:2306.00597},
	publisher = {{arXiv}},
	author = {Sadik, Ahmed R. and Ceravola, Antonello and Joublin, Frank and Patra, Jibesh},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2306.00597 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209471/Sadik 等 - 2023 - Analysis of ChatGPT on Source Code.pdf:application/pdf;arXiv.org Snapshot:files/209712/2306.html:text/html},
}

@misc{chan_transformer-based_2023,
	title = {Transformer-based Vulnerability Detection in Code at {EditTime}: Zero-shot, Few-shot, or Fine-tuning?},
	url = {http://arxiv.org/abs/2306.01754},
	shorttitle = {Transformer-based Vulnerability Detection in Code at {EditTime}},
	abstract = {Software vulnerabilities bear enterprises significant costs. Despite extensive efforts in research and development of software vulnerability detection methods, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build before attempting detection. This, unfortunately, introduces a long latency between the time a vulnerability is injected to the time it is removed, which can substantially increases the cost of fixing a vulnerability. We recognize that the current advances in machine learning can be used to detect vulnerable code patterns on syntactically incomplete code snippets as the developer is writing the code at {EditTime}. In this paper we present a practical system that leverages deep learning on a large-scale data set of vulnerable code patterns to learn complex manifestations of more than 250 vulnerability types and detect vulnerable code patterns at {EditTime}. We discuss zero-shot, few-shot, and fine-tuning approaches on state of the art pre-trained Large Language Models ({LLMs}). We show that in comparison with state of the art vulnerability detection models our approach improves the state of the art by 10\%. We also evaluate our approach to detect vulnerability in auto-generated code by code {LLMs}. Evaluation on a benchmark of high-risk code scenarios shows a reduction of up to 90\% vulnerability reduction.},
	number = {{arXiv}:2306.01754},
	publisher = {{arXiv}},
	author = {Chan, Aaron and Kharkar, Anant and Moghaddam, Roshanak Zilouchian and Mohylevskyy, Yevhen and Helyar, Alec and Kamal, Eslam and Elkamhawy, Mohamed and Sundaresan, Neel},
	urldate = {2023-06-10},
	date = {2023-05-22},
	eprinttype = {arxiv},
	eprint = {2306.01754 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209174/Chan 等 - 2023 - Transformer-based Vulnerability Detection in Code .pdf:application/pdf;arXiv.org Snapshot:files/209258/2306.html:text/html},
}

@misc{ahmed_majority_2023,
	title = {Majority Rule: better patching via Self-Consistency},
	url = {http://arxiv.org/abs/2306.00108},
	shorttitle = {Majority Rule},
	abstract = {Large Language models ({LLMs}) can be induced to solve non-trivial problems with "few-shot" prompts including illustrative problem-solution examples. Now if the few-shots also include "chain of thought" ({CoT}) explanations, which are of the form problem-explanation-solution, {LLMs} will generate a "explained" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the {LLM} is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant S-C (or even {CoT}) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the {MODIT} dataset; we also find evidence suggesting that the correct commit messages are helping the {LLM} learn to produce better patches.},
	number = {{arXiv}:2306.00108},
	publisher = {{arXiv}},
	author = {Ahmed, Toufique and Devanbu, Premkumar},
	urldate = {2023-06-10},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2306.00108 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209467/Ahmed 和 Devanbu - 2023 - Majority Rule better patching via Self-Consistenc.pdf:application/pdf;arXiv.org Snapshot:files/209469/2306.html:text/html},
}

@misc{shi_lambdabeam_2023,
	title = {{LambdaBeam}: Neural Program Search with Higher-Order Functions and Lambdas},
	url = {http://arxiv.org/abs/2306.02049},
	shorttitle = {{LambdaBeam}},
	abstract = {Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called {LambdaBeam} that can construct arbitrary lambda functions that compose operations within a given {DSL}. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that {LambdaBeam} outperforms neural, symbolic, and {LLM}-based techniques in an integer list manipulation domain.},
	number = {{arXiv}:2306.02049},
	publisher = {{arXiv}},
	author = {Shi, Kensen and Dai, Hanjun and Li, Wen-Ding and Ellis, Kevin and Sutton, Charles},
	urldate = {2023-06-10},
	date = {2023-06-03},
	eprinttype = {arxiv},
	eprint = {2306.02049 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209178/Shi 等 - 2023 - LambdaBeam Neural Program Search with Higher-Orde.pdf:application/pdf;arXiv.org Snapshot:files/209262/2306.html:text/html},
}

@misc{xing_prompt_2023,
	title = {Prompt Sapper: {LLM}-Empowered Software Engineering Infrastructure for {AI}-Native Services},
	url = {http://arxiv.org/abs/2306.02230},
	shorttitle = {Prompt Sapper},
	abstract = {Foundation models, such as {GPT}-4, {DALL}-E have brought unprecedented {AI} "operating system" effect and new forms of human-{AI} interaction, sparking a wave of innovation in {AI}-native services, where natural language prompts serve as executable "code" directly (prompt as executable code), eliminating the need for programming language as an intermediary and opening up the door to personal {AI}. Prompt Sapper has emerged in response, committed to support the development of {AI}-native services by {AI} chain engineering. It creates a large language model ({LLM}) empowered software engineering infrastructure for authoring {AI} chains through human-{AI} collaborative intelligence, unleashing the {AI} innovation potential of every individual, and forging a future where everyone can be a master of {AI} innovation. This article will introduce the R{\textbackslash}\&D motivation behind Prompt Sapper, along with its corresponding {AI} chain engineering methodology and technical practices.},
	number = {{arXiv}:2306.02230},
	publisher = {{arXiv}},
	author = {Xing, Zhenchang and Huang, Qing and Cheng, Yu and Zhu, Liming and Lu, Qinghua and Xu, Xiwei},
	urldate = {2023-06-10},
	date = {2023-06-03},
	eprinttype = {arxiv},
	eprint = {2306.02230 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209478/Xing 等 - 2023 - Prompt Sapper LLM-Empowered Software Engineering .pdf:application/pdf;arXiv.org Snapshot:files/209483/2306.html:text/html},
}

@misc{chen_beyond_2023,
	title = {Beyond Generating Code: Evaluating {GPT} on a Data Visualization Course},
	url = {http://arxiv.org/abs/2306.02914},
	shorttitle = {Beyond Generating Code},
	abstract = {This paper presents an empirical evaluation of the performance of the Generative Pre-trained Transformer ({GPT}) model in Harvard's {CS}171 data visualization course. While previous studies have focused on {GPT}'s ability to generate code for visualizations, this study goes beyond code generation to evaluate {GPT}'s abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication. The evaluation utilized {GPT}-3.5 and {GPT}-4 to complete assignments of {CS}171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of {GPT}'s capabilities in completing border visualization tasks. Findings show that {GPT}-4 scored 80\% on quizzes and homework, and {TFs} could distinguish between {GPT}- and human-generated homework with 70\% accuracy. The study also demonstrates {GPT}'s potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication. The paper concludes by discussing the strengths and limitations of {GPT} in data visualization, potential avenues for incorporating {GPT} in broader visualization tasks, and the need to redesign visualization education.},
	number = {{arXiv}:2306.02914},
	publisher = {{arXiv}},
	author = {Chen, Zhutian and Zhang, Chenyang and Wang, Qianwen and Troidl, Jakob and Warchol, Simon and Beyer, Johanna and Gehlenborg, Nils and Pfister, Hanspeter},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.02914 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:files/209473/Chen 等 - 2023 - Beyond Generating Code Evaluating GPT on a Data V.pdf:application/pdf;arXiv.org Snapshot:files/209477/2306.html:text/html},
}

@misc{liu_repobench_2023,
	title = {{RepoBench}: Benchmarking Repository-Level Code Auto-Completion Systems},
	url = {http://arxiv.org/abs/2306.03091},
	shorttitle = {{RepoBench}},
	abstract = {Large Language Models ({LLMs}) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce {RepoBench}, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. {RepoBench} consists of three interconnected evaluation tasks: {RepoBench}-R (Retrieval), {RepoBench}-C (Code Completion), and {RepoBench}-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. {RepoBench} aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. {RepoBench} is publicly available at https://github.com/Leolty/repobench.},
	number = {{arXiv}:2306.03091},
	publisher = {{arXiv}},
	author = {Liu, Tianyang and Xu, Canwen and {McAuley}, Julian},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03091 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209474/Liu 等 - 2023 - RepoBench Benchmarking Repository-Level Code Auto.pdf:application/pdf;arXiv.org Snapshot:files/209479/2306.html:text/html},
}

@misc{schroder_autoscrum_2023,
	title = {{AutoScrum}: Automating Project Planning Using Large Language Models},
	url = {http://arxiv.org/abs/2306.03197},
	shorttitle = {{AutoScrum}},
	abstract = {Recent advancements in the field of large language models have made it possible to use language models for advanced reasoning. In this paper we leverage this ability for designing complex project plans based only on knowing the current state and the desired state. Two approaches are demonstrated - a scrum based approach and a shortcut plan approach. The scrum based approach executes an automated process of requirements gathering, user story mapping, feature identification, task decomposition and finally generates questions and search terms for seeking out domain specific information to assist with task completion. The shortcut approach looks at most recent snapshot of the current and desired state and generates the next most reasonable task to do in order to get to the desired state as quickly as possible. In this paper we automate everything using a novel concept of "Language Programs". These are programs written in natural language designed to process input data through the language model. Guidance language is used for all {LLM} programs. All demo source code for this paper is available at https://github.com/autoscrum/autoscrum},
	number = {{arXiv}:2306.03197},
	publisher = {{arXiv}},
	author = {Schroder, Martin},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03197 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209177/Schroder - 2023 - AutoScrum Automating Project Planning Using Large.pdf:application/pdf;arXiv.org Snapshot:files/209261/2306.html:text/html},
}

@misc{wang_towards_2023,
	title = {Towards Adapting Computer Science Courses to {AI} Assistants' Capabilities},
	url = {http://arxiv.org/abs/2306.03289},
	abstract = {The use of {AI} assistants, along with the challenges they present, has sparked significant debate within the community of computer science education. While these tools demonstrate the potential to support students' learning and instructors' teaching, they also raise concerns about enabling unethical uses by students. Previous research has suggested various strategies aimed at addressing these issues. However, they concentrate on the introductory programming courses and focus on one specific type of problem. The present research evaluated the performance of {ChatGPT}, a state-of-the-art {AI} assistant, at solving 187 problems spanning three distinct types that were collected from six undergraduate computer science. The selected courses covered different topics and targeted different program levels. We then explored methods to modify these problems to adapt them to {ChatGPT}'s capabilities to reduce potential misuse by students. Finally, we conducted semi-structured interviews with 11 computer science instructors. The aim was to gather their opinions on our problem modification methods, understand their perspectives on the impact of {AI} assistants on computer science education, and learn their strategies for adapting their courses to leverage these {AI} capabilities for educational improvement. The results revealed issues ranging from academic fairness to long-term impact on students' mental models. From our results, we derived design implications and recommended tools to help instructors design and create future course material that could more effectively adapt to {AI} assistants' capabilities.},
	number = {{arXiv}:2306.03289},
	publisher = {{arXiv}},
	author = {Wang, Tianjia and Vargas-Diaz, Daniel and Brown, Chris and Chen, Yan},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03289 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209476/Wang 等 - 2023 - Towards Adapting Computer Science Courses to AI As.pdf:application/pdf;arXiv.org Snapshot:files/209481/2306.html:text/html},
}

@misc{dinh_large_2023,
	title = {Large Language Models of Code Fail at Completing Code with Potential Bugs},
	url = {http://arxiv.org/abs/2306.03438},
	abstract = {Large language models of code (Code-{LLMs}) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-{HumanEval}) and one with realistic bugs derived from user submissions to coding problems (buggy-{FixEval}). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-{LLMs}. For instance, the passing rates of {CodeGen}-2B-mono on test cases of buggy-{HumanEval} drop more than 50\% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a large gap in post-mitigation performance.},
	number = {{arXiv}:2306.03438},
	publisher = {{arXiv}},
	author = {Dinh, Tuan and Zhao, Jinman and Tan, Samson and Negrinho, Renato and Lausen, Leonard and Zha, Sheng and Karypis, George},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2306.03438 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209482/Dinh 等 - 2023 - Large Language Models of Code Fail at Completing C.pdf:application/pdf;arXiv.org Snapshot:files/209487/2306.html:text/html},
}

@misc{gandhi_natural_2023,
	title = {Natural Language Commanding via Program Synthesis},
	url = {http://arxiv.org/abs/2306.03460},
	abstract = {We present Semantic Interpreter, a natural language-friendly {AI} system for productivity software such as Microsoft Office that leverages large language models ({LLMs}) to execute user intent across application features. While {LLMs} are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language ({ODSL}), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with {LLMs} for program synthesis, translating natural language user utterances to {ODSL} programs that can be transpiled to application {APIs} and then executed. We focus our discussion primarily on a research exploration for Microsoft {PowerPoint}.},
	number = {{arXiv}:2306.03460},
	publisher = {{arXiv}},
	author = {Gandhi, Apurva and Nguyen, Thong Q. and Jiao, Huitian and Steen, Robert and Bhatawdekar, Ameya},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2306.03460 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209486/Gandhi 等 - 2023 - Natural Language Commanding via Program Synthesis.pdf:application/pdf;arXiv.org Snapshot:files/209493/2306.html:text/html},
}

@misc{ling_deductive_2023,
	title = {Deductive Verification of Chain-of-Thought Reasoning},
	url = {http://arxiv.org/abs/2306.03872},
	abstract = {Large Language Models ({LLMs}) significantly benefit from Chain-of-Thought ({CoT}) prompting in performing various reasoning tasks. While {CoT} allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like {ChatGPT}. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify\_cot.},
	number = {{arXiv}:2306.03872},
	publisher = {{arXiv}},
	author = {Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2306.03872 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209480/Ling 等 - 2023 - Deductive Verification of Chain-of-Thought Reasoni.pdf:application/pdf;arXiv.org Snapshot:files/209691/2306.html:text/html},
}

@misc{babe_studenteval_2023,
	title = {{StudentEval}: A Benchmark of Student-Written Prompts for Large Language Models of Code},
	url = {http://arxiv.org/abs/2306.04556},
	shorttitle = {{StudentEval}},
	abstract = {Code {LLMs} are being rapidly deployed and there is evidence that they can make professional programmers more productive. Current benchmarks for code generation measure whether models generate correct programs given an expert prompt. In this paper, we present a new benchmark containing multiple prompts per problem, written by a specific population of non-expert prompters: beginning programmers. {StudentEval} contains 1,749 prompts for 48 problems, written by 80 students who have only completed one semester of Python programming. Our students wrote these prompts while working interactively with a Code {LLM}, and we observed very mixed success rates. We use {StudentEval} to evaluate 5 Code {LLMs} and find that {StudentEval} is a better discriminator of model performance than existing benchmarks. We analyze the prompts and find significant variation in students' prompting techniques. We also find that nondeterministic {LLM} sampling could mislead students into thinking that their prompts are more (or less) effective than they actually are, which has implications for how to teach with Code {LLMs}.},
	number = {{arXiv}:2306.04556},
	publisher = {{arXiv}},
	author = {Babe, Hannah {McLean} and Nguyen, Sydney and Zi, Yangtian and Guha, Arjun and Feldman, Molly Q. and Anderson, Carolyn Jane},
	urldate = {2023-06-10},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2306.04556 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/208773/Babe 等 - 2023 - StudentEval A Benchmark of Student-Written Prompt.pdf:application/pdf;arXiv.org Snapshot:files/208790/2306.html:text/html},
}

@misc{talebirad_multi-agent_2023,
	title = {Multi-Agent Collaboration: Harnessing the Power of Intelligent {LLM} Agents},
	url = {http://arxiv.org/abs/2306.03314},
	shorttitle = {Multi-Agent Collaboration},
	abstract = {In this paper, we present a novel framework for enhancing the capabilities of large language models ({LLMs}) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence ({AGI}), specifically focusing on the Auto-{GPT} and {BabyAGI} models. We also examine the "Gorilla" model, which integrates external {APIs} into the {LLM}. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of {LLMs} through collaboration and knowledge exchange among intelligent agents.},
	number = {{arXiv}:2306.03314},
	publisher = {{arXiv}},
	author = {Talebirad, Yashar and Nadiri, Amirhossein},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03314 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:files/209175/Talebirad 和 Nadiri - 2023 - Multi-Agent Collaboration Harnessing the Power of.pdf:application/pdf;arXiv.org Snapshot:files/209722/2306.html:text/html},
}

@misc{ding_static_2023,
	title = {A Static Evaluation of Code Completion by Large Language Models},
	url = {http://arxiv.org/abs/2306.03203},
	abstract = {Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.},
	number = {{arXiv}:2306.03203},
	publisher = {{arXiv}},
	author = {Ding, Hantian and Kumar, Varun and Tian, Yuchen and Wang, Zijian and Kwiatkowski, Rob and Li, Xiaopeng and Ramanathan, Murali Krishna and Ray, Baishakhi and Bhatia, Parminder and Sengupta, Sudipta and Roth, Dan and Xiang, Bing},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03203 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208769/Ding 等 - 2023 - A Static Evaluation of Code Completion by Large La.pdf:application/pdf;arXiv.org Snapshot:files/209696/2306.html:text/html},
}

@misc{mukherjee_stack_2023,
	title = {Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models},
	url = {http://arxiv.org/abs/2306.03268},
	shorttitle = {Stack Over-Flowing with Results},
	abstract = {Large pre-trained neural language models have brought immense progress to both {NLP} and software engineering. Models in {OpenAI}'s {GPT} series now dwarf Google's {BERT} and Meta's {RoBERTa}, which previously set new benchmarks on a wide range of {NLP} applications. These models are trained on massive corpora of heterogeneous data from web crawls, which enables them to learn general language patterns and semantic relationships. However, the largest models are both expensive to train and deploy and are often closed-source, so we lack access to their data and design decisions. We argue that this trend towards large, general-purpose models should be complemented with single-purpose, more modestly sized pre-trained models. In this work, we take {StackOverflow} ({SO}) as a domain example in which large volumes of rich aligned code and text data is available. We adopt standard practices for pre-training large language models, including using a very large context size (2,048 tokens), batch size (0.5M tokens) and training set (27B tokens), coupled with a powerful toolkit (Megatron-{LM}), to train two models: {SOBertBase}, with 109M parameters, and {SOBertLarge} with 762M parameters, at a budget of just \${\textbackslash}\$187\$ and \${\textbackslash}\$800\$ each. We compare the performance of our models with both the previous {SOTA} model trained on {SO} data exclusively as well general-purpose {BERT} models and {OpenAI}'s {ChatGPT} on four {SO}-specific downstream tasks - question quality prediction, closed question prediction, named entity recognition and obsoletion prediction (a new task we introduce). Not only do our models consistently outperform all baselines, the smaller model is often sufficient for strong results. Both models are released to the public. These results demonstrate that pre-training both extensively and properly on in-domain data can yield a powerful and affordable alternative to leveraging closed-source general-purpose models.},
	number = {{arXiv}:2306.03268},
	publisher = {{arXiv}},
	author = {Mukherjee, Manisha and Hellendoorn, Vincent J.},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03268 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209492/Mukherjee 和 Hellendoorn - 2023 - Stack Over-Flowing with Results The Case for Doma.pdf:application/pdf;arXiv.org Snapshot:files/209721/2306.html:text/html},
}

@misc{lew_sequential_2023,
	title = {Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs},
	url = {http://arxiv.org/abs/2306.03081},
	abstract = {Even after fine-tuning and reinforcement learning, large language models ({LLMs}) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of {LLMs}, called sequential Monte Carlo ({SMC}) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, {SMC} can steer {LLMs} to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with {SMC} steering, we present a probabilistic programming library, {LLaMPPL} (https://github.com/probcomp/{LLaMPPL}), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of {LLaMA}-family Transformers.},
	number = {{arXiv}:2306.03081},
	publisher = {{arXiv}},
	author = {Lew, Alexander K. and Zhi-Xuan, Tan and Grand, Gabriel and Mansinghka, Vikash K.},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03081 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Programming Languages, Statistics - Computation},
	file = {arXiv Fulltext PDF:files/209485/Lew 等 - 2023 - Sequential Monte Carlo Steering of Large Language .pdf:application/pdf;arXiv.org Snapshot:files/209808/2306.html:text/html},
}

@misc{xu_lmpa_2023,
	title = {{LmPa}: Improving Decompilation by Synergy of Large Language Model and Program Analysis},
	url = {http://arxiv.org/abs/2306.02546},
	shorttitle = {{LmPa}},
	abstract = {Decompilation aims to recover the source code form of a binary executable. It has many applications in security and software engineering such as malware analysis, vulnerability detection and code reuse. A prominent challenge in decompilation is to recover variable names. We propose a novel method that leverages the synergy of large language model ({LLM}) and program analysis. Language models encode rich multi-modal knowledge, but its limited input size prevents providing sufficient global context for name recovery. We propose to divide the task to many {LLM} queries and use program analysis to correlate and propagate the query results, which in turn improves the performance of {LLM} by providing additional contextual information. Our results show that 75\% of the recovered names are considered good by users and our technique outperforms the state-of-the-art technique by 16.5\% and 20.23\% in precision and recall, respectively.},
	number = {{arXiv}:2306.02546},
	publisher = {{arXiv}},
	author = {Xu, Xiangzhe and Zhang, Zhuo and Feng, Shiwei and Ye, Yapeng and Su, Zian and Jiang, Nan and Cheng, Siyuan and Tan, Lin and Zhang, Xiangyu},
	urldate = {2023-06-10},
	date = {2023-06-04},
	eprinttype = {arxiv},
	eprint = {2306.02546 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209489/Xu 等 - 2023 - LmPa Improving Decompilation by Synergy of Large .pdf:application/pdf;arXiv.org Snapshot:files/209770/2306.html:text/html},
}

@misc{jiang_selfevolve_2023,
	title = {{SelfEvolve}: A Code Evolution Framework via Large Language Models},
	url = {http://arxiv.org/abs/2306.02907},
	shorttitle = {{SelfEvolve}},
	abstract = {Large language models ({LLMs}) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment {LLMs} with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while {LLMs} show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called {\textbackslash}autoknow, that leverages {LLMs} as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, {\textbackslash}autoknow{\textasciitilde}obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, {\textbackslash}autoknow{\textasciitilde}asks {LLM} to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate {\textbackslash}autoknow{\textasciitilde}on three code generation datasets, including {DS}-1000 for data science code, {HumanEval} for software engineering code, and {TransCoder} for C++-to-Python translation. Our empirical experiments show that {\textbackslash}autoknow{\textasciitilde}outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of {\textbackslash}autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that {\textbackslash}autoknow{\textasciitilde}can be adapted to other more advanced models, such as {GPT}-4, and bring consistent efficacy improvement.},
	number = {{arXiv}:2306.02907},
	publisher = {{arXiv}},
	author = {Jiang, Shuyang and Wang, Yuhao and Wang, Yu},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.02907 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209181/Jiang 等 - 2023 - SelfEvolve A Code Evolution Framework via Large L.pdf:application/pdf;arXiv.org Snapshot:files/209263/2306.html:text/html},
}

@misc{bui_codetf_2023,
	title = {{CodeTF}: One-stop Transformer Library for State-of-the-art Code {LLM}},
	url = {http://arxiv.org/abs/2306.00029},
	shorttitle = {{CodeTF}},
	abstract = {Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models ({LLMs}), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present {CodeTF}, an open-source Transformer-based library for state-of-the-art Code {LLMs} and code intelligence. Following the principles of modular design and extensible framework, we design {CodeTF} with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code {LLM} models and popular code benchmarks, including a standardized interface to train and serve code {LLMs} efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope {CodeTF} is able to bridge the gap between machine learning/generative {AI} and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.},
	number = {{arXiv}:2306.00029},
	publisher = {{arXiv}},
	author = {Bui, Nghi D. Q. and Le, Hung and Wang, Yue and Li, Junnan and Gotmare, Akhilesh Deepak and Hoi, Steven C. H.},
	urldate = {2023-06-10},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2306.00029 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209490/Bui 等 - 2023 - CodeTF One-stop Transformer Library for State-of-.pdf:application/pdf;arXiv.org Snapshot:files/209496/2306.html:text/html},
}

@misc{darm_knowledge_2023,
	title = {Knowledge Base Question Answering for Space Debris Queries},
	url = {http://arxiv.org/abs/2305.19734},
	abstract = {Space agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge bases ({KB}) are an effective way of storing and accessing such information at scale. In this work we present a system, developed for the European Space Agency ({ESA}), that can answer complex natural language queries, to support engineers in accessing the information contained in a {KB} that models the orbital space debris environment. Our system is based on a pipeline which first generates a sequence of basic database operations, called a \%program sketch, from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by {GPT}-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data. Our code can be found at {\textbackslash}url\{https://github.com/{PaulDrm}/{DISCOSQA}\}.},
	number = {{arXiv}:2305.19734},
	publisher = {{arXiv}},
	author = {Darm, Paul and Miceli-Barone, Antonio Valerio and Cohen, Shay B. and Riccardi, Annalisa},
	urldate = {2023-06-10},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2305.19734 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7, Computer Science - Databases},
	file = {arXiv Fulltext PDF:files/208785/Darm 等 - 2023 - Knowledge Base Question Answering for Space Debris.pdf:application/pdf;arXiv.org Snapshot:files/209810/2305.html:text/html},
}

@misc{guo_what_2023,
	title = {What indeed can {GPT} models do in chemistry? A comprehensive benchmark on eight tasks},
	url = {http://arxiv.org/abs/2305.18365},
	shorttitle = {What indeed can {GPT} models do in chemistry?},
	abstract = {Large Language Models ({LLMs}) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of {LLMs} to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including {BBBP}, Tox21, {PubChem}, {USPTO}, and {ChEBI}, facilitating a broad exploration of the capacities of {LLMs} within the context of practical chemistry. Three {GPT} models ({GPT}-4, {GPT}-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. The key results of our investigation are 1) {GPT}-4 outperforms the other two models among the three evaluated; 2) {GPT} models exhibit less competitive performance in tasks demanding precise understanding of molecular {SMILES} representation, such as reaction prediction and retrosynthesis;3) {GPT} models demonstrate strong capabilities in text-related explanation tasks such as molecule captioning; and 4) {GPT} models exhibit comparable or better performance to classical machine learning models when applied to chemical problems that can be transformed into classification or ranking tasks, such as property prediction, and yield prediction.},
	number = {{arXiv}:2305.18365},
	publisher = {{arXiv}},
	author = {Guo, Taicheng and Guo, Kehan and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
	urldate = {2023-06-10},
	date = {2023-05-27},
	eprinttype = {arxiv},
	eprint = {2305.18365 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208852/Guo 等 - 2023 - What indeed can GPT models do in chemistry A comp.pdf:application/pdf;arXiv.org Snapshot:files/209200/2305.html:text/html},
}

@misc{huang_anpl_2023,
	title = {{ANPL}: Compiling Natural Programs with Interactive Decomposition},
	url = {http://arxiv.org/abs/2305.18498},
	shorttitle = {{ANPL}},
	abstract = {The advents of Large Language Models ({LLMs}) have shown promise in augmenting programming using natural interactions. However, while {LLMs} are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an {LLM}-generated program. We introduce {ANPL}, a programming system that allows users to decompose user-specific tasks. In an {ANPL} program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the {LLM}. Given an {ANPL} program, the {ANPL} compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy {ANPL} on the Abstraction and Reasoning Corpus ({ARC}), a set of unique tasks that are challenging for state-of-the-art {AI} systems, showing it outperforms baseline programming systems that (a) without the ability to decompose tasks interactively and (b) without the guarantee that the modules can be correctly composed together. We obtain a dataset consisting of 300/400 {ARC} tasks that were successfully decomposed and grounded in Python, providing valuable insights into how humans decompose programmatic tasks. See the dataset at https://iprc-dip.github.io/{DARC}.},
	number = {{arXiv}:2305.18498},
	publisher = {{arXiv}},
	author = {Huang, Di and Nan, Ziyuan and Hu, Xing and Jin, Pengwei and Peng, Shaohui and Wen, Yuanbo and Zhang, Rui and Du, Zidong and Guo, Qi and Pu, Yewen and Chen, Yunji},
	urldate = {2023-06-10},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2305.18498 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208806/Huang 等 - 2023 - ANPL Compiling Natural Programs with Interactive .pdf:application/pdf;arXiv.org Snapshot:files/208824/2305.html:text/html},
}

@misc{cassano_type_2023,
	title = {Type Prediction With Program Decomposition and Fill-in-the-Type Training},
	url = {http://arxiv.org/abs/2305.17145},
	abstract = {{TypeScript} and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models ({LLMs}) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building {OpenTau}, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for {LLMs}. We evaluate our work with a new dataset for {TypeScript} type prediction, and show that 47.4\% of files type check (14.5\% absolute improvement) with an overall rate of 3.3 type errors per file. All code, data, and models are available at: https://github.com/{GammaTauAI}/opentau.},
	number = {{arXiv}:2305.17145},
	publisher = {{arXiv}},
	author = {Cassano, Federico and Yee, Ming-Ho and Shinn, Noah and Guha, Arjun and Holtzen, Steven},
	urldate = {2023-06-10},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2305.17145 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208792/Cassano 等 - 2023 - Type Prediction With Program Decomposition and Fil.pdf:application/pdf;arXiv.org Snapshot:files/208812/2305.html:text/html},
}

@misc{sridhara_chatgpt_2023,
	title = {{ChatGPT}: A Study on its Utility for Ubiquitous Software Engineering Tasks},
	url = {http://arxiv.org/abs/2305.16837},
	shorttitle = {{ChatGPT}},
	abstract = {{ChatGPT} (Chat Generative Pre-trained Transformer) is a chatbot launched by {OpenAI} on November 30, 2022. {OpenAI}'s {GPT}-3 family of large language models serve as the foundation for {ChatGPT}. {ChatGPT} is fine-tuned with both supervised and reinforcement learning techniques and has received widespread attention for its articulate responses across diverse domains of knowledge. In this study, we explore how {ChatGPT} can be used to help with common software engineering tasks. Many of the ubiquitous tasks covering the breadth of software engineering such as ambiguity resolution in software requirements, method name suggestion, test case prioritization, code review, log summarization can potentially be performed using {ChatGPT}. In this study, we explore fifteen common software engineering tasks using {ChatGPT}. We juxtapose and analyze {ChatGPT}'s answers with the respective state of the art outputs (where available) and/or human expert ground truth. Our experiments suggest that for many tasks, {ChatGPT} does perform credibly and the response from it is detailed and often better than the human expert output or the state of the art output. However, for a few other tasks, {ChatGPT} in its present form provides incorrect answers and hence is not suited for such tasks.},
	number = {{arXiv}:2305.16837},
	publisher = {{arXiv}},
	author = {Sridhara, Giriprasad and G., Ranjani H. and Mazumdar, Sourav},
	urldate = {2023-06-10},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2305.16837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208796/Sridhara 等 - 2023 - ChatGPT A Study on its Utility for Ubiquitous Sof.pdf:application/pdf;arXiv.org Snapshot:files/209184/2305.html:text/html},
}

@misc{liu_uncovering_2023,
	title = {Uncovering and Quantifying Social Biases in Code Generation},
	url = {http://arxiv.org/abs/2305.15377},
	abstract = {With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, {InCoder}, and {CodeGen}) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias. (This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.)},
	number = {{arXiv}:2305.15377},
	publisher = {{arXiv}},
	author = {Liu, Yan and Chen, Xiaokang and Gao, Yan and Su, Zhe and Zhang, Fengji and Zan, Daoguang and Lou, Jian-Guang and Chen, Pin-Yu and Ho, Tsung-Yi},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15377 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208795/Liu 等 - 2023 - Uncovering and Quantifying Social Biases in Code G.pdf:application/pdf;arXiv.org Snapshot:files/209807/2305.html:text/html},
}

@misc{koziolek_chatgpt_2023,
	title = {{ChatGPT} for {PLC}/{DCS} Control Logic Generation},
	url = {http://arxiv.org/abs/2305.15809},
	abstract = {Large language models ({LLMs}) providing generative {AI} have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how {LLMs} can support control engineers using typical control programming languages in programming tasks. Researchers have explored {GitHub} {CoPilot} or {DeepMind} {AlphaCode} for source code generation but did not yet tackle control logic programming. The contribution of this paper is an exploratory study, for which we created 100 {LLM} prompts in 10 representative categories to analyze control logic generation for of {PLCs} and {DCS} from natural language. We tested the prompts by generating answers with {ChatGPT} using the {GPT}-4 {LLM}. It generated syntactically correct {IEC} 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal {LLM} benchmark to test and compare such models for control logic generation.},
	number = {{arXiv}:2305.15809},
	publisher = {{arXiv}},
	author = {Koziolek, Heiko and Gruener, Sten and Ashiwal, Virendra},
	urldate = {2023-06-10},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2305.15809 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, D.2.2},
	file = {arXiv Fulltext PDF:files/209494/Koziolek 等 - 2023 - ChatGPT for PLCDCS Control Logic Generation.pdf:application/pdf;arXiv.org Snapshot:files/209500/2305.html:text/html},
}

@misc{wang_voyager_2023,
	title = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
	url = {http://arxiv.org/abs/2305.16291},
	shorttitle = {Voyager},
	abstract = {We introduce Voyager, the first {LLM}-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with {GPT}-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior {SOTA}. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
	number = {{arXiv}:2305.16291},
	publisher = {{arXiv}},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	urldate = {2023-06-10},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2305.16291 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209249/Wang 等 - 2023 - Voyager An Open-Ended Embodied Agent with Large L.pdf:application/pdf;arXiv.org Snapshot:files/209286/2305.html:text/html},
}

@misc{lee_who_2023,
	title = {Who Wrote this Code? Watermarking for Code Generation},
	url = {http://arxiv.org/abs/2305.15060},
	shorttitle = {Who Wrote this Code?},
	abstract = {Large language models for code have recently shown remarkable performance in generating executable code. However, this rapid advancement has been accompanied by many legal and ethical concerns, such as code licensing issues, code plagiarism, and malware generation, making watermarking machine-generated code a very timely problem. Despite such imminent needs, we discover that existing watermarking and machine-generated text detection methods for {LLMs} fail to function with code generation tasks properly. Hence, in this work, we propose a new watermarking method, {SWEET}, that significantly improves upon previous approaches when watermarking machine-generated code. Our proposed method selectively applies watermarking to the tokens with high enough entropy, surpassing a defined threshold. The experiments on code generation benchmarks show that our watermarked code has superior quality compared to code produced by the previous state-of-the-art {LLM} watermarking method. Furthermore, our watermark method also outperforms {DetectGPT} for the task of machine-generated code detection.},
	number = {{arXiv}:2305.15060},
	publisher = {{arXiv}},
	author = {Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15060 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209495/Lee 等 - 2023 - Who Wrote this Code Watermarking for Code Generat.pdf:application/pdf;arXiv.org Snapshot:files/209835/2305.html:text/html},
}

@misc{charalambous_new_2023,
	title = {A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification},
	url = {http://arxiv.org/abs/2305.14752},
	shorttitle = {A New Era in Software Security},
	abstract = {In this paper we present a novel solution that combines the capabilities of Large Language Models ({LLMs}) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking ({BMC}) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the {LLM} engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use {BMC} to verify the corrected version of the code generated by the {LLM}. As a proof of concept, we create {ESBMC}-{AI} based on the Efficient {SMT}-based Context-Bounded Model Checker ({ESBMC}) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C programs. Our experimentation involved generating a dataset comprising 1000 C code samples, each consisting of 20 to 50 lines of code. Notably, our proposed method achieved an impressive success rate of up to 80\% in repairing vulnerable code encompassing buffer overflow and pointer dereference failures. We assert that this automated approach can effectively incorporate into the software development lifecycle's continuous integration and deployment ({CI}/{CD}) process.},
	number = {{arXiv}:2305.14752},
	publisher = {{arXiv}},
	author = {Charalambous, Yiannis and Tihanyi, Norbert and Jain, Ridhi and Sun, Youcheng and Ferrag, Mohamed Amine and Cordeiro, Lucas C.},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.14752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv Fulltext PDF:files/209183/Charalambous 等 - 2023 - A New Era in Software Security Towards Self-Heali.pdf:application/pdf;arXiv.org Snapshot:files/209264/2305.html:text/html},
}

@misc{zhao_automatic_2023,
	title = {Automatic Model Selection with Large Language Models for Reasoning},
	url = {http://arxiv.org/abs/2305.14333},
	abstract = {Chain-of-Thought and Program-Aided Language Models represent two distinct reasoning methods, each with its own strengths and weaknesses. We demonstrate that it is possible to combine the best of both worlds by using different models for different problems, employing a large language model ({LLM}) to perform model selection. Through a theoretical analysis, we discover that the performance improvement is determined by the differences between the combined methods and the success rate of choosing the correct model. On eight reasoning datasets, our proposed approach shows significant improvements. Furthermore, we achieve new state-of-the-art results on {GSM}8K and {SVAMP} with accuracies of 96.5\% and 93.7\%, respectively. Our code is publicly available at https://github.com/{XuZhao}0/Model-Selection-Reasoning.},
	number = {{arXiv}:2305.14333},
	publisher = {{arXiv}},
	author = {Zhao, Xu and Xie, Yuxi and Kawaguchi, Kenji and He, Junxian and Xie, Qizhe},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14333 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209498/Zhao 等 - 2023 - Automatic Model Selection with Large Language Mode.pdf:application/pdf;arXiv.org Snapshot:files/209702/2305.html:text/html},
}

@misc{gupta_grace_2023,
	title = {{GrACE}: Generation using Associated Code Edits},
	url = {http://arxiv.org/abs/2305.14129},
	shorttitle = {{GrACE}},
	abstract = {Developers expend a significant amount of time in editing code for a variety of reasons such as bug fixing or adding new features. Designing effective methods to predict code edits has been an active yet challenging area of research due to the diversity of code edits and the difficulty of capturing the developer intent. In this work, we address these challenges by endowing pre-trained large language models ({LLMs}) of code with the knowledge of prior, relevant edits. The generative capability of the {LLMs} helps address the diversity in code changes and conditioning code generation on prior edits helps capture the latent developer intent. We evaluate two well-known {LLMs}, Codex and {CodeT}5, in zero-shot and fine-tuning settings respectively. In our experiments with two datasets, the knowledge of prior edits boosts the performance of the {LLMs} significantly and enables them to generate 29\% and 54\% more correctly edited code in top-1 suggestions relative to the current state-of-the-art symbolic and neural approaches, respectively.},
	number = {{arXiv}:2305.14129},
	publisher = {{arXiv}},
	author = {Gupta, Priyanshu and Khare, Avishree and Bajpai, Yasharth and Chakraborty, Saikat and Gulwani, Sumit and Kanade, Aditya and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14129 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209501/Gupta 等 - 2023 - GrACE Generation using Associated Code Edits.pdf:application/pdf;arXiv.org Snapshot:files/209508/2305.html:text/html},
}

@misc{ye_generating_2023,
	title = {Generating Data for Symbolic Language with Large Language Models},
	url = {http://arxiv.org/abs/2305.13917},
	abstract = {While large language models ({LLMs}) bring not only performance but also complexity, recent work has started to turn {LLMs} into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose {SymGen} which utilizes {LLMs} for generating various annotation-expensive symbolic language data. {SymGen} consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the {LLMs}, we demonstrate the 1{\textbackslash}\%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. {SymGen} sheds new light on data generation for complex tasks, and we release the code at {\textbackslash}href\{https://github.com/{HKUNLP}/{SymGen}\}\{https://github.com/{HKUNLP}/{SymGen}\}.},
	number = {{arXiv}:2305.13917},
	publisher = {{arXiv}},
	author = {Ye, Jiacheng and Li, Chengzu and Kong, Lingpeng and Yu, Tao},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.13917 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209502/Ye 等 - 2023 - Generating Data for Symbolic Language with Large L.pdf:application/pdf;arXiv.org Snapshot:files/209510/2305.html:text/html},
}

@misc{sun_automatic_2023,
	title = {Automatic Code Summarization via {ChatGPT}: How Far Are We?},
	url = {http://arxiv.org/abs/2305.12865},
	shorttitle = {Automatic Code Summarization via {ChatGPT}},
	abstract = {To support software developers in understanding and maintaining programs, various automatic code summarization techniques have been proposed to generate a concise natural language comment for a given code snippet. Recently, the emergence of large language models ({LLMs}) has led to a great boost in the performance of natural language processing tasks. Among them, {ChatGPT} is the most popular one which has attracted wide attention from the software engineering community. However, it still remains unclear how {ChatGPT} performs in (automatic) code summarization. Therefore, in this paper, we focus on evaluating {ChatGPT} on a widely-used Python dataset called {CSN}-Python and comparing it with several state-of-the-art ({SOTA}) code summarization models. Specifically, we first explore an appropriate prompt to guide {ChatGPT} to generate in-distribution comments. Then, we use such a prompt to ask {ChatGPT} to generate comments for all code snippets in the {CSN}-Python test set. We adopt three widely-used metrics (including {BLEU}, {METEOR}, and {ROUGE}-L) to measure the quality of the comments generated by {ChatGPT} and {SOTA} models (including {NCS}, {CodeBERT}, and {CodeT}5). The experimental results show that in terms of {BLEU} and {ROUGE}-L, {ChatGPT}'s code summarization performance is significantly worse than all three {SOTA} models. We also present some cases and discuss the advantages and disadvantages of {ChatGPT} in code summarization. Based on the findings, we outline several open challenges and opportunities in {ChatGPT}-based code summarization.},
	number = {{arXiv}:2305.12865},
	publisher = {{arXiv}},
	author = {Sun, Weisong and Fang, Chunrong and You, Yudu and Miao, Yun and Liu, Yi and Li, Yuekang and Deng, Gelei and Huang, Shenghan and Chen, Yuchen and Zhang, Quanjun and Qian, Hanwei and Liu, Yang and Chen, Zhenyu},
	urldate = {2023-06-10},
	date = {2023-05-22},
	eprinttype = {arxiv},
	eprint = {2305.12865 [cs]},
	keywords = {68T50, Computer Science - Artificial Intelligence, Computer Science - Software Engineering, D.2.3},
	file = {arXiv Fulltext PDF:files/208839/Sun 等 - 2023 - Automatic Code Summarization via ChatGPT How Far .pdf:application/pdf;arXiv.org Snapshot:files/209192/2305.html:text/html},
}

@misc{allen-zhu_physics_2023,
	title = {Physics of Language Models: Part 1, Context-Free Grammar},
	url = {http://arxiv.org/abs/2305.13673},
	shorttitle = {Physics of Language Models},
	abstract = {We design experiments to study \${\textbackslash}textit\{how\}\$ generative language models, like {GPT}, learn context-free grammars ({CFGs}) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. {CFGs} are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging {CFGs}, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable \${\textbackslash}textit\{diversity\}\$. More importantly, we delve into the \${\textbackslash}textit\{physical principles\}\$ behind how transformers learns {CFGs}. We discover that the hidden states within the transformer implicitly and \${\textbackslash}textit\{precisely\}\$ encode the {CFG} structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extension of {CFGs} as well as the robustness aspect of transformers against grammar mistakes. Overall, our research provides a comprehensive and empirical understanding of how transformers learn {CFGs}, and reveals the physical mechanisms utilized by transformers to capture the structure and rules of languages.},
	number = {{arXiv}:2305.13673},
	publisher = {{arXiv}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	urldate = {2023-06-10},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.13673 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209197/Allen-Zhu 和 Li - 2023 - Physics of Language Models Part 1, Context-Free G.pdf:application/pdf;arXiv.org Snapshot:files/209267/2305.html:text/html},
}

@misc{armengol-estape_slade_2023,
	title = {{SLaDe}: A Portable Small Language Model Decompiler for Optimized Assembler},
	url = {http://arxiv.org/abs/2305.12520},
	shorttitle = {{SLaDe}},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and {ISAs}. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents {SLaDe}, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, {SLaDe} can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate {SLaDe} on over 4,000 functions from {AnghaBench} on two {ISAs} and at two optimizations levels. {SLaDe} is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model {ChatGPT} and generates significantly more readable code than both.},
	number = {{arXiv}:2305.12520},
	publisher = {{arXiv}},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F. P.},
	urldate = {2023-06-10},
	date = {2023-05-21},
	eprinttype = {arxiv},
	eprint = {2305.12520 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208819/Armengol-Estapé 等 - 2023 - SLaDe A Portable Small Language Model Decompiler .pdf:application/pdf;arXiv.org Snapshot:files/209186/2305.html:text/html},
}

@misc{nascimento_comparing_2023,
	title = {Comparing Software Developers with {ChatGPT}: An Empirical Investigation},
	url = {http://arxiv.org/abs/2305.11837},
	shorttitle = {Comparing Software Developers with {ChatGPT}},
	abstract = {The advent of automation in particular Software Engineering ({SE}) tasks has transitioned from theory to reality. Numerous scholarly articles have documented the successful application of Artificial Intelligence to address issues in areas such as project management, modeling, testing, and development. A recent innovation is the introduction of {ChatGPT}, an {ML}-infused chatbot, touted as a resource proficient in generating programming codes and formulating software testing strategies for developers and testers respectively. Although there is speculation that {AI}-based computation can increase productivity and even substitute software engineers in software development, there is currently a lack of empirical evidence to verify this. Moreover, despite the primary focus on enhancing the accuracy of {AI} systems, non-functional requirements including energy efficiency, vulnerability, fairness (i.e., human bias), and safety frequently receive insufficient attention. This paper posits that a comprehensive comparison of software engineers and {AI}-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of {AI}-based methods, and understanding task suitability for humans or {AI}. Furthermore, it facilitates the effective implementation of cooperative work structures and human-in-the-loop processes. This paper conducts an empirical investigation, contrasting the performance of software engineers and {AI} systems, like {ChatGPT}, across different evaluation metrics. The empirical study includes a case of assessing {ChatGPT}-generated code versus code produced by developers and uploaded in Leetcode.},
	number = {{arXiv}:2305.11837},
	publisher = {{arXiv}},
	author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
	urldate = {2023-06-10},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2305.11837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208815/Nascimento 等 - 2023 - Comparing Software Developers with ChatGPT An Emp.pdf:application/pdf;arXiv.org Snapshot:files/208838/2305.html:text/html},
}

@misc{ma_scope_2023,
	title = {The Scope of {ChatGPT} in Software Engineering: A Thorough Investigation},
	url = {http://arxiv.org/abs/2305.12138},
	shorttitle = {The Scope of {ChatGPT} in Software Engineering},
	abstract = {{ChatGPT} demonstrates immense potential to transform software engineering ({SE}) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of {SE} make the lack of interpretability for {ChatGPT} a concern. To address this issue, we carried out a study evaluating {ChatGPT}'s capabilities and limitations in {SE}. We broke down the abilities needed for {AI} models to tackle {SE} tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on {ChatGPT}'s ability to comprehend code syntax and semantic structures, including abstract syntax trees ({AST}), control flow graphs ({CFG}), and call graphs ({CG}). We assessed {ChatGPT}'s performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while {ChatGPT} excels at understanding code syntax ({AST}), it struggles with comprehending code semantics, particularly dynamic semantics. We conclude that {ChatGPT} possesses capabilities akin to an Abstract Syntax Tree ({AST}) parser, demonstrating initial competencies in static code analysis. Additionally, our study highlights that {ChatGPT} is susceptible to hallucination when interpreting code semantic structures and fabricating non-existent facts. These results underscore the need to explore methods for verifying the correctness of {ChatGPT}'s outputs to ensure its dependability in {SE}. More importantly, our study provide an iniital answer why the generated codes from {LLMs} are usually synatx correct but vulnerabale.},
	number = {{arXiv}:2305.12138},
	publisher = {{arXiv}},
	author = {Ma, Wei and Liu, Shangqing and Wang, Wenhan and Hu, Qiang and Liu, Ye and Zhang, Cen and Nie, Liming and Liu, Yang},
	urldate = {2023-06-10},
	date = {2023-05-20},
	eprinttype = {arxiv},
	eprint = {2305.12138 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209185/Ma 等 - 2023 - The Scope of ChatGPT in Software Engineering A Th.pdf:application/pdf;arXiv.org Snapshot:files/209265/2305.html:text/html},
}

@misc{silver_generalized_2023,
	title = {Generalized Planning in {PDDL} Domains with Pretrained Large Language Models},
	url = {http://arxiv.org/abs/2305.11014},
	abstract = {Recent work has considered whether large language models ({LLMs}) can function as planners: given a task, generate a plan. We investigate whether {LLMs} can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider {PDDL} domains and use {GPT}-4 to synthesize Python programs. We also consider (1) Chain-of-Thought ({CoT}) summarization, where the {LLM} is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the {LLM} is re-prompted with four types of feedback. We evaluate this approach in seven {PDDL} domains and compare it to four ablations and four baselines. Overall, we find that {GPT}-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that {CoT} summarization has non-uniform impact, that {GPT}-4 is far superior to {GPT}-3.5, and that just two training tasks are often sufficient for strong generalization.},
	number = {{arXiv}:2305.11014},
	publisher = {{arXiv}},
	author = {Silver, Tom and Dan, Soham and Srinivas, Kavitha and Tenenbaum, Joshua B. and Kaelbling, Leslie Pack and Katz, Michael},
	urldate = {2023-06-10},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.11014 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:files/209503/Silver 等 - 2023 - Generalized Planning in PDDL Domains with Pretrain.pdf:application/pdf;arXiv.org Snapshot:files/209708/2305.html:text/html},
}

@misc{phelps_investigating_2023,
	title = {Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics},
	url = {http://arxiv.org/abs/2305.07970},
	abstract = {In this study, we investigate the capacity of large language models ({LLMs}), specifically {GPT}-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated {LLM}-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that {LLMs} can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the {LLM}'s ability to generalize its knowledge about human behavior in social dilemmas. We call upon the research community to further explore the factors contributing to the emergent behavior of {LLM}-generated agents in a wider array of social dilemmas, examining the impact of model architecture, training parameters, and various partner strategies on agent behavior. As more advanced {LLMs} like {GPT}-4 become available, it is crucial to investigate whether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ultimately fostering the development of {AI} systems that better align with human values and social norms.},
	number = {{arXiv}:2305.07970},
	publisher = {{arXiv}},
	author = {Phelps, Steve and Russell, Yvan I.},
	urldate = {2023-06-10},
	date = {2023-05-13},
	eprinttype = {arxiv},
	eprint = {2305.07970 [cs, econ, q-fin]},
	keywords = {I.2.0, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computer Science and Game Theory, Economics - General Economics},
	file = {arXiv Fulltext PDF:files/209499/Phelps 和 Russell - 2023 - Investigating Emergent Goal-Like Behaviour in Larg.pdf:application/pdf;arXiv.org Snapshot:files/209504/2305.html:text/html},
}

@misc{wang_plan-and-solve_2023,
	title = {Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
	url = {http://arxiv.org/abs/2305.04091},
	shorttitle = {Plan-and-Solve Prompting},
	abstract = {Large language models ({LLMs}) have recently been shown to deliver impressive performance in various {NLP} tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought ({CoT}) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable {LLMs} to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-{CoT} concatenates the target problem statement with "Let's think step by step" as an input prompt to {LLMs}. Despite the success of Zero-shot-{CoT}, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve ({PS}) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend {PS} prompting with more detailed instructions and derive {PS}+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over {GPT}-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-{CoT} across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot {CoT} prompting on the math reasoning problem. The code can be found at https://github.com/{AGI}-Edgerunners/Plan-and-Solve-Prompting.},
	number = {{arXiv}:2305.04091},
	publisher = {{arXiv}},
	author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
	urldate = {2023-06-10},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2305.04091 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208862/Wang 等 - 2023 - Plan-and-Solve Prompting Improving Zero-Shot Chai.pdf:application/pdf;arXiv.org Snapshot:files/209767/2305.html:text/html},
}

@misc{rahmani_improving_2023,
	title = {Improving Code Example Recommendations on Informal Documentation Using {BERT} and Query-Aware {LSH}: A Comparative Study},
	url = {http://arxiv.org/abs/2305.03017},
	shorttitle = {Improving Code Example Recommendations on Informal Documentation Using {BERT} and Query-Aware {LSH}},
	abstract = {The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized {BERT} in our approach, which is a Large Language Model ({LLM}) for text representation that can effectively extract semantic information from textual data. Our first step involved using {BERT} to convert code examples into numerical vectors. Subsequently, we applied {LSH} to identify Approximate Nearest Neighbors ({ANN}). Our research involved the implementation of two variants of this approach, namely the Random Hyperplane-based {LSH} and the Query-Aware {LSH}. Our study compared two algorithms using four parameters: {HitRate}, Mean Reciprocal Rank ({MRR}), Average Execution Time, and Relevance. The results of our analysis revealed that the Query- Aware ({QA}) approach outperformed the Random Hyperplane-based ({RH}) approach in terms of {HitRate}. Specifically, the {QA} approach achieved a {HitRate} improvement of 20\% to 35\% for query pairs compared to the {RH} approach. Creating hashing tables and assigning data samples to buckets using the {QA} approach is at least four times faster than the {RH} approach. The {QA} approach returns code examples within milliseconds, while it takes several seconds (sec) for the {RH} approach to recommend code examples.},
	number = {{arXiv}:2305.03017},
	publisher = {{arXiv}},
	author = {Rahmani, Sajjad and Naghshzan, {AmirHossein} and Guerrouj, Latifa},
	urldate = {2023-06-10},
	date = {2023-05-04},
	eprinttype = {arxiv},
	eprint = {2305.03017 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209505/Rahmani 等 - 2023 - Improving Code Example Recommendations on Informal.pdf:application/pdf;arXiv.org Snapshot:files/209513/2305.html:text/html},
}

@misc{patton_program_2023,
	title = {Program Synthesis for Robot Learning from Demonstrations},
	url = {http://arxiv.org/abs/2305.03129},
	abstract = {This paper presents a new synthesis-based approach for solving the Learning from Demonstration ({LfD}) problem in robotics. Given a set of user demonstrations, the goal of programmatic {LfD} is to learn a policy in a programming language that can be used to control a robot's behavior. We address this problem through a novel program synthesis algorithm that leverages two key ideas: First, to perform fast and effective generalization from user demonstrations, our synthesis algorithm views these demonstrations as strings over a finite alphabet and abstracts programs in our {DSL} as regular expressions over the same alphabet. This regex abstraction facilitates synthesis by helping infer useful program sketches and pruning infeasible parts of the search space. Second, to deal with the large number of object types in the environment, our method leverages a Large Language Model ({LLM}) to guide search. We have implemented our approach in a tool called Prolex and present the results of a comprehensive experimental evaluation on 120 benchmarks involving 40 unique tasks in three different environments. We show that, given a 120 second time limit, Prolex can find a program consistent with the demonstrations in 80\% of the cases. Furthermore, for 81\% of the tasks for which a solution is returned, Prolex is able to find the ground truth program with just one demonstration. To put these results in perspective, we conduct a comparison against two baselines and show that both perform much worse.},
	number = {{arXiv}:2305.03129},
	publisher = {{arXiv}},
	author = {Patton, Noah and Rahmani, Kia and Missula, Meghana and Biswas, Joydeep and Dillig, Işil},
	urldate = {2023-06-10},
	date = {2023-05-04},
	eprinttype = {arxiv},
	eprint = {2305.03129 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/209188/Patton 等 - 2023 - Program Synthesis for Robot Learning from Demonstr.pdf:application/pdf;arXiv.org Snapshot:files/209838/2305.html:text/html},
}

@misc{edwards_advise_2023,
	title = {{ADVISE}: {AI}-accelerated Design of Evidence Synthesis for Global Development},
	url = {http://arxiv.org/abs/2305.01145},
	shorttitle = {{ADVISE}},
	abstract = {When designing evidence-based policies and programs, decision-makers must distill key information from a vast and rapidly growing literature base. Identifying relevant literature from raw search results is time and resource intensive, and is often done by manual screening. In this study, we develop an {AI} agent based on a bidirectional encoder representations from transformers ({BERT}) model and incorporate it into a human team designing an evidence synthesis product for global development. We explore the effectiveness of the human-{AI} hybrid team in accelerating the evidence synthesis process. To further improve team efficiency, we enhance the human-{AI} hybrid team through active learning ({AL}). Specifically, we explore different sampling strategies, including random sampling, least confidence ({LC}) sampling, and highest priority ({HP}) sampling, to study their influence on the collaborative screening process. Results show that incorporating the {BERT}-based {AI} agent into the human team can reduce the human screening effort by 68.5\% compared to the case of no {AI} assistance and by 16.8\% compared to the case of using a support vector machine ({SVM})-based {AI} agent for identifying 80\% of all relevant documents. When we apply the {HP} sampling strategy for {AL}, the human screening effort can be reduced even more: by 78.3\% for identifying 80\% of all relevant documents compared to no {AI} assistance. We apply the {AL}-enhanced human-{AI} hybrid teaming workflow in the design process of three evidence gap maps ({EGMs}) for {USAID} and find it to be highly effective. These findings demonstrate how {AI} can accelerate the development of evidence synthesis products and promote timely evidence-based decision making in global development in a human-{AI} hybrid teaming context.},
	number = {{arXiv}:2305.01145},
	publisher = {{arXiv}},
	author = {Edwards, Kristen M. and Song, Binyang and Porciello, Jaron and Engelbert, Mark and Huang, Carolyn and Ahmed, Faez},
	urldate = {2023-06-10},
	date = {2023-05-01},
	eprinttype = {arxiv},
	eprint = {2305.01145 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209511/Edwards 等 - 2023 - ADVISE AI-accelerated Design of Evidence Synthesi.pdf:application/pdf;arXiv.org Snapshot:files/209804/2305.html:text/html},
}

@misc{liu_is_2023,
	title = {Is Your Code Generated by {ChatGPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
	url = {http://arxiv.org/abs/2305.01210},
	shorttitle = {Is Your Code Generated by {ChatGPT} Really Correct?},
	abstract = {Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models ({LLMs}) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various {LLMs} on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of {LLMs}, is the code generated really correct? To answer this, we propose {EvalPlus} -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of {LLM}-synthesized code. In short, {EvalPlus} takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both {LLM}-based and mutation-based input generators to further validate the synthesized code. We extend the popular {HUMANEVAL} benchmark and build {HUMANEVAL}+ with 81x additionally generated tests. Our extensive evaluation across 14 popular {LLMs} demonstrates that {HUMANEVAL}+ is able to catch significant amounts of previously undetected wrong code synthesized by {LLMs}, reducing the pass@k by 15.1\% on average! Moreover, we even found several incorrect ground-truth implementations in {HUMANEVAL}. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of {LLMs} for code synthesis but also opens up a new direction to improve programming benchmarks through automated test input generation.},
	number = {{arXiv}:2305.01210},
	publisher = {{arXiv}},
	author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2023-05-02},
	eprinttype = {arxiv},
	eprint = {2305.01210 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209512/Liu 等 - 2023 - Is Your Code Generated by ChatGPT Really Correct .pdf:application/pdf;arXiv.org Snapshot:files/209515/2305.html:text/html},
}

@misc{khatry_words_2023,
	title = {From Words to Code: Harnessing Data for Program Synthesis from Natural Language},
	url = {http://arxiv.org/abs/2305.01598},
	shorttitle = {From Words to Code},
	abstract = {Creating programs to correctly manipulate data is a difficult task, as the underlying programming languages and {APIs} can be challenging to learn for many users who are not skilled programmers. Large language models ({LLMs}) demonstrate remarkable potential for generating code from natural language, but in the data manipulation domain, apart from the natural language ({NL}) description of the intended task, we also have the dataset on which the task is to be performed, or the "data context". Existing approaches have utilized data context in a limited way by simply adding relevant information from the input data into the prompts sent to the {LLM}. In this work, we utilize the available input data to execute the candidate programs generated by the {LLMs} and gather their outputs. We introduce semantic reranking, a technique to rerank the programs generated by {LLMs} based on three signals coming the program outputs: (a) semantic filtering and well-formedness based score tuning: do programs even generate well-formed outputs, (b) semantic interleaving: how do the outputs from different candidates compare to each other, and (c) output-based score tuning: how do the outputs compare to outputs predicted for the same task. We provide theoretical justification for semantic interleaving. We also introduce temperature mixing, where we combine samples generated by {LLMs} using both high and low temperatures. We extensively evaluate our approach in three domains, namely databases ({SQL}), data science (Pandas) and business intelligence (Excel's Power Query M) on a variety of new and existing benchmarks. We observe substantial gains across domains, with improvements of up to 45\% in top-1 accuracy and 34\% in top-3 accuracy.},
	number = {{arXiv}:2305.01598},
	publisher = {{arXiv}},
	author = {Khatry, Anirudh and Cahoon, Joyce and Henkel, Jordan and Deep, Shaleen and Emani, Venkatesh and Floratou, Avrilia and Gulwani, Sumit and Le, Vu and Raza, Mohammad and Shi, Sherry and Singh, Mukul and Tiwari, Ashish},
	urldate = {2023-06-10},
	date = {2023-05-03},
	eprinttype = {arxiv},
	eprint = {2305.01598 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Databases},
	file = {arXiv Fulltext PDF:files/208848/Khatry 等 - 2023 - From Words to Code Harnessing Data for Program Sy.pdf:application/pdf;arXiv.org Snapshot:files/209194/2305.html:text/html},
}

@misc{zhuo_large_2023,
	title = {Large Language Models Are State-of-the-Art Evaluators of Code Generation},
	url = {http://arxiv.org/abs/2304.14317},
	abstract = {Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code generation tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as {BLEU}, have demonstrated weak correlations with human practitioners in code generation tasks. Moreover, the utilization of human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose a new evaluation framework based on the {GPT}-3.5 ({\textbackslash}texttt\{{GPT}-3.5-turbo\}), for code generation assessments. Our framework addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our framework on two different tasks and four programming languages, comparing its performance with the state-of-the-art {CodeBERTScore} metric, which relies on a pre-trained model. Our results demonstrate that our framework surpasses {CodeBERTScore}, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation framework and datasets available to the public at {\textbackslash}url\{https://github.com/terryyz/llm-code-eval\}, encouraging further research in the evaluation of code generation.},
	number = {{arXiv}:2304.14317},
	publisher = {{arXiv}},
	author = {Zhuo, Terry Yue},
	urldate = {2023-06-10},
	date = {2023-04-27},
	eprinttype = {arxiv},
	eprint = {2304.14317 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209509/Zhuo - 2023 - Large Language Models Are State-of-the-Art Evaluat.pdf:application/pdf;arXiv.org Snapshot:files/209816/2304.html:text/html},
}

@misc{fraiwan_review_2023,
	title = {A Review of {ChatGPT} Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions},
	url = {http://arxiv.org/abs/2305.00237},
	shorttitle = {A Review of {ChatGPT} Applications in Education, Marketing, Software Engineering, and Healthcare},
	abstract = {{ChatGPT} is a type of artificial intelligence language model that uses deep learning algorithms to generate human-like responses to text-based prompts. The introduction of the latest {ChatGPT} version in November of 2022 has caused shockwaves in the industrial and academic communities for its powerful capabilities, plethora of possible applications, and the great possibility for abuse. At the time of writing this work, several other language models (e.g., Google Bard and Meta {LLaMA}) just came out in an attempt to get a foothold in the vast possible market. These models have the ability to revolutionize the way we interact with computers and have potential applications in many fields, including education, software engineering, healthcare, and marketing. In this paper, we will discuss the possible applications, drawbacks, and research directions using advanced language Chatbots (e.g., {ChatGPT}) in each of these fields. We first start with a brief introduction and the development timeline of artificial intelligence based language models, then we go through possible applications of such models, after that we discuss the limitations and drawbacks of the current technological state of the art, and finally we point out future possible research directions.},
	number = {{arXiv}:2305.00237},
	publisher = {{arXiv}},
	author = {Fraiwan, Mohammad and Khasawneh, Natheer},
	urldate = {2023-06-10},
	date = {2023-04-29},
	eprinttype = {arxiv},
	eprint = {2305.00237 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209506/Fraiwan 和 Khasawneh - 2023 - A Review of ChatGPT Applications in Education, Mar.pdf:application/pdf;arXiv.org Snapshot:files/209743/2305.html:text/html},
}

@misc{khoury_how_2023,
	title = {How Secure is Code Generated by {ChatGPT}?},
	url = {http://arxiv.org/abs/2304.09655},
	abstract = {In recent years, large language models have been responsible for great advances in the field of artificial intelligence ({AI}). {ChatGPT} in particular, an {AI} chatbot developed and recently released by {OpenAI}, has taken the field to the next level. The conversational model is able not only to process human-like text, but also to translate natural language into code. However, the safety of programs generated by {ChatGPT} should not be overlooked. In this paper, we perform an experiment to address this issue. Specifically, we ask {ChatGPT} to generate a number of program and evaluate the security of the resulting source code. We further investigate whether {ChatGPT} can be prodded to improve the security by appropriate prompts, and discuss the ethical aspects of using {AI} to generate code. Results suggest that {ChatGPT} is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.},
	number = {{arXiv}:2304.09655},
	publisher = {{arXiv}},
	author = {Khoury, Raphaël and Avila, Anderson R. and Brunelle, Jacob and Camara, Baba Mamadou},
	urldate = {2023-06-10},
	date = {2023-04-19},
	eprinttype = {arxiv},
	eprint = {2304.09655 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209507/Khoury 等 - 2023 - How Secure is Code Generated by ChatGPT.pdf:application/pdf;arXiv.org Snapshot:files/209698/2304.html:text/html},
}

@misc{mandal_large_2023,
	title = {Large Language Models Based Automatic Synthesis of Software Specifications},
	url = {http://arxiv.org/abs/2304.09181},
	abstract = {Software configurations play a crucial role in determining the behavior of software systems. In order to ensure safe and error-free operation, it is necessary to identify the correct configuration, along with their valid bounds and rules, which are commonly referred to as software specifications. As software systems grow in complexity and scale, the number of configurations and associated specifications required to ensure the correct operation can become large and prohibitively difficult to manipulate manually. Due to the fast pace of software development, it is often the case that correct software specifications are not thoroughly checked or validated within the software itself. Rather, they are frequently discussed and documented in a variety of external sources, including software manuals, code comments, and online discussion forums. Therefore, it is hard for the system administrator to know the correct specifications of configurations due to the lack of clarity, organization, and a centralized unified source to look at. To address this challenge, we propose {SpecSyn} a framework that leverages a state-of-the-art large language model to automatically synthesize software specifications from natural language sources. Our approach formulates software specification synthesis as a sequence-to-sequence learning problem and investigates the extraction of specifications from large contextual texts. This is the first work that uses a large language model for end-to-end specification synthesis from natural language texts. Empirical results demonstrate that our system outperforms prior the state-of-the-art specification synthesis tool by 21\% in terms of F1 score and can find specifications from single as well as multiple sentences.},
	number = {{arXiv}:2304.09181},
	publisher = {{arXiv}},
	author = {Mandal, Shantanu and Chethan, Adhrik and Janfaza, Vahid and Mahmud, S. M. Farabi and Anderson, Todd A. and Turek, Javier and Tithi, Jesmin Jahan and Muzahid, Abdullah},
	urldate = {2023-06-10},
	date = {2023-04-17},
	eprinttype = {arxiv},
	eprint = {2304.09181 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209187/Mandal 等 - 2023 - Large Language Models Based Automatic Synthesis of.pdf:application/pdf;arXiv.org Snapshot:files/209753/2304.html:text/html},
}

@misc{xia_keep_2023,
	title = {Keep the Conversation Going: Fixing 162 out of 337 bugs for \$0.42 each using {ChatGPT}},
	url = {http://arxiv.org/abs/2304.00385},
	shorttitle = {Keep the Conversation Going},
	abstract = {Automated Program Repair ({APR}) aims to automatically generate patches for buggy programs. Recent {APR} work has been focused on leveraging modern Large Language Models ({LLMs}) to directly generate patches for {APR}. Such {LLM}-based {APR} tools work by first constructing an input prompt built using the original buggy code and then queries the {LLM} to generate patches. While the {LLM}-based {APR} tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches. To address these limitations, we propose {ChatRepair}, the first fully automated conversation-driven {APR} approach that interleaves patch generation with instant feedback to perform {APR} in a conversational style. {ChatRepair} first feeds the {LLM} with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful {APR}. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the {LLM} to generate the next patch. In this way, we can avoid making the same mistakes. For earlier patches that passed all the tests, we further ask the {LLM} to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement {ChatRepair} using state-of-the-art dialogue-based {LLM} -- {ChatGPT}. By calculating the cost of accessing {ChatGPT}, we can fix 162 out of 337 bugs for {\textbackslash}\$0.42 each!},
	number = {{arXiv}:2304.00385},
	publisher = {{arXiv}},
	author = {Xia, Chunqiu Steven and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2023-04-01},
	eprinttype = {arxiv},
	eprint = {2304.00385 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209523/Xia 和 Zhang - 2023 - Keep the Conversation Going Fixing 162 out of 337.pdf:application/pdf;arXiv.org Snapshot:files/209806/2304.html:text/html},
}

@misc{ren_pangu-sigma_2023,
	title = {{PanGu}-\{{\textbackslash}Sigma\}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},
	url = {http://arxiv.org/abs/2303.10845},
	shorttitle = {{PanGu}-\{{\textbackslash}Sigma\}},
	abstract = {The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 {AI} processors and {MindSpore} framework, and present the language model with 1.085T parameters named {PanGu}-\{{\textbackslash}Sigma\}. With parameter inherent from {PanGu}-\{{\textbackslash}alpha\}, we extend the dense Transformer model to sparse one with Random Routed Experts ({RRE}), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation({ECSS}). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that {PanGu}-\{{\textbackslash}Sigma\} provides state-of-the-art performance in zero-shot learning of various Chinese {NLP} downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.},
	number = {{arXiv}:2303.10845},
	publisher = {{arXiv}},
	author = {Ren, Xiaozhe and Zhou, Pingyi and Meng, Xinfan and Huang, Xinjing and Wang, Yadao and Wang, Weichao and Li, Pengfei and Zhang, Xiaoda and Podolskiy, Alexander and Arshinov, Grigory and Bout, Andrey and Piontkovskaya, Irina and Wei, Jiansheng and Jiang, Xin and Su, Teng and Liu, Qun and Yao, Jun},
	urldate = {2023-06-10},
	date = {2023-03-19},
	eprinttype = {arxiv},
	eprint = {2303.10845 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209520/Ren 等 - 2023 - PanGu- Sigma Towards Trillion Parameter Languag.pdf:application/pdf;arXiv.org Snapshot:files/209528/2303.html:text/html},
}

@misc{he_representation_2023,
	title = {Representation Learning for Stack Overflow Posts: How Far are We?},
	url = {http://arxiv.org/abs/2303.06853},
	shorttitle = {Representation Learning for Stack Overflow Posts},
	abstract = {The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content.The performance of such solutions hinges significantly on the selection of representation model for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers' interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art ({SOTA}) Stack Overflow post representation models are Post2Vec and {BERTOverflow}, which are built upon trendy neural networks such as convolutional neural network ({CNN}) and Transformer architecture (e.g., {BERT}). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and {BERTOverflow}) in a wide range of related tasks, i.e., tag recommendation, relatedness prediction, and {API} recommendation. To find more suitable representation models for the posts, we further explore a diverse set of {BERT}-based models, including (1) general domain language models ({RoBERTa} and Longformer) and (2) language models built with software engineering-related textual artifacts ({CodeBERT}, {GraphCodeBERT}, and {seBERT}). However, it also illustrates the ``No Silver Bullet'' concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose {SOBERT}, which employs a simple-yet-effective strategy to improve the best-performing model by continuing the pre-training phase with the textual artifact from Stack Overflow.},
	number = {{arXiv}:2303.06853},
	publisher = {{arXiv}},
	author = {He, Junda and Xin, Zhou and Xu, Bowen and Zhang, Ting and Kim, Kisub and Yang, Zhou and Thung, Ferdian and Irsan, Ivana and Lo, David},
	urldate = {2023-06-10},
	date = {2023-03-13},
	eprinttype = {arxiv},
	eprint = {2303.06853 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209514/He 等 - 2023 - Representation Learning for Stack Overflow Posts .pdf:application/pdf;arXiv.org Snapshot:files/209729/2303.html:text/html},
}

@misc{ji_exploring_2023,
	title = {Exploring {ChatGPT}'s Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences},
	url = {http://arxiv.org/abs/2303.07610},
	shorttitle = {Exploring {ChatGPT}'s Ability to Rank Content},
	abstract = {As a natural language assistant, {ChatGPT} is capable of performing various tasks, including but not limited to article generation, code completion, and data analysis. Furthermore, {ChatGPT} has consistently demonstrated a remarkable level of accuracy and reliability in terms of content evaluation, exhibiting the capability of mimicking human preferences. To further explore {ChatGPT}'s potential in this regard, a study is conducted to assess its ability to rank content. In order to do so, a test set consisting of prompts is created, covering a wide range of use cases, and five models are utilized to generate corresponding responses. {ChatGPT} is then instructed to rank the responses generated by these models. The results on the test set show that {ChatGPT}'s ranking preferences are consistent with human to a certain extent. This preliminary experimental finding implies that {ChatGPT}'s zero-shot ranking capability could be used to reduce annotation pressure in a number of ranking tasks.},
	number = {{arXiv}:2303.07610},
	publisher = {{arXiv}},
	author = {Ji, Yunjie and Gong, Yan and Peng, Yiping and Ni, Chao and Sun, Peiyan and Pan, Dongyu and Ma, Baochang and Li, Xiangang},
	urldate = {2023-06-10},
	date = {2023-03-13},
	eprinttype = {arxiv},
	eprint = {2303.07610 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209517/Ji 等 - 2023 - Exploring ChatGPT's Ability to Rank Content A Pre.pdf:application/pdf;arXiv.org Snapshot:files/209522/2303.html:text/html},
}

@misc{jin_inferfix_2023,
	title = {{InferFix}: End-to-End Program Repair with {LLMs}},
	url = {http://arxiv.org/abs/2303.07263},
	shorttitle = {{InferFix}},
	abstract = {Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose {InferFix}: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. {InferFix} combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated {InferredBugs}, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C\# repositories. Our evaluation demonstrates that {InferFix} outperforms strong {LLM} baselines, with a top-1 accuracy of 65.6\% for generating fixes in C\# and 76.8\% in Java. We discuss the deployment of {InferFix} alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow.},
	number = {{arXiv}:2303.07263},
	publisher = {{arXiv}},
	author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
	urldate = {2023-06-10},
	date = {2023-03-13},
	eprinttype = {arxiv},
	eprint = {2303.07263 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209193/Jin 等 - 2023 - InferFix End-to-End Program Repair with LLMs.pdf:application/pdf;arXiv.org Snapshot:files/209266/2303.html:text/html},
}

@misc{ahmad_towards_2023,
	title = {Towards Human-Bot Collaborative Software Architecting with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.14600},
	abstract = {Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders' perspectives, designers' intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering ({ACSE}) inherits a multitude of challenges. {ACSE} challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software (e.g., {IoTs}, blockchain, quantum systems). Software Development Bots ({DevBots}) trained on large language models can help synergise architects' knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative {ACSE}. An emerging solution to enable this collaboration is {ChatGPT}, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and {ChatGPT} for architectural analysis, synthesis, and evaluation of a services-driven software application. Preliminary results indicate that {ChatGPT} can mimic an architect's role to support and often lead {ACSE}, however; it requires human oversight and decision support for collaborative architecting. Future research focuses on harnessing empirical evidence about architects' productivity and exploring socio-technical aspects of architecting with {ChatGPT} to tackle emerging and futuristic challenges of {ACSE}.},
	number = {{arXiv}:2302.14600},
	publisher = {{arXiv}},
	author = {Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fehmideh, Mahdi and Aktar, Mst Shamima and Mikkonen, Tommi},
	urldate = {2023-06-10},
	date = {2023-02-26},
	eprinttype = {arxiv},
	eprint = {2302.14600 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209201/Ahmad 等 - 2023 - Towards Human-Bot Collaborative Software Architect.pdf:application/pdf;arXiv.org Snapshot:files/209733/2302.html:text/html},
}

@misc{khan_xcodeeval_2023,
	title = {{xCodeEval}: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval},
	url = {http://arxiv.org/abs/2303.03004},
	shorttitle = {{xCodeEval}},
	abstract = {The ability to solve problems is a hallmark of intelligence and has been an enduring goal in {AI}. {AI} systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', i.e., being able to get the same output for a given input.},
	number = {{arXiv}:2303.03004},
	publisher = {{arXiv}},
	author = {Khan, Mohammad Abdullah Matin and Bari, M. Saiful and Do, Xuan Long and Wang, Weishi and Parvez, Md Rizwan and Joty, Shafiq},
	urldate = {2023-06-10},
	date = {2023-04-17},
	eprinttype = {arxiv},
	eprint = {2303.03004 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209196/Khan 等 - 2023 - xCodeEval A Large Scale Multilingual Multitask Be.pdf:application/pdf;arXiv.org Snapshot:files/209689/2303.html:text/html},
}

@misc{pudari_copilot_2023,
	title = {From Copilot to Pilot: Towards {AI} Supported Software Development},
	url = {http://arxiv.org/abs/2303.04142},
	shorttitle = {From Copilot to Pilot},
	abstract = {{AI}-supported programming has arrived, as shown by the introduction and successes of large language models for code, such as Copilot/Codex (Github/{OpenAI}) and {AlphaCode} ({DeepMind}). Above human average performance on programming challenges is now possible. However, software engineering is much more than solving programming contests. Moving beyond code completion to {AI}-supported software engineering will require an {AI} system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs. In this study, we explore the current limitations of {AI}-supported code completion tools like Copilot and offer a simple taxonomy for understanding the classification of {AI}-supported code completion tools in this space. We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid code smells in most of our test scenarios. We then conduct additional investigation to determine the current boundaries of {AI}-supported code completion tools like Copilot by introducing a taxonomy of software abstraction hierarchies where 'basic programming functionality' such as code compilation and syntax checking is at the least abstract level, software architecture analysis and design are at the most abstract level. We conclude by providing a discussion on challenges for future development of {AI}-supported code completion tools to reach the design level of abstraction in our taxonomy.},
	number = {{arXiv}:2303.04142},
	publisher = {{arXiv}},
	author = {Pudari, Rohith and Ernst, Neil A.},
	urldate = {2023-06-10},
	date = {2023-03-07},
	eprinttype = {arxiv},
	eprint = {2303.04142 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209195/Pudari 和 Ernst - 2023 - From Copilot to Pilot Towards AI Supported Softwa.pdf:application/pdf;arXiv.org Snapshot:files/209737/2303.html:text/html},
}

@misc{he_large_2023,
	title = {Large Language Models for Code: Security Hardening and Adversarial Testing},
	url = {http://arxiv.org/abs/2302.05319},
	shorttitle = {Large Language Models for Code},
	abstract = {Large language models ({LMs}) are increasingly pretrained on massive codebases and used to generate code. However, {LMs} lack awareness of security and are found to frequently produce unsafe code. This work studies the security of {LMs} along two important axes: (i) security hardening, which aims to enhance {LMs}' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate {LMs}' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the {LM} to generate secure or unsafe code, while preserving the {LM}'s capability of generating functionally correct code. We propose a novel learning-based approach called {SVEN} to solve this task. {SVEN} leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the {LM}'s weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that {SVEN} is highly effective in achieving strong security control. For instance, a state-of-the-art {CodeGen} {LM} with 2.7B parameters generates secure code for 59.1\% of the time. When we employ {SVEN} to perform security hardening (or adversarial testing) on this {LM}, the ratio is significantly boosted to 92.3\% (or degraded to 36.8\%). Importantly, {SVEN} closely matches the original {LMs} in functional correctness.},
	number = {{arXiv}:2302.05319},
	publisher = {{arXiv}},
	author = {He, Jingxuan and Vechev, Martin},
	urldate = {2023-06-10},
	date = {2023-05-05},
	eprinttype = {arxiv},
	eprint = {2302.05319 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209203/He 和 Vechev - 2023 - Large Language Models for Code Security Hardening.pdf:application/pdf;arXiv.org Snapshot:files/209748/2302.html:text/html},
}

@misc{zhou_codebertscore_2023,
	title = {{CodeBERTScore}: Evaluating Code Generation with Pretrained Models of Code},
	url = {http://arxiv.org/abs/2302.05527},
	shorttitle = {{CodeBERTScore}},
	abstract = {Since the rise of neural models of code that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose {CodeBERTScore}: an automatic evaluation metric for code generation, which builds on {BERTScore} (Zhang et al., 2020). Instead of measuring exact token matching as {BLEU}, {CodeBERTScore} computes a soft similarity score between each token in the generated code and in the reference code, using the contextual encodings of large pretrained models. Further, instead of encoding only the generated tokens as in {BERTScore}, {CodeBERTScore} also encodes the programmatic context surrounding the generated code. We perform an extensive evaluation of {CodeBERTScore} across four programming languages. We find that {CodeBERTScore} achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by {CodeBERTScore} is more likely to be preferred by humans, as well as to function correctly when executed. Finally, while {CodeBERTScore} can be used with a multilingual {CodeBERT} as its base model, we release five language-specific pretrained models to use with our publicly available code at https://github.com/neulab/code-bert-score . Our language-specific models have been downloaded more than 25,000 times from the Huggingface Hub.},
	number = {{arXiv}:2302.05527},
	publisher = {{arXiv}},
	author = {Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},
	urldate = {2023-06-10},
	date = {2023-02-10},
	eprinttype = {arxiv},
	eprint = {2302.05527 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208873/Zhou 等 - 2023 - CodeBERTScore Evaluating Code Generation with Pre.pdf:application/pdf;arXiv.org Snapshot:files/209836/2302.html:text/html},
}

@misc{schafer_adaptive_2023,
	title = {Adaptive Test Generation Using a Large Language Model},
	url = {http://arxiv.org/abs/2302.06527},
	abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. This paper presents {TestPilot}, an adaptive test generation technique that leverages Large Language Models ({LLMs}). {TestPilot} uses Codex, an off-the-shelf {LLM}, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests. In our approach, Codex is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. If a generated test fails, {TestPilot}'s adaptive component attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We created an implementation of {TestPilot} for {JavaScript} and evaluated it on 25 npm packages with a total of 1,684 {API} functions to generate tests for. Our results show that the generated tests achieve up to 93.1\% statement coverage (median 68.2\%). Moreover, on average, 58.5\% of the generated tests contain at least one assertion that exercises functionality from the package under test. Our experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. Finally, we find that {TestPilot} does not generate memorized tests: 92.7\% of our generated tests have \${\textbackslash}leq\$ 50\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies.},
	number = {{arXiv}:2302.06527},
	publisher = {{arXiv}},
	author = {Schäfer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
	urldate = {2023-06-10},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.06527 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209202/Schäfer 等 - 2023 - Adaptive Test Generation Using a Large Language Mo.pdf:application/pdf;arXiv.org Snapshot:files/209268/2302.html:text/html},
}

@misc{xia_conversational_2023,
	title = {Conversational Automated Program Repair},
	url = {http://arxiv.org/abs/2301.13246},
	abstract = {Automated Program Repair ({APR}) can help developers automatically generate patches for bugs. Due to the impressive performance obtained using Large Pre-Trained Language Models ({LLMs}) on many code related tasks, researchers have started to directly use {LLMs} for {APR}. However, prior approaches simply repeatedly sample the {LLM} given the same constructed input/prompt created from the original buggy code, which not only leads to generating the same incorrect patches repeatedly but also miss the critical information in testcases. To address these limitations, we propose conversational {APR}, a new paradigm for program repair that alternates between patch generation and validation in a conversational manner. In conversational {APR}, we iteratively build the input to the model by combining previously generated patches with validation feedback. As such, we leverage the long-term context window of {LLMs} to not only avoid generating previously incorrect patches but also incorporate validation feedback to help the model understand the semantic meaning of the program under test. We evaluate 10 different {LLM} including the newly developed {ChatGPT} model to demonstrate the improvement of conversational {APR} over the prior {LLM} for {APR} approach.},
	number = {{arXiv}:2301.13246},
	publisher = {{arXiv}},
	author = {Xia, Chunqiu Steven and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2023-01-30},
	eprinttype = {arxiv},
	eprint = {2301.13246 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208877/Xia 和 Zhang - 2023 - Conversational Automated Program Repair.pdf:application/pdf;arXiv.org Snapshot:files/209787/2301.html:text/html},
}

@misc{yu_codereval_2023,
	title = {{CoderEval}: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
	url = {http://arxiv.org/abs/2302.00288},
	shorttitle = {{CoderEval}},
	abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, {CodeGen}, and {PanGu}-Coder. To validate the performance of these models, multiple existing benchmarks (e.g., {AiXBench} and {HumanEval}) are proposed, including only cases of generating a standalone function, i.e., a function that invokes or accesses only built-in functions and standard libraries. However, standalone functions constitute only about 30{\textbackslash}\% of functions from real open-source projects. To assess a model's performance for pragmatic code generation (i.e., code generation for real settings of open source or proprietary code), in this paper, we propose a benchmark named {CoderEval} of pragmatic code generation with generative pre-trained models. Compared with the widely-used {HumanEval} benchmark from {OpenAI}, {CoderEval} can be used to assess the performance of models against pragmatic code generation beyond just generating standalone functions. Through the evaluation of three public available models ({CodeGen}, {PanGu}-Coder, and Codex) on {CoderEval}, we analyze and discuss the current progress and future directions of pragmatic code generation with a generative pre-trained model.},
	number = {{arXiv}:2302.00288},
	publisher = {{arXiv}},
	author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Xie, Tao and Wang, Qianxiang},
	urldate = {2023-06-10},
	date = {2023-02-01},
	eprinttype = {arxiv},
	eprint = {2302.00288 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208901/Yu 等 - 2023 - CoderEval A Benchmark of Pragmatic Code Generatio.pdf:application/pdf;arXiv.org Snapshot:files/208919/2302.html:text/html},
}

@misc{sobania_analysis_2023,
	title = {An Analysis of the Automatic Bug Fixing Performance of {ChatGPT}},
	url = {http://arxiv.org/abs/2301.08653},
	abstract = {To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is {ChatGPT}. The bug fixing performance of {ChatGPT}, however, is so far unclear. Therefore, in this paper we evaluate {ChatGPT} on the standard bug fixing benchmark set, {QuixBugs}, and compare the performance with the results of several other approaches reported in the literature. We find that {ChatGPT}'s bug fixing performance is competitive to the common deep learning approaches {CoCoNut} and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, {ChatGPT} offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to {ChatGPT}, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.},
	number = {{arXiv}:2301.08653},
	publisher = {{arXiv}},
	author = {Sobania, Dominik and Briesch, Martin and Hanna, Carol and Petke, Justyna},
	urldate = {2023-06-10},
	date = {2023-01-20},
	eprinttype = {arxiv},
	eprint = {2301.08653 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208900/Sobania 等 - 2023 - An Analysis of the Automatic Bug Fixing Performanc.pdf:application/pdf;arXiv.org Snapshot:files/208918/2301.html:text/html},
}

@misc{ross_case_2023,
	title = {A Case Study in Engineering a Conversational Programming Assistant's Persona},
	url = {http://arxiv.org/abs/2301.10016},
	abstract = {The Programmer's Assistant is an experimental prototype software development environment that integrates a chatbot with a code editor. Conversational capability was achieved by using an existing code-fluent Large Language Model and providing it with a prompt that establishes a conversational interaction pattern, a set of conventions, and a style of interaction appropriate for the application. A discussion of the evolution of the prompt provides a case study in how to coax an existing foundation model to behave in a desirable manner for a particular application.},
	number = {{arXiv}:2301.10016},
	publisher = {{arXiv}},
	author = {Ross, Steven I. and Muller, Michael and Martinez, Fernando and Houde, Stephanie and Weisz, Justin D.},
	urldate = {2023-06-10},
	date = {2023-01-13},
	eprinttype = {arxiv},
	eprint = {2301.10016 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209516/Ross 等 - 2023 - A Case Study in Engineering a Conversational Progr.pdf:application/pdf;arXiv.org Snapshot:files/209521/2301.html:text/html},
}

@misc{treude_navigating_2023,
	title = {Navigating Complexity in Software Engineering: A Prototype for Comparing {GPT}-n Solutions},
	url = {http://arxiv.org/abs/2301.12169},
	shorttitle = {Navigating Complexity in Software Engineering},
	abstract = {Navigating the diverse solution spaces of non-trivial software engineering tasks requires a combination of technical knowledge, problem-solving skills, and creativity. With multiple possible solutions available, each with its own set of trade-offs, it is essential for programmers to evaluate the various options and select the one that best suits the specific requirements and constraints of a project. Whether it is choosing from a range of libraries, weighing the pros and cons of different architecture and design solutions, or finding unique ways to fulfill user requirements, the ability to think creatively is crucial for making informed decisions that will result in efficient and effective software. However, the interfaces of current chatbot tools for programmers, such as {OpenAI}'s {ChatGPT} or {GitHub} Copilot, are optimized for presenting a single solution, even for complex queries. While other solutions can be requested, they are not displayed by default and are not intuitive to access. In this paper, we present our work-in-progress prototype "{GPTCompare}", which allows programmers to visually compare multiple source code solutions generated by {GPT}-n models for the same programming-related query by highlighting their similarities and differences.},
	number = {{arXiv}:2301.12169},
	publisher = {{arXiv}},
	author = {Treude, Christoph},
	urldate = {2023-06-10},
	date = {2023-01-28},
	eprinttype = {arxiv},
	eprint = {2301.12169 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/208881/Treude - 2023 - Navigating Complexity in Software Engineering A P.pdf:application/pdf;arXiv.org Snapshot:files/209206/2301.html:text/html},
}

@misc{ding_unified_2023,
	title = {A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific {NLP} Tasks},
	url = {http://arxiv.org/abs/2212.05251},
	abstract = {By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models ({PLMs}) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the {PLM} fine-tuning stage. We propose {KnowledgeDA}, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, {KnowledgeDA} can automatically generate a domain-specific language model following three steps: (i) localize domain knowledge entities in texts via an embedding-similarity approach; (ii) generate augmented samples by retrieving replaceable domain entity pairs from two views of both knowledge graph and training data; (iii) select high-quality augmented samples for fine-tuning via confidence-based assessment. We implement a prototype of {KnowledgeDA} to learn language models for two domains, healthcare and software development. Experiments on domain-specific text classification and {QA} tasks verify the effectiveness and generalizability of {KnowledgeDA}.},
	number = {{arXiv}:2212.05251},
	publisher = {{arXiv}},
	author = {Ding, Ruiqing and Han, Xiao and Wang, Leye},
	urldate = {2023-06-10},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2212.05251 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209518/Ding 等 - 2023 - A Unified Knowledge Graph Augmentation Service for.pdf:application/pdf;arXiv.org Snapshot:files/209524/2212.html:text/html},
}

@misc{haque_i_2022,
	title = {"I think this is the most disruptive technology": Exploring Sentiments of {ChatGPT} Early Adopters using Twitter Data},
	url = {http://arxiv.org/abs/2212.05856},
	shorttitle = {"I think this is the most disruptive technology"},
	abstract = {Large language models have recently attracted significant attention due to their impressive performance on a variety of tasks. {ChatGPT} developed by {OpenAI} is one such implementation of a large, pre-trained language model that has gained immense popularity among early adopters, where certain users go to the extent of characterizing it as a disruptive technology in many domains. Understanding such early adopters' sentiments is important because it can provide insights into the potential success or failure of the technology, as well as its strengths and weaknesses. In this paper, we conduct a mixed-method study using 10,732 tweets from early {ChatGPT} users. We first use topic modelling to identify the main topics and then perform an in-depth qualitative sentiment analysis of each topic. Our results show that the majority of the early adopters have expressed overwhelmingly positive sentiments related to topics such as Disruptions to software development, Entertainment and exercising creativity. Only a limited percentage of users expressed concerns about issues such as the potential for misuse of {ChatGPT}, especially regarding topics such as Impact on educational aspects. We discuss these findings by providing specific examples for each topic and then detail implications related to addressing these concerns for both researchers and users.},
	number = {{arXiv}:2212.05856},
	publisher = {{arXiv}},
	author = {Haque, Mubin Ul and Dharmadasa, Isuru and Sworna, Zarrin Tasnim and Rajapakse, Roshan Namal and Ahmad, Hussain},
	urldate = {2023-06-10},
	date = {2022-12-12},
	eprinttype = {arxiv},
	eprint = {2212.05856 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208896/Haque 等 - 2022 - I think this is the most disruptive technology .pdf:application/pdf;arXiv.org Snapshot:files/209716/2212.html:text/html},
}

@misc{sun_pre-trained_2022,
	title = {A Pre-Trained {BERT} Model for Android Applications},
	url = {http://arxiv.org/abs/2212.05976},
	abstract = {The automation of an increasingly large number of software engineering tasks is becoming possible thanks to Machine Learning ({ML}). One foundational building block in the application of {ML} to software artifacts is the representation of these artifacts (e.g., source code or executable code) into a form that is suitable for learning. Many studies have leveraged representation learning, delegating to {ML} itself the job of automatically devising suitable representations. Yet, in the context of Android problems, existing models are either limited to coarse-grained whole-app level (e.g., apk2vec) or conducted for one specific downstream task (e.g., smali2vec). Our work is part of a new line of research that investigates effective, task-agnostic, and fine-grained universal representations of bytecode to mitigate both of these two limitations. Such representations aim to capture information relevant to various low-level downstream tasks (e.g., at the class-level). We are inspired by the field of Natural Language Processing, where the problem of universal representation was addressed by building Universal Language Models, such as {BERT}, whose goal is to capture abstract semantic information about sentences, in a way that is reusable for a variety of tasks. We propose {DexBERT}, a {BERT}-like Language Model dedicated to representing chunks of {DEX} bytecode, the main binary format used in Android applications. We empirically assess whether {DexBERT} is able to model the {DEX} language and evaluate the suitability of our model in two distinct class-level software engineering tasks: Malicious Code Localization and Defect Prediction. We also experiment with strategies to deal with the problem of catering to apps having vastly different sizes, and we demonstrate one example of using our technique to investigate what information is relevant to a given task.},
	number = {{arXiv}:2212.05976},
	publisher = {{arXiv}},
	author = {Sun, Tiezhu and Allix, Kevin and Kim, Kisub and Zhou, Xin and Kim, Dongsun and Lo, David and Bissyandé, Tegawendé F. and Klein, Jacques},
	urldate = {2023-06-10},
	date = {2022-12-12},
	eprinttype = {arxiv},
	eprint = {2212.05976 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209519/Sun 等 - 2022 - A Pre-Trained BERT Model for Android Applications.pdf:application/pdf;arXiv.org Snapshot:files/209525/2212.html:text/html},
}

@misc{zan_when_2022,
	title = {When Language Model Meets Private Library},
	url = {http://arxiv.org/abs/2210.17236},
	abstract = {With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries. In practice, it is common for programmers to write code using private libraries. However, this is a challenge for language models since they have never seen private {APIs} during training. Motivated by the fact that private libraries usually come with elaborate {API} documentation, we propose a novel framework with two modules: the {APIRetriever} finds useful {APIs}, and then the {APICoder} generates code using these {APIs}. For {APIRetriever}, we present a dense retrieval system and also design a friendly interaction to involve uses. For {APICoder}, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing {API} information. Both modules are trained with data from public libraries and can be generalized to private ones. Furthermore, we craft three benchmarks for private libraries, named {TorchDataEval}, {MonkeyEval}, and {BeatNumEval}. Experimental results demonstrate the impressive performance of our framework.},
	number = {{arXiv}:2210.17236},
	publisher = {{arXiv}},
	author = {Zan, Daoguang and Chen, Bei and Lin, Zeqi and Guan, Bei and Wang, Yongji and Lou, Jian-Guang},
	urldate = {2023-06-10},
	date = {2022-10-31},
	eprinttype = {arxiv},
	eprint = {2210.17236 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209527/Zan 等 - 2022 - When Language Model Meets Private Library.pdf:application/pdf;arXiv.org Snapshot:files/209811/2210.html:text/html},
}

@misc{leinonen_using_2022,
	title = {Using Large Language Models to Enhance Programming Error Messages},
	url = {http://arxiv.org/abs/2210.11630},
	abstract = {A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix the error. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.},
	number = {{arXiv}:2210.11630},
	publisher = {{arXiv}},
	author = {Leinonen, Juho and Hellas, Arto and Sarsa, Sami and Reeves, Brent and Denny, Paul and Prather, James and Becker, Brett A.},
	urldate = {2023-06-10},
	date = {2022-10-20},
	eprinttype = {arxiv},
	eprint = {2210.11630 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/208889/Leinonen 等 - 2022 - Using Large Language Models to Enhance Programming.pdf:application/pdf;arXiv.org Snapshot:files/209209/2210.html:text/html},
}

@misc{taesiri_large_2022,
	title = {Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors},
	url = {http://arxiv.org/abs/2210.02506},
	abstract = {Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While {AI}-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the {GameBugDescriptions} benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the {OPT} and {InstructGPT} large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66\%, and on some video games, up to 78.94\%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/{LLMxBugs}},
	number = {{arXiv}:2210.02506},
	publisher = {{arXiv}},
	author = {Taesiri, Mohammad Reza and Macklon, Finlay and Wang, Yihe and Shen, Hengshuo and Bezemer, Cor-Paul},
	urldate = {2023-06-10},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2210.02506 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209210/Taesiri 等 - 2022 - Large Language Models are Pretty Good Zero-Shot Vi.pdf:application/pdf;arXiv.org Snapshot:files/209271/2210.html:text/html},
}

@misc{cheng_binding_2023,
	title = {Binding Language Models in Symbolic Languages},
	url = {http://arxiv.org/abs/2210.02875},
	abstract = {Though end-to-end neural approaches have recently been dominating {NLP} tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified {API} of language model ({LM}) functionalities to a programming language (e.g., {SQL}, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an {LM} as both the program parser and the underlying model called by the {API} during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ {GPT}-3 Codex as the {LM}. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate {API} calls to prompt Codex to solve the unanswerable part, and identify where to place the {API} calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense {QA}, information extraction) given proper prompts in the {API} calls. Binder achieves state-of-the-art results on {WikiTableQuestions} and {TabFact} datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/{HKUNLP}/Binder .},
	number = {{arXiv}:2210.02875},
	publisher = {{arXiv}},
	author = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
	urldate = {2023-06-10},
	date = {2023-02-28},
	eprinttype = {arxiv},
	eprint = {2210.02875 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209211/Cheng 等 - 2023 - Binding Language Models in Symbolic Languages.pdf:application/pdf;arXiv.org Snapshot:files/209802/2210.html:text/html},
}

@misc{meng_cobert_2022,
	title = {{CoBERT}: Self-Supervised Speech Representation Learning Through Code Representation Learning},
	url = {http://arxiv.org/abs/2210.04062},
	shorttitle = {{CoBERT}},
	abstract = {Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code {BERT} ({CoBERT}) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. {CoBERT} outperforms the most recent state-of-the-art performance on the {ASR} task and brings significant improvements on the {SUPERB} speech translation ({ST}) task. Our code and models are released at https://github.com/mct10/{CoBERT}.},
	number = {{arXiv}:2210.04062},
	publisher = {{arXiv}},
	author = {Meng, Chutong and Ao, Junyi and Ko, Tom and Wang, Mingxuan and Li, Haizhou},
	urldate = {2023-06-10},
	date = {2022-12-01},
	eprinttype = {arxiv},
	eprint = {2210.04062 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:files/208899/Meng 等 - 2022 - CoBERT Self-Supervised Speech Representation Lear.pdf:application/pdf;arXiv.org Snapshot:files/209212/2210.html:text/html},
}

@misc{sun_dont_2023,
	title = {Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems},
	url = {http://arxiv.org/abs/2209.05948},
	abstract = {Currently, large pre-trained language models are widely applied in neural code completion systems. Though large code models significantly outperform their smaller counterparts, around 70\% displayed code completions from Copilot are not accepted by developers. Being reviewed but not accepted, their help to developer productivity is considerably limited. Even worse, considering the high cost of the large code models, it is a huge waste of computing resources and energy. To fill this significant gap, we first investigate the prompts of unhelpful code completions, and empirically find four observable patterns that cause such prompts, all of which are inherent, namely, they can hardly be addressed by improving the accuracy of the model. This demonstrates the feasibility of identifying such prompts based on the prompts themselves. Motivated by this finding, we propose an early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities without sending them to the code completion system. Furthermore, we propose a lightweight Transformer-based estimator to demonstrate the feasibility of the mechanism. The experimental results show that the proposed estimator helps save 23.3\% of computational cost measured in floating-point operations for the code completion systems, and 80.2\% of rejected prompts lead to unhelpful completion},
	number = {{arXiv}:2209.05948},
	publisher = {{arXiv}},
	author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Ni, Mingze and Li, Li},
	urldate = {2023-06-10},
	date = {2023-02-01},
	eprinttype = {arxiv},
	eprint = {2209.05948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208953/Sun 等 - 2023 - Don't Complete It! Preventing Unhelpful Code Compl.pdf:application/pdf;arXiv.org Snapshot:files/209834/2209.html:text/html},
}

@misc{liang_code_2023,
	title = {Code as Policies: Language Model Programs for Embodied Control},
	url = {http://arxiv.org/abs/2209.07753},
	shorttitle = {Code as Policies},
	abstract = {Large language models ({LLMs}) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing {LLMs} can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive {APIs}. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), {LLMs} can take in new commands and autonomously re-compose {API} calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., {NumPy}, Shapely) to perform arithmetic, {LLMs} used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs ({LMPs}) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the {HumanEval} [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
	number = {{arXiv}:2209.07753},
	publisher = {{arXiv}},
	author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2209.07753 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/208959/Liang 等 - 2023 - Code as Policies Language Model Programs for Embo.pdf:application/pdf;arXiv.org Snapshot:files/208980/2209.html:text/html},
}

@misc{li_cctest_2023,
	title = {{CCTEST}: Testing and Repairing Code Completion Systems},
	url = {http://arxiv.org/abs/2208.08289},
	shorttitle = {{CCTEST}},
	abstract = {Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models ({LLMs}). To date, visible {LLM}-based code completion frameworks such as {GitHub} Copilot and {GPT} are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes {CCTEST}, a framework to test and repair code completion systems in blackbox settings. {CCTEST} features a set of novel mutation strategies, namely program structure-correlated ({PSC}) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, {CCTEST} repairs the code completion outputs by selecting the output that mostly reflects the "average" appearance of all output cases, as the final output of the code completion systems. We detected a total of 33,540 inputs (with a true positive rate of 86\%) that can trigger erroneous cases from eight popular {LLM}-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40\% and 67\% with respect to {BLEU} score and Levenshtein edit similarity.},
	number = {{arXiv}:2208.08289},
	publisher = {{arXiv}},
	author = {Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun},
	urldate = {2023-06-10},
	date = {2023-05-08},
	eprinttype = {arxiv},
	eprint = {2208.08289 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209535/Li 等 - 2023 - CCTEST Testing and Repairing Code Completion Syst.pdf:application/pdf;arXiv.org Snapshot:files/209539/2208.html:text/html},
}

@misc{lahiri_interactive_2022,
	title = {Interactive Code Generation via Test-Driven User-Intent Formalization},
	url = {http://arxiv.org/abs/2208.05950},
	abstract = {Pre-trained large language models ({LLMs}) such as {OpenAI} Codex have shown immense potential in automating significant aspects of coding by producing natural code from informal natural language ({NL}) intent. However, the code produced does not have any correctness guarantees around satisfying user's intent. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics. In this paper, we take a first step towards addressing the problem above by proposing the workflow of test-driven user-intent formalization ({TDUIF}), which leverages lightweight user feedback to jointly (a) formalize the user intent as tests (a partial specification), and (b) generates code that meets the formal user intent. To perform a scalable and large-scale automated evaluation of the algorithms without requiring a user in the loop, we describe how to simulate user interaction with high-fidelity using a reference solution. We also describe and implement alternate implementations of several algorithmic components (including mutating and ranking a set of tests) that can be composed for efficient solutions to the {TDUIF} problem. We have developed a system {TICODER} that implements several solutions to {TDUIF}, and compare their relative effectiveness on the {MBPP} academic code generation benchmark. Our results are promising with using the {OpenAI} Codex {LLM} on {MBPP}: our best algorithm improves the pass@1 code generation accuracy metric from 48.39\% to 70.49\% with a single user query, and up to 85.48\% with up to 5 user queries. Second, we can generate a non-trivial functional unit test consistent with the user intent within an average of 1.69 user queries for 90.40\% of the examples for this dataset.},
	number = {{arXiv}:2208.05950},
	publisher = {{arXiv}},
	author = {Lahiri, Shuvendu K. and Naik, Aaditya and Sakkas, Georgios and Choudhury, Piali and von Veh, Curtis and Musuvathi, Madanlal and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng},
	urldate = {2023-06-10},
	date = {2022-08-11},
	eprinttype = {arxiv},
	eprint = {2208.05950 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208934/Lahiri 等 - 2022 - Interactive Code Generation via Test-Driven User-I.pdf:application/pdf;arXiv.org Snapshot:files/209799/2208.html:text/html},
}

@misc{zhou_docprompting_2023,
	title = {{DocPrompting}: Generating Code by Retrieving the Docs},
	url = {http://arxiv.org/abs/2207.05987},
	shorttitle = {{DocPrompting}},
	abstract = {Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available {APIs} by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce {DocPrompting}: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an {NL} intent, and (2) generating code based on the {NL} intent and the retrieved documentation. {DocPrompting} is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that {DocPrompting} consistently improves {NL}-to-code models: {DocPrompting} improves strong base models such as {CodeT}5 by 2.85\% in pass@1 (52\% relative gain) and 4.39\% in pass@10 (30\% relative gain) in execution-based evaluation on the popular Python {CoNaLa} benchmark; on a new Bash dataset tldr, {DocPrompting} improves {CodeT}5 and {GPT}-Neo1.3B by up to absolute 6.9\% exact match.},
	number = {{arXiv}:2207.05987},
	publisher = {{arXiv}},
	author = {Zhou, Shuyan and Alon, Uri and Xu, Frank F. and Wang, Zhiruo and Jiang, Zhengbao and Neubig, Graham},
	urldate = {2023-06-10},
	date = {2023-02-18},
	eprinttype = {arxiv},
	eprint = {2207.05987 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208926/Zhou 等 - 2023 - DocPrompting Generating Code by Retrieving the Do.pdf:application/pdf;arXiv.org Snapshot:files/208946/2207.html:text/html},
}

@misc{patil_poet_2022,
	title = {{POET}: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging},
	url = {http://arxiv.org/abs/2207.07697},
	shorttitle = {{POET}},
	abstract = {Fine-tuning models on edge devices like mobile phones would enable privacy-preserving personalization over sensitive data. However, edge training has historically been limited to relatively small models with simple architectures because training is both memory and energy intensive. We present {POET}, an algorithm to enable training large neural networks on memory-scarce battery-operated edge devices. {POET} jointly optimizes the integrated search search spaces of rematerialization and paging, two algorithms to reduce the memory consumption of backpropagation. Given a memory budget and a run-time constraint, we formulate a mixed-integer linear program ({MILP}) for energy-optimal training. Our approach enables training significantly larger models on embedded devices while reducing energy consumption while not modifying mathematical correctness of backpropagation. We demonstrate that it is possible to fine-tune both {ResNet}-18 and {BERT} within the memory constraints of a Cortex-M class embedded device while outperforming current edge training methods in energy efficiency. {POET} is an open-source project available at https://github.com/{ShishirPatil}/poet},
	number = {{arXiv}:2207.07697},
	publisher = {{arXiv}},
	author = {Patil, Shishir G. and Jain, Paras and Dutta, Prabal and Stoica, Ion and Gonzalez, Joseph E.},
	urldate = {2023-06-10},
	date = {2022-07-15},
	eprinttype = {arxiv},
	eprint = {2207.07697 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:files/208923/Patil 等 - 2022 - POET Training Neural Networks on Tiny Devices wit.pdf:application/pdf;arXiv.org Snapshot:files/209839/2207.html:text/html},
}

@misc{kassianik_bigissue_2023,
	title = {{BigIssue}: A Realistic Bug Localization Benchmark},
	url = {http://arxiv.org/abs/2207.10739},
	shorttitle = {{BigIssue}},
	abstract = {As machine learning tools progress, the inevitable question arises: How can machine learning help us write better code? With significant progress being achieved in natural language processing with models like {GPT}-3 and Bert, the applications of natural language processing techniques to code are starting to be explored. Most of the research has been focused on automatic program repair ({APR}), and while the results on synthetic or highly filtered datasets are promising, such models are hard to apply in real-world scenarios because of inadequate bug localization. We propose {BigIssue}: a benchmark for realistic bug localization. The goal of the benchmark is two-fold. We provide (1) a general benchmark with a diversity of real and synthetic Java bugs and (2) a motivation to improve bug localization capabilities of models through attention to the full repository context. With the introduction of {BigIssue}, we hope to advance the state of the art in bug localization, in turn improving {APR} performance and increasing its applicability to the modern development cycle.},
	number = {{arXiv}:2207.10739},
	publisher = {{arXiv}},
	author = {Kassianik, Paul and Nijkamp, Erik and Pang, Bo and Zhou, Yingbo and Xiong, Caiming},
	urldate = {2023-06-10},
	date = {2023-05-04},
	eprinttype = {arxiv},
	eprint = {2207.10739 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209213/Kassianik 等 - 2023 - BigIssue A Realistic Bug Localization Benchmark.pdf:application/pdf;arXiv.org Snapshot:files/209694/2207.html:text/html},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
	url = {http://arxiv.org/abs/2206.04615},
	shorttitle = {Beyond the Imitation Game},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark ({BIG}-bench). {BIG}-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. {BIG}-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of {OpenAI}'s {GPT} models, Google-internal dense transformer architectures, and Switch-style sparse transformers on {BIG}-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	number = {{arXiv}:2206.04615},
	publisher = {{arXiv}},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and {McDonell}, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and {McElrath}, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and {LeBras}, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	urldate = {2023-06-10},
	date = {2022-06-10},
	eprinttype = {arxiv},
	eprint = {2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209540/Srivastava 等 - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf:application/pdf;arXiv.org Snapshot:files/209545/2206.html:text/html},
}

@misc{mastropaolo_using_2022-2,
	title = {Using Transfer Learning for Code-Related Tasks},
	url = {http://arxiv.org/abs/2206.08574},
	abstract = {Deep learning ({DL}) techniques have been used to support several code-related tasks such as code summarization and bug-fixing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing ({NLP}) tasks. The basic idea behind these models is to first pre-train them on a generic dataset using a self-supervised task (e.g, filling masked words in sentences). Then, these models are fine-tuned to support specific tasks of interest (e.g, language translation). A single model can be fine-tuned to support multiple tasks, possibly exploiting the benefits of transfer learning. This means that knowledge acquired to solve a specific task (e.g, language translation) can be useful to boost performance on another task (e.g, sentiment classification). While the benefits of transfer learning have been widely studied in {NLP}, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-fixing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task fine-tuning on the model's performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks benefit from a multi-task fine-tuning.},
	number = {{arXiv}:2206.08574},
	publisher = {{arXiv}},
	author = {Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2022-06-17},
	eprinttype = {arxiv},
	eprint = {2206.08574 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209589/Mastropaolo 等 - 2022 - Using Transfer Learning for Code-Related Tasks.pdf:application/pdf;arXiv.org Snapshot:files/209593/2206.html:text/html},
}

@misc{bareis_code_2022,
	title = {Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code},
	url = {http://arxiv.org/abs/2206.01335},
	shorttitle = {Code Generation Tools (Almost) for Free?},
	abstract = {Few-shot learning with large-scale, pre-trained language models is a powerful way to answer questions about code, e.g., how to complete a given code example, or even generate code snippets from scratch. The success of these models raises the question whether they could serve as a basis for building a wide range code generation tools. Traditionally, such tools are built manually and separately for each task. Instead, few-shot learning may allow to obtain different tools from a single pre-trained language model by simply providing a few examples or a natural language description of the expected tool behavior. This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose. We consider three code manipulation and code generation tasks targeted by a range of traditional tools: (i) code mutation; (ii) test oracle generation from natural language documentation; and (iii) test case generation. For each task, we compare few-shot learning to a manually built tool. Our results show that the model-based tools complement (code mutation), are on par (test oracle generation), or even outperform their respective traditionally built tool (test case generation), while imposing far less effort to develop them. By comparing the effectiveness of different variants of the model-based tools, we provide insights on how to design an appropriate input ("prompt") to the model and what influence the size of the model has. For example, we find that providing a small natural language description of the code generation task is an easy way to improve predictions. Overall, we conclude that few-shot language models are surprisingly effective, yet there is still more work to be done, such as exploring more diverse ways of prompting and tackling even more involved tasks.},
	number = {{arXiv}:2206.01335},
	publisher = {{arXiv}},
	author = {Bareiß, Patrick and Souza, Beatriz and d'Amorim, Marcelo and Pradel, Michael},
	urldate = {2023-06-10},
	date = {2022-06-12},
	eprinttype = {arxiv},
	eprint = {2206.01335 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209526/Bareiß 等 - 2022 - Code Generation Tools (Almost) for Free A Study o.pdf:application/pdf;arXiv.org Snapshot:files/209791/2206.html:text/html},
}

@misc{lu_neuro-symbolic_2023,
	title = {Neuro-Symbolic Procedural Planning with Commonsense Prompting},
	url = {http://arxiv.org/abs/2206.02928},
	abstract = {Procedural planning aims to implement complex high-level goals by decomposition into sequential simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models ({LLMs}) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from {LLMs} in the zero-shot setting. However, such elicited pre-trained knowledge in {LLMs} induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural {PLANner} ({PLAN}) that elicits procedural planning knowledge from the {LLMs} with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on {WikiHow} and {RobotHow} show the superiority of {PLAN} on procedural planning without further training or manual exemplars.},
	number = {{arXiv}:2206.02928},
	publisher = {{arXiv}},
	author = {Lu, Yujie and Feng, Weixi and Zhu, Wanrong and Xu, Wenda and Wang, Xin Eric and Eckstein, Miguel and Wang, William Yang},
	urldate = {2023-06-10},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2206.02928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209003/Lu 等 - 2023 - Neuro-Symbolic Procedural Planning with Commonsens.pdf:application/pdf;arXiv.org Snapshot:files/209841/2206.html:text/html},
}

@misc{zhao_gap-gen_2023,
	title = {{GAP}-Gen: Guided Automatic Python Code Generation},
	url = {http://arxiv.org/abs/2201.08810},
	shorttitle = {{GAP}-Gen},
	abstract = {Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose {GAP}-Gen, a Guided Automatic Python Code Generation method based on Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree ({AST}) reducing the size and high complexity of Abstract Syntax Tree but maintaining crucial syntactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently through out the code. In our work, rather than pretraining, we focus on modifying the finetuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. {GAP}-Gen fine-tunes the transformer based language models T5 and {CodeT}5 using the Code-to-Docstring datasets {CodeSearchNet}, {CodeSearchNet} {AdvTest} and Code-Docstring Corpus from {EdinburghNLP}. Our experiments show that {GAP}-Gen achieves better results on automatic Python code generation task than previous works.},
	number = {{arXiv}:2201.08810},
	publisher = {{arXiv}},
	author = {Zhao, Junchen and Song, Yurun and Wang, Junlin and Harris, Ian G.},
	urldate = {2023-06-10},
	date = {2023-05-09},
	eprinttype = {arxiv},
	eprint = {2201.08810 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209216/Zhao 等 - 2023 - GAP-Gen Guided Automatic Python Code Generation.pdf:application/pdf;arXiv.org Snapshot:files/209273/2201.html:text/html},
}

@misc{poesia_synchromesh_2022,
	title = {Synchromesh: Reliable code generation from pre-trained language models},
	url = {http://arxiv.org/abs/2201.11227},
	shorttitle = {Synchromesh},
	abstract = {Large pre-trained language models have been used to generate code,providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning ({TST}), a novel method for semantic example selection. {TST} learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding ({CSD}): a general framework for constraining the output to a set of valid programs in the target language. {CSD} leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using {GPT}-3 and Codex in three real-world languages: {SQL} queries, Vega-Lite visualizations and {SMCalFlow} programs. These domains showcase rich constraints that {CSD} is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from {CSD} and {TST} in prediction accuracy and in effectively preventing run-time errors.},
	number = {{arXiv}:2201.11227},
	publisher = {{arXiv}},
	author = {Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
	urldate = {2023-06-10},
	date = {2022-01-26},
	eprinttype = {arxiv},
	eprint = {2201.11227 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209530/Poesia 等 - 2022 - Synchromesh Reliable code generation from pre-tra.pdf:application/pdf;arXiv.org Snapshot:files/209533/2201.html:text/html},
}

@misc{chen_varclr_2021,
	title = {{VarCLR}: Variable Semantic Representation Pre-training via Contrastive Learning},
	url = {http://arxiv.org/abs/2112.02650},
	shorttitle = {{VarCLR}},
	abstract = {Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture relatedness (whether two variables are linked at all), rather than similarity (whether they actually have the same meaning). We propose {VarCLR}, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from {GitHub} edits. We show that {VarCLR} enables the effective application of sophisticated, general-purpose language models like {BERT}, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. {VarCLR} produces models that significantly outperform the state-of-the-art on {IdBench}, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names.},
	number = {{arXiv}:2112.02650},
	publisher = {{arXiv}},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J. and Neubig, Graham and Vasilescu, Bogdan and Goues, Claire Le},
	urldate = {2023-06-10},
	date = {2021-12-05},
	eprinttype = {arxiv},
	eprint = {2112.02650 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/208977/Chen 等 - 2021 - VarCLR Variable Semantic Representation Pre-train.pdf:application/pdf;arXiv.org Snapshot:files/209224/2112.html:text/html},
}

@misc{prenner_automatic_2021,
	title = {Automatic Program Repair with {OpenAI}'s Codex: Evaluating {QuixBugs}},
	url = {http://arxiv.org/abs/2111.03922},
	shorttitle = {Automatic Program Repair with {OpenAI}'s Codex},
	abstract = {{OpenAI}'s Codex, a {GPT}-3 like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair. Our initial evaluation uses the multi-language {QuixBugs} benchmark (40 bugs in both Python and Java). We find that, despite not being trained for {APR}, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is slightly more successful at repairing Python than Java.},
	number = {{arXiv}:2111.03922},
	publisher = {{arXiv}},
	author = {Prenner, Julian Aron and Robbes, Romain},
	urldate = {2023-06-10},
	date = {2021-11-06},
	eprinttype = {arxiv},
	eprint = {2111.03922 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209214/Prenner 和 Robbes - 2021 - Automatic Program Repair with OpenAI's Codex Eval.pdf:application/pdf;arXiv.org Snapshot:files/209272/2111.html:text/html},
}

@misc{cito_counterfactual_2021,
	title = {Counterfactual Explanations for Models of Code},
	url = {http://arxiv.org/abs/2111.05711},
	abstract = {Machine learning ({ML}) models play an increasingly prevalent role in many software engineering tasks. However, because most models are now powered by opaque deep neural networks, it can be difficult for developers to understand why the model came to a certain conclusion and how to act upon the model's prediction. Motivated by this problem, this paper explores counterfactual explanations for models of source code. Such counterfactual explanations constitute minimal changes to the source code under which the model "changes its mind". We integrate counterfactual explanation generation to models of source code in a real-world setting. We describe considerations that impact both the ability to find realistic and plausible counterfactual explanations, as well as the usefulness of such explanation to the user of the model. In a series of experiments we investigate the efficacy of our approach on three different models, each based on a {BERT}-like architecture operating over source code.},
	number = {{arXiv}:2111.05711},
	publisher = {{arXiv}},
	author = {Cito, Jürgen and Dillig, Isil and Murali, Vijayaraghavan and Chandra, Satish},
	urldate = {2023-06-10},
	date = {2021-11-10},
	eprinttype = {arxiv},
	eprint = {2111.05711 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209529/Cito 等 - 2021 - Counterfactual Explanations for Models of Code.pdf:application/pdf;arXiv.org Snapshot:files/209531/2111.html:text/html},
}

@misc{karakus_amazon_2021,
	title = {Amazon {SageMaker} Model Parallelism: A General and Flexible Framework for Large Model Training},
	url = {http://arxiv.org/abs/2111.05972},
	shorttitle = {Amazon {SageMaker} Model Parallelism},
	abstract = {With deep learning models rapidly growing in size, systems-level solutions for large-model training are required. We present Amazon {SageMaker} model parallelism, a software library that integrates with {PyTorch}, and enables easy training of large models using model parallelism and other memory-saving features. In contrast to existing solutions, the implementation of the {SageMaker} library is much more generic and flexible, in that it can automatically partition and run pipeline parallelism over arbitrary model architectures with minimal code change, and also offers a general and extensible framework for tensor parallelism, which supports a wider range of use cases, and is modular enough to be easily applied to new training scripts. The library also preserves the native {PyTorch} user experience to a much larger degree, supporting module re-use and dynamic graphs, while giving the user full control over the details of the training step. We evaluate performance over {GPT}-3, {RoBERTa}, {BERT}, and neural collaborative filtering, and demonstrate competitive performance over existing solutions.},
	number = {{arXiv}:2111.05972},
	publisher = {{arXiv}},
	author = {Karakus, Can and Huilgol, Rahul and Wu, Fei and Subramanian, Anirudh and Daniel, Cade and Cavdar, Derya and Xu, Teng and Chen, Haohan and Rahnama, Arash and Quintela, Luis},
	urldate = {2023-06-10},
	date = {2021-11-10},
	eprinttype = {arxiv},
	eprint = {2111.05972 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:files/208960/Karakus 等 - 2021 - Amazon SageMaker Model Parallelism A General and .pdf:application/pdf;arXiv.org Snapshot:files/209792/2111.html:text/html},
}

@misc{so_primer_2022,
	title = {Primer: Searching for Efficient Transformers for Language Modeling},
	url = {http://arxiv.org/abs/2109.08668},
	shorttitle = {Primer},
	abstract = {Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer {TensorFlow} program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring {ReLU} activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to {GPT}-3 {XL}, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.},
	number = {{arXiv}:2109.08668},
	publisher = {{arXiv}},
	author = {So, David R. and Mańke, Wojciech and Liu, Hanxiao and Dai, Zihang and Shazeer, Noam and Le, Quoc V.},
	urldate = {2023-06-10},
	date = {2022-01-24},
	eprinttype = {arxiv},
	eprint = {2109.08668 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/209537/So 等 - 2022 - Primer Searching for Efficient Transformers for L.pdf:application/pdf;arXiv.org Snapshot:files/209542/2109.html:text/html},
}

@misc{von_der_mosel_validity_2022,
	title = {On the validity of pre-trained transformers for natural language processing in the software engineering domain},
	url = {http://arxiv.org/abs/2109.04738},
	abstract = {Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare {BERT} transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.},
	number = {{arXiv}:2109.04738},
	publisher = {{arXiv}},
	author = {von der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen},
	urldate = {2023-06-10},
	date = {2022-05-12},
	eprinttype = {arxiv},
	eprint = {2109.04738 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/208952/von der Mosel 等 - 2022 - On the validity of pre-trained transformers for na.pdf:application/pdf;arXiv.org Snapshot:files/208974/2109.html:text/html},
}

@misc{yang_exploring_2021,
	title = {Exploring Decomposition for Table-based Fact Verification},
	url = {http://arxiv.org/abs/2109.11020},
	abstract = {Fact verification based on structured data is challenging as it requires models to understand both natural language and symbolic operations performed over tables. Although pre-trained language models have demonstrated a strong capability in verifying simple statements, they struggle with complex statements that involve multiple operations. In this paper, we improve fact verification by decomposing complex statements into simpler subproblems. Leveraging the programs synthesized by a weakly supervised semantic parser, we propose a program-guided approach to constructing a pseudo dataset for decomposition model training. The subproblems, together with their predicted answers, serve as the intermediate evidence to enhance our fact verification model. Experiments show that our proposed approach achieves the new state-of-the-art performance, an 82.7{\textbackslash}\% accuracy, on the {TabFact} benchmark.},
	number = {{arXiv}:2109.11020},
	publisher = {{arXiv}},
	author = {Yang, Xiaoyu and Zhu, Xiaodan},
	urldate = {2023-06-10},
	date = {2021-09-22},
	eprinttype = {arxiv},
	eprint = {2109.11020 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208965/Yang 和 Zhu - 2021 - Exploring Decomposition for Table-based Fact Verif.pdf:application/pdf;arXiv.org Snapshot:files/209765/2109.html:text/html},
}

@misc{deshpande_how_2021,
	title = {How Much Data Analytics is Enough? The {ROI} of Machine Learning Classification and its Application to Requirements Dependency Classification},
	url = {http://arxiv.org/abs/2109.14097},
	shorttitle = {How Much Data Analytics is Enough?},
	abstract = {Machine Learning ({ML}) can substantially improve the efficiency and effectiveness of organizations and is widely used for different purposes within Software Engineering. However, the selection and implementation of {ML} techniques rely almost exclusively on accuracy criteria. Thus, for organizations wishing to realize the benefits of {ML} investments, this narrow approach ignores crucial considerations around the anticipated costs of the {ML} activities across the {ML} lifecycle, while failing to account for the benefits that are likely to accrue from the proposed activity. We present findings for an approach that addresses this gap by enhancing the accuracy criterion with return on investment ({ROI}) considerations. Specifically, we analyze the performance of the two state-of-the-art {ML} techniques: Random Forest and Bidirectional Encoder Representations from Transformers ({BERT}), based on accuracy and {ROI} for two publicly available data sets. Specifically, we compare decision-making on requirements dependency extraction (i) exclusively based on accuracy and (ii) extended to include {ROI} analysis. As a result, we propose recommendations for selecting {ML} classification techniques based on the degree of training data used. Our findings indicate that considering {ROI} as additional criteria can drastically influence {ML} selection when compared to decisions based on accuracy as the sole criterion},
	number = {{arXiv}:2109.14097},
	publisher = {{arXiv}},
	author = {Deshpande, Gouri and Ruhe, Guenther and Saunders, Chad},
	urldate = {2023-06-10},
	date = {2021-09-28},
	eprinttype = {arxiv},
	eprint = {2109.14097 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/209534/Deshpande 等 - 2021 - How Much Data Analytics is Enough The ROI of Mach.pdf:application/pdf;arXiv.org Snapshot:files/209845/2109.html:text/html},
}

@misc{narasimhan_cgems_2021,
	title = {{CGEMs}: A Metric Model for Automatic Code Generation using {GPT}-3},
	url = {http://arxiv.org/abs/2108.10168},
	shorttitle = {{CGEMs}},
	abstract = {Today, {AI} technology is showing its strengths in almost every industry and walks of life. From text generation, text summarization, chatbots, {NLP} is being used widely. One such paradigm is automatic code generation. An {AI} could be generating anything; hence the output space is unconstrained. A self-driving car is driven for 100 million miles to validate its safety, but tests cannot be written to monitor and cover an unconstrained space. One of the solutions to validate {AI}-generated content is to constrain the problem and convert it from abstract to realistic, and this can be accomplished by either validating the unconstrained algorithm using theoretical proofs or by using Monte-Carlo simulation methods. In this case, we use the latter approach to test/validate a statistically significant number of samples. This hypothesis of validating the {AI}-generated code is the main motive of this work and to know if {AI}-generated code is reliable, a metric model {CGEMs} is proposed. This is an extremely challenging task as programs can have different logic with different naming conventions, but the metrics must capture the structure and logic of the program. This is similar to the importance grammar carries in {AI}-based text generation, Q\&A, translations, etc. The various metrics that are garnered in this work to support the evaluation of generated code are as follows: Compilation, {NL} description to logic conversion, number of edits needed, some of the commonly used static-code metrics and {NLP} metrics. These metrics are applied to 80 codes generated using {OpenAI}'s {GPT}-3. Post which a Neural network is designed for binary classification (acceptable/not acceptable quality of the generated code). The inputs to this network are the values of the features obtained from the metrics. The model achieves a classification accuracy of 76.92\% and an F1 score of 55.56\%. {XAI} is augmented for model interpretability.},
	number = {{arXiv}:2108.10168},
	publisher = {{arXiv}},
	author = {Narasimhan, Aishwarya and Rao, Krishna Prasad Agara Venkatesha and B, Veena M.},
	urldate = {2023-06-10},
	date = {2021-08-23},
	eprinttype = {arxiv},
	eprint = {2108.10168 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:files/209532/Narasimhan 等 - 2021 - CGEMs A Metric Model for Automatic Code Generatio.pdf:application/pdf;arXiv.org Snapshot:files/209778/2108.html:text/html},
}

@misc{karmakar_what_2021,
	title = {What do pre-trained code models know about code?},
	url = {http://arxiv.org/abs/2108.11308},
	abstract = {Pre-trained models of code built on the transformer architecture have performed well on software engineering ({SE}) tasks such as predictive code generation, code summarization, among others. However, whether the vector representations from these pre-trained models comprehensively encode characteristics of source code well enough to be applicable to a broad spectrum of downstream tasks remains an open question. One way to investigate this is with diagnostic tasks called probes. In this paper, we construct four probing tasks (probing for surface-level, syntactic, structural, and semantic information) for pre-trained code models. We show how probes can be used to identify whether models are deficient in (understanding) certain code properties, characterize different model layers, and get insight into the model sample-efficiency. We probe four models that vary in their expected knowledge of code properties: {BERT} (pre-trained on English), {CodeBERT} and {CodeBERTa} (pre-trained on source code, and natural language documentation), and {GraphCodeBERT} (pre-trained on source code with dataflow). While {GraphCodeBERT} performs more consistently overall, we find that {BERT} performs surprisingly well on some code tasks, which calls for further investigation.},
	number = {{arXiv}:2108.11308},
	publisher = {{arXiv}},
	author = {Karmakar, Anjan and Robbes, Romain},
	urldate = {2023-06-10},
	date = {2021-08-25},
	eprinttype = {arxiv},
	eprint = {2108.11308 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209536/Karmakar 和 Robbes - 2021 - What do pre-trained code models know about code.pdf:application/pdf;arXiv.org Snapshot:files/209790/2108.html:text/html},
}

@misc{ahmad_avatar_2023,
	title = {{AVATAR}: A Parallel Corpus for Java-Python Program Translation},
	url = {http://arxiv.org/abs/2108.11590},
	shorttitle = {{AVATAR}},
	abstract = {Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present {AVATAR}, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. {AVATAR} is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, {AVATAR} includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on {AVATAR}. Experiment results show that the models lack in generating functionally accurate code.},
	number = {{arXiv}:2108.11590},
	publisher = {{arXiv}},
	author = {Ahmad, Wasi Uddin and Tushar, Md Golam Rahman and Chakraborty, Saikat and Chang, Kai-Wei},
	urldate = {2023-06-10},
	date = {2023-05-04},
	eprinttype = {arxiv},
	eprint = {2108.11590 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209538/Ahmad 等 - 2023 - AVATAR A Parallel Corpus for Java-Python Program .pdf:application/pdf;arXiv.org Snapshot:files/209543/2108.html:text/html},
}

@misc{liang_mwp-bert_2022,
	title = {{MWP}-{BERT}: Numeracy-Augmented Pre-training for Math Word Problem Solving},
	url = {http://arxiv.org/abs/2107.13435},
	shorttitle = {{MWP}-{BERT}},
	abstract = {Math word problem ({MWP}) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for {MWP} solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, {MWP} solving has extra requirements in numerical reasoning. In other words, instead of the number value itself, it is the reusable numerical property that matters more in numerical reasoning. Therefore, we argue that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here. In this work, we introduce this idea to the popular pre-training language model ({PLM}) techniques and build {MWP}-{BERT}, an effective contextual number representation {PLM}. We demonstrate the effectiveness of our {MWP}-{BERT} on {MWP} solving and several {MWP}-specific understanding tasks on both English and Chinese benchmarks.},
	number = {{arXiv}:2107.13435},
	publisher = {{arXiv}},
	author = {Liang, Zhenwen and Zhang, Jipeng and Wang, Lei and Qin, Wei and Lan, Yunshi and Shao, Jie and Zhang, Xiangliang},
	urldate = {2023-06-10},
	date = {2022-05-11},
	eprinttype = {arxiv},
	eprint = {2107.13435 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:files/209541/Liang 等 - 2022 - MWP-BERT Numeracy-Augmented Pre-training for Math.pdf:application/pdf;arXiv.org Snapshot:files/209547/2107.html:text/html},
}

@misc{ciniselli_empirical_2021-2,
	title = {An Empirical Study on the Usage of {BERT} Models for Code Completion},
	url = {http://arxiv.org/abs/2103.07115},
	abstract = {Code completion is one of the main features of modern Integrated Development Environments ({IDEs}). Its objective is to speed up code writing by predicting the next code token(s) the developer is likely to write. Research in this area has substantially bolstered the predictive performance of these techniques. However, the support to developers is still limited to the prediction of the next few tokens to type. In this work, we take a step further in this direction by presenting a large-scale empirical study aimed at exploring the capabilities of state-of-the-art deep learning ({DL}) models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). To this aim, we train and test several adapted variants of the recently proposed {RoBERTa} model, and evaluate its predictions from several perspectives, including: (i) metrics usually adopted when assessing {DL} generative models (i.e., {BLEU} score and Levenshtein distance); (ii) the percentage of perfect predictions (i.e., the predicted code snippets that match those written by developers); and (iii) the "semantic" equivalence of the generated code as compared to the one written by developers. The achieved results show that {BERT} models represent a viable solution for code completion, with perfect predictions ranging from {\textasciitilde}7\%, obtained when asking the model to guess entire blocks, up to {\textasciitilde}58\%, reached in the simpler scenario of few tokens masked from the same code statement.},
	number = {{arXiv}:2103.07115},
	publisher = {{arXiv}},
	author = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021-03-12},
	eprinttype = {arxiv},
	eprint = {2103.07115 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209548/Ciniselli 等 - 2021 - An Empirical Study on the Usage of BERT Models for.pdf:application/pdf;arXiv.org Snapshot:files/209554/2103.html:text/html},
}

@misc{van_der_zee_code_2021,
	title = {Code Word Detection in Fraud Investigations using a Deep-Learning Approach},
	url = {http://arxiv.org/abs/2103.09606},
	abstract = {In modern litigation, fraud investigators often face an overwhelming number of documents that must be reviewed throughout a matter. In the majority of legal cases, fraud investigators do not know beforehand, exactly what they are looking for, nor where to find it. In addition, fraudsters may use deception to hide their behaviour and intentions by using code words. Effectively, this means fraud investigators are looking for a needle in the haystack without knowing what the needle looks like. As part of a larger research program, we use a framework to expedite the investigation process applying text-mining and machine learning techniques. We structure this framework using three well-known methods in fraud investigations: (i) the fraud triangle (ii) the golden ("W") investigation questions, and (iii) the analysis of competing hypotheses. With this framework, it is possible to automatically organize investigative data, so it is easier for investigators to find answers to typical investigative questions. In this research, we focus on one of the components of this framework: the identification of the usage of code words by fraudsters. Here for, a novel (annotated) synthetic data set is created containing such code words, hidden in normal email communication. Subsequently, a range of machine learning techniques are employed to detect such code words. We show that the state-of-the-art {BERT} model significantly outperforms other methods on this task. With this result, we demonstrate that deep neural language models can reliably (F1 score of 0.9) be applied in fraud investigations for the detection of code words.},
	number = {{arXiv}:2103.09606},
	publisher = {{arXiv}},
	author = {van der Zee, Youri and Scholtes, Jan C. and Westerhoud, Marcel and Rossi, Julien},
	urldate = {2023-06-10},
	date = {2021-03-17},
	eprinttype = {arxiv},
	eprint = {2103.09606 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209217/van der Zee 等 - 2021 - Code Word Detection in Fraud Investigations using .pdf:application/pdf;arXiv.org Snapshot:files/209679/2103.html:text/html},
}

@misc{mastropaolo_studying_2021-1,
	title = {Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks},
	url = {http://arxiv.org/abs/2102.02017},
	abstract = {Deep learning ({DL}) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing ({NLP}) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of {NLP} tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task ( e.g: filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task ( e.g: language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used {DL} techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing {DL}-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.},
	number = {{arXiv}:2102.02017},
	publisher = {{arXiv}},
	author = {Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Palacio, David Nader and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021-02-03},
	eprinttype = {arxiv},
	eprint = {2102.02017 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209546/Mastropaolo 等 - 2021 - Studying the Usage of Text-To-Text Transfer Transf.pdf:application/pdf;arXiv.org Snapshot:files/209553/2102.html:text/html},
}

@misc{shi_design_2020,
	title = {The design and implementation of Language Learning Chatbot with {XAI} using Ontology and Transfer Learning},
	url = {http://arxiv.org/abs/2009.13984},
	abstract = {In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by {GPT}-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of {XAI} (Explainable Artificial Intelligence) to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in {WeChat} as front-end, and fine-tuned {GPT}-2 model of transfer learning as back-end to interpret the responses by ontology graph.},
	number = {{arXiv}:2009.13984},
	publisher = {{arXiv}},
	author = {Shi, Nuobei and Zeng, Qin and Lee, Raymond},
	urldate = {2023-06-10},
	date = {2020-09-29},
	eprinttype = {arxiv},
	eprint = {2009.13984 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209223/Shi 等 - 2020 - The design and implementation of Language Learning.pdf:application/pdf;arXiv.org Snapshot:files/209276/2009.html:text/html},
}

@misc{oren_improving_2020,
	title = {Improving Compositional Generalization in Semantic Parsing},
	url = {http://arxiv.org/abs/2010.05647},
	abstract = {Generalization of models to out-of-distribution ({OOD}) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in semantic parsing, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional generalization. We find that the following factors improve compositional generalization: (a) using contextual representations, such as {ELMo} and {BERT}, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and {OOD} generalization, performance on {OOD} compositions is still substantially lower.},
	number = {{arXiv}:2010.05647},
	publisher = {{arXiv}},
	author = {Oren, Inbar and Herzig, Jonathan and Gupta, Nitish and Gardner, Matt and Berant, Jonathan},
	urldate = {2023-06-10},
	date = {2020-10-12},
	eprinttype = {arxiv},
	eprint = {2010.05647 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208976/Oren 等 - 2020 - Improving Compositional Generalization in Semantic.pdf:application/pdf;arXiv.org Snapshot:files/208993/2010.html:text/html},
}

@inproceedings{jain_contrastive_2021,
	title = {Contrastive Code Representation Learning},
	url = {http://arxiv.org/abs/2007.04973},
	doi = {10.18653/v1/2021.emnlp-main.482},
	abstract = {Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like summarizing code in English, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based {BERT} model is sensitive to source code edits, even when the edits preserve semantics. We propose {ContraCode}: a contrastive pre-training task that learns code functionality, not form. {ContraCode} pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training improves {JavaScript} summarization and {TypeScript} type inference accuracy by 2\% to 13\%. We also propose a new zero-shot {JavaScript} code clone detection dataset, showing that {ContraCode} is both more robust and semantically meaningful. On it, we outperform {RoBERTa} by 39\% {AUROC} in an adversarial setting and up to 5\% on natural code.},
	pages = {5954--5971},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	author = {Jain, Paras and Jain, Ajay and Zhang, Tianjun and Abbeel, Pieter and Gonzalez, Joseph E. and Stoica, Ion},
	urldate = {2023-06-10},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2007.04973 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209010/Jain 等 - 2021 - Contrastive Code Representation Learning.pdf:application/pdf;arXiv.org Snapshot:files/209030/2007.html:text/html},
}

@misc{tabassum_code_2020,
	title = {Code and Named Entity Recognition in {StackOverflow}},
	url = {http://arxiv.org/abs/2005.01634},
	abstract = {There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, {StackOverflow} currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental {NLP} techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition ({NER}) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types. We trained in-domain {BERT} representations ({BERTOverflow}) on 152 million sentences from {StackOverflow}, which lead to an absolute increase of +10 F-1 score over off-the-shelf {BERT}. We also present the {SoftNER} model which achieves an overall 79.10 F\$\_1\$ score for code and named entity recognition on {StackOverflow} data. Our {SoftNER} model incorporates a context-independent code token classifier with corpus-level features to improve the {BERT}-based tagging model. Our code and data are available at: https://github.com/jeniyat/{StackOverflowNER}/},
	number = {{arXiv}:2005.01634},
	publisher = {{arXiv}},
	author = {Tabassum, Jeniya and Maddela, Mounica and Xu, Wei and Ritter, Alan},
	urldate = {2023-06-10},
	date = {2020-11-15},
	eprinttype = {arxiv},
	eprint = {2005.01634 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/208996/Tabassum 等 - 2020 - Code and Named Entity Recognition in StackOverflow.pdf:application/pdf;arXiv.org Snapshot:files/209229/2005.html:text/html},
}

@misc{thiergart_understanding_2021,
	title = {Understanding Emails and Drafting Responses -- An Approach Using {GPT}-3},
	url = {http://arxiv.org/abs/2102.03062},
	abstract = {Providing computer systems with the ability to understand and generate natural language has long been a challenge of engineers. Recent progress in natural language processing ({NLP}), like the {GPT}-3 language model released by {OpenAI}, has made both possible to an extent. In this paper, we explore the possibility of rationalising email communication using {GPT}-3. First, we demonstrate the technical feasibility of understanding incoming emails and generating responses, drawing on literature from the disciplines of software engineering as well as data science. Second, we apply knowledge from both business studies and, again, software engineering to identify ways to tackle challenges we encountered. Third, we argue for the economic viability of such a solution by analysing costs and market demand. We conclude that applying {GPT}-3 to rationalising email communication is feasible both technically and economically.},
	number = {{arXiv}:2102.03062},
	publisher = {{arXiv}},
	author = {Thiergart, Jonas and Huber, Stefan and Übellacker, Thomas},
	urldate = {2023-06-10},
	date = {2021-02-15},
	eprinttype = {arxiv},
	eprint = {2102.03062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/208981/Thiergart 等 - 2021 - Understanding Emails and Drafting Responses -- An .pdf:application/pdf;arXiv.org Snapshot:files/209786/2102.html:text/html},
}

@misc{yan_fastseq_2021,
	title = {{FastSeq}: Make Sequence Generation Faster},
	url = {http://arxiv.org/abs/2106.04718},
	shorttitle = {{FastSeq}},
	abstract = {Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop {FastSeq} framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough to be applicable to Transformer-based models (e.g., T5, {GPT}2, and {UniLM}). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, {FastSeq} is easy to use with a simple one-line code change. The source code is available at https://github.com/microsoft/fastseq.},
	number = {{arXiv}:2106.04718},
	publisher = {{arXiv}},
	author = {Yan, Yu and Hu, Fei and Chen, Jiusheng and Bhendawade, Nikhil and Ye, Ting and Gong, Yeyun and Duan, Nan and Cui, Desheng and Chi, Bingyu and Zhang, Ruofei},
	urldate = {2023-06-10},
	date = {2021-07-12},
	eprinttype = {arxiv},
	eprint = {2106.04718 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209025/Yan 等 - 2021 - FastSeq Make Sequence Generation Faster.pdf:application/pdf;arXiv.org Snapshot:files/209049/2106.html:text/html},
}

@misc{koo_semantic-aware_2021,
	title = {Semantic-aware Binary Code Representation with {BERT}},
	url = {http://arxiv.org/abs/2106.05478},
	abstract = {A wide range of binary analysis applications, such as bug discovery, malware analysis and code clone detection, require recovery of contextual meanings on a binary code. Recently, binary analysis techniques based on machine learning have been proposed to automatically reconstruct the code representation of a binary instead of manually crafting specifics of the analysis algorithm. However, the existing approaches utilizing machine learning are still specialized to solve one domain of problems, rendering recreation of models for different types of binary analysis. In this paper, we propose {DeepSemantic} utilizing {BERT} in producing the semantic-aware code representation of a binary code. To this end, we introduce well-balanced instruction normalization that holds rich information for each of instructions yet minimizing an out-of-vocabulary ({OOV}) problem. {DeepSemantic} has been carefully designed based on our study with large swaths of binaries. Besides, {DeepSemantic} leverages the essence of the {BERT} architecture into re-purposing a pre-trained generic model that is readily available as a one-time processing, followed by quickly applying specific downstream tasks with a fine-tuning process. We demonstrate {DeepSemantic} with two downstream tasks, namely, binary similarity comparison and compiler provenance (i.e., compiler and optimization level) prediction. Our experimental results show that the binary similarity model outperforms two state-of-the-art binary similarity tools, {DeepBinDiff} and {SAFE}, 49.84\% and 15.83\% on average, respectively.},
	number = {{arXiv}:2106.05478},
	publisher = {{arXiv}},
	author = {Koo, Hyungjoon and Park, Soyeon and Choi, Daejin and Kim, Taesoo},
	urldate = {2023-06-10},
	date = {2021-06-09},
	eprinttype = {arxiv},
	eprint = {2106.05478 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209552/Koo 等 - 2021 - Semantic-aware Binary Code Representation with BER.pdf:application/pdf;arXiv.org Snapshot:files/209557/2106.html:text/html},
}

@misc{schuster_programming_2021,
	title = {Programming Puzzles},
	url = {http://arxiv.org/abs/2106.05784},
	abstract = {We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program \$f\$, and the goal is to find an input which makes \$f\$ return True. The puzzles are objective in that each one is specified entirely by the source code of its verifier \$f\$, so evaluating \$f\$ is all that is needed to test a candidate solution. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems, to classic programming puzzles (e.g., Tower of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). We develop baseline enumerative program synthesis, {GPT}-3 and Codex solvers that are capable of solving puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Codex performs best, solving up to 18\% of 397 test problems with a single try and 80\% of the problems with 1,000 tries per problem. In a small user study, we find a positive correlation between puzzle-solving performance and coding experience, and between the puzzle difficulty for humans and {AI} solvers. Therefore, further improvements on P3 could have a significant impact on many program synthesis areas.},
	number = {{arXiv}:2106.05784},
	publisher = {{arXiv}},
	author = {Schuster, Tal and Kalyan, Ashwin and Polozov, Oleksandr and Kalai, Adam Tauman},
	urldate = {2023-06-10},
	date = {2021-11-06},
	eprinttype = {arxiv},
	eprint = {2106.05784 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209556/Schuster 等 - 2021 - Programming Puzzles.pdf:application/pdf;arXiv.org Snapshot:files/209559/2106.html:text/html},
}

@misc{kervadec_supervising_2021,
	title = {Supervising the Transfer of Reasoning Patterns in {VQA}},
	url = {http://arxiv.org/abs/2106.05597},
	abstract = {Methods for Visual Question Anwering ({VQA}) are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. It has been recently shown that better reasoning patterns emerge in attention layers of a state-of-the-art {VQA} model when they are trained on perfect (oracle) visual inputs. This provides evidence that deep neural networks can learn to reason when training conditions are favorable enough. However, transferring this learned knowledge to deployable models is a challenge, as much of it is lost during the transfer. We propose a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on {PAC}-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. We also demonstrate the effectiveness of this approach experimentally on the {GQA} dataset and show its complementarity to {BERT}-like self-supervised pre-training.},
	number = {{arXiv}:2106.05597},
	publisher = {{arXiv}},
	author = {Kervadec, Corentin and Wolf, Christian and Antipov, Grigory and Baccouche, Moez and Nadri, Madiha},
	urldate = {2023-06-10},
	date = {2021-06-10},
	eprinttype = {arxiv},
	eprint = {2106.05597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209026/Kervadec 等 - 2021 - Supervising the Transfer of Reasoning Patterns in .pdf:application/pdf;arXiv.org Snapshot:files/209050/2106.html:text/html},
}

@misc{williams_towards_2021,
	title = {Towards a corpus for credibility assessment in software practitioner blog articles},
	url = {http://arxiv.org/abs/2106.11159},
	abstract = {Blogs are a source of grey literature which are widely adopted by software practitioners for disseminating opinion and experience. Analysing such articles can provide useful insights into the state-of-practice for software engineering research. However, there are challenges in identifying higher quality content from the large quantity of articles available. Credibility assessment can help in identifying quality content, though there is a lack of existing corpora. Credibility is typically measured through a series of conceptual criteria, with 'argumentation' and 'evidence' being two important criteria. We create a corpus labelled for argumentation and evidence that can aid the credibility community. The corpus consists of articles from the blog of a single software practitioner and is publicly available. Three annotators label the corpus with a series of conceptual credibility criteria, reaching an agreement of 0.82 (Fleiss' Kappa). We present preliminary analysis of the corpus by using it to investigate the identification of claim sentences (one of our ten labels). We train four systems (Bert, {KNN}, Decision Tree and {SVM}) using three feature sets (Bag of Words, Topic Modelling and {InferSent}), achieving an F1 score of 0.64 using {InferSent} and a Linear {SVM}. Our preliminary results are promising, indicating that the corpus can help future studies in detecting the credibility of grey literature. Future research will investigate the degree to which the sentence level annotations can infer the credibility of the overall document.},
	number = {{arXiv}:2106.11159},
	publisher = {{arXiv}},
	author = {Williams, Ashley and Shardlow, Matthew and Rainer, Austen},
	urldate = {2023-06-10},
	date = {2021-06-21},
	eprinttype = {arxiv},
	eprint = {2106.11159 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209544/Williams 等 - 2021 - Towards a corpus for credibility assessment in sof.pdf:application/pdf;arXiv.org Snapshot:files/209803/2106.html:text/html},
}

@misc{tang_solving_2021,
	title = {Solving Probability and Statistics Problems by Program Synthesis},
	url = {http://arxiv.org/abs/2111.08267},
	abstract = {We solve university level probability and statistics questions by program synthesis using {OpenAI}'s Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from {MIT}'s 18.05 Introduction to Probability and Statistics and Harvard's {STAT}110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.},
	number = {{arXiv}:2111.08267},
	publisher = {{arXiv}},
	author = {Tang, Leonard and Ke, Elizabeth and Singh, Nikhil and Verma, Nakul and Drori, Iddo},
	urldate = {2023-06-10},
	date = {2021-11-16},
	eprinttype = {arxiv},
	eprint = {2111.08267 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209228/Tang 等 - 2021 - Solving Probability and Statistics Problems by Pro.pdf:application/pdf;arXiv.org Snapshot:files/209719/2111.html:text/html},
}

@misc{jain_jigsaw_2021,
	title = {Jigsaw: Large Language Models meet Program Synthesis},
	url = {http://arxiv.org/abs/2112.02969},
	shorttitle = {Jigsaw},
	abstract = {Large pre-trained language models such as {GPT}-3, Codex, and Google's language model are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated {AI} pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool jigsaw, targeted at synthesizing code for using Python Pandas {API} using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, jigsaw has an important role to play in improving the accuracy of the systems.},
	number = {{arXiv}:2112.02969},
	publisher = {{arXiv}},
	author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
	urldate = {2023-06-10},
	date = {2021-12-06},
	eprinttype = {arxiv},
	eprint = {2112.02969 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209017/Jain 等 - 2021 - Jigsaw Large Language Models meet Program Synthes.pdf:application/pdf;arXiv.org Snapshot:files/209820/2112.html:text/html},
}

@inproceedings{favero_bert_se_2021,
	title = {{BERT}\_SE: A Pre-trained Language Representation Model for Software Engineering},
	url = {http://arxiv.org/abs/2112.00699},
	doi = {10.5121/csit.2021.111909},
	shorttitle = {{BERT}\_SE},
	abstract = {The application of Natural Language Processing ({NLP}) has achieved a high level of relevance in several areas. In the field of software engineering ({SE}), {NLP} applications are based on the classification of similar texts (e.g. software requirements), applied in tasks of estimating software effort, selection of human resources, etc. Classifying software requirements has been a complex task, considering the informality and complexity inherent in the texts produced during the software development process. The pre-trained embedding models are shown as a viable alternative when considering the low volume of textual data labeled in the area of software engineering, as well as the lack of quality of these data. Although there is much research around the application of word embedding in several areas, to date, there is no knowledge of studies that have explored its application in the creation of a specific model for the domain of the {SE} area. Thus, this article presents the proposal for a contextualized embedding model, called {BERT}\_SE, which allows the recognition of specific and relevant terms in the context of {SE}. The assessment of {BERT}\_SE was performed using the software requirements classification task, demonstrating that this model has an average improvement rate of 13\% concerning the {BERT}\_base model, made available by the authors of {BERT}. The code and pre-trained models are available at https://github.com/elianedb.},
	pages = {115--130},
	booktitle = {{NLP} Techniques and Applications},
	author = {Fávero, Eliane Maria De Bortoli and Casanova, Dalcimar},
	urldate = {2023-06-10},
	date = {2021-11-27},
	eprinttype = {arxiv},
	eprint = {2112.00699 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209549/Fávero 和 Casanova - 2021 - BERT_SE A Pre-trained Language Representation Mod.pdf:application/pdf;arXiv.org Snapshot:files/209744/2112.html:text/html},
}

@misc{niu_spt-code_2022-1,
	title = {{SPT}-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations},
	url = {http://arxiv.org/abs/2201.01549},
	shorttitle = {{SPT}-Code},
	abstract = {Recent years have seen the successful application of large pre-trained models to code representation learning, resulting in substantial improvements on many code-related downstream tasks. But there are issues surrounding their application to {SE} tasks. First, the majority of the pre-trained models focus on pre-training only the encoder of the Transformer. For generation tasks that are addressed using models with the encoder-decoder architecture, however, there is no reason why the decoder should be left out during pre-training. Second, many existing pre-trained models, including state-of-the-art models such as T5-learning, simply reuse the pre-training tasks designed for natural languages. Moreover, to learn the natural language description of source code needed eventually for code-related tasks such as code summarization, existing pre-training tasks require a bilingual corpus composed of source code and the associated natural language description, which severely limits the amount of data for pre-training. To this end, we propose {SPT}-Code, a sequence-to-sequence pre-trained model for source code. In order to pre-train {SPT}-Code in a sequence-to-sequence manner and address the aforementioned weaknesses associated with existing pre-training tasks, we introduce three pre-training tasks that are specifically designed to enable {SPT}-Code to learn knowledge of source code, the corresponding code structure, as well as a natural language description of the code without relying on any bilingual corpus, and eventually exploit these three sources of information when it is applied to downstream tasks. Experimental results demonstrate that {SPT}-Code achieves state-of-the-art performance on five code-related downstream tasks after fine-tuning.},
	number = {{arXiv}:2201.01549},
	publisher = {{arXiv}},
	author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Ge, Jidong and Huang, Liguo and Luo, Bin},
	urldate = {2023-06-10},
	date = {2022-05-25},
	eprinttype = {arxiv},
	eprint = {2201.01549 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209027/Niu 等 - 2022 - SPT-Code Sequence-to-Sequence Pre-Training for Le.pdf:application/pdf;arXiv.org Snapshot:files/209779/2201.html:text/html},
}

@misc{yang_aspect-based_2022,
	title = {Aspect-Based {API} Review Classification: How Far Can Pre-Trained Transformer Model Go?},
	url = {http://arxiv.org/abs/2201.11327},
	shorttitle = {Aspect-Based {API} Review Classification},
	abstract = {{APIs} (Application Programming Interfaces) are reusable software libraries and are building blocks for modern rapid software development. Previous research shows that programmers frequently share and search for reviews of {APIs} on the mainstream software question and answer (Q\&A) platforms like Stack Overflow, which motivates researchers to design tasks and approaches related to process {API} reviews automatically. Among these tasks, classifying {API} reviews into different aspects (e.g., performance or security), which is called the aspect-based {API} review classification, is of great importance. The current state-of-the-art ({SOTA}) solution to this task is based on the traditional machine learning algorithm. Inspired by the great success achieved by pre-trained models on many software engineering tasks, this study fine-tunes six pre-trained models for the aspect-based {API} review classification task and compares them with the current {SOTA} solution on an {API} review benchmark collected by Uddin et al. The investigated models include four models ({BERT}, {RoBERTa}, {ALBERT} and {XLNet}) that are pre-trained on natural languages, {BERTOverflow} that is pre-trained on text corpus extracted from posts on Stack Overflow, and {CosSensBERT} that is designed for handling imbalanced data. The results show that all the six fine-tuned models outperform the traditional machine learning-based tool. More specifically, the improvement on the F1-score ranges from 21.0\% to 30.2\%. We also find that {BERTOverflow}, a model pre-trained on the corpus from Stack Overflow, does not show better performance than {BERT}. The result also suggests that {CosSensBERT} also does not exhibit better performance than {BERT} in terms of F1, but it is still worthy of being considered as it achieves better performance on {MCC} and {AUC}.},
	number = {{arXiv}:2201.11327},
	publisher = {{arXiv}},
	author = {Yang, chengran and Xu, Bowen and Khan, Junaed younus and Uddin, Gias and Han, Donggyun and Yang, Zhou and Lo, David},
	urldate = {2023-06-10},
	date = {2022-01-27},
	eprinttype = {arxiv},
	eprint = {2201.11327 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209551/Yang 等 - 2022 - Aspect-Based API Review Classification How Far Ca.pdf:application/pdf;arXiv.org Snapshot:files/209754/2201.html:text/html},
}

@misc{pearce_pop_2022,
	title = {Pop Quiz! Can a Large Language Model Help With Reverse Engineering?},
	url = {http://arxiv.org/abs/2202.01142},
	abstract = {Large language models (such as {OpenAI}'s Codex) have demonstrated impressive zero-shot multi-task capabilities in the software domain, including code explanation. In this work, we examine if this ability can be used to help with reverse engineering. Specifically, we investigate prompting Codex to identify the purpose, capabilities, and important variable names or values from code, even when the code is produced through decompilation. Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model. We present an extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks: of the 136,260 questions we posed, it answered 72,754 correctly. A key takeaway is that while promising, {LLMs} are not yet ready for zero-shot reverse engineering.},
	number = {{arXiv}:2202.01142},
	publisher = {{arXiv}},
	author = {Pearce, Hammond and Tan, Benjamin and Krishnamurthy, Prashanth and Khorrami, Farshad and Karri, Ramesh and Dolan-Gavitt, Brendan},
	urldate = {2023-06-10},
	date = {2022-02-02},
	eprinttype = {arxiv},
	eprint = {2202.01142 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209033/Pearce 等 - 2022 - Pop Quiz! Can a Large Language Model Help With Rev.pdf:application/pdf;arXiv.org Snapshot:files/209053/2202.html:text/html},
}

@misc{chen_transferability_2022-1,
	title = {On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages},
	url = {http://arxiv.org/abs/2204.09653},
	abstract = {A recent study by Ahmed and Devanbu reported that using a corpus of code written in multilingual datasets to fine-tune multilingual Pre-trained Language Models ({PLMs}) achieves higher performance as opposed to using a corpus of code written in just one programming language. However, no analysis was made with respect to fine-tuning monolingual {PLMs}. Furthermore, some programming languages are inherently different and code written in one language usually cannot be interchanged with the others, i.e., Ruby and Java code possess very different structure. To better understand how monolingual and multilingual {PLMs} affect different programming languages, we investigate 1) the performance of {PLMs} on Ruby for two popular Software Engineering tasks: Code Summarization and Code Search, 2) the strategy (to select programming languages) that works well on fine-tuning multilingual {PLMs} for Ruby, and 3) the performance of the fine-tuned {PLMs} on Ruby given different code lengths. In this work, we analyze over a hundred of pre-trained and fine-tuned models. Our results show that 1) multilingual {PLMs} have a lower Performance-to-Time Ratio (the {BLEU}, {METEOR}, or {MRR} scores over the fine-tuning duration) as compared to monolingual {PLMs}, 2) our proposed strategy to select target programming languages to fine-tune multilingual {PLMs} is effective: it reduces the time to fine-tune yet achieves higher performance in Code Summarization and Code Search tasks, and 3) our proposed strategy consistently shows good performance on different code lengths.},
	number = {{arXiv}:2204.09653},
	publisher = {{arXiv}},
	author = {Chen, Fuxiang and Fard, Fatemeh and Lo, David and Bryksin, Timofey},
	urldate = {2023-06-10},
	date = {2022-04-05},
	eprinttype = {arxiv},
	eprint = {2204.09653 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209555/Chen 等 - 2022 - On the Transferability of Pre-trained Language Mod.pdf:application/pdf;arXiv.org Snapshot:files/209824/2204.html:text/html},
}

@misc{sharma_exploratory_2022-1,
	title = {An Exploratory Study on Code Attention in {BERT}},
	url = {http://arxiv.org/abs/2204.10200},
	abstract = {Many recent models in software engineering introduced deep neural models based on the Transformer architecture or use transformer-based Pre-trained Language Models ({PLM}) trained on code. Although these models achieve the state of the arts results in many downstream tasks such as code summarization and bug detection, they are based on Transformer and {PLM}, which are mainly studied in the Natural Language Processing ({NLP}) field. The current studies rely on the reasoning and practices from {NLP} for these models in code, despite the differences between natural languages and programming languages. There is also limited literature on explaining how code is modeled. Here, we investigate the attention behavior of {PLM} on code and compare it with natural language. We pre-trained {BERT}, a Transformer based {PLM}, on code and explored what kind of information it learns, both semantic and syntactic. We run several experiments to analyze the attention values of code constructs on each other and what {BERT} learns in each layer. Our analyses show that {BERT} pays more attention to syntactic entities, specifically identifiers and separators, in contrast to the most attended token [{CLS}] in {NLP}. This observation motivated us to leverage identifiers to represent the code sequence instead of the [{CLS}] token when used for code clone detection. Our results show that employing embeddings from identifiers increases the performance of {BERT} by 605\% and 4\% F1-score in its lower layers and the upper layers, respectively. When identifiers' embeddings are used in {CodeBERT}, a code-based {PLM}, the performance is improved by 21-24\% in the F1-score of clone detection. The findings can benefit the research community by using code-specific representations instead of applying the common embeddings used in {NLP}, and open new directions for developing smaller models with similar performance.},
	number = {{arXiv}:2204.10200},
	publisher = {{arXiv}},
	author = {Sharma, Rishab and Chen, Fuxiang and Fard, Fatemeh and Lo, David},
	urldate = {2023-06-10},
	date = {2022-04-05},
	eprinttype = {arxiv},
	eprint = {2204.10200 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209034/Sharma 等 - 2022 - An Exploratory Study on Code Attention in BERT.pdf:application/pdf;arXiv.org Snapshot:files/209781/2204.html:text/html},
}

@misc{zhang_skillspan_2022,
	title = {{SkillSpan}: Hard and Soft Skill Extraction from English Job Postings},
	url = {http://arxiv.org/abs/2204.12811},
	shorttitle = {{SkillSpan}},
	abstract = {Skill Extraction ({SE}) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce {SKILLSPAN}, a novel {SE} dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a {BERT} baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models significantly outperform their non-adapted counterparts, and single-task outperforms multi-task learning.},
	number = {{arXiv}:2204.12811},
	publisher = {{arXiv}},
	author = {Zhang, Mike and Jensen, Kristian Nørgaard and Sonniks, Sif Dam and Plank, Barbara},
	urldate = {2023-06-10},
	date = {2022-04-27},
	eprinttype = {arxiv},
	eprint = {2204.12811 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209042/Zhang 等 - 2022 - SkillSpan Hard and Soft Skill Extraction from Eng.pdf:application/pdf;arXiv.org Snapshot:files/209067/2204.html:text/html},
}

@misc{trautsch_predicting_2022,
	title = {Predicting Issue Types with {seBERT}},
	url = {http://arxiv.org/abs/2205.01335},
	abstract = {Pre-trained transformer models are the current state-of-the-art for natural language models processing. {seBERT} is such a model, that was developed based on the {BERT} architecture, but trained from scratch with software engineering data. We fine-tuned this model for the {NLBSE} challenge for the task of issue type prediction. Our model dominates the baseline {fastText} for all three issue types in both recall and precisio\} to achieve an overall F1-score of 85.7\%, which is an increase of 4.1\% over the baseline.},
	number = {{arXiv}:2205.01335},
	publisher = {{arXiv}},
	author = {Trautsch, Alexander and Herbold, Steffen},
	urldate = {2023-06-10},
	date = {2022-05-03},
	eprinttype = {arxiv},
	eprint = {2205.01335 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209024/Trautsch 和 Herbold - 2022 - Predicting Issue Types with seBERT.pdf:application/pdf;arXiv.org Snapshot:files/209233/2205.html:text/html},
}

@misc{roper_transformer-based_2022,
	title = {Transformer-based Program Synthesis for Low-Data Environments},
	url = {http://arxiv.org/abs/2205.09246},
	abstract = {Recent advancements in large pre-trained transformer models ({GPT}2/3, T5) have found use in program synthesis to generate programs that satisfy a set of input/output examples. However, these models perform poorly on long-horizon and low-data tasks, and often don't seem to understand the semantics of the languages they generate. We investigate an approach that tackles both of these issues, by using attributed context-free-grammars of programming languages to generate programs, and then analyzing generated programs so that they can be annotated with compile and runtime attributes, such as types, so that information about the program can be remembered during long-horizon generation. We firstly find that synthesized datasets can be made efficiently and can provide transformer models with enough data in order to perform well on some synthesis tasks. We also find that giving models access to program attributes is especially effective in low-data environments, and tends improve the quality and reduce errors of transformer-generated programs.},
	number = {{arXiv}:2205.09246},
	publisher = {{arXiv}},
	author = {Roper, Jack},
	urldate = {2023-06-10},
	date = {2022-05-18},
	eprinttype = {arxiv},
	eprint = {2205.09246 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209550/Roper - 2022 - Transformer-based Program Synthesis for Low-Data E.pdf:application/pdf;arXiv.org Snapshot:files/209714/2205.html:text/html},
}

@misc{fan_automated_2023,
	title = {Automated Repair of Programs from Large Language Models},
	url = {http://arxiv.org/abs/2205.10583},
	abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair ({APR}) techniques can fix the incorrect solutions produced by language models in {LeetCode} contests. The goal is to study whether {APR} techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating {APR} techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools {TBar} and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing {APR} tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future {APR} tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with {APR} to curate patch ingredients, is worth studying.},
	number = {{arXiv}:2205.10583},
	publisher = {{arXiv}},
	author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
	urldate = {2023-06-10},
	date = {2023-01-01},
	eprinttype = {arxiv},
	eprint = {2205.10583 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209235/Fan 等 - 2023 - Automated Repair of Programs from Large Language M.pdf:application/pdf;arXiv.org Snapshot:files/209777/2205.html:text/html},
}

@misc{inala_fault-aware_2022,
	title = {Fault-Aware Neural Code Rankers},
	url = {http://arxiv.org/abs/2206.03865},
	abstract = {Large language models ({LLMs}) have demonstrated an impressive ability to generate code for various programming tasks. In many instances, {LLMs} can generate a correct program for a task when given numerous trials. Consequently, a recent trend is to do large scale sampling of programs using a model and then filtering/ranking the programs based on the program execution on a small number of known unit tests to select one candidate solution. However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development. In this paper, we propose {CodeRanker}, a neural ranker that can predict the correctness of a sampled program without executing it. Our {CodeRanker} is fault-aware i.e., it is trained to predict different kinds of execution information such as predicting the exact compile/runtime error type (e.g., an {IndexError} or a {TypeError}). We show that {CodeRanker} can significantly increase the pass@1 accuracy of various code generation models (including Codex, {GPT}-Neo, {GPT}-J) on {APPS}, {HumanEval} and {MBPP} datasets.},
	number = {{arXiv}:2206.03865},
	publisher = {{arXiv}},
	author = {Inala, Jeevana Priya and Wang, Chenglong and Yang, Mei and Codas, Andres and Encarnación, Mark and Lahiri, Shuvendu K. and Musuvathi, Madanlal and Gao, Jianfeng},
	urldate = {2023-06-10},
	date = {2022-12-09},
	eprinttype = {arxiv},
	eprint = {2206.03865 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209062/Inala 等 - 2022 - Fault-Aware Neural Code Rankers.pdf:application/pdf;arXiv.org Snapshot:files/209724/2206.html:text/html},
}

@misc{li_auger_2022-1,
	title = {{AUGER}: Automatically Generating Review Comments with Pre-training Models},
	url = {http://arxiv.org/abs/2208.08014},
	shorttitle = {{AUGER}},
	abstract = {Code review is one of the best practices as a powerful safeguard for software quality. In practice, senior or highly skilled reviewers inspect source code and provide constructive comments, considering what authors may ignore, for example, some special cases. The collaborative validation between contributors results in code being highly qualified and less chance of bugs. However, since personal knowledge is limited and varies, the efficiency and effectiveness of code review practice are worthy of further improvement. In fact, it still takes a colossal and time-consuming effort to deliver useful review comments. This paper explores a synergy of multiple practical review comments to enhance code review and proposes {AUGER} ({AUtomatically} {GEnerating} Review comments): a review comments generator with pre-training models. We first collect empirical review data from 11 notable Java projects and construct a dataset of 10,882 code changes. By leveraging Text-to-Text Transfer Transformer (T5) models, the framework synthesizes valuable knowledge in the training stage and effectively outperforms baselines by 37.38\% in {ROUGE}-L. 29\% of our automatic review comments are considered useful according to prior studies. The inference generates just in 20 seconds and is also open to training further. Moreover, the performance also gets improved when thoroughly analyzed in case study.},
	number = {{arXiv}:2208.08014},
	publisher = {{arXiv}},
	author = {Li, Lingwei and Yang, Li and Jiang, Huaxi and Yan, Jun and Luo, Tiejian and Hua, Zihan and Liang, Geng and Zuo, Chun},
	urldate = {2023-06-10},
	date = {2022-08-31},
	eprinttype = {arxiv},
	eprint = {2208.08014 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209239/Li 等 - 2022 - AUGER Automatically Generating Review Comments wi.pdf:application/pdf;arXiv.org Snapshot:files/209731/2208.html:text/html},
}

@misc{joshi_repair_2022,
	title = {Repair Is Nearly Generation: Multilingual Program Repair with {LLMs}},
	url = {http://arxiv.org/abs/2208.11640},
	shorttitle = {Repair Is Nearly Generation},
	abstract = {Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program -- a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce {RING}, a multilingual repair engine powered by a large language model trained on code ({LLMC}) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the {AI} assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that {RING} can outperform language-specific repair engines for three of these languages.},
	number = {{arXiv}:2208.11640},
	publisher = {{arXiv}},
	author = {Joshi, Harshit and Cambronero, José and Gulwani, Sumit and Le, Vu and Radicek, Ivan and Verbruggen, Gust},
	urldate = {2023-06-10},
	date = {2022-12-05},
	eprinttype = {arxiv},
	eprint = {2208.11640 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209048/Joshi 等 - 2022 - Repair Is Nearly Generation Multilingual Program .pdf:application/pdf;arXiv.org Snapshot:files/209798/2208.html:text/html},
}

@misc{oshingbesan_extreme_2022,
	title = {Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers},
	url = {http://arxiv.org/abs/2209.10106},
	abstract = {Text-to-text transformers have shown remarkable success in the task of multi-task transfer learning, especially in natural language processing ({NLP}). However, while there have been several attempts to train transformers on different domains, there is usually a clear relationship between these domains, e.g.,, code summarization, where the natural language summary describes the code. There have been very few attempts to study how multi-task transfer learning works on tasks in significantly different domains. In this project, we investigated the behavior of multi-domain, multi-task learning using multi-domain text-to-text transfer transformers ({MD}-T5) on four tasks across two domains - Python Code and Chess. We carried out extensive experiments using three popular training strategies: Bert-style joint pretraining + successive finetuning, {GPT}-style joint pretraining + successive finetuning, and {GPT}-style joint pretraining + joint finetuning. Also, we evaluate the model on four metrics - Play Score, Eval Score, {BLEU} Score, and Multi-Domain Learning Score ({MDLS}). These metrics measure performance across the various tasks and multi-domain learning. We show that while negative knowledge transfer and catastrophic forgetting are still considerable challenges for all the models, the {GPT}-style joint pretraining + joint finetuning strategy showed the most promise in multi-domain, multi-task learning as it performs well across all four tasks while still keeping its multi-domain knowledge.},
	number = {{arXiv}:2209.10106},
	publisher = {{arXiv}},
	author = {Oshingbesan, Adebayo and Ekoh, Courage and Atakpa, Germann and Byaruagaba, Yonah},
	urldate = {2023-06-10},
	date = {2022-09-21},
	eprinttype = {arxiv},
	eprint = {2209.10106 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209056/Oshingbesan 等 - 2022 - Extreme Multi-Domain, Multi-Task Learning With Uni.pdf:application/pdf;arXiv.org Snapshot:files/209673/2209.html:text/html},
}

@misc{wang_code4struct_2023,
	title = {Code4Struct: Code Generation for Few-Shot Event Structure Prediction},
	url = {http://arxiv.org/abs/2210.12810},
	shorttitle = {Code4Struct},
	abstract = {Large Language Model ({LLM}) trained on a mixture of text and code has demonstrated impressive capability in translating natural language ({NL}) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction ({EAE}) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language ({PL}) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating {EAE} as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art ({SOTA}) trained on 20-shot data by 29.5\% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12\% absolute F1.},
	number = {{arXiv}:2210.12810},
	publisher = {{arXiv}},
	author = {Wang, Xingyao and Li, Sha and Ji, Heng},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2210.12810 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209560/Wang 等 - 2023 - Code4Struct Code Generation for Few-Shot Event St.pdf:application/pdf;arXiv.org Snapshot:files/209833/2210.html:text/html},
}

@misc{xia_practical_2022,
	title = {Practical Program Repair in the Era of Large Pre-trained Language Models},
	url = {http://arxiv.org/abs/2210.14179},
	abstract = {Automated Program Repair ({APR}) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based {APR} techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates or directly predict potential patches. Large Pre-Trained Language Models ({PLMs}), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged {PLMs} for {APR} without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art {PLMs} or was not evaluated on realistic datasets. In this work, we perform the first extensive study on directly applying {PLMs} for {APR}. We select 9 recent state-of-the-art {PLMs}, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use {PLMs} to generate patches. We apply the {PLMs} under these repair settings on 5 datasets across 3 different languages and compare different {PLMs} in the number of bugs fixed, generation speed and compilation rate. Our study demonstrates that directly applying state-of-the-art {PLMs} can already substantially outperform all existing {APR} techniques on all our datasets. Among the studied {PLMs}, the scaling effect exists for {APR} where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style {APR}) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the {PLMs} consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking.},
	number = {{arXiv}:2210.14179},
	publisher = {{arXiv}},
	author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2022-10-25},
	eprinttype = {arxiv},
	eprint = {2210.14179 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209236/Xia 等 - 2022 - Practical Program Repair in the Era of Large Pre-t.pdf:application/pdf;arXiv.org Snapshot:files/209817/2210.html:text/html},
}

@misc{mouselinos_simple_2023,
	title = {A Simple, Yet Effective Approach to Finding Biases in Code Generation},
	url = {http://arxiv.org/abs/2211.00609},
	abstract = {Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances. To investigate the effect, we propose the "block of influence" concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.},
	number = {{arXiv}:2211.00609},
	publisher = {{arXiv}},
	author = {Mouselinos, Spyridon and Malinowski, Mateusz and Michalewski, Henryk},
	urldate = {2023-06-10},
	date = {2023-05-09},
	eprinttype = {arxiv},
	eprint = {2211.00609 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209564/Mouselinos 等 - 2023 - A Simple, Yet Effective Approach to Finding Biases.pdf:application/pdf;arXiv.org Snapshot:files/209676/2211.html:text/html},
}

@misc{zhou_large_2023,
	title = {Large Language Models Are Human-Level Prompt Engineers},
	url = {http://arxiv.org/abs/2211.01910},
	abstract = {By conditioning on natural language instructions, large language models ({LLMs}) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer ({APE}) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an {LLM} in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another {LLM} following the selected instruction. Experiments on 24 {NLP} tasks show that our automatically generated instructions outperform the prior {LLM} baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of {APE}. We show that {APE}-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
	number = {{arXiv}:2211.01910},
	publisher = {{arXiv}},
	author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
	urldate = {2023-06-10},
	date = {2023-03-10},
	eprinttype = {arxiv},
	eprint = {2211.01910 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209565/Zhou 等 - 2023 - Large Language Models Are Human-Level Prompt Engin.pdf:application/pdf;arXiv.org Snapshot:files/209569/2211.html:text/html},
}

@misc{gao_pal_2023,
	title = {{PAL}: Program-aided Language Models},
	url = {http://arxiv.org/abs/2211.10435},
	shorttitle = {{PAL}},
	abstract = {Large language models ({LLMs}) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ {LLMs} for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While {LLMs} seem to be adept at this sort of step-by-step decomposition, {LLMs} often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models ({PAL}): a novel approach that uses the {LLM} to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With {PAL}, decomposing the natural language problem into runnable steps remains the only learning task for the {LLM}, while solving is delegated to the interpreter. We demonstrate this synergy between a neural {LLM} and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from {BIG}-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an {LLM} and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, {PAL} using Codex achieves state-of-the-art few-shot accuracy on the {GSM}8K benchmark of math word problems, surpassing {PaLM}-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
	number = {{arXiv}:2211.10435},
	publisher = {{arXiv}},
	author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
	urldate = {2023-06-10},
	date = {2023-01-27},
	eprinttype = {arxiv},
	eprint = {2211.10435 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209089/Gao 等 - 2023 - PAL Program-aided Language Models.pdf:application/pdf;arXiv.org Snapshot:files/209110/2211.html:text/html},
}

@misc{li_pre-trained_2022,
	title = {Do Pre-trained Language Models Indeed Understand Software Engineering Tasks?},
	url = {http://arxiv.org/abs/2211.10623},
	abstract = {Artificial intelligence ({AI}) for software engineering ({SE}) tasks has recently achieved promising performance. In this paper, we investigate to what extent the pre-trained language model truly understands those {SE} tasks such as code search, code summarization, etc. We conduct a comprehensive empirical study on a board set of {AI} for {SE} ({AI}4SE) tasks by feeding them with variant inputs: 1) with various masking rates and 2) with sufficient input subset method. Then, the trained models are evaluated on different {SE} tasks, including code search, code summarization, and duplicate bug report detection. Our experimental results show that pre-trained language models are insensitive to the given input, thus they achieve similar performance in these three {SE} tasks. We refer to this phenomenon as overinterpretation, where a model confidently makes a decision without salient features, or where a model finds some irrelevant relationships between the final decision and the dataset. Our study investigates two approaches to mitigate the overinterpretation phenomenon: whole word mask strategy and ensembling. To the best of our knowledge, we are the first to reveal this overinterpretation phenomenon to the {AI}4SE community, which is an important reminder for researchers to design the input for the models and calls for necessary future work in understanding and implementing {AI}4SE tasks.},
	number = {{arXiv}:2211.10623},
	publisher = {{arXiv}},
	author = {Li, Yao and Zhang, Tao and Luo, Xiapu and Cai, Haipeng and Fang, Sen and Yuan, Dawei},
	urldate = {2023-06-10},
	date = {2022-11-19},
	eprinttype = {arxiv},
	eprint = {2211.10623 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209240/Li 等 - 2022 - Do Pre-trained Language Models Indeed Understand S.pdf:application/pdf;arXiv.org Snapshot:files/209710/2211.html:text/html},
}

@misc{chen_program_2022,
	title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
	url = {http://arxiv.org/abs/2211.12588},
	shorttitle = {Program of Thoughts Prompting},
	abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting ({CoT}) is by far the state-of-art method for these tasks. {CoT} uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' ({PoT}), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate {PoT} on five math word problem datasets ({GSM}, {AQuA}, {SVAMP}, {TabMWP}, {MultiArith}) and three financial-{QA} datasets ({FinQA}, {ConvFinQA}, {TATQA}) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, {PoT} can show an average performance gain over {CoT} by around 12{\textbackslash}\% across all the evaluated datasets. By combining {PoT} with self-consistency decoding, we can achieve {SoTA} performance on all math problem datasets and near-{SoTA} performance on financial datasets. All of our data and code are released in Github{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/wenhuchen/Program-of-Thoughts\}\}.},
	number = {{arXiv}:2211.12588},
	publisher = {{arXiv}},
	author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
	urldate = {2023-06-10},
	date = {2022-11-28},
	eprinttype = {arxiv},
	eprint = {2211.12588 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209570/Chen 等 - 2022 - Program of Thoughts Prompting Disentangling Compu.pdf:application/pdf;arXiv.org Snapshot:files/209578/2211.html:text/html},
}

@misc{gupta_visual_2022,
	title = {Visual Programming: Compositional visual reasoning without training},
	url = {http://arxiv.org/abs/2211.11559},
	shorttitle = {Visual Programming},
	abstract = {We present {VISPROG}, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. {VISPROG} avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of {VISPROG} on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like {VISPROG} are an exciting avenue to easily and effectively expand the scope of {AI} systems to serve the long tail of complex tasks that people may wish to perform.},
	number = {{arXiv}:2211.11559},
	publisher = {{arXiv}},
	author = {Gupta, Tanmay and Kembhavi, Aniruddha},
	urldate = {2023-06-10},
	date = {2022-11-18},
	eprinttype = {arxiv},
	eprint = {2211.11559 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209652/Gupta 和 Kembhavi - 2022 - Visual Programming Compositional visual reasoning.pdf:application/pdf;arXiv.org Snapshot:files/209655/2211.html:text/html},
}

@misc{azerbayev_explicit_2023,
	title = {Explicit Knowledge Transfer for Weakly-Supervised Code Generation},
	url = {http://arxiv.org/abs/2211.16740},
	abstract = {Large language models ({LLMs}) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific {NL}-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an {LLM} to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer ({EKT}), which uses the few-shot capabilities of a teacher {LLM} to create {NL}-code pairs that we then filter for correctness and fine-tune the student on. We evaluate {EKT} on the task of generating code solutions to math word problems from the {GSM}8k dataset. We find that {EKT} not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A {GPT}-Neo 1.3B model trained using {EKT} with a {GPT}-J teacher achieves a 12.4\% pass@100 on {GSM}8k, while the same student and teacher trained with knowledge distillation yield only a 3.7\% pass@100. We also show that it is possible for a student model to outperform the teacher using {EKT}.},
	number = {{arXiv}:2211.16740},
	publisher = {{arXiv}},
	author = {Azerbayev, Zhangir and Ni, Ansong and Schoelkopf, Hailey and Radev, Dragomir},
	urldate = {2023-06-10},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2211.16740 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209078/Azerbayev 等 - 2023 - Explicit Knowledge Transfer for Weakly-Supervised .pdf:application/pdf;arXiv.org Snapshot:files/209099/2211.html:text/html},
}

@misc{zong_survey_2022,
	title = {a survey on {GPT}-3},
	url = {http://arxiv.org/abs/2212.00857},
	abstract = {This paper provides an introductory survey to {GPT}-3. We cover some of the historical development behind this technology, some of the key features of {GPT}-3, and discuss the machine learning model and the datasets used. We survey both academic and commercial efforts applying {GPT}-3 in diverse domains such as developing conversational {AI} chatbots, software development, creative work, domain knowledge, and business productivity. We discuss some of the challenges that {GPT}-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers. We also discuss the future research opportunities in this area.},
	number = {{arXiv}:2212.00857},
	publisher = {{arXiv}},
	author = {Zong, Mingyu and Krishnamachari, Bhaskar},
	urldate = {2023-06-10},
	date = {2022-12-01},
	eprinttype = {arxiv},
	eprint = {2212.00857 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209082/Zong 和 Krishnamachari - 2022 - a survey on GPT-3.pdf:application/pdf;arXiv.org Snapshot:files/209812/2212.html:text/html},
}

@misc{zelikman_parsel_2023,
	title = {Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions},
	url = {http://arxiv.org/abs/2212.10561},
	shorttitle = {Parsel},
	abstract = {Despite recent success in large language model ({LLM}) reasoning, {LLMs} struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code {LLMs}. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, {LLMs} solve more competition-level problems in the {APPS} dataset, resulting in pass rates over 75{\textbackslash}\% higher than prior results from directly sampling {AlphaCode} and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on {HumanEval} from 67{\textbackslash}\% to 85{\textbackslash}\%. We also find that {LLM}-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses {LLM} limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel},
	number = {{arXiv}:2212.10561},
	publisher = {{arXiv}},
	author = {Zelikman, Eric and Huang, Qian and Poesia, Gabriel and Goodman, Noah D. and Haber, Nick},
	urldate = {2023-06-10},
	date = {2023-05-28},
	eprinttype = {arxiv},
	eprint = {2212.10561 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209241/Zelikman 等 - 2023 - Parsel Algorithmic Reasoning with Language Models.pdf:application/pdf;arXiv.org Snapshot:files/209282/2212.html:text/html},
}

@misc{hara_np4g_2022,
	title = {{NP}4G : Network Programming for Generalization},
	url = {http://arxiv.org/abs/2212.11118},
	shorttitle = {{NP}4G},
	abstract = {Automatic programming has been actively studied for a long time by various approaches including genetic programming. In recent years, automatic programming using neural networks such as {GPT}-3 has been actively studied and is attracting a lot of attention. However, these methods are illogical inference based on experience by enormous learning, and their thinking process is unclear. Even using the method by logical inference with a clear thinking process, the system that automatically generates any programs has not yet been realized. Especially, the inductive inference generalized by logical inference from one example is an important issue that the artificial intelligence can acquire knowledge by itself. In this study, we propose {NP}4G: Network Programming for Generalization, which can automatically generate programs by inductive inference. Because the proposed method can realize "sequence", "selection", and "iteration" in programming and can satisfy the conditions of the structured program theorem, it is expected that {NP}4G is a method automatically acquire any programs by inductive inference. As an example, we automatically construct a bitwise {NOT} operation program from several training data by generalization using {NP}4G. Although {NP}4G only randomly selects and connects nodes, by adjusting the number of nodes and the number of phase of "Phased Learning", we show the bitwise {NOT} operation programs are acquired in a comparatively short time and at a rate of about 7 in 10 running. The source code of {NP}4G is available on {GitHub} as a public repository.},
	number = {{arXiv}:2212.11118},
	publisher = {{arXiv}},
	author = {Hara, Shoichiro and Watanabe, Yuji},
	urldate = {2023-06-10},
	date = {2022-12-08},
	eprinttype = {arxiv},
	eprint = {2212.11118 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/209237/Hara 和 Watanabe - 2022 - NP4G  Network Programming for Generalization.pdf:application/pdf;arXiv.org Snapshot:files/209278/2212.html:text/html},
}

@misc{thakur_benchmarking_2022,
	title = {Benchmarking Large Language Models for Automated Verilog {RTL} Code Generation},
	url = {http://arxiv.org/abs/2212.11140},
	abstract = {Automating hardware design could obviate a significant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models ({LLMs}) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of {LLMs} to generate useful Verilog. For this, we fine-tune pre-trained {LLMs} on Verilog datasets collected from {GitHub} and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in {LLMs} more capable of producing syntactically correct code (25.9\% overall). Further, when analyzing functional correctness, a fine-tuned open-source {CodeGen} {LLM} can outperform the state-of-the-art commercial Codex {LLM} (6.5\% overall). Training/evaluation scripts and {LLM} checkpoints are available: https://github.com/shailja-thakur/{VGen}.},
	number = {{arXiv}:2212.11140},
	publisher = {{arXiv}},
	author = {Thakur, Shailja and Ahmad, Baleegh and Fan, Zhenxing and Pearce, Hammond and Tan, Benjamin and Karri, Ramesh and Dolan-Gavitt, Brendan and Garg, Siddharth},
	urldate = {2023-06-10},
	date = {2022-12-13},
	eprinttype = {arxiv},
	eprint = {2212.11140 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209238/Thakur 等 - 2022 - Benchmarking Large Language Models for Automated V.pdf:application/pdf;arXiv.org Snapshot:files/209281/2212.html:text/html},
}

@misc{khattab_demonstrate-search-predict_2023,
	title = {Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive {NLP}},
	url = {http://arxiv.org/abs/2212.14024},
	shorttitle = {Demonstrate-Search-Predict},
	abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models ({LM}) and retrieval models ({RM}). Existing work has combined these in simple "retrieve-then-read" pipelines in which the {RM} retrieves passages that are inserted into the {LM} prompt. To begin to fully realize the potential of frozen {LMs} and {RMs}, we propose Demonstrate-Search-Predict ({DSP}), a framework that relies on passing natural language texts in sophisticated pipelines between an {LM} and an {RM}. {DSP} can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the {LM} and {RM} can handle more reliably. We have written novel {DSP} programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120\%, 8-39\%, and 80-290\% relative gains against the vanilla {LM} ({GPT}-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release {DSP} at https://github.com/stanfordnlp/dsp},
	number = {{arXiv}:2212.14024},
	publisher = {{arXiv}},
	author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	urldate = {2023-06-10},
	date = {2023-01-23},
	eprinttype = {arxiv},
	eprint = {2212.14024 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/209566/Khattab 等 - 2023 - Demonstrate-Search-Predict Composing retrieval an.pdf:application/pdf;arXiv.org Snapshot:files/209577/2212.html:text/html},
}

@misc{deng_large_2023-1,
	title = {Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models},
	url = {http://arxiv.org/abs/2212.14834},
	shorttitle = {Large Language Models are Zero-Shot Fuzzers},
	abstract = {Detecting bugs in Deep Learning ({DL}) libraries (e.g., {TensorFlow}/{PyTorch}) is critical for almost all downstream {DL} systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input {DL} programs need to satisfy both the input language (e.g., Python) syntax/semantics and the {DL} {API} input/shape constraints for tensor computations. To address these limitations, we propose {TitanFuzz} - the first approach to directly leveraging Large Language Models ({LLMs}) to generate input programs for fuzzing {DL} libraries. {LLMs} are titanic models trained on billions of code snippets and can auto-regressively generate human-like code snippets. Our key insight is that modern {LLMs} can also include numerous code snippets invoking {DL} library {APIs} in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate {DL} {API} constraints for valid {DL} program generation. More specifically, we use both generative and infilling {LLMs} (e.g., Codex/{InCoder}) to generate and mutate valid/diverse input {DL} programs for fuzzing. Our experimental results demonstrate that {TitanFuzz} can achieve 30.38\%/50.84\% higher code coverage than state-of-the-art fuzzers on {TensorFlow}/{PyTorch}. Furthermore, {TitanFuzz} is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs. This paper demonstrates that modern titanic {LLMs} can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as {DL} systems). We hope {TitanFuzz} can stimulate more work in this promising direction of {LLMs} for fuzzing.},
	number = {{arXiv}:2212.14834},
	publisher = {{arXiv}},
	author = {Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming},
	urldate = {2023-06-10},
	date = {2023-03-07},
	eprinttype = {arxiv},
	eprint = {2212.14834 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209580/Deng 等 - 2023 - Large Language Models are Zero-Shot Fuzzers Fuzzi.pdf:application/pdf;arXiv.org Snapshot:files/209782/2212.html:text/html},
}

@misc{yildirim_adaptive_2023,
	title = {Adaptive Fine-tuning for Multiclass Classification over Software Requirement Data},
	url = {http://arxiv.org/abs/2301.00495},
	abstract = {The analysis of software requirement specifications ({SRS}) using Natural Language Processing ({NLP}) methods has been an important study area in the software engineering field in recent years. Especially thanks to the advances brought by deep learning and transfer learning approaches in {NLP}, {SRS} data can be utilized for various learning tasks more easily. In this study, we employ a three-stage domain-adaptive fine-tuning approach for three prediction tasks regarding software requirements, which improve the model robustness on a real distribution shift. The multi-class classification tasks involve predicting the type, priority and severity of the requirement texts specified by the users. We compare our results with strong classification baselines such as word embedding pooling and Sentence {BERT}, and show that the adaptive fine-tuning leads to performance improvements across the tasks. We find that an adaptively fine-tuned model can be specialized to particular data distribution, which is able to generate accurate results and learns from abundantly available textual data in software engineering task management systems.},
	number = {{arXiv}:2301.00495},
	publisher = {{arXiv}},
	author = {Yildirim, Savas and Cevik, Mucahit and Parikh, Devang and Basar, Ayse},
	urldate = {2023-06-10},
	date = {2023-01-01},
	eprinttype = {arxiv},
	eprint = {2301.00495 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209579/Yildirim 等 - 2023 - Adaptive Fine-tuning for Multiclass Classification.pdf:application/pdf;arXiv.org Snapshot:files/209585/2301.html:text/html},
}

@article{beurer-kellner_prompting_2023,
	title = {Prompting Is Programming: A Query Language for Large Language Models},
	volume = {7},
	issn = {2475-1421},
	url = {http://arxiv.org/abs/2212.06094},
	doi = {10.1145/3591300},
	shorttitle = {Prompting Is Programming},
	abstract = {Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming ({LMP}). {LMP} generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, {LMP} allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics. To enable {LMP}, we implement {LMQL}(short for Language Model Query Language), which leverages the constraints and control flow from an {LMP} prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that {LMQL} can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level {APIs}. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use {APIs} (26-85\% cost savings).},
	pages = {1946--1969},
	issue = {{PLDI}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
	urldate = {2023-06-10},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2212.06094 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209124/Beurer-Kellner 等 - 2023 - Prompting Is Programming A Query Language for Lar.pdf:application/pdf;arXiv.org Snapshot:files/209771/2212.html:text/html},
}

@inproceedings{macneil_automatically_2022,
	title = {Automatically Generating {CS} Learning Materials with Large Language Models},
	url = {http://arxiv.org/abs/2212.05113},
	doi = {10.1145/3545947.3569630},
	abstract = {Recent breakthroughs in Large Language Models ({LLMs}), such as {GPT}-3 and Codex, now enable software developers to generate code based on a natural language prompt. Within computer science education, researchers are exploring the potential for {LLMs} to generate code explanations and programming assignments using carefully crafted prompts. These advances may enable students to interact with code in new ways while helping instructors scale their learning materials. However, {LLMs} also introduce new implications for academic integrity, curriculum design, and software engineering careers. This workshop will demonstrate the capabilities of {LLMs} to help attendees evaluate whether and how {LLMs} might be integrated into their pedagogy and research. We will also engage attendees in brainstorming to consider how {LLMs} will impact our field.},
	pages = {1176--1176},
	booktitle = {Proceedings of the 54th {ACM} Technical Symposium on Computer Science Education V. 2},
	author = {{MacNeil}, Stephen and Tran, Andrew and Leinonen, Juho and Denny, Paul and Kim, Joanne and Hellas, Arto and Bernstein, Seth and Sarsa, Sami},
	urldate = {2023-06-10},
	date = {2022-03},
	eprinttype = {arxiv},
	eprint = {2212.05113 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:files/209584/MacNeil 等 - 2022 - Automatically Generating CS Learning Materials wit.pdf:application/pdf;arXiv.org Snapshot:files/209819/2212.html:text/html},
}

@misc{huang_api_2023,
	title = {{API} Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model},
	url = {http://arxiv.org/abs/2301.03987},
	abstract = {Extraction of Application Programming Interfaces ({APIs}) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., {API} recommendation). However, existing approaches are rule-based and sequence-labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling {API} entities and relations, this paper formulates heterogeneous {API} extraction and {API} relation extraction task as a sequence-to-sequence generation task, and proposes {AERJE}, an {API} entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, {AERJE} builds a multi-task architecture that extracts {API} entities and relations from unstructured text using dynamic prompts. We systematically evaluate {AERJE} on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that {AERJE} achieves high accuracy and discrimination ability in {API} entity-relation joint extraction, even with zero or few-shot fine-tuning.},
	number = {{arXiv}:2301.03987},
	publisher = {{arXiv}},
	author = {Huang, Qing and Sun, Yanbang and Xing, Zhenchang and Yu, Min and Xu, Xiwei and Lu, Qinghua},
	urldate = {2023-06-10},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2301.03987 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209594/Huang 等 - 2023 - API Entity and Relation Joint Extraction from Text.pdf:application/pdf;arXiv.org Snapshot:files/209604/2301.html:text/html},
}

@misc{allal_santacoder_2023,
	title = {{SantaCoder}: don't reach for the stars!},
	url = {http://arxiv.org/abs/2301.03988},
	shorttitle = {{SantaCoder}},
	abstract = {The {BigCode} project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information ({PII}) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, {JavaScript}, and Python subsets of The Stack and evaluate them on the {MultiPL}-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ {GitHub} stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models ({InCoder}-6.7B and {CodeGen}-Multi-2.7B) in both left-to-right generation and infilling on the Java, {JavaScript}, and Python portions of {MultiPL}-E, despite being a substantially smaller model. All models are released under an {OpenRAIL} license at https://hf.co/bigcode.},
	number = {{arXiv}:2301.03988},
	publisher = {{arXiv}},
	author = {Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and Umapathi, Logesh Kumar and Anderson, Carolyn Jane and Zi, Yangtian and Poirier, Joel Lamy and Schoelkopf, Hailey and Troshin, Sergey and Abulkhanov, Dmitry and Romero, Manuel and Lappert, Michael and De Toni, Francesco and del Río, Bernardo García and Liu, Qian and Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue and Yu, Ian and Villegas, Paulo and Zocca, Marco and Mangrulkar, Sourab and Lansky, David and Nguyen, Huu and Contractor, Danish and Villa, Luis and Li, Jia and Bahdanau, Dzmitry and Jernite, Yacine and Hughes, Sean and Fried, Daniel and Guha, Arjun and de Vries, Harm and von Werra, Leandro},
	urldate = {2023-06-10},
	date = {2023-02-24},
	eprinttype = {arxiv},
	eprint = {2301.03988 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209141/Allal 等 - 2023 - SantaCoder don't reach for the stars!.pdf:application/pdf;arXiv.org Snapshot:files/209247/2301.html:text/html},
}

@misc{hajipour_systematically_2023,
	title = {Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models},
	url = {http://arxiv.org/abs/2302.04012},
	abstract = {Recently, large language models for code generation have achieved breakthroughs in several programming language tasks. Their advances in competition-level programming problems have made them an emerging pillar in {AI}-assisted pair programming. Tools such as {GitHub} Copilot are already part of the daily programming workflow and are used by more than a million developers. The training data for these models is usually collected from open-source repositories (e.g., {GitHub}) that contain software faults and security vulnerabilities. This unsanitized training data can lead language models to learn these vulnerabilities and propagate them in the code generation procedure. Given the wide use of these models in the daily workflow of developers, it is crucial to study the security aspects of these models systematically. In this work, we propose the first approach to automatically finding security vulnerabilities in black-box code generation models. To achieve this, we propose a novel black-box inversion approach based on few-shot prompting. We evaluate the effectiveness of our approach by examining code generation models in the generation of high-risk security weaknesses. We show that our approach automatically and systematically finds 1000s of security vulnerabilities in various code generation models, including the commercial black-box model {GitHub} Copilot.},
	number = {{arXiv}:2302.04012},
	publisher = {{arXiv}},
	author = {Hajipour, Hossein and Holz, Thorsten and Schönherr, Lea and Fritz, Mario},
	urldate = {2023-06-10},
	date = {2023-02-08},
	eprinttype = {arxiv},
	eprint = {2302.04012 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:files/209280/Hajipour 等 - 2023 - Systematically Finding Security Vulnerabilities in.pdf:application/pdf;arXiv.org Snapshot:files/209677/2302.html:text/html},
}

@misc{bang_multitask_2023,
	title = {A Multitask, Multilingual, Multimodal Evaluation of {ChatGPT} on Reasoning, Hallucination, and Interactivity},
	url = {http://arxiv.org/abs/2302.04023},
	abstract = {This paper proposes a framework for quantitatively evaluating interactive {LLMs} such as {ChatGPT} using publicly available data sets. We carry out an extensive technical evaluation of {ChatGPT} using 23 data sets covering 8 different common {NLP} application tasks. We evaluate the multitask, multilingual and multi-modal aspects of {ChatGPT} based on these data sets and a newly designed multimodal dataset. We find that {ChatGPT} outperforms {LLMs} with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that {ChatGPT} is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. {ChatGPT} suffers from hallucination problems like other {LLMs} and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of {ChatGPT} enables human collaboration with the underlying {LLM} to improve its performance, i.e, 8\% {ROUGE}-1 on summarization and 2\% {ChrF}++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
	number = {{arXiv}:2302.04023},
	publisher = {{arXiv}},
	author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
	urldate = {2023-06-10},
	date = {2023-02-28},
	eprinttype = {arxiv},
	eprint = {2302.04023 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209199/Bang 等 - 2023 - A Multitask, Multilingual, Multimodal Evaluation o.pdf:application/pdf;arXiv.org Snapshot:files/209686/2302.html:text/html},
}

@misc{daull_complex_2023,
	title = {Complex {QA} and language models hybrid architectures, Survey},
	url = {http://arxiv.org/abs/2302.09051},
	abstract = {This paper reviews the state-of-the-art of language models architectures and strategies for "complex" question-answering ({QA}, {CQA}, {CPS}) with a focus on hybridization. Large Language Models ({LLM}) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like {ChatGPT} and {GALACTICA} have allowed non-specialists to grasp the great potential as well as the equally strong limitations of {LLM} in complex {QA}. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers {BIG}, {BLOOM} and {HELM} which open source, benchmark and analyze limits and challenges of {LLM} in terms of tasks complexity and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as a baseline. We discuss some challenges associated with complex {QA}, including domain adaptation, decomposition and efficient multi-step {QA}, long form and non-factoid {QA}, safety and multi-sensitivity data protection, multimodal search, hallucinations, explainability and truthfulness, temporal reasoning. We analyze current solutions and promising research trends, using elements such as: hybrid {LLM} architectural patterns, training and prompting strategies, active human reinforcement learning supervised with {AI}, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.},
	number = {{arXiv}:2302.09051},
	publisher = {{arXiv}},
	author = {Daull, Xavier and Bellot, Patrice and Bruno, Emmanuel and Martin, Vincent and Murisasco, Elisabeth},
	urldate = {2023-06-10},
	date = {2023-04-07},
	eprinttype = {arxiv},
	eprint = {2302.09051 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/209600/Daull 等 - 2023 - Complex QA and language models hybrid architecture.pdf:application/pdf;arXiv.org Snapshot:files/209675/2302.html:text/html},
}

@misc{khakhar_pac_2023,
	title = {{PAC} Prediction Sets for Large Language Models of Code},
	url = {http://arxiv.org/abs/2302.08703},
	abstract = {Prediction sets have recently been shown to be a promising strategy for quantifying the uncertainty of deep neural networks in a way that provides theoretical guarantees. However, existing techniques have largely targeted settings where the space of labels is simple, so prediction sets can be arbitrary subsets of labels. For structured prediction problems where the space of labels is exponential in size, even prediction sets containing a small fraction of all labels can be exponentially large. In the context of code generation, we propose a solution that considers a restricted set of prediction sets that can compactly be represented as partial programs, which are programs with portions replaced with holes. Given a trained code generation model, our algorithm leverages a programming language's abstract syntax tree to generate a set of programs such that the correct program is in the set with high-confidence. Valuable applications of our algorithm include a Codex-style code generator with holes in uncertain parts of the generated code, which provides a partial program with theoretical guarantees. We evaluate our approach on {PICARD} (a T5 model for {SQL} semantic parsing) and Codex (a {GPT} model for over a dozen programming languages, including Python), demonstrating that our approach generates compact {PAC} prediction sets. This is the first research contribution that generates {PAC} prediction sets for generative code models.},
	number = {{arXiv}:2302.08703},
	publisher = {{arXiv}},
	author = {Khakhar, Adam and Mell, Stephen and Bastani, Osbert},
	urldate = {2023-06-10},
	date = {2023-02-17},
	eprinttype = {arxiv},
	eprint = {2302.08703 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209590/Khakhar 等 - 2023 - PAC Prediction Sets for Large Language Models of C.pdf:application/pdf;arXiv.org Snapshot:files/209595/2302.html:text/html},
}

@misc{white_prompt_2023,
	title = {A Prompt Pattern Catalog to Enhance Prompt Engineering with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models ({LLMs}), such as {ChatGPT}. Prompts are instructions given to an {LLM} to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an {LLM}. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with {LLMs}. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with {LLMs}. This paper provides the following contributions to research on prompt engineering that apply {LLMs} to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of {LLM} conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	number = {{arXiv}:2302.11382},
	publisher = {{arXiv}},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	urldate = {2023-06-10},
	date = {2023-02-21},
	eprinttype = {arxiv},
	eprint = {2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209106/White 等 - 2023 - A Prompt Pattern Catalog to Enhance Prompt Enginee.pdf:application/pdf;arXiv.org Snapshot:files/209135/2302.html:text/html},
}

@misc{paranjape_art_2023,
	title = {{ART}: Automatic multi-step reasoning and tool-use for large language models},
	url = {http://arxiv.org/abs/2303.09014},
	shorttitle = {{ART}},
	abstract = {Large language models ({LLMs}) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought ({CoT}) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core {LLM} capabilities (e.g. search/running code). Prior work on {CoT} prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use ({ART}), a framework that uses frozen {LLMs} to automatically generate intermediate reasoning steps as a program. Given a new task to solve, {ART} selects demonstrations of multi-step reasoning and tool use from a task library. At test time, {ART} seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. {ART} achieves a substantial improvement over few-shot prompting and automatic {CoT} on unseen tasks in the {BigBench} and {MMLU} benchmarks, and matches performance of hand-crafted {CoT} prompts on a majority of these tasks. {ART} is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.},
	number = {{arXiv}:2303.09014},
	publisher = {{arXiv}},
	author = {Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
	urldate = {2023-06-10},
	date = {2023-03-15},
	eprinttype = {arxiv},
	eprint = {2303.09014 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209242/Paranjape 等 - 2023 - ART Automatic multi-step reasoning and tool-use f.pdf:application/pdf;arXiv.org Snapshot:files/209284/2303.html:text/html},
}

@misc{arakelyan_exploring_2023,
	title = {Exploring Distributional Shifts in Large Language Models for Code Analysis},
	url = {http://arxiv.org/abs/2303.09128},
	abstract = {We systematically study the capacity of two large language models for code - {CodeT}5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that in the case of code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to each domain individually.},
	number = {{arXiv}:2303.09128},
	publisher = {{arXiv}},
	author = {Arakelyan, Shushan and Das, Rocktim Jyoti and Mao, Yi and Ren, Xiang},
	urldate = {2023-06-10},
	date = {2023-03-16},
	eprinttype = {arxiv},
	eprint = {2303.09128 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209592/Arakelyan 等 - 2023 - Exploring Distributional Shifts in Large Language .pdf:application/pdf;arXiv.org Snapshot:files/209699/2303.html:text/html},
}

@misc{tony_llmseceval_2023,
	title = {{LLMSecEval}: A Dataset of Natural Language Prompts for Security Evaluations},
	url = {http://arxiv.org/abs/2303.09384},
	shorttitle = {{LLMSecEval}},
	abstract = {Large Language Models ({LLMs}) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language ({NL}) descriptions by learning languages and programming practices from public {GitHub} repositories. Although {LLMs} promise an effortless {NL}-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present {LLMSecEval}, a dataset containing 150 {NL} prompts that can be leveraged for assessing the security performance of such models. Such prompts are {NL} descriptions of code snippets prone to various security vulnerabilities listed in {MITRE}'s Top 25 Common Weakness Enumeration ({CWE}) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by {LLMs}. As a practical application, we show how {LLMSecEval} can be used for evaluating the security of snippets automatically generated from {NL} descriptions.},
	number = {{arXiv}:2303.09384},
	publisher = {{arXiv}},
	author = {Tony, Catherine and Mutas, Markus and Ferreyra, Nicolás E. Díaz and Scandariato, Riccardo},
	urldate = {2023-06-10},
	date = {2023-03-16},
	eprinttype = {arxiv},
	eprint = {2303.09384 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/209591/Tony 等 - 2023 - LLMSecEval A Dataset of Natural Language Prompts .pdf:application/pdf;arXiv.org Snapshot:files/209800/2303.html:text/html},
}

@misc{kashefi_chatgpt_2023,
	title = {{ChatGPT} for Programming Numerical Methods},
	url = {http://arxiv.org/abs/2303.12093},
	abstract = {{ChatGPT} is a large language model recently released by the {OpenAI} company. In this technical report, we explore for the first time the capability of {ChatGPT} for programming numerical algorithms. Specifically, we examine the capability of {GhatGPT} for generating codes for numerical algorithms in different programming languages, for debugging and improving written codes by users, for completing missed parts of numerical codes, rewriting available codes in other programming languages, and for parallelizing serial codes. Additionally, we assess if {ChatGPT} can recognize if given codes are written by humans or machines. To reach this goal, we consider a variety of mathematical problems such as the Poisson equation, the diffusion equation, the incompressible Navier-Stokes equations, compressible inviscid flow, eigenvalue problems, solving linear systems of equations, storing sparse matrices, etc. Furthermore, we exemplify scientific machine learning such as physics-informed neural networks and convolutional neural networks with applications to computational physics. Through these examples, we investigate the successes, failures, and challenges of {ChatGPT}. Examples of failures are producing singular matrices, operations on arrays with incompatible sizes, programming interruption for relatively long codes, etc. Our outcomes suggest that {ChatGPT} can successfully program numerical algorithms in different programming languages, but certain limitations and challenges exist that require further improvement of this machine learning model.},
	number = {{arXiv}:2303.12093},
	publisher = {{arXiv}},
	author = {Kashefi, Ali and Mukerji, Tapan},
	urldate = {2023-06-10},
	date = {2023-04-26},
	eprinttype = {arxiv},
	eprint = {2303.12093 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:files/209638/Kashefi 和 Mukerji - 2023 - ChatGPT for Programming Numerical Methods.pdf:application/pdf;arXiv.org Snapshot:files/209741/2303.html:text/html},
}

@misc{zhang_multimodal_2023,
	title = {Multimodal Pre-training Framework for Sequential Recommendation via Contrastive Learning},
	url = {http://arxiv.org/abs/2303.11879},
	abstract = {Sequential recommendation systems utilize the sequential interactions of users with items as their main supervision signals in learning users' preferences. However, existing methods usually generate unsatisfactory results due to the sparsity of user behavior data. To address this issue, we propose a novel pre-training framework, named Multimodal Sequence Mixup for Sequential Recommendation ({MSM}4SR), which leverages both users' sequential behaviors and items' multimodal content ({\textbackslash}ie text and images) for effectively recommendation. Specifically, {MSM}4SR tokenizes each item image into multiple textual keywords and uses the pre-trained {BERT} model to obtain initial textual and visual features of items, for eliminating the discrepancy between the text and image modalities. A novel backbone network, {\textbackslash}ie Multimodal Mixup Sequence Encoder (M\${\textasciicircum}2\${SE}), is proposed to bridge the gap between the item multimodal content and the user behavior, using a complementary sequence mixup strategy. In addition, two contrastive learning tasks are developed to assist M\${\textasciicircum}2\${SE} in learning generalized multimodal representations of the user behavior sequence. Extensive experiments on real-world datasets demonstrate that {MSM}4SR outperforms state-of-the-art recommendation methods. Moreover, we further verify the effectiveness of {MSM}4SR on other challenging tasks including cold-start and cross-domain recommendation.},
	number = {{arXiv}:2303.11879},
	publisher = {{arXiv}},
	author = {Zhang, Lingzi and Zhou, Xin and Shen, Zhiqi},
	urldate = {2023-06-10},
	date = {2023-03-21},
	eprinttype = {arxiv},
	eprint = {2303.11879 [cs]},
	keywords = {Computer Science - Multimedia, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:files/209610/Zhang 等 - 2023 - Multimodal Pre-training Framework for Sequential R.pdf:application/pdf;arXiv.org Snapshot:files/209626/2303.html:text/html},
}

@misc{nori_capabilities_2023,
	title = {Capabilities of {GPT}-4 on Medical Challenge Problems},
	url = {http://arxiv.org/abs/2303.13375},
	abstract = {Large language models ({LLMs}) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of {GPT}-4, a state-of-the-art {LLM}, on medical competency examinations and benchmark datasets. {GPT}-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the {USMLE}, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the {MultiMedQA} suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that {GPT}-4, without any specialized prompt crafting, exceeds the passing score on {USMLE} by over 20 points and outperforms earlier general-purpose models ({GPT}-3.5) as well as models specifically fine-tuned on medical knowledge (Med-{PaLM}, a prompt-tuned version of Flan-{PaLM} 540B). In addition, {GPT}-4 is significantly better calibrated than {GPT}-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of {GPT}-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of {GPT}-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.},
	number = {{arXiv}:2303.13375},
	publisher = {{arXiv}},
	author = {Nori, Harsha and King, Nicholas and {McKinney}, Scott Mayer and Carignan, Dean and Horvitz, Eric},
	urldate = {2023-06-10},
	date = {2023-04-12},
	eprinttype = {arxiv},
	eprint = {2303.13375 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209599/Nori 等 - 2023 - Capabilities of GPT-4 on Medical Challenge Problem.pdf:application/pdf;arXiv.org Snapshot:files/209617/2303.html:text/html},
}

@misc{liu_comprehensive_2023,
	title = {A comprehensive evaluation of {ChatGPT}'s zero-shot Text-to-{SQL} capability},
	url = {http://arxiv.org/abs/2303.13547},
	abstract = {This paper presents the first comprehensive analysis of {ChatGPT}'s Text-to-{SQL} ability. Given the recent emergence of large-scale conversational language model {ChatGPT} and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-{SQL} performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that {ChatGPT} has strong text-to-{SQL} abilities. Although there is still a gap from the current state-of-the-art ({SOTA}) model performance, considering that the experiment was conducted in a zero-shot scenario, {ChatGPT}'s performance is still impressive. Notably, in the {ADVETA} ({RPL}) scenario, the zero-shot {ChatGPT} even outperforms the {SOTA} model that requires fine-tuning on the Spider dataset by 4.1{\textbackslash}\%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by {ChatGPT} publicly available at https://github.com/{THU}-{BPM}/chatgpt-sql.},
	number = {{arXiv}:2303.13547},
	publisher = {{arXiv}},
	author = {Liu, Aiwei and Hu, Xuming and Wen, Lijie and Yu, Philip S.},
	urldate = {2023-06-10},
	date = {2023-03-11},
	eprinttype = {arxiv},
	eprint = {2303.13547 [cs]},
	keywords = {68T50, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7},
	file = {arXiv Fulltext PDF:files/209118/Liu 等 - 2023 - A comprehensive evaluation of ChatGPT's zero-shot .pdf:application/pdf;arXiv.org Snapshot:files/209704/2303.html:text/html},
}

@misc{bull_generative_2023,
	title = {Generative {AI} Assistants in Software Development Education},
	url = {http://arxiv.org/abs/2303.13936},
	abstract = {The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative {AI} ({GAI}) assistants for software development. Whilst {AI} is already used in various areas of software engineering, {GAI} technologies, such as {GitHub} Copilot and {ChatGPT}, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft ({GitHub}, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations.},
	number = {{arXiv}:2303.13936},
	publisher = {{arXiv}},
	author = {Bull, Christopher and Kharrufa, Ahmed},
	urldate = {2023-06-10},
	date = {2023-03-24},
	eprinttype = {arxiv},
	eprint = {2303.13936 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209614/Bull 和 Kharrufa - 2023 - Generative AI Assistants in Software Development E.pdf:application/pdf;arXiv.org Snapshot:files/209715/2303.html:text/html},
}

@misc{skreta_errors_2023,
	title = {Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting},
	url = {http://arxiv.org/abs/2303.14100},
	shorttitle = {Errors are Useful Prompts},
	abstract = {Generating low-level robot task plans from high-level natural language instructions remains a challenging problem. Although large language models have shown promising results in generating plans, the accuracy of the output remains unverified. Furthermore, the lack of domain-specific language data poses a limitation on the applicability of these models. In this paper, we propose {CLAIRIFY}, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints. Our approach provides effective guidance to the language model on generating structured-like task plans by incorporating any errors as feedback, while the verifier ensures the syntactic accuracy of the generated plans. We demonstrate the effectiveness of {CLAIRIFY} in planning chemistry experiments by achieving state-of-the-art results. We also show that the generated plans can be executed on a real robot by integrating them with a task and motion planner.},
	number = {{arXiv}:2303.14100},
	publisher = {{arXiv}},
	author = {Skreta, Marta and Yoshikawa, Naruki and Arellano-Rubach, Sebastian and Ji, Zhi and Kristensen, Lasse Bjørn and Darvish, Kourosh and Aspuru-Guzik, Alán and Shkurti, Florian and Garg, Animesh},
	urldate = {2023-06-10},
	date = {2023-03-24},
	eprinttype = {arxiv},
	eprint = {2303.14100 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/209660/Skreta 等 - 2023 - Errors are Useful Prompts Instruction Guided Task.pdf:application/pdf;arXiv.org Snapshot:files/209826/2303.html:text/html},
}

@misc{liang_taskmatrixai_2023,
	title = {{TaskMatrix}.{AI}: Completing Tasks by Connecting Foundation Models with Millions of {APIs}},
	url = {http://arxiv.org/abs/2303.16434},
	shorttitle = {{TaskMatrix}.{AI}},
	abstract = {Artificial Intelligence ({AI}) has made incredible progress recently. On the one hand, advanced foundation models like {ChatGPT} can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce {TaskMatrix}.{AI} as a new {AI} ecosystem that connects foundation models with millions of {APIs} for task completion. Unlike most previous work that aimed to improve a single {AI} model, {TaskMatrix}.{AI} focuses more on using existing foundation models (as a brain-like central system) and {APIs} of other {AI} models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.},
	number = {{arXiv}:2303.16434},
	publisher = {{arXiv}},
	author = {Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and Wang, Yun and Shou, Linjun and Gong, Ming and Duan, Nan},
	urldate = {2023-06-10},
	date = {2023-03-28},
	eprinttype = {arxiv},
	eprint = {2303.16434 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209639/Liang 等 - 2023 - TaskMatrix.AI Completing Tasks by Connecting Foun.pdf:application/pdf;arXiv.org Snapshot:files/209642/2303.html:text/html},
}

@misc{chen_improving_2023,
	title = {Improving Code Generation by Training with Natural Language Feedback},
	url = {http://arxiv.org/abs/2303.16749},
	abstract = {The potential for pre-trained large language models ({LLMs}) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback ({ILF}). {ILF} requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that {ILF} can be seen as a form of minimizing the {KL} divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use {ILF} to improve a Codegen-Mono 6.1B model's pass@1 rate by 38\% relative (and 10\% absolute) on the Mostly Basic Python Problems ({MBPP}) benchmark, outperforming both fine-tuning on {MBPP} and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from human-written natural language feedback is both more effective and sample-efficient than training exclusively on demonstrations for improving an {LLM}'s performance on code generation tasks.},
	number = {{arXiv}:2303.16749},
	publisher = {{arXiv}},
	author = {Chen, Angelica and Scheurer, Jérémy and Korbak, Tomasz and Campos, Jon Ander and Chan, Jun Shern and Bowman, Samuel R. and Cho, Kyunghyun and Perez, Ethan},
	urldate = {2023-06-10},
	date = {2023-03-28},
	eprinttype = {arxiv},
	eprint = {2303.16749 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209627/Chen 等 - 2023 - Improving Code Generation by Training with Natural.pdf:application/pdf;arXiv.org Snapshot:files/209695/2303.html:text/html},
}

@misc{zheng_codegeex_2023,
	title = {{CodeGeeX}: A Pre-Trained Model for Code Generation with Multilingual Evaluations on {HumanEval}-X},
	url = {http://arxiv.org/abs/2303.17568},
	shorttitle = {{CodeGeeX}},
	abstract = {Large pre-trained code generation models, such as {OpenAI} Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce {CodeGeeX}, a multilingual model with 13 billion parameters for code generation. {CodeGeeX} is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that {CodeGeeX} outperforms multilingual code models of similar scale for both the tasks of code generation and translation on {HumanEval}-X. Building upon {HumanEval} (Python only), we develop the {HumanEval}-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, {JavaScript}, and Go. In addition, we build {CodeGeeX}-based extensions on Visual Studio Code, {JetBrains}, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that {CodeGeeX} can help to increase coding efficiency for 83.4\% of its users. Finally, {CodeGeeX} is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), {API}, extensions, and {HumanEval}-X at https://github.com/{THUDM}/{CodeGeeX}.},
	number = {{arXiv}:2303.17568},
	publisher = {{arXiv}},
	author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie},
	urldate = {2023-06-10},
	date = {2023-03-30},
	eprinttype = {arxiv},
	eprint = {2303.17568 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209703/Zheng 等 - 2023 - CodeGeeX A Pre-Trained Model for Code Generation .pdf:application/pdf;arXiv.org Snapshot:files/209852/2303.html:text/html},
}

@misc{li_towards_2023,
	title = {Towards Enhancing In-Context Learning for Code Generation},
	url = {http://arxiv.org/abs/2303.17780},
	abstract = {In-context learning ({ICL}) with pre-trained language models ({PTLMs}) has shown great success in code generation. {ICL} does not require training. {PTLMs} take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse {ICL} techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard {ICL}. Inspired by observations of the human coding process, we propose a novel {ICL} approach for code generation named {AceCoder}. Compared to standard {ICL}, {AceCoder} has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, {APIs}) from them. (2) Guided Code Generation. It encourages {PTLMs} to output an intermediate preliminary (e.g., test cases, {APIs}) before generating programs. The preliminary can help {PTLMs} understand requirements and guide the next code generation. We apply {AceCoder} to six {PTLMs} (e.g., Codex) and evaluate it on three public benchmarks using the Pass@k. Results show that {AceCoder} can significantly improve the performance of {PTLMs} on code generation. (1) In terms of Pass@1, {AceCoder} outperforms standard {ICL} by up to 79.7\% and fine-tuned models by up to 171\%. (2) {AceCoder} is effective in {PTLMs} with different sizes (e.g., 1B to 175B) and different languages (e.g., Python, Java, and {JavaScript}). (3) We investigate multiple choices of the intermediate preliminary. (4) We manually evaluate generated programs in three aspects and prove the superiority of {AceCoder}. (5) Finally, we discuss some insights about {ICL} for practitioners.},
	number = {{arXiv}:2303.17780},
	publisher = {{arXiv}},
	author = {Li, Jia and Zhao, Yunfei and Li, Yongmin and Li, Ge and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2023-03-30},
	eprinttype = {arxiv},
	eprint = {2303.17780 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209644/Li 等 - 2023 - Towards Enhancing In-Context Learning for Code Gen.pdf:application/pdf;arXiv.org Snapshot:files/209646/2303.html:text/html},
}

@misc{chen_teaching_2023,
	title = {Teaching Large Language Models to Self-Debug},
	url = {http://arxiv.org/abs/2304.05128},
	abstract = {Large language models ({LLMs}) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-{SQL} generation, {TransCoder} for C++-to-Python translation, and {MBPP} for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest label by 9\%. On {TransCoder} and {MBPP} where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
	number = {{arXiv}:2304.05128},
	publisher = {{arXiv}},
	author = {Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and Zhou, Denny},
	urldate = {2023-06-10},
	date = {2023-04-11},
	eprinttype = {arxiv},
	eprint = {2304.05128 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209645/Chen 等 - 2023 - Teaching Large Language Models to Self-Debug.pdf:application/pdf;arXiv.org Snapshot:files/209763/2304.html:text/html},
}

@misc{fakhoury_towards_2023,
	title = {Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions},
	url = {http://arxiv.org/abs/2304.03816},
	abstract = {Large language models ({LLMs}), such as {OpenAI}'s Codex, have demonstrated their potential to generate code from natural language descriptions across a wide range of programming tasks. Several benchmarks have recently emerged to evaluate the ability of {LLMs} to generate functionally correct code from natural language intent with respect to a set of hidden test cases. This has enabled the research community to identify significant and reproducible advancements in {LLM} capabilities. However, there is currently a lack of benchmark datasets for assessing the ability of {LLMs} to generate functionally correct code edits based on natural language descriptions of intended changes. This paper aims to address this gap by motivating the problem {NL}2Fix of translating natural language descriptions of code changes (namely bug fixes described in Issue reports in repositories) into correct code fixes. To this end, we introduce Defects4J-{NL}2Fix, a dataset of 283 Java programs from the popular Defects4J dataset augmented with high-level descriptions of bug fixes, and empirically evaluate the performance of several state-of-the-art {LLMs} for the this task. Results show that these {LLMS} together are capable of generating plausible fixes for 64.6\% of the bugs, and the best {LLM}-based technique can achieve up to 21.20\% top-1 and 35.68\% top-5 accuracy on this benchmark.},
	number = {{arXiv}:2304.03816},
	publisher = {{arXiv}},
	author = {Fakhoury, Sarah and Chakraborty, Saikat and Musuvathi, Madan and Lahiri, Shuvendu K.},
	urldate = {2023-06-10},
	date = {2023-04-07},
	eprinttype = {arxiv},
	eprint = {2304.03816 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209647/Fakhoury 等 - 2023 - Towards Generating Functionally Correct Code Edits.pdf:application/pdf;arXiv.org Snapshot:files/209822/2304.html:text/html},
}

@misc{wang_evaluating_2023,
	title = {Evaluating {AIGC} Detectors on Code Content},
	url = {http://arxiv.org/abs/2304.05193},
	abstract = {Artificial Intelligence Generated Content ({AIGC}) has garnered considerable attention for its impressive performance, with {ChatGPT} emerging as a leading {AIGC} model that produces high-quality responses across various applications, including software development and maintenance. Despite its potential, the misuse of {ChatGPT} poses significant concerns, especially in education and safetycritical domains. Numerous {AIGC} detectors have been developed and evaluated on natural language data. However, their performance on code-related content generated by {ChatGPT} remains unexplored. To fill this gap, in this paper, we present the first empirical study on evaluating existing {AIGC} detectors in the software domain. We created a comprehensive dataset including 492.5K samples comprising code-related content produced by {ChatGPT}, encompassing popular software activities like Q\&A (115K), code summarization (126K), and code generation (226.5K). We evaluated six {AIGC} detectors, including three commercial and three open-source solutions, assessing their performance on this dataset. Additionally, we conducted a human study to understand human detection capabilities and compare them with the existing {AIGC} detectors. Our results indicate that {AIGC} detectors demonstrate lower performance on code-related data compared to natural language data. Fine-tuning can enhance detector performance, especially for content within the same domain; but generalization remains a challenge. The human evaluation reveals that detection by humans is quite challenging.},
	number = {{arXiv}:2304.05193},
	publisher = {{arXiv}},
	author = {Wang, Jian and Liu, Shangqing and Xie, Xiaofei and Li, Yi},
	urldate = {2023-06-10},
	date = {2023-04-11},
	eprinttype = {arxiv},
	eprint = {2304.05193 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209245/Wang 等 - 2023 - Evaluating AIGC Detectors on Code Content.pdf:application/pdf;arXiv.org Snapshot:files/209678/2304.html:text/html},
}

@misc{ahmed_improving_2023,
	title = {Improving Few-Shot Prompts with Relevant Static Analysis Products},
	url = {http://arxiv.org/abs/2304.06815},
	abstract = {Large Language Models ({LLM}) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these {LLMs} to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc. One might assume that the powerful multi-layer architecture of transformer-style {LLMs} makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an {LLM}'s prompt with semantic facts explicitly, actually helps. Prior work shows that {LLM} performance on code summarization benefits from few-shot samples drawn either from the same-project or from examples found via information retrieval methods (such as {BM}25). While summarization performance has steadily increased since the early days, there is still room for improvement: {LLM} performance on code summarization still lags its performance on natural-language tasks like translation and text summarization. We find that adding semantic facts actually does help! This approach improves performance in several different settings suggested by prior work, including for two different Large Language Models. In most cases, improvement nears or exceeds 2 {BLEU}; for the {PHP} language in the challenging {CodeSearchNet} dataset, this augmentation actually yields performance surpassing 30 {BLEU}.},
	number = {{arXiv}:2304.06815},
	publisher = {{arXiv}},
	author = {Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl T.},
	urldate = {2023-06-10},
	date = {2023-04-13},
	eprinttype = {arxiv},
	eprint = {2304.06815 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209207/Ahmed 等 - 2023 - Improving Few-Shot Prompts with Relevant Static An.pdf:application/pdf;arXiv.org Snapshot:files/209269/2304.html:text/html},
}

@misc{dong_self-collaboration_2023,
	title = {Self-collaboration Code Generation via {ChatGPT}},
	url = {http://arxiv.org/abs/2304.07590},
	abstract = {Although Large Language Models ({LLMs}) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing {LLMs}, exemplified by {ChatGPT}. Specifically, through role instructions, 1) Multiple {LLMs} act as distinct ``experts'', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three {ChatGPT} roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\%-47.1\% Pass@1 compared to direct code generation, achieving state-of-the-art performance and even surpassing {GPT}-4. Moreover, we showcase that self-collaboration could potentially enable {LLMs} to efficiently handle complex real-world tasks that are not readily solved by direct code generation, as evidenced in case study.},
	number = {{arXiv}:2304.07590},
	publisher = {{arXiv}},
	author = {Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2304.07590 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209296/Dong 等 - 2023 - Self-collaboration Code Generation via ChatGPT.pdf:application/pdf;arXiv.org Snapshot:files/209301/2304.html:text/html},
}

@misc{cao_study_2023,
	title = {A study on Prompt Design, Advantages and Limitations of {ChatGPT} for Deep Learning Program Repair},
	url = {http://arxiv.org/abs/2304.08191},
	abstract = {{ChatGPT} has revolutionized many research and industrial fields. {ChatGPT} has shown great potential in software engineering to boost various traditional tasks such as program repair, code understanding, and code generation. However, whether automatic program repair ({APR}) applies to deep learning ({DL}) programs is still unknown. {DL} programs, whose decision logic is not explicitly encoded in the source code, have posed unique challenges to {APR}. While to repair {DL} programs, an {APR} approach needs to not only parse the source code syntactically but also needs to understand the code intention. With the best prior work, the performance of fault localization is still far less than satisfactory (only about 30{\textbackslash}\%). Therefore, in this paper, we explore {ChatGPT}'s capability for {DL} program repair by asking three research questions. (1) Can {ChatGPT} debug {DL} programs effectively? (2) How can {ChatGPT}'s repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? On top of that, we categorize the common aspects useful for prompt design for {DL} program repair. Also, we propose various prompt templates to facilitate the performance and summarize the advantages and disadvantages of {ChatGPT}'s abilities such as detecting bad code smell, code refactoring, and detecting {API} misuse/deprecation.},
	number = {{arXiv}:2304.08191},
	publisher = {{arXiv}},
	author = {Cao, Jialun and Li, Meiziniu and Wen, Ming and Cheung, Shing-chi},
	urldate = {2023-06-10},
	date = {2023-04-17},
	eprinttype = {arxiv},
	eprint = {2304.08191 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209152/Cao 等 - 2023 - A study on Prompt Design, Advantages and Limitatio.pdf:application/pdf;arXiv.org Snapshot:files/209717/2304.html:text/html},
}

@misc{sharma_stochastic_2023,
	title = {Stochastic Code Generation},
	url = {http://arxiv.org/abs/2304.08243},
	abstract = {Large language models pre-trained for code generation can generate high-quality short code but often struggle with generating coherent long code and understanding higher-level or system-level specifications. This issue is also observed in language modeling for long text generation, and one proposed solution is the use of a latent stochastic process. This approach involves generating a document plan and then producing text that is consistent with it. In this study, we investigate whether this technique can be applied to code generation to improve coherence. We base our proposed encoder and decoder on the pre-trained {GPT}-2 based {CodeParrot} model and utilize the {APPS} dataset for training. We evaluate our results using the {HumanEval} benchmark and observe that the modified Time Control model performs similarly to {CodeParrot} on this evaluation.},
	number = {{arXiv}:2304.08243},
	publisher = {{arXiv}},
	author = {Sharma, Swapnil and Anand, Nikita and G. V, Kranthi Kiran},
	urldate = {2023-06-10},
	date = {2023-04-13},
	eprinttype = {arxiv},
	eprint = {2304.08243 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209244/Sharma 等 - 2023 - Stochastic Code Generation.pdf:application/pdf;arXiv.org Snapshot:files/209825/2304.html:text/html},
}

@misc{tian_is_2023,
	title = {Is {ChatGPT} the Ultimate Programming Assistant -- How far is it?},
	url = {http://arxiv.org/abs/2304.11938},
	abstract = {The recent progress in generative {AI} techniques has significantly influenced software engineering, as {AI}-driven methods tackle common developer challenges such as code synthesis from descriptions, program repair, and natural language summaries for existing programs. Large-scale language models ({LLMs}), like {OpenAI}'s Codex, are increasingly adopted in {AI}-driven software engineering. {ChatGPT}, another {LLM}, has gained considerable attention for its potential as a bot for discussing source code, suggesting changes, providing descriptions, and generating code. To evaluate the practicality of {LLMs} as programming assistant bots, it is essential to examine their performance on unseen problems and various tasks. In our paper, we conduct an empirical analysis of {ChatGPT}'s potential as a fully automated programming assistant, emphasizing code generation, program repair, and code summarization. Our study assesses {ChatGPT}'s performance on common programming problems and compares it to state-of-the-art approaches using two benchmarks. Our research indicates that {ChatGPT} effectively handles typical programming challenges. However, we also discover the limitations in its attention span: comprehensive descriptions can restrict {ChatGPT}'s focus and impede its ability to utilize its extensive knowledge for problem-solving. Surprisingly, we find that {ChatGPT}'s summary explanations of incorrect code provide valuable insights into the developer's original intentions. This insight can be served as a foundation for future work addressing the oracle problem. Our study offers valuable perspectives on the development of {LLMs} for programming assistance, specifically by highlighting the significance of prompt engineering and enhancing our comprehension of {ChatGPT}'s practical applications in software engineering.},
	number = {{arXiv}:2304.11938},
	publisher = {{arXiv}},
	author = {Tian, Haoye and Lu, Weiqi and Li, Tsz On and Tang, Xunzhu and Cheung, Shing-Chi and Klein, Jacques and Bissyandé, Tegawendé F.},
	urldate = {2023-06-10},
	date = {2023-04-24},
	eprinttype = {arxiv},
	eprint = {2304.11938 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209649/Tian 等 - 2023 - Is ChatGPT the Ultimate Programming Assistant -- H.pdf:application/pdf;arXiv.org Snapshot:files/209651/2304.html:text/html},
}

@misc{ling_chatgpt_2023,
	title = {{ChatGPT} (Feb 13 Version) is a Chinese Room},
	url = {http://arxiv.org/abs/2304.12411},
	abstract = {{ChatGPT} has gained both positive and negative publicity after reports suggesting that it is able to pass various professional and licensing examinations. This suggests that {ChatGPT} may pass Turing Test in the near future. However, a computer program that passing Turing Test can either mean that it is a Chinese Room or artificially conscious. Hence, the question of whether the current state of {ChatGPT} is more of a Chinese Room or approaching artificial consciousness remains. Here, I demonstrate that the current version of {ChatGPT} (Feb 13 version) is a Chinese Room. Despite potential evidence of cognitive connections, {ChatGPT} exhibits critical errors in causal reasoning. At the same time, I demonstrate that {ChatGPT} can generate all possible categorical responses to the same question and response with erroneous examples; thus, questioning its utility as a learning tool. I also show that {ChatGPT} is capable of artificial hallucination, which is defined as generating confidently wrong replies. It is likely that errors in causal reasoning leads to hallucinations. More critically, {ChatGPT} generates false references to mimic real publications. Therefore, its utility is cautioned.},
	number = {{arXiv}:2304.12411},
	publisher = {{arXiv}},
	author = {Ling, Maurice {HT}},
	urldate = {2023-06-10},
	date = {2023-02-18},
	eprinttype = {arxiv},
	eprint = {2304.12411 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209653/Ling - 2023 - ChatGPT (Feb 13 Version) is a Chinese Room.pdf:application/pdf;arXiv.org Snapshot:files/209789/2304.html:text/html},
}

@misc{gilbert_semantic_2023,
	title = {Semantic Compression With Large Language Models},
	url = {http://arxiv.org/abs/2304.12512},
	abstract = {The rise of large language models ({LLMs}) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as "hallucinations"), {LLMs} are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed. This paper presents three contributions to research on {LLMs}. First, we present the results from experiments exploring the viability of approximate compression using {LLMs}, focusing specifically on {GPT}-3.5 and {GPT}-4 via {ChatGPT} interfaces. Second, we investigate and quantify the capability of {LLMs} to compress text and code, as well as to recall and manipulate compressed representations of prompts. Third, we present two novel metrics -- Exact Reconstructive Effectiveness ({ERE}) and Semantic Reconstruction Effectiveness ({SRE}) -- that quantify the level of preserved intent between text compressed and decompressed by the {LLMs} we studied. Our initial results indicate that {GPT}-4 can effectively compress and reconstruct text while preserving the semantic essence of the original text, providing a path to leverage \${\textbackslash}sim\$5\${\textbackslash}times\$ more tokens than present limits allow.},
	number = {{arXiv}:2304.12512},
	publisher = {{arXiv}},
	author = {Gilbert, Henry and Sandborn, Michael and Schmidt, Douglas C. and Spencer-Smith, Jesse and White, Jules},
	urldate = {2023-06-10},
	date = {2023-04-24},
	eprinttype = {arxiv},
	eprint = {2304.12512 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:files/209648/Gilbert 等 - 2023 - Semantic Compression With Large Language Models.pdf:application/pdf;arXiv.org Snapshot:files/209650/2304.html:text/html},
}

@misc{liu_code_2023,
	title = {Code Execution with Pre-trained Language Models},
	url = {http://arxiv.org/abs/2305.05383},
	abstract = {Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present {CodeExecutor}, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate {CodeExecutor} on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution.},
	number = {{arXiv}:2305.05383},
	publisher = {{arXiv}},
	author = {Liu, Chenxiao and Lu, Shuai and Chen, Weizhu and Jiang, Daxin and Svyatkovskiy, Alexey and Fu, Shengyu and Sundaresan, Neel and Duan, Nan},
	urldate = {2023-06-10},
	date = {2023-05-08},
	eprinttype = {arxiv},
	eprint = {2305.05383 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209155/Liu 等 - 2023 - Code Execution with Pre-trained Language Models.pdf:application/pdf;arXiv.org Snapshot:files/209250/2305.html:text/html},
}

@misc{li_codeie_2023,
	title = {{CodeIE}: Large Code Generation Models are Better Few-Shot Information Extractors},
	url = {http://arxiv.org/abs/2305.05711},
	shorttitle = {{CodeIE}},
	abstract = {Large language models ({LLMs}) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many {NLP} tasks. A common practice is to recast the task into a text-to-text format such that generative {LLMs} of natural language ({NL}-{LLMs}) like {GPT}-3 can be prompted to solve it. However, it is nontrivial to perform information extraction ({IE}) tasks with {NL}-{LLMs} since the output of the {IE} task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative {LLMs} of code (Code-{LLMs}) such as Codex to perform {IE} tasks, in particular, named entity recognition and relation extraction. In contrast to {NL}-{LLMs}, we show that Code-{LLMs} can be well-aligned with these {IE} tasks by designing code-style prompts and formulating these {IE} tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for {IE} tasks (e.g., {UIE}) and prompting {NL}-{LLMs} under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-{LLMs} for {IE} tasks.},
	number = {{arXiv}:2305.05711},
	publisher = {{arXiv}},
	author = {Li, Peng and Sun, Tianxiang and Tang, Qiong and Yan, Hang and Wu, Yuanbin and Huang, Xuanjing and Qiu, Xipeng},
	urldate = {2023-06-10},
	date = {2023-05-10},
	eprinttype = {arxiv},
	eprint = {2305.05711 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209654/Li 等 - 2023 - CodeIE Large Code Generation Models are Better Fe.pdf:application/pdf;arXiv.org Snapshot:files/209658/2305.html:text/html},
}

@misc{manh_vault_2023,
	title = {The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation},
	url = {http://arxiv.org/abs/2305.06156},
	shorttitle = {The Vault},
	abstract = {We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models ({LLMs}). Existing open-source datasets for training code-based {LLMs} often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused {LLMs} or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric {LLMs}, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and software development practices.},
	number = {{arXiv}:2305.06156},
	publisher = {{arXiv}},
	author = {Manh, Dung Nguyen and Hai, Nam Le and Dau, Anh T. V. and Nguyen, Anh Minh and Nghiem, Khanh and Guo, Jin and Bui, Nghi D. Q.},
	urldate = {2023-06-10},
	date = {2023-05-09},
	eprinttype = {arxiv},
	eprint = {2305.06156 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:files/209656/Manh 等 - 2023 - The Vault A Comprehensive Multilingual Dataset fo.pdf:application/pdf;arXiv.org Snapshot:files/209659/2305.html:text/html},
}

@misc{li_autonomous_2023,
	title = {Autonomous {GIS}: the next-generation {AI}-powered {GIS}},
	url = {http://arxiv.org/abs/2305.06453},
	shorttitle = {Autonomous {GIS}},
	abstract = {Large Language Models ({LLMs}), such as {ChatGPT}, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting {LLM} as the reasoning core, we introduce Autonomous {GIS} as an {AI}-powered geographic information system ({GIS}) that leverages the {LLM}'s general abilities in natural language understanding, reasoning, and coding for addressing spatial problems with automatic spatial data collection, analysis, and visualization. We envision that autonomous {GIS} will need to achieve five autonomous goals: self-generating, self-organizing, self-verifying, self-executing, and self-growing. We developed a prototype system called {LLM}-Geo using the {GPT}-4 {API} in a Python environment, demonstrating what an autonomous {GIS} looks like and how it delivers expected results without human intervention using three case studies. For all case studies, {LLM}-Geo was able to return accurate results, including aggregated numbers, graphs, and maps, significantly reducing manual operation time. Although still in its infancy and lacking several important modules such as logging and code testing, {LLM}-Geo demonstrates a potential path toward the next-generation {AI}-powered {GIS}. We advocate for the {GIScience} community to dedicate more effort to the research and development of autonomous {GIS}, making spatial analysis easier, faster, and more accessible to a broader audience.},
	number = {{arXiv}:2305.06453},
	publisher = {{arXiv}},
	author = {Li, Zhenlong and Ning, Huan},
	urldate = {2023-06-10},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2305.06453 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209662/Li 和 Ning - 2023 - Autonomous GIS the next-generation AI-powered GIS.pdf:application/pdf;arXiv.org Snapshot:files/209773/2305.html:text/html},
}

@misc{li_enabling_2023,
	title = {Enabling Programming Thinking in Large Language Models Toward Code Generation},
	url = {http://arxiv.org/abs/2305.06599},
	abstract = {Large Language Models ({LLMs}) (e.g., {ChatGPT}) have shown impressive performance in code generation. A large-scale study released that writing programs requires programming thinking, i.e., analyzing and implementing requirements in programming logic (e.g., sequence, branch, loop). Existing studies use {LLMs} to generate programs from requirements directly and do not explicitly introduce the programming thinking. This paper explores how to unlock the programming thinking of {LLMs} in code generation and proposes an approach named {TiP}. Our idea is to decompose code generation into two steps and progressively lead {LLMs} to analyze\&implement requirements in programming logic. Specifically, {TiP} first generates a code sketch, which provides a high-level solving process using programming logic but omits implementation details (e.g., {APIs}). Then, {TiP} implements the sketch into a program using specific programming languages. We conduct extensive experiments on three public benchmarks (i.e., {HumanEval}, {MBPP}, and {MBCPP}). (1) {TiP} outperforms the state-of-the-art baseline - {ChatGPT} by up to 17.5\% in Pass@1, 11.02\% in Pass@3, and 9.84\% in Pass@5. (2) Human evaluation shows that {TiP} outperforms {ChatGPT} in three aspects (i.e., correctness, code quality, and maintainability). (3) {TiP} is effective for different {LLMs}. (4) We explore multiple choices (e.g., chain-of-thought) for the code sketch and validate the superiority of our design. (5) We discuss the complementarity between {TiP} and post-processing approaches (e.g., {CodeT}).},
	number = {{arXiv}:2305.06599},
	publisher = {{arXiv}},
	author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
	urldate = {2023-06-10},
	date = {2023-05-11},
	eprinttype = {arxiv},
	eprint = {2305.06599 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209248/Li 等 - 2023 - Enabling Programming Thinking in Large Language Mo.pdf:application/pdf;arXiv.org Snapshot:files/209285/2305.html:text/html},
}

@misc{jhamtani_natural_2023,
	title = {Natural Language Decomposition and Interpretation of Complex Utterances},
	url = {http://arxiv.org/abs/2305.08677},
	abstract = {Natural language interfaces often require supervised data to translate user requests into programs, database queries, or other structured intent representations. During data collection, it can be difficult to anticipate and formalize the full range of user needs -- for example, in a system designed to handle simple requests (like \${\textbackslash}textit\{find my meetings tomorrow\}\$ or \${\textbackslash}textit\{move my meeting with my manager to noon\})\$, users may also express more elaborate requests (like \${\textbackslash}textit\{swap all my calls on Monday and Tuesday\}\$). We introduce an approach for equipping a simple language-to-code model to handle complex utterances via a process of hierarchical natural language decomposition. Our approach uses a pre-trained language model to decompose a complex utterance into a sequence of smaller natural language steps, then interprets each step using the language-to-code model. To test our approach, we collect and release {DeCU} -- a new {NL}-to-program benchmark to evaluate Decomposition of Complex Utterances. Experiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches.},
	number = {{arXiv}:2305.08677},
	publisher = {{arXiv}},
	author = {Jhamtani, Harsh and Fang, Hao and Xia, Patrick and Levy, Eran and Andreas, Jacob and Van Durme, Ben},
	urldate = {2023-06-10},
	date = {2023-05-15},
	eprinttype = {arxiv},
	eprint = {2305.08677 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209254/Jhamtani 等 - 2023 - Natural Language Decomposition and Interpretation .pdf:application/pdf;arXiv.org Snapshot:files/209287/2305.html:text/html},
}

@misc{ye_satisfiability-aided_2023,
	title = {Satisfiability-Aided Language Models Using Declarative Prompting},
	url = {http://arxiv.org/abs/2305.09656},
	abstract = {Prior work has combined chain-of-thought prompting in large language models ({LLMs}) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving problems that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling ({SATLM}) approach for improving the reasoning capabilities of {LLMs}. We use an {LLM} to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the {LLM} can parse it out of the description more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness of the answer with respect to the parsed specification and avoid planning errors in the solving process. We evaluate {SATLM} on 6 different datasets and show that it consistently outperforms program-aided {LMs} in an imperative paradigm. In particular, {SATLM} outperforms program-aided {LMs} by 23\% on a challenging subset of the {GSM} arithmetic reasoning dataset; {SATLM} also achieves a new {SoTA} on {LSAT}, surpassing previous models that are trained on the full training set.},
	number = {{arXiv}:2305.09656},
	publisher = {{arXiv}},
	author = {Ye, Xi and Chen, Qiaochu and Dillig, Isil and Durrett, Greg},
	urldate = {2023-06-10},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2305.09656 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209657/Ye 等 - 2023 - Satisfiability-Aided Language Models Using Declara.pdf:application/pdf;arXiv.org Snapshot:files/209661/2305.html:text/html},
}

@misc{zhang_neural_2023,
	title = {Neural Program Repair with Program Dependence Analysis and Effective Filter Mechanism},
	url = {http://arxiv.org/abs/2305.09315},
	abstract = {Automated program repair is a crucial task for improving the efficiency of software developers. Recently, neural-based techniques have demonstrated significant promise in generating correct patches for buggy code snippets. However, most existing approaches arbitrarily treat the buggy context without any analysis to capture the semantic relationship between the buggy statement and its context. Additionally, we observe that existing neural models may output an unaltered patch consistent with the input buggy code snippet, which fails to be the correct human-written one for fixing the given bug. To address the aforementioned limitations, we present in this paper a novel neural program repair framework called {\textbackslash}approach, which adapts the general pre-trained language model for fixing single-line Java bugs. We make the first attempt to use program slicing to extract contextual information directly related to the given buggy statement as repair ingredients from the corresponding program dependence graph and eliminate unaltered patches using an intuitive but effective filter mechanism. We demonstrate the effectiveness of {\textbackslash}approach on five benchmarks when compared with state-of-the-art baselines.},
	number = {{arXiv}:2305.09315},
	publisher = {{arXiv}},
	author = {Zhang, Yuwei and Li, Ge and Jin, Zhi and Xing, Ying},
	urldate = {2023-06-10},
	date = {2023-05-16},
	eprinttype = {arxiv},
	eprint = {2305.09315 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209854/Zhang 等 - 2023 - Neural Program Repair with Program Dependence Anal.pdf:application/pdf;arXiv.org Snapshot:files/209855/2305.html:text/html},
}

@misc{druga_scratch_2023,
	title = {Scratch Copilot Evaluation: Assessing {AI}-Assisted Creative Coding for Families},
	url = {http://arxiv.org/abs/2305.10417},
	shorttitle = {Scratch Copilot Evaluation},
	abstract = {How can {AI} enhance creative coding experiences for families? This study explores the potential of large language models ({LLMs}) in helping families with creative coding using Scratch. Based on our previous user study involving a prototype {AI} assistant, we devised three evaluation scenarios to determine if {LLMs} could help families comprehend game code, debug programs, and generate new ideas for future projects. We utilized 22 Scratch projects for each scenario and generated responses from {LLMs} with and without practice tasks, resulting in 120 creative coding support scenario datasets. In addition, the authors independently evaluated their precision, pedagogical value, and age-appropriate language. Our findings show that {LLMs} achieved an overall success rate of more than 80{\textbackslash}\% on the different tasks and evaluation criteria. This research offers valuable information on using {LLMs} for creative family coding and presents design guidelines for future {AI}-supported coding applications. Our evaluation framework, together with our labeled evaluation data, is publicly available.},
	number = {{arXiv}:2305.10417},
	publisher = {{arXiv}},
	author = {Druga, Stefania and Otero, Nancy},
	urldate = {2023-06-10},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2305.10417 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209663/Druga 和 Otero - 2023 - Scratch Copilot Evaluation Assessing AI-Assisted .pdf:application/pdf;arXiv.org Snapshot:files/209664/2305.html:text/html},
}

@misc{bai_progsg_2023,
	title = {{ProgSG}: Cross-Modality Representation Learning for Programs in Electronic Design Automation},
	url = {http://arxiv.org/abs/2305.10838},
	shorttitle = {{ProgSG}},
	abstract = {Recent years have witnessed the growing popularity of domain-specific accelerators ({DSAs}), such as Google's {TPUs}, for accelerating various applications such as deep learning, search, autonomous driving, etc. To facilitate {DSA} designs, high-level synthesis ({HLS}) is used, which allows a developer to compile a high-level description in the form of software code in C and C++ into a design in low-level hardware description languages (such as {VHDL} or Verilog) and eventually synthesized into a {DSA} on an {ASIC} (application-specific integrated circuit) or {FPGA} (field-programmable gate arrays). However, existing {HLS} tools still require microarchitecture decisions, expressed in terms of pragmas (such as directives for parallelization and pipelining). To enable more people to design {DSAs}, it is desirable to automate such decisions with the help of deep learning for predicting the quality of {HLS} designs. This requires us a deeper understanding of the program, which is a combination of original code and pragmas. Naturally, these programs can be considered as sequence data, for which large language models ({LLM}) can help. In addition, these programs can be compiled and converted into a control data flow graph ({CDFG}), and the compiler also provides fine-grained alignment between the code tokens and the {CDFG} nodes. However, existing works either fail to leverage both modalities or combine the two in shallow or coarse ways. We propose {ProgSG} allowing the source code sequence modality and the graph modalities to interact with each other in a deep and fine-grained way. To alleviate the scarcity of labeled designs, a pre-training method is proposed based on a suite of compiler's data flow analysis tasks. Experimental results on two benchmark datasets show the superiority of {ProgSG} over baseline methods that either only consider one modality or combine the two without utilizing the alignment information.},
	number = {{arXiv}:2305.10838},
	publisher = {{arXiv}},
	author = {Bai, Yunsheng and Sohrabizadeh, Atefeh and Qin, Zongyue and Hu, Ziniu and Sun, Yizhou and Cong, Jason},
	urldate = {2023-06-10},
	date = {2023-06-02},
	eprinttype = {arxiv},
	eprint = {2305.10838 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:files/209171/2305.html:text/html},
}

@misc{li_think_2023,
	title = {Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation},
	url = {http://arxiv.org/abs/2305.10679},
	shorttitle = {Think Outside the Code},
	abstract = {Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models ({LLMs}) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of {LLMs} to solve competition-level programming problems, resulting in a more than 50\% increase in the pass@\$k\$ metrics for {ChatGPT} on the {CodeContests} benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on {LeetCode} contests show that our framework boosts the ability of {ChatGPT} to a level comparable to that of human programmers.},
	number = {{arXiv}:2305.10679},
	publisher = {{arXiv}},
	author = {Li, Xin-Ye and Xue, Jiang-Tian and Xie, Zheng and Li, Ming},
	urldate = {2023-06-10},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2305.10679 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209253/Li 等 - 2023 - Think Outside the Code Brainstorming Boosts Large.pdf:application/pdf;arXiv.org Snapshot:files/209720/2305.html:text/html},
}

@misc{heisler_android_2023,
	title = {An Android Robot Head as Embodied Conversational Agent},
	url = {http://arxiv.org/abs/2305.10945},
	abstract = {This paper describes, how current Machine Learning ({ML}) techniques combined with simple rule-based animation routines make an android robot head an embodied conversational agent with {ChatGPT} as its core component. The android robot head is described, technical details are given of how lip-sync animation is being achieved, and general software design decisions are presented. A public presentation of the system revealed improvement opportunities that are reported and that lead our iterative implementation approach.},
	number = {{arXiv}:2305.10945},
	publisher = {{arXiv}},
	author = {Heisler, Marcel and Becker-Asano, Christian},
	urldate = {2023-06-10},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.10945 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/209665/Heisler 和 Becker-Asano - 2023 - An Android Robot Head as Embodied Conversational A.pdf:application/pdf;arXiv.org Snapshot:files/209674/2305.html:text/html},
}

@misc{patil_gorilla_2023,
	title = {Gorilla: Large Language Model Connected with Massive {APIs}},
	url = {http://arxiv.org/abs/2305.15334},
	shorttitle = {Gorilla},
	abstract = {Large Language Models ({LLMs}) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via {API} calls remains unfulfilled. This is a challenging task even for today's state-of-the-art {LLMs} such as {GPT}-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an {API} call. We release Gorilla, a finetuned {LLaMA}-based model that surpasses the performance of {GPT}-4 on writing {API} calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting {LLMs} directly. To evaluate the model's ability, we introduce {APIBench}, a comprehensive dataset consisting of {HuggingFace}, {TorchHub}, and {TensorHub} {APIs}. The successful integration of the retrieval system with Gorilla demonstrates the potential for {LLMs} to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
	number = {{arXiv}:2305.15334},
	publisher = {{arXiv}},
	author = {Patil, Shishir G. and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E.},
	urldate = {2023-06-10},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.15334 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/209256/Patil 等 - 2023 - Gorilla Large Language Model Connected with Massi.pdf:application/pdf;arXiv.org Snapshot:files/209693/2305.html:text/html},
}

@misc{li_sheetcopilot_2023,
	title = {{SheetCopilot}: Bringing Software Productivity to the Next Level through Large Language Models},
	url = {http://arxiv.org/abs/2305.19308},
	shorttitle = {{SheetCopilot}},
	abstract = {Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models ({LLMs}), directing software with natural language user requests become a reachable goal. In this work, we propose a {SheetCopilot} agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for {LLMs} to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of {LLMs} in software control tasks. Our {SheetCopilot} correctly completes 44.3{\textbackslash}\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page:https://sheetcopilot-demo.github.io/.},
	number = {{arXiv}:2305.19308},
	publisher = {{arXiv}},
	author = {Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and Zhang, Zhaoxiang},
	urldate = {2023-06-10},
	date = {2023-05-30},
	eprinttype = {arxiv},
	eprint = {2305.19308 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209730/Li 等 - 2023 - SheetCopilot Bringing Software Productivity to th.pdf:application/pdf;arXiv.org Snapshot:files/209853/2305.html:text/html},
}

@misc{nasir_llmatic_2023,
	title = {{LLMatic}: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization},
	url = {http://arxiv.org/abs/2306.01102},
	shorttitle = {{LLMatic}},
	abstract = {Large Language Models ({LLMs}) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view {LLMs} as mutation and crossover tools. Meanwhile, Quality-Diversity ({QD}) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of {LLMs} with the diversity and robustness of {QD} solutions, we introduce {LLMatic}, a Neural Architecture Search ({NAS}) algorithm. While {LLMs} struggle to conduct {NAS} directly through prompts, {LLMatic} uses a procedural approach, leveraging {QD} for prompts and network architecture to create diverse and highly performant networks. We test {LLMatic} on the {CIFAR}-10 image classification benchmark, demonstrating that it can produce competitive networks with just \$2,000\$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark.},
	number = {{arXiv}:2306.01102},
	publisher = {{arXiv}},
	author = {Nasir, Muhammad U. and Earle, Sam and Togelius, Julian and James, Steven and Cleghorn, Christopher},
	urldate = {2023-06-10},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2306.01102 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/209208/Nasir 等 - 2023 - LLMatic Neural Architecture Search via Large Lang.pdf:application/pdf;arXiv.org Snapshot:files/209755/2306.html:text/html},
}

@misc{kou_is_2023,
	title = {Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation},
	url = {http://arxiv.org/abs/2306.01220},
	shorttitle = {Is Model Attention Aligned with Human Attention?},
	abstract = {Large Language Models ({LLMs}) have been demonstrated effective for code generation. Due to the complexity and opacity of {LLMs}, little is known about how these models generate code. To deepen our understanding, we investigate whether {LLMs} attend to the same parts of a natural language description as human programmers during code generation. An analysis of five {LLMs} on a popular benchmark, {HumanEval}, revealed a consistent misalignment between {LLMs}' and programmers' attention. Furthermore, we found that there is no correlation between the code generation accuracy of {LLMs} and their alignment with human programmers. Through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. Our findings highlight the need for human-aligned {LLMs} for better interpretability and programmer trust.},
	number = {{arXiv}:2306.01220},
	publisher = {{arXiv}},
	author = {Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi},
	urldate = {2023-06-10},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2306.01220 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:files/209259/Kou 等 - 2023 - Is Model Attention Aligned with Human Attention A.pdf:application/pdf;arXiv.org Snapshot:files/209288/2306.html:text/html},
}

@misc{le_evaluation_2023,
	title = {An Evaluation of Log Parsing with {ChatGPT}},
	url = {http://arxiv.org/abs/2306.01590},
	abstract = {Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, {ChatGPT}, the current cutting-edge large language model ({LLM}), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate {ChatGPT}'s ability to undertake log parsing by addressing two research questions. (1) Can {ChatGPT} effectively parse logs? (2) How does {ChatGPT} perform with different prompting methods? Our results show that {ChatGPT} can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for {ChatGPT}-based log parsing.},
	number = {{arXiv}:2306.01590},
	publisher = {{arXiv}},
	author = {Le, Van-Hoang and Zhang, Hongyu},
	urldate = {2023-06-10},
	date = {2023-06-02},
	eprinttype = {arxiv},
	eprint = {2306.01590 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209666/Le 和 Zhang - 2023 - An Evaluation of Log Parsing with ChatGPT.pdf:application/pdf;arXiv.org Snapshot:files/209758/2306.html:text/html},
}

@misc{feng_prompting_2023,
	title = {Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models},
	url = {http://arxiv.org/abs/2306.01987},
	shorttitle = {Prompting Is All Your Need},
	abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models ({LLMs}) in natural language understanding, we propose {AdbGPT}, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. {AdbGPT} leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from {LLMs} to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our {AdbGPT} to reproduce 81.3\% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of {AdbGPT} in enhancing developers' bug replay capabilities.},
	number = {{arXiv}:2306.01987},
	publisher = {{arXiv}},
	author = {Feng, Sidong and Chen, Chunyang},
	urldate = {2023-06-10},
	date = {2023-06-02},
	eprinttype = {arxiv},
	eprint = {2306.01987 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209669/Feng 和 Chen - 2023 - Prompting Is All Your Need Automated Android Bug .pdf:application/pdf;arXiv.org Snapshot:files/209840/2306.html:text/html},
}

@article{le-cong_invalidator_2023,
	title = {Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {http://arxiv.org/abs/2301.01113},
	doi = {10.1109/TSE.2023.3255177},
	shorttitle = {Invalidator},
	abstract = {Automated program repair ({APR}) faces the challenge of test overfitting, where generated patches pass validation tests but fail to generalize. Existing methods for patch assessment involve generating new tests or manual inspection, which can be time-consuming or biased. In this paper, we propose a novel technique, {INVALIDATOR}, to automatically assess the correctness of {APR}-generated patches via semantic and syntactic reasoning. {INVALIDATOR} leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model. Given a buggy program and the developer-patched program, {INVALIDATOR} infers likely invariants on both programs. Then, {INVALIDATOR} determines that an {APR}-generated patch overfits if: (1) it violates correct specifications or (2) maintains erroneous behaviors from the original buggy program. In case our approach fails to determine an overfitting patch based on invariants, {INVALIDATOR} utilizes a trained model from labeled patches to assess patch correctness based on program syntax. The benefit of {INVALIDATOR} is threefold. First, {INVALIDATOR} leverages both semantic and syntactic reasoning to enhance its discriminative capability. Second, {INVALIDATOR} does not require new test cases to be generated, but instead only relies on the current test suite and uses invariant inference to generalize program behaviors. Third, {INVALIDATOR} is fully automated. Experimental results demonstrate that {INVALIDATOR} outperforms existing methods in terms of Accuracy and F-measure, correctly identifying 79\% of overfitting patches and detecting 23\% more overfitting patches than the best baseline.},
	pages = {1--20},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Le-Cong, Thanh and Luong, Duc-Minh and Le, Xuan Bach D. and Lo, David and Tran, Nhat-Hoa and Quang-Huy, Bui and Huynh, Quyet-Thang},
	urldate = {2023-06-10},
	date = {2023},
	eprinttype = {arxiv},
	eprint = {2301.01113 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209668/Le-Cong 等 - 2023 - Invalidator Automated Patch Correctness Assessmen.pdf:application/pdf;arXiv.org Snapshot:files/209761/2301.html:text/html},
}

@inproceedings{madi_how_2022,
	title = {How Readable is Model-generated Code? Examining Readability and Visual Inspection of {GitHub} Copilot},
	url = {http://arxiv.org/abs/2208.14613},
	doi = {10.1145/3551349.3560438},
	shorttitle = {How Readable is Model-generated Code?},
	abstract = {Background: Recent advancements in large language models have motivated the practical use of such models in code generation and program synthesis. However, little is known about the effects of such tools on code readability and visual attention in practice. Objective: In this paper, we focus on {GitHub} Copilot to address the issues of readability and visual inspection of model generated code. Readability and low complexity are vital aspects of good source code, and visual inspection of generated code is important in light of automation bias. Method: Through a human experiment (n=21) we compare model generated code to code written completely by human programmers. We use a combination of static code analysis and human annotators to assess code readability, and we use eye tracking to assess the visual inspection of code. Results: Our results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers. At the same time, eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generated code. Conclusion: Our findings highlight that reading code is more important than ever, and programmers should beware of complacency and automation bias with model generated code.},
	pages = {1--5},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	author = {Madi, Naser Al},
	urldate = {2023-06-10},
	date = {2022-10-10},
	eprinttype = {arxiv},
	eprint = {2208.14613 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209667/Madi - 2022 - How Readable is Model-generated Code Examining Re.pdf:application/pdf;arXiv.org Snapshot:files/209670/2208.html:text/html},
}

@article{salza_effectiveness_2023-2,
	title = {On the Effectiveness of Transfer Learning for Code Search},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {http://arxiv.org/abs/2108.05890},
	doi = {10.1109/TSE.2022.3192755},
	abstract = {The Transformer architecture and transfer learning have marked a quantum leap in natural language processing, improving the state of the art across a range of text-based tasks. This paper examines how these advancements can be applied to and improve code search. To this end, we pre-train a {BERT}-based model on combinations of natural language and source code data and fine-tune it on pairs of {StackOverflow} question titles and code answers. Our results show that the pre-trained models consistently outperform the models that were not pre-trained. In cases where the model was pre-trained on natural language "and" source code data, it also outperforms an information retrieval baseline based on Lucene. Also, we demonstrated that the combined use of an information retrieval-based approach followed by a Transformer leads to the best results overall, especially when searching into a large search pool. Transfer learning is particularly effective when much pre-training data is available and fine-tuning data is limited. We demonstrate that natural language processing models based on the Transformer architecture can be directly applied to source code analysis tasks, such as code search. With the development of Transformer models designed more specifically for dealing with source code data, we believe the results of source code analysis tasks can be further improved.},
	pages = {1804--1822},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C.},
	urldate = {2023-06-10},
	date = {2023-04-01},
	eprinttype = {arxiv},
	eprint = {2108.05890 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:files/209279/Salza 等 - 2023 - On the Effectiveness of Transfer Learning for Code.pdf:application/pdf;arXiv.org Snapshot:files/209291/2108.html:text/html},
}

@article{drori_neural_2022,
	title = {A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/2112.15594},
	doi = {10.1073/pnas.2123433119},
	abstract = {We demonstrate that a neural network pre-trained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and {OpenAI}'s Codex transformer and execute them to solve course problems at 81\% automatic accuracy. We curate a new dataset of questions from {MIT}'s largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a {MATH} dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest {GPT}-3 language model pre-trained on text automatically solves only 18.8\% of these university questions using zero-shot learning and 30.8\% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81\% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8\% to 81.1\%. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.},
	pages = {e2123433119},
	number = {32},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and Wang, Roman and Singh, Nikhil and Patti, Taylor L. and Lynch, Jayson and Shporer, Avi and Verma, Nakul and Wu, Eugene and Strang, Gilbert},
	urldate = {2023-06-10},
	date = {2022-08-09},
	eprinttype = {arxiv},
	eprint = {2112.15594 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/209307/Drori 等 - 2022 - A Neural Network Solves, Explains, and Generates U.pdf:application/pdf;arXiv.org Snapshot:files/209775/2112.html:text/html},
}

@inproceedings{akimova_pytracebugs_2021,
	location = {Taipei, Taiwan},
	title = {{PyTraceBugs}: A Large Python Code Dataset for Supervised Machine Learning in Software Defect Prediction},
	isbn = {978-1-66543-784-4},
	url = {https://ieeexplore.ieee.org/document/9712116/},
	doi = {10.1109/APSEC53868.2021.00022},
	shorttitle = {{PyTraceBugs}},
	eventtitle = {2021 28th Asia-Pacific Software Engineering Conference ({APSEC})},
	pages = {141--151},
	booktitle = {2021 28th Asia-Pacific Software Engineering Conference ({APSEC})},
	publisher = {{IEEE}},
	author = {Akimova, Elena N. and Bersenev, Alexander Yu. and Deikov, Artem A. and Kobylkin, Konstantin S. and Konygin, Anton V. and Mezentsev, Ilya P. and Misilov, Vladimir E.},
	urldate = {2023-06-10},
	date = {2021-12},
}

@inproceedings{chen_transferability_2022-2,
	location = {Virtual Event},
	title = {On the transferability of pre-trained language models for low-resource programming languages},
	isbn = {978-1-4503-9298-3},
	url = {https://dl.acm.org/doi/10.1145/3524610.3527917},
	doi = {10.1145/3524610.3527917},
	eventtitle = {{ICPC} '22: 30th International Conference on Program Comprehension},
	pages = {401--412},
	booktitle = {Proceedings of the 30th {IEEE}/{ACM} International Conference on Program Comprehension},
	publisher = {{ACM}},
	author = {Chen, Fuxiang and Fard, Fatemeh H. and Lo, David and Bryksin, Timofey},
	urldate = {2023-06-10},
	date = {2022-05-16},
	langid = {english},
	file = {已提交版本:files/209576/Chen 等 - 2022 - On the transferability of pre-trained language mod.pdf:application/pdf},
}

@inproceedings{biswas_geometric_2022,
	location = {Padua, Italy},
	title = {Geometric Analysis and Metric Learning of Instruction Embeddings},
	isbn = {978-1-72818-671-9},
	url = {https://ieeexplore.ieee.org/document/9892426/},
	doi = {10.1109/IJCNN55064.2022.9892426},
	eventtitle = {2022 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2022 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Biswas, Sajib and Barao, Timothy and Lazzari, John and {McCoy}, Jeret and Liu, Xiuwen and Kostandarithes, Alexander},
	urldate = {2023-06-10},
	date = {2022-07-18},
}

@article{el-allaly_mttlade_2021,
	title = {{MTTLADE}: A multi-task transfer learning-based method for adverse drug events extraction},
	volume = {58},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457320309626},
	doi = {10.1016/j.ipm.2020.102473},
	shorttitle = {{MTTLADE}},
	pages = {102473},
	number = {3},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {El-allaly, Ed-drissiya and Sarrouti, Mourad and En-Nahnahi, Noureddine and Ouatik El Alaoui, Said},
	urldate = {2023-06-10},
	date = {2021-05},
	langid = {english},
}

@article{ghadhab_augmenting_2021,
	title = {Augmenting commit classification by using fine-grained source code changes and a pre-trained deep neural language model},
	volume = {135},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584921000495},
	doi = {10.1016/j.infsof.2021.106566},
	pages = {106566},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Ghadhab, Lobna and Jenhani, Ilyes and Mkaouer, Mohamed Wiem and Ben Messaoud, Montassar},
	urldate = {2023-06-10},
	date = {2021-07},
	langid = {english},
}

@incollection{noel_software_2023,
	location = {Cham},
	title = {Software Parts Classification for Agile and Efficient Product Life Cycle Management},
	volume = {667},
	isbn = {978-3-031-25181-8 978-3-031-25182-5},
	url = {https://link.springer.com/10.1007/978-3-031-25182-5_2},
	pages = {15--24},
	booktitle = {Product Lifecycle Management. {PLM} in Transition Times: The Place of Humans and Transformative Technologies},
	publisher = {Springer Nature Switzerland},
	author = {Gaurav, Anmol and Ila, Bhanu Prakash and Kondamudi, Naveen Mehata and Deshwal, Pinky},
	editor = {Noël, Frédéric and Nyffenegger, Felix and Rivest, Louis and Bouras, Abdelaziz},
	urldate = {2023-06-10},
	date = {2023},
	langid = {english},
	doi = {10.1007/978-3-031-25182-5_2},
	note = {Series Title: {IFIP} Advances in Information and Communication Technology},
}

@article{isotani_sentence_2023,
	title = {Sentence embedding and fine-tuning to automatically identify duplicate bugs},
	volume = {4},
	issn = {2624-9898},
	url = {https://www.frontiersin.org/articles/10.3389/fcomp.2022.1032452/full},
	doi = {10.3389/fcomp.2022.1032452},
	abstract = {Industrial software maintenance is critical but burdensome. Activities such as detecting duplicate bug reports are often performed manually. Herein an automated duplicate bug report detection system improves maintenance efficiency using vectorization of the contents and deep learning–based sentence embedding to calculate the similarity of the whole report from vectors of individual elements. Specifically, sentence embedding is realized using Sentence-{BERT} fine tuning. Additionally, its performance is experimentally compared to baseline methods to validate the proposed system. The proposed system detects duplicate bug reports more effectively than existing methods.},
	pages = {1032452},
	journaltitle = {Frontiers in Computer Science},
	shortjournal = {Front. Comput. Sci.},
	author = {Isotani, Haruna and Washizaki, Hironori and Fukazawa, Yoshiaki and Nomoto, Tsutomu and Ouji, Saori and Saito, Shinobu},
	urldate = {2023-06-10},
	date = {2023-01-19},
	file = {全文:files/209586/Isotani 等 - 2023 - Sentence embedding and fine-tuning to automaticall.pdf:application/pdf},
}

@inproceedings{isotani_duplicate_2021,
	location = {Luxembourg},
	title = {Duplicate Bug Report Detection by Using Sentence Embedding and Fine-tuning},
	isbn = {978-1-66542-882-8},
	url = {https://ieeexplore.ieee.org/document/9609125/},
	doi = {10.1109/ICSME52107.2021.00054},
	eventtitle = {2021 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {535--544},
	booktitle = {2021 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Isotani, Haruna and Washizaki, Hironori and Fukazawa, Yoshiaki and Nomoto, Tsutomu and Ouji, Saori and Saito, Shinobu},
	urldate = {2023-06-10},
	date = {2021-09},
}

@article{koo_binary_2023,
	title = {Binary Code Representation With Well-Balanced Instruction Normalization},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10077368/},
	doi = {10.1109/ACCESS.2023.3259481},
	pages = {29183--29198},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Koo, Hyungjoon and Park, Soyeon and Choi, Daejin and Kim, Taesoo},
	urldate = {2023-06-10},
	date = {2023},
}

@article{kim_data-analytics-based_2022,
	title = {Data-analytics-based factory operation strategies for die-casting quality enhancement},
	volume = {119},
	issn = {0268-3768, 1433-3015},
	url = {https://link.springer.com/10.1007/s00170-021-08625-8},
	doi = {10.1007/s00170-021-08625-8},
	pages = {3865--3890},
	number = {5},
	journaltitle = {The International Journal of Advanced Manufacturing Technology},
	shortjournal = {Int J Adv Manuf Technol},
	author = {Kim, Jun and Lee, Ju Yeon},
	urldate = {2023-06-10},
	date = {2022-03},
	langid = {english},
	file = {已提交版本:files/209587/Kim 和 Lee - 2022 - Data-analytics-based factory operation strategies .pdf:application/pdf},
}

@article{khaliq_deep_2022,
	title = {A deep learning-based automated framework for functional User Interface testing},
	volume = {150},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584922001070},
	doi = {10.1016/j.infsof.2022.106969},
	pages = {106969},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Khaliq, Zubair and Farooq, Sheikh Umar and Khan, Dawood Ashraf},
	urldate = {2023-06-10},
	date = {2022-10},
	langid = {english},
}

@article{li_deep_2022-1,
	title = {Deep semantic mining of big multimedia data advertisements based on needs ontology construction},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-021-11892-y},
	doi = {10.1007/s11042-021-11892-y},
	pages = {28079--28102},
	number = {20},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Li, Zhiyi and Shen, Zhirui},
	urldate = {2023-06-10},
	date = {2022-08},
	langid = {english},
}

@article{song_multi-label_2022,
	title = {Multi-label legal document classification: A deep learning-based approach with label-attention and domain-specific pre-training},
	volume = {106},
	issn = {03064379},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306437921000016},
	doi = {10.1016/j.is.2021.101718},
	shorttitle = {Multi-label legal document classification},
	pages = {101718},
	journaltitle = {Information Systems},
	shortjournal = {Information Systems},
	author = {Song, Dezhao and Vold, Andrew and Madan, Kanika and Schilder, Frank},
	urldate = {2023-06-10},
	date = {2022-05},
	langid = {english},
}

@inproceedings{wang_use_2022,
	location = {Guangzhou, China},
	title = {The Use of Pretrained Model for Matching App Reviews and Bug Reports},
	isbn = {978-1-66547-704-8},
	url = {https://ieeexplore.ieee.org/document/10062438/},
	doi = {10.1109/QRS57517.2022.00034},
	eventtitle = {2022 {IEEE} 22nd International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {242--251},
	booktitle = {2022 {IEEE} 22nd International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Wang, Xiaojuan and Zhang, Wenyu and Lai, Shanyan and Ye, Chunyang and Zhou, Hui},
	urldate = {2023-06-10},
	date = {2022-12},
}

@article{oliveira_square-pulse_2020,
	title = {Square-pulse shearography inspections of metallic parts repaired with a glass fiber reinforced polymer using pressure, radiation, vibration, and induction loading methods},
	volume = {187},
	issn = {03080161},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0308016120301642},
	doi = {10.1016/j.ijpvp.2020.104187},
	pages = {104187},
	journaltitle = {International Journal of Pressure Vessels and Piping},
	shortjournal = {International Journal of Pressure Vessels and Piping},
	author = {Oliveira, B.C.F. and Seibert, A.A. and Staub, D. and Albertazzi, A.},
	urldate = {2023-06-10},
	date = {2020-11},
	langid = {english},
}

@article{tian_cross-level_2023,
	title = {A Cross-Level Requirement Trace Link Update Model Based on Bidirectional Encoder Representations from Transformers},
	volume = {11},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/3/623},
	doi = {10.3390/math11030623},
	abstract = {Cross-level requirement trace links (i.e., links between high-level requirements ({HLRs}) and low-level requirements ({LLRs})) record the top-down decomposition process of requirements and support various development and management activities (e.g., requirement validation). Undoubtedly, updating trace links synchronously with requirement changes is critical for their constant availability. However, large-scale open-source software that is rapidly iterative and continually released has numerous requirements that are dynamic. These requirements render timely update of trace links challenging. To address these problems, in this study, a novel deep-learning-based method, deep requirement trace analyzer fusing heterogeneous features ({DRAFT}), was proposed for updating trace links between various levels of requirements. Considering both the semantic information of requirement text descriptions and the process features based on metadata, trace link data accumulated in the early stage are comprehensively used to train the trace link identification model. Particularly, first, we performed second-phase pre-training for the bidirectional encoder representations from transformers ({BERT}) language model based on the project document corpus to realize project-related knowledge transfer, which yields superior text embedding. Second, we designed 11 heuristic features based on the requirement metadata in the open-source system. Based on these features and semantic similarity between {HLRs} and {LLRs}, we designed a cross-level requirement tracing model for new requirements. The superiority of {DRAFT} was verified based on the requirement datasets of eight open-source projects. The average F1 and F2 scores of {DRAFT} were 69.3\% and 76.9\%, respectively, which were 16.5\% and 22.3\% higher than baselines. An ablation experiment proved the positive role of two key steps in trace link construction.},
	pages = {623},
	number = {3},
	journaltitle = {Mathematics},
	shortjournal = {Mathematics},
	author = {Tian, Jiahao and Zhang, Li and Lian, Xiaoli},
	urldate = {2023-06-10},
	date = {2023-01-26},
	langid = {english},
	file = {全文:files/209616/Tian 等 - 2023 - A Cross-Level Requirement Trace Link Update Model .pdf:application/pdf},
}

@article{miftahutdinov_medical_2021,
	title = {Medical concept normalization in clinical trials with drug and disease representation learning},
	volume = {37},
	issn = {1367-4803, 1367-4811},
	url = {https://academic.oup.com/bioinformatics/article/37/21/3856/6313159},
	doi = {10.1093/bioinformatics/btab474},
	abstract = {Abstract
            
              Motivation
              Clinical trials are the essential stage of every drug development program for the treatment to become available to patients. Despite the importance of well-structured clinical trial databases and their tremendous value for drug discovery and development such instances are very rare. Presently large-scale information on clinical trials is stored in clinical trial registers which are relatively structured, but the mappings to external databases of drugs and diseases are increasingly lacking. The precise production of such links would enable us to interrogate richer harmonized datasets for invaluable insights.
            
            
              Results
              We present a neural approach for medical concept normalization of diseases and drugs. Our two-stage approach is based on Bidirectional Encoder Representations from Transformers ({BERT}). In the training stage, we optimize the relative similarity of mentions and concept names from a terminology via triplet loss. In the inference stage, we obtain the closest concept name representation in a common embedding space to a given mention representation. We performed a set of experiments on a dataset of abstracts and a real-world dataset of trial records with interventions and conditions mapped to drug and disease terminologies. The latter includes mentions associated with one or more concepts (in-{KB}) or zero (out-of-{KB}, nil prediction). Experiments show that our approach significantly outperforms baseline and state-of-the-art architectures. Moreover, we demonstrate that our approach is effective in knowledge transfer from the scientific literature to clinical trial data.
            
            
              Availability and implementation
              We make code and data freely available at https://github.com/insilicomedicine/{DILBERT}.},
	pages = {3856--3864},
	number = {21},
	journaltitle = {Bioinformatics},
	author = {Miftahutdinov, Zulfat and Kadurin, Artur and Kudrin, Roman and Tutubalina, Elena},
	editor = {Wren, Jonathan},
	urldate = {2023-06-10},
	date = {2021-11-05},
	langid = {english},
	file = {全文:files/209637/Miftahutdinov 等 - 2021 - Medical concept normalization in clinical trials w.pdf:application/pdf},
}

@article{wang_miprobert_2023,
	title = {{miProBERT}: identification of {microRNA} promoters based on the pre-trained model {BERT}},
	volume = {24},
	issn = {1467-5463, 1477-4054},
	url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbad093/7079709},
	doi = {10.1093/bib/bbad093},
	shorttitle = {{miProBERT}},
	abstract = {Abstract
            Accurate prediction of promoter regions driving {miRNA} gene expression has become a major challenge due to the lack of annotation information for pri-{miRNA} transcripts. This defect hinders our understanding of {miRNA}-mediated regulatory networks. Some algorithms have been designed during the past decade to detect {miRNA} promoters. However, these methods rely on biosignal data such as {CpG} islands and still need to be improved. Here, we propose {miProBERT}, a {BERT}-based model for predicting promoters directly from gene sequences without using any structural or biological signals. According to our information, it is the first time a {BERT}-based model has been employed to identify {miRNA} promoters. We use the pre-trained model {DNABERT}, fine-tune the pre-trained model on the gene promoter dataset so that the model includes information about the richer biological properties of promoter sequences in its representation, and then systematically scan the upstream regions of each intergenic {miRNA} using the fine-tuned model. About, 665 {miRNA} promoters are found. The innovative use of a random substitution strategy to construct a negative dataset improves the discriminative ability of the model and further reduces the false positive rate ({FPR}) to as low as 0.0421. On independent datasets, {miProBERT} outperformed other gene promoter prediction methods. With comparison on 33 experimentally validated {miRNA} promoter datasets, {miProBERT} significantly outperformed previously developed {miRNA} promoter prediction programs with 78.13\% precision and 75.76\% recall. We further verify the predicted promoter regions by analyzing conservation, {CpG} content and histone marks. The effectiveness and robustness of {miProBERT} are highlighted.},
	pages = {bbad093},
	number = {3},
	journaltitle = {Briefings in Bioinformatics},
	author = {Wang, Xin and Gao, Xin and Wang, Guohua and Li, Dan},
	urldate = {2023-06-10},
	date = {2023-05-19},
	langid = {english},
}

@inproceedings{tol_fastspec_2021-1,
	location = {Vienna, Austria},
	title = {{FastSpec}: Scalable Generation and Detection of Spectre Gadgets Using Neural Embeddings},
	isbn = {978-1-66541-491-3},
	url = {https://ieeexplore.ieee.org/document/9581258/},
	doi = {10.1109/EuroSP51992.2021.00047},
	shorttitle = {{FastSpec}},
	eventtitle = {2021 {IEEE} European Symposium on Security and Privacy ({EuroS}\&P)},
	pages = {616--632},
	booktitle = {2021 {IEEE} European Symposium on Security and Privacy ({EuroS}\&P)},
	publisher = {{IEEE}},
	author = {Tol, M. Caner and Gulmezoglu, Berk and Yurtseven, Koray and Sunar, Berk},
	urldate = {2023-06-10},
	date = {2021-09},
	file = {已提交版本:files/209631/Tol 等 - 2021 - FastSpec Scalable Generation and Detection of Spe.pdf:application/pdf},
}

@article{tang_attensy-sner_2023,
	title = {{AttenSy}-{SNER}: software knowledge entity extraction with syntactic features and semantic augmentation information},
	volume = {9},
	issn = {2199-4536, 2198-6053},
	url = {https://link.springer.com/10.1007/s40747-022-00742-5},
	doi = {10.1007/s40747-022-00742-5},
	shorttitle = {{AttenSy}-{SNER}},
	abstract = {Abstract
            Software knowledge community contains a large scale of software knowledge entity information, complex structure and rich semantic correlations. It is significant to recognize and extract software knowledge entity from software knowledge community, as it has great impact on entity-centric tasks such as software knowledge graph construction, software document generation and expert recommendation. Since the texts of the software knowledge community are unstructured by user-generated texts, it is difficult to apply the traditional entity extraction method in the domain of the software knowledge community due to the problems of entity variation, entity sparsity, entity ambiguity, out-of-vocabulary ({OOV}) words and the lack of annotated data sets. This paper proposes a novel software knowledge entity extraction model, named {AttenSy}-{SNER}, which integrates syntactic features and semantic augmentation information, to extract fine-grained software knowledge entities from unstructured user-generated content. The input representation layer utilizes Bidirectional Encoder Representations from Transformers ({BERT}) model to extract the feature representation of the input sequence. The contextual coding layer leverages the Bidirectional Long Short-Term Memory ({BiLSTM}) network and Graph Convolutional Network ({GCN}) for contextual information and syntactic dependency information, and a semantic augmentation strategy based on attention mechanism is introduced to enrich the semantic feature representation of sequences as well. The tag decoding layer leverages Conditional Random Fields ({CRF}) to solve the dependency between the output tags and obtain the global optimal label sequence. The results of model comparison experiments show that the proposed model has better performance than the benchmark model in software engineering domain.},
	pages = {25--39},
	number = {1},
	journaltitle = {Complex \& Intelligent Systems},
	shortjournal = {Complex Intell. Syst.},
	author = {Tang, Mingjing and Li, Tong and Gao, Wei and Xia, Yu},
	urldate = {2023-06-10},
	date = {2023-02},
	langid = {english},
	file = {全文:files/209628/Tang 等 - 2023 - AttenSy-SNER software knowledge entity extraction.pdf:application/pdf},
}

@article{mohsen_enhancing_2023,
	title = {Enhancing Bug Localization Using Phase-Based Approach},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10097736/},
	doi = {10.1109/ACCESS.2023.3265731},
	pages = {35901--35913},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Mohsen, Amr Mansour and Hassan, Hesham A. and Wassif, Khaled T. and Moawad, Ramadan and Makady, Soha H.},
	urldate = {2023-06-10},
	date = {2023},
}

@inproceedings{mamede_exploring_2022,
	location = {Guangzhou, China},
	title = {Exploring Transformers for Multi-Label Classification of Java Vulnerabilities},
	isbn = {978-1-66547-704-8},
	url = {https://ieeexplore.ieee.org/document/10062434/},
	doi = {10.1109/QRS57517.2022.00015},
	eventtitle = {2022 {IEEE} 22nd International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {43--52},
	booktitle = {2022 {IEEE} 22nd International Conference on Software Quality, Reliability and Security ({QRS})},
	publisher = {{IEEE}},
	author = {Mamede, Cláudia and Pinconschi, Eduard and Abreu, Rui and Campos, José},
	urldate = {2023-06-10},
	date = {2022-12},
}

@inproceedings{li_toward_2021-1,
	location = {Luxembourg},
	title = {Toward Less Hidden Cost of Code Completion with Acceptance and Ranking Models},
	isbn = {978-1-66542-882-8},
	url = {https://ieeexplore.ieee.org/document/9609202/},
	doi = {10.1109/ICSME52107.2021.00024},
	eventtitle = {2021 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {195--205},
	booktitle = {2021 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Li, Jingxuan and Huang, Rui and Li, Wei and Yao, Kai and Tan, Weiguo},
	urldate = {2023-06-10},
	date = {2021-09},
	file = {已提交版本:files/209635/Li 等 - 2021 - Toward Less Hidden Cost of Code Completion with Ac.pdf:application/pdf},
}

@article{son_study_2021,
	title = {A Study on Classification of Mobile Application Reviews Using Deep Learning},
	volume = {10},
	issn = {22871322},
	url = {https://scholar.kyobobook.co.kr/article/detail/4010028403497},
	doi = {10.30693/SMJ.2021.10.2.76},
	pages = {76--83},
	number = {2},
	journaltitle = {Korean Institute of Smart Media},
	shortjournal = {Korean Institute Smart Media},
	author = {Son, Jaeik and Noh, Mijin and Rahman, Tazizur and Pyo, Gyujin and Han, Mumoungcho and Kim, Yangsok},
	urldate = {2023-06-10},
	date = {2021-06-30},
}

@inproceedings{siddiq_empirical_2022,
	location = {Limassol, Cyprus},
	title = {An Empirical Study of Code Smells in Transformer-based Code Generation Techniques},
	isbn = {978-1-66549-609-4},
	url = {https://ieeexplore.ieee.org/document/10006873/},
	doi = {10.1109/SCAM55253.2022.00014},
	eventtitle = {2022 {IEEE} 22nd International Working Conference on Source Code Analysis and Manipulation ({SCAM})},
	pages = {71--82},
	booktitle = {2022 {IEEE} 22nd International Working Conference on Source Code Analysis and Manipulation ({SCAM})},
	publisher = {{IEEE}},
	author = {Siddiq, Mohammed Latif and Majumder, Shafayat H. and Mim, Maisha R. and Jajodia, Sourov and Santos, Joanna C. S.},
	urldate = {2023-06-10},
	date = {2022-10},
}

@inproceedings{shi_cross-modal_2022,
	location = {Limassol, Cyprus},
	title = {Cross-Modal Contrastive Learning for Code Search},
	isbn = {978-1-66547-956-1},
	url = {https://ieeexplore.ieee.org/document/9978195/},
	doi = {10.1109/ICSME55016.2022.00017},
	eventtitle = {2022 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {94--105},
	booktitle = {2022 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Shi, Zejian and Xiong, Yun and Zhang, Xiaolong and Zhang, Yao and Li, Shanshan and Zhu, Yangyong},
	urldate = {2023-06-10},
	date = {2022-10},
}

@article{paik_achieving_2022,
	title = {Achieving the Performance of All-Bank In-{DRAM} {PIM} With Standard Memory Interface: Memory-Computation Decoupling},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9870805/},
	doi = {10.1109/ACCESS.2022.3203051},
	shorttitle = {Achieving the Performance of All-Bank In-{DRAM} {PIM} With Standard Memory Interface},
	pages = {93256--93272},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Paik, Yoonah and Kim, Chang Hyun and Lee, Won Jun and Kim, Seon Wook},
	urldate = {2023-06-10},
	date = {2022},
}

@article{lu_bi-gru_2020,
	title = {Bi-{GRU} Sentiment Classification for Chinese Based on Grammar Rules and {BERT}:},
	volume = {13},
	issn = {1875-6883},
	url = {https://www.atlantis-press.com/article/125940732},
	doi = {10.2991/ijcis.d.200423.001},
	shorttitle = {Bi-{GRU} Sentiment Classification for Chinese Based on Grammar Rules and {BERT}},
	pages = {538},
	number = {1},
	journaltitle = {International Journal of Computational Intelligence Systems},
	shortjournal = {{IJCIS}},
	author = {Lu, Qiang and Zhu, Zhenfang and Xu, Fuyong and Zhang, Dianyuan and Wu, Wenqing and Guo, Qiangqiang},
	urldate = {2023-06-10},
	date = {2020},
	langid = {english},
	file = {全文:files/209630/Lu 等 - 2020 - Bi-GRU Sentiment Classification for Chinese Based .pdf:application/pdf},
}

@article{sharma_self-admitted_2022-1,
	title = {Self-admitted technical debt in R: detection and causes},
	volume = {29},
	issn = {0928-8910, 1573-7535},
	url = {https://link.springer.com/10.1007/s10515-022-00358-6},
	doi = {10.1007/s10515-022-00358-6},
	shorttitle = {Self-admitted technical debt in R},
	abstract = {Abstract
            
              Self-Admitted Technical Debt ({SATD}) is primarily studied in Object-Oriented ({OO}) languages and traditionally commercial software. However, scientific software coded in dynamically-typed languages such as R differs in paradigm, and the source code comments’ semantics are different (i.e., more aligned with algorithms and statistics when compared to traditional software). Additionally, many Software Engineering topics are understudied in scientific software development, with {SATD} detection remaining a challenge for this domain. This gap adds complexity since prior works determined {SATD} in scientific software does not adjust to many of the keywords identified for {OO} {SATD}, possibly hindering its automated detection. Therefore, we investigated how classification models (traditional machine learning, deep neural networks, and deep neural Pre-Trained Language Models ({PTMs})) automatically detect {SATD} in R packages. This study aims to study the capabilities of these models to classify different {TD} types in this domain and manually analyze the causes of each in a representative sample. Our results show that {PTMs} (i.e., {RoBERTa}) outperform other models and work well when the number of comments labelled as a particular {SATD} type has low occurrences. We also found that some {SATD} types are more challenging to detect. We manually identified sixteen causes, including eight new causes detected by our study. The most common cause was
              failure to remember
              , in agreement with previous studies. These findings will help the R package authors automatically identify {SATD} in their source code and improve their code quality. In the future, checklists for R developers can also be developed by scientific communities such as {rOpenSci} to guarantee a higher quality of packages before submission.},
	pages = {53},
	number = {2},
	journaltitle = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Sharma, Rishab and Shahbazi, Ramin and Fard, Fatemeh H. and Codabux, Zadia and Vidoni, Melina},
	urldate = {2023-06-10},
	date = {2022-11},
	langid = {english},
	file = {全文:files/209629/Sharma 等 - 2022 - Self-admitted technical debt in R detection and c.pdf:application/pdf},
}

@article{li_automatic_2023,
	title = {Automatic classification of interactive texts in online collaborative discussion based on multi-feature fusion},
	volume = {107},
	issn = {00457906},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790623000733},
	doi = {10.1016/j.compeleceng.2023.108648},
	pages = {108648},
	journaltitle = {Computers and Electrical Engineering},
	shortjournal = {Computers and Electrical Engineering},
	author = {Li, Shuhong and Deng, Mingming and Shao, Zheng and Chen, Xu and Zheng, Yafeng},
	urldate = {2023-06-10},
	date = {2023-04},
	langid = {english},
}

@inproceedings{liu_chis_2021,
	location = {Taipei, Taiwan},
	title = {{CHIS}: A Novel Hybrid Granularity Identifier Splitting Approach},
	isbn = {978-1-66543-784-4},
	url = {https://ieeexplore.ieee.org/document/9712111/},
	doi = {10.1109/APSEC53868.2021.00027},
	shorttitle = {{CHIS}},
	eventtitle = {2021 28th Asia-Pacific Software Engineering Conference ({APSEC})},
	pages = {192--201},
	booktitle = {2021 28th Asia-Pacific Software Engineering Conference ({APSEC})},
	publisher = {{IEEE}},
	author = {Liu, Siyuan and Zhang, Jingxuan and Liang, Jiahui and Luo, Junpeng and Xu, Yong and Sun, Chenxing},
	urldate = {2023-06-10},
	date = {2021-12},
}

@inproceedings{tian_evaluating_2020-2,
	location = {Virtual Event Australia},
	title = {Evaluating representation learning of code changes for predicting patch correctness in program repair},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416532},
	doi = {10.1145/3324884.3416532},
	eventtitle = {{ASE} '20: 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {981--992},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Tian, Haoye and Liu, Kui and Kaboré, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyandé, Tegawendé F.},
	urldate = {2023-06-10},
	date = {2020-12-21},
	langid = {english},
	file = {已提交版本:files/209643/Tian 等 - 2020 - Evaluating representation learning of code changes.pdf:application/pdf},
}

@article{yu_graph-based_2023,
	title = {Graph-based code semantics learning for efficient semantic code clone detection},
	volume = {156},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584922002397},
	doi = {10.1016/j.infsof.2022.107130},
	pages = {107130},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Yu, Dongjin and Yang, Quanxin and Chen, Xin and Chen, Jie and Xu, Yihang},
	urldate = {2023-06-10},
	date = {2023-04},
	langid = {english},
}

@article{zaidi_toward_2022,
	title = {Toward an Effective Bug Triage System Using Transformers to Add New Developers},
	volume = {2022},
	issn = {1687-7268, 1687-725X},
	url = {https://www.hindawi.com/journals/js/2022/4347004/},
	doi = {10.1155/2022/4347004},
	abstract = {As defects become more widespread in software development and advancement, bug triaging has become imperative for software testing and maintenance. The bug triage process assigns an appropriate developer to a bug report. Many automated and semiautomated systems have been proposed in the last decade, and some recent techniques have provided direction for developing an effective triage system. However, these techniques still require improvement. Another open challenge related to this problem is adding new developers to the existing triage system, which is challenging because the developers have no listed triage history. This paper proposes a transformer-based bug triage system that uses bidirectional encoder representation from transformers ({BERT}) for word representation. The proposed model can add a new developer to the existing system without building a training model from scratch. To add new developers, we assumed that new developers had a triage history created by a manual triager or human triage manager after learning their skills from the existing developer history. Then, the existing model was fine-tuned to add new developers using the manual triage history. Experiments were conducted using datasets from well-known large-scale open-source projects, such as Eclipse and Mozilla, and top-k accuracy was used as a criterion for assessment. The experimental outcome suggests that the proposed triage system is better than other word-embedding-based triage methods for the bug triage problem. Additionally, the proposed method performs the best for adding new developers to an existing bug triage system without requiring retraining using a whole dataset.},
	pages = {1--19},
	journaltitle = {Journal of Sensors},
	shortjournal = {Journal of Sensors},
	author = {Zaidi, Syed Farhan Alam and Woo, Honguk and Lee, Chan-Gun},
	editor = {Lim, Sangsoon},
	urldate = {2023-06-10},
	date = {2022-04-08},
	langid = {english},
	file = {全文:files/209641/Zaidi 等 - 2022 - Toward an Effective Bug Triage System Using Transf.pdf:application/pdf},
}

@inproceedings{ardimento_using_2020,
	title = {Using {BERT} to Predict Bug-Fixing Time},
	isbn = {2330-4863},
	abstract = {Prediction of the resolution time of a newly-submitted bug is a relevant aspect during the bug triage process since it can help project managers to better estimate software maintenance efforts and better manage software projects. Once a bug is reported, it is typically recorded in a Bug Tracking System, and it is assigned to a developer in order to solve the issue. The contribution of this paper is to provide a deep learning approach for the resolution of the bug-fixing time prediction, proposing a new feature set, consisting of the description of the issue and comments of the developers, in order to perform transfer learning on a pre-trained language representations model, called {BERT}. The problem of predicting the resolution time of a bug is therefore formulated as a supervised text categorization task. {BERT} makes use of a self-attention mechanism that allows learning the bidirectional context representation of a word in a sentence, which constitutes one of the main advantages over the previously proposed solutions. Experimental results show the proposed approach has effective bug-fixing time prediction ability.},
	author = {Ardimento, Pasquale and Mele, Costantino},
	editor = {Castellano, G and Castiello, C and Mencar, C},
	urldate = {2021-03-02},
	date = {2020},
}

@inproceedings{csuvik_utilizing_2020,
	title = {Utilizing Source Code Embeddings to Identify Correct Patches},
	isbn = {978-1-72816-280-5},
	abstract = {The so called Generate-and-Validate approach of Automatic Program Repair consists of two main activities, the generate activity, which produces candidate solutions to the problem, and the validate activity, which checks the correctness of the generated solutions. The latter however might not give a reliable result, since most of the techniques establish the correctness of the solutions by (re-)running the available test cases. A program is marked as a possible fix, if it passes all the available test cases. Although tests can be run automatically, in real life applications the problem of over- and underfitting often occurs, resulting in inadequate patches. At this point manual investigation of repair candidates is needed although they passed the tests. Our goal is to investigate ways to predict correct patches. The core idea is to exploit textual and structural similarity between the original (buggy) program and the generated patches. To do so we apply Doc2vec and Bert embedding methods on source code. So far {APR} tools generate mostly one-line fixes, leaving most of the original source code intact. Our observation was, that patches which bring in new variables, make larger changes in the code are usually the incorrect ones. The proposed approach was evaluated on the {QuixBugs} dataset consisting of 40 bugs and fixes belonging to them. Our approach successfully filtered out 45\% of the incorrect patches.},
	pages = {18--25},
	author = {Csuvik, Viktor and Horvath, Daniel and Horvath, Ferenc and Vidacs, Laszlo},
	editor = {Luo, X and Shang, W and Sun, X and Zhang, T},
	urldate = {2020-01-01},
	date = {2020},
}

@inproceedings{kanade_learning_2020-2,
	title = {Learning and Evaluating Contextual Embedding of Source Code},
	volume = {119},
	isbn = {2640-3498},
	abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as {BERT}, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from {GitHub}, which we use to pre-train {CuBERT}, an open-sourced code-understanding {BERT} model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune {CuBERT} on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, {BiLSTM} and Transformer models, as well as published state-of-the-art models, showing that {CuBERT} outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against {CuBERT} models as a strong baseline.},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	editor = {Daume, H and Singh, A},
	urldate = {2021-09-17},
	date = {2020},
}

@inproceedings{korbak_controlling_2022-1,
	title = {Controlling Conditional Language Models without Catastrophic Forgetting},
	isbn = {2640-3498},
	abstract = {Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities ("catastrophic forgetting"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models ({EBMs}) and approximating these {EBMs} using distributional policy gradients ({DPG}). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend {DPG} to conditional tasks by proposing Conditional {DPG} ({CDPG}). We evaluate {CDPG} on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and {GPT}-Neo). Our results show that fine-tuning using {CDPG} robustly moves these pretrained models closer towards meeting control objectives and - in contrast with baseline approaches - does not result in catastrophic forgetting.},
	author = {Korbak, Tomasz and Elsahar, Hady and Kruszewski, German and Dymetman, Marc},
	editor = {Chaudhuri, K and Jegelka, S and Song, L and Szepesvari, C and Niu, G and Sabato, S},
	urldate = {2023-05-03},
	date = {2022},
}

@inproceedings{wan_leveraging_2020-1,
	title = {Leveraging Personal Navigation Assistant Systems Using Automated Social Media Traffic Reporting},
	isbn = {978-1-72814-224-1},
	abstract = {Modern urbanization is demanding smarter technologies to improve a variety of applications in intelligent transportation systems to relieve the increasing amount of vehicular traffic congestion and incidents. Existing incident detection techniques are limited to the use of sensors in the transportation network and hang on human-inputs. Despite of its data abundance, social media is not well-exploited in such context. In this paper, we develop an automated traffic alert system based on Natural Language Processing ({NLP}) that filters this flood of information and extract important traffic-related bullets. To this end, we employ the fine-tuning Bidirectional Encoder Representations from Transformers ({BERT}) language embedding model to filter the related traffic information from social media. Then, we apply a question-answering model to extract necessary information characterizing the report event such as its exact location, occurrence time, and nature of the events. We demonstrate the adopted {NLP} approaches outperform other existing approach and, after effectively training them, we focus on real-world situation and show how the developed approach can, in real-time, extract traffic-related information and automatically convert them into alerts for navigation assistance applications such as navigation apps.},
	author = {Wan, Xiangpeng and Ghazzai, Hakim and Massoud, Yehia and {IEEE}},
	urldate = {2021-02-26},
	date = {2020},
}

@inproceedings{yan_fastseq_2021-1,
	title = {{FastSeq}: Make Sequence Generation Faster},
	isbn = {978-1-954085-56-5},
	abstract = {Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop {FastSeq} framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough to be applicable to Transformer-based models (e.g., T5, {GPT}2, and {UniLM}). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, {FastSeq} is easy to use with a simple one-line code change. The source code is available at https://github.com/microsoft/fastseq.},
	pages = {218--226},
	author = {Yan, Yu and Hu, Fei and Chen, Jiusheng and Bhendawade, Nikhil and Ye, Ting and Gong, Yeyun and Duan, Nan and Cui, Desheng and Chi, Bingyu and Zhang, Ruifei and {Assoc Computat Linguist}},
	urldate = {2021-10-02},
	date = {2021},
}

@article{uddin_software_2022-2,
	title = {Software defect prediction employing {BiLSTM} and {BERT}-based semantic feature},
	volume = {26},
	issn = {1432-7643, 1433-7479},
	url = {https://link.springer.com/10.1007/s00500-022-06830-5},
	doi = {10.1007/s00500-022-06830-5},
	pages = {7877--7891},
	number = {16},
	journaltitle = {Soft Computing},
	shortjournal = {Soft Comput},
	author = {Uddin, Md Nasir and Li, Bixin and Ali, Zafar and Kefalas, Pavlos and Khan, Inayat and Zada, Islam},
	urldate = {2023-06-10},
	date = {2022-08},
	langid = {english},
}

@article{badia_aranda_evolution_2022,
	title = {Evolution of patients with chronic hepatitis C infection with advanced fibrosis or cirrhosis cured with direct-acting antivirals. Long-term follow-up},
	volume = {45},
	issn = {24443824},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2444382422001870},
	doi = {10.1016/j.gastre.2022.02.005},
	pages = {767--779},
	number = {10},
	journaltitle = {Gastroenterología y Hepatología (English Edition)},
	shortjournal = {Gastroenterología y Hepatología (English Edition)},
	author = {Badia Aranda, Ester and Fernández Marcos, Cristina and Puebla Maestu, Aida and Gozalo Marín, Visitación and Vinuesa Campo, Raquel and Calvo Simal, Sara and Gómez Camarero, Judith},
	urldate = {2023-06-10},
	date = {2022-12},
	langid = {english},
}

@article{ferreira_codex_2018,
	title = {Codex: A metamodel ontology to guide the execution of coding experiments},
	volume = {59},
	issn = {09205489},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0920548917303197},
	doi = {10.1016/j.csi.2018.02.003},
	shorttitle = {Codex},
	pages = {35--44},
	journaltitle = {Computer Standards \& Interfaces},
	shortjournal = {Computer Standards \& Interfaces},
	author = {Ferreira, Waldemar and Baldassarre, Maria Teresa and Soares, Sergio},
	urldate = {2023-06-10},
	date = {2018-08},
	langid = {english},
	file = {已提交版本:files/209906/Ferreira 等 - 2018 - Codex A metamodel ontology to guide the execution.pdf:application/pdf},
}

@article{betancourt-pena_diferencias_2020,
	title = {Diferencias entre pacientes con {EPOC} no adherentes y adherentes al tratamiento farmacológico según la {GOLD} 2018 en variables clínicas, los índices {CODEX}, {COTE} y {BODE}},
	volume = {42},
	issn = {02115638},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0211563819301397},
	doi = {10.1016/j.ft.2019.10.005},
	pages = {24--32},
	number = {1},
	journaltitle = {Fisioterapia},
	shortjournal = {Fisioterapia},
	author = {Betancourt-Peña, J. and Rodríguez-Castro, J. and Escobar-Vidal, D.A.},
	urldate = {2023-06-10},
	date = {2020-01},
	langid = {spanish},
}

@article{bella_mineral_2022,
	title = {Mineral content and physico-chemical parameters of honey from North regions of Algeria},
	volume = {36},
	issn = {1478-6419, 1478-6427},
	url = {https://www.tandfonline.com/doi/full/10.1080/14786419.2020.1791110},
	doi = {10.1080/14786419.2020.1791110},
	pages = {636--643},
	number = {2},
	journaltitle = {Natural Product Research},
	shortjournal = {Natural Product Research},
	author = {Bella, Giuseppa Di and Licata, Patrizia and Potortì, Angela Giorgia and Crupi, Rosalia and Nava, Vincenzo and Qada, Benameur and Rando, Rossana and Bartolomeo, Giovanni and Dugo, Giacomo and Turco, Vincenzo Lo},
	urldate = {2023-06-10},
	date = {2022-01-17},
	langid = {english},
	file = {已提交版本:files/209882/Bella 等 - 2022 - Mineral content and physico-chemical parameters of.pdf:application/pdf},
}

@article{atuesta_rodriguez_factores_2022,
	title = {Factores asociados con fracturas vertebrales asintomáticas en pacientes con artritis reumatoide en un servicio de reumatología de Bogotá (Colombia)},
	volume = {29},
	issn = {01218123},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0121812321001055},
	doi = {10.1016/j.rcreu.2021.05.017},
	pages = {274--282},
	number = {4},
	journaltitle = {Revista Colombiana de Reumatología},
	shortjournal = {Revista Colombiana de Reumatología},
	author = {Atuesta Rodríguez, Alexis Javier and Rondón Herrera, Federico and Calvo Páramo, Enrique and Motta Quimbaya, Orfa Yaneth and Caicedo Gélvez, Yazmín Adriana},
	urldate = {2023-06-10},
	date = {2022-10},
	langid = {spanish},
}

@article{palombo_genome-wide_2018,
	title = {Genome-wide association study of milk fatty acid composition in Italian Simmental and Italian Holstein cows using single nucleotide polymorphism arrays},
	volume = {101},
	issn = {00220302},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022030218308695},
	doi = {10.3168/jds.2018-14413},
	pages = {11004--11019},
	number = {12},
	journaltitle = {Journal of Dairy Science},
	shortjournal = {Journal of Dairy Science},
	author = {Palombo, V. and Milanesi, M. and Sgorlon, S. and Capomaccio, S. and Mele, M. and Nicolazzi, E. and Ajmone-Marsan, P. and Pilla, F. and Stefanon, B. and D'Andrea, M.},
	urldate = {2023-06-10},
	date = {2018-12},
	langid = {english},
	file = {全文:files/209908/Palombo 等 - 2018 - Genome-wide association study of milk fatty acid c.pdf:application/pdf},
}

@article{jimenez-murcia_developmental_2019,
	title = {Developmental trajectories of gambling severity after cognitive-behavioral therapy},
	volume = {60},
	issn = {0924-9338, 1778-3585},
	url = {https://www.cambridge.org/core/product/identifier/S0924933800000067/type/journal_article},
	doi = {10.1016/j.eurpsy.2019.04.001},
	abstract = {Abstract
            
              Aims:
              To estimate trajectories of the gambling disorder ({GD}) severity for 12 months following a manualized cognitive-behavior-therapy ({CBT}) program, and to identify the main variables associated with each trajectory.
            
            
              Methods:
              
                Latent Class Growth Analysis examined the longitudinal changes of
                n
                = 603 treatment-seeking patients with {GD}.
              
            
            
              Results:
              
                Five separate empirical trajectories were identified: T1 (
                n
                = 383, 63.5\%) was characterized by the most highest baseline gambling severity levels and positive progress to recovery during the follow-up period; T2 (
                n
                = 154, 25.5\%) featured participants with high baseline gambling severity and good progress to recovery; T3 (
                n
                = 30, 5.0\%) was made up of patients with high gambling baseline severity and slow progress to recovery; T4 (
                n
                = 13, 2.2\%) and T5 (
                n
                = 23, 3.8\%) contained participants with high baseline gambling severity and moderate (T4) and poor (T5) progress in {GD} severity during the follow-up. Psychopathological state and personality traits discriminated between trajectories. Poor compliance with the therapy guidelines and the presence of relapses also differed between the trajectories.
              
            
            
              Conclusions:
              Our findings show that patients seeking treatment for {GD} are heterogeneous and that trends in progress following treatment can be identified considering sociodemographic features, psychopathological state and personality traits. These results could be useful in developing more efficient interventions for {GD} patients.},
	pages = {28--40},
	journaltitle = {European Psychiatry},
	shortjournal = {Eur. psychiatr.},
	author = {Jiménez-Murcia, Susana and Granero, Roser and Fernández-Aranda, Fernando and Aymamí, Neus and Gómez-Peña, Mónica and Mestre-Bach, Gemma and Steward, Trevor and Del Pino-Gutiérrez, Amparo and Mena-Moreno, Teresa and Vintró-Alcaraz, Cristina and Agüera, Zaida and Sánchez-González, Jéssica and Moragas, Laura and Codina, Ester and Menchón, José M.},
	urldate = {2023-06-10},
	date = {2019-08},
	langid = {english},
	file = {全文:files/209892/Jiménez-Murcia 等 - 2019 - Developmental trajectories of gambling severity af.pdf:application/pdf},
}

@article{mazzetto_acercamiento_2017,
	title = {Un acercamiento al léxico del sabor entre los antiguos nahuas},
	volume = {51},
	issn = {01851225},
	url = {http://www.revistas.unam.mx/index.php/antropologia/article/view/61984},
	doi = {10.1016/j.antro.2017.03.003},
	pages = {154--170},
	number = {2},
	journaltitle = {Anales de Antropología},
	shortjournal = {Anales de Antropología},
	author = {Mazzetto, Elena},
	urldate = {2023-06-10},
	date = {2017-07},
	langid = {spanish},
	file = {全文:files/209901/Mazzetto - 2017 - Un acercamiento al léxico del sabor entre los anti.pdf:application/pdf},
}

@article{polioudakis_single-cell_2019,
	title = {A Single-Cell Transcriptomic Atlas of Human Neocortical Development during Mid-gestation},
	volume = {103},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319305616},
	doi = {10.1016/j.neuron.2019.06.011},
	pages = {785--801.e8},
	number = {5},
	journaltitle = {Neuron},
	shortjournal = {Neuron},
	author = {Polioudakis, Damon and De La Torre-Ubieta, Luis and Langerman, Justin and Elkins, Andrew G. and Shi, Xu and Stein, Jason L. and Vuong, Celine K. and Nichterwitz, Susanne and Gevorgian, Melinda and Opland, Carli K. and Lu, Daning and Connell, William and Ruzzo, Elizabeth K. and Lowe, Jennifer K. and Hadzic, Tarik and Hinz, Flora I. and Sabri, Shan and Lowry, William E. and Gerstein, Mark B. and Plath, Kathrin and Geschwind, Daniel H.},
	urldate = {2023-06-10},
	date = {2019-09},
	langid = {english},
	file = {全文:files/209883/Polioudakis 等 - 2019 - A Single-Cell Transcriptomic Atlas of Human Neocor.pdf:application/pdf},
}

@article{shawahna_prevalence_2018,
	title = {Prevalence and association of clinical characteristics and biochemical factors with complications of diabetes mellitus in Palestinians treated in primary healthcare practice},
	volume = {12},
	issn = {18714021},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1871402118301115},
	doi = {10.1016/j.dsx.2018.04.024},
	pages = {693--704},
	number = {5},
	journaltitle = {Diabetes \& Metabolic Syndrome: Clinical Research \& Reviews},
	shortjournal = {Diabetes \& Metabolic Syndrome: Clinical Research \& Reviews},
	author = {Shawahna, Ramzi and Shanti, Yousef and Al Zabadi, Hamzeh and Sharabati, Mutassem and Alawneh, Ammar and Shaqu, Rakan and Taha, Ibrahim and Bustami, Adnan},
	urldate = {2023-06-10},
	date = {2018-09},
	langid = {english},
}

@article{reis_agronomic_2018,
	title = {Agronomic biofortification of upland rice with selenium and nitrogen and its relation to grain quality},
	volume = {79},
	issn = {07335210},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0733521017307920},
	doi = {10.1016/j.jcs.2018.01.004},
	pages = {508--515},
	journaltitle = {Journal of Cereal Science},
	shortjournal = {Journal of Cereal Science},
	author = {Reis, Heitor Pontes Gestal and Barcelos, Jéssica Pigatto De Queiroz and Junior, Enes Furlani and Santos, Elcio Ferreira and Silva, Vinícius Martins and Moraes, Milton Ferreira and Putti, Fernando Ferrari and Reis, André Rodrigues Dos},
	urldate = {2023-06-10},
	date = {2018-01},
	langid = {english},
}

@article{khan_complementary_2022,
	title = {Complementary effect of zoo compost with mineral nitrogen fertilisation increases wheat yield and nutrition in a low-nutrient soil},
	volume = {32},
	issn = {10020160},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1002016021600784},
	doi = {10.1016/S1002-0160(21)60078-4},
	pages = {339--347},
	number = {2},
	journaltitle = {Pedosphere},
	shortjournal = {Pedosphere},
	author = {Khan, Muhammad S.A. and Abbott, Lynette K. and Solaiman, Zakaria M. and Mawson, Peter R. and Waite, Ian S. and Jenkins, Sasha N.},
	urldate = {2023-06-10},
	date = {2022-04},
	langid = {english},
	file = {已接受版本:files/209904/Khan 等 - 2022 - Complementary effect of zoo compost with mineral n.pdf:application/pdf},
}

@article{rojas_presencia_2017,
	title = {Presencia de Acanthamoeba spp. en agua para consumo ganadero en la provincia de La Pampa, Argentina},
	volume = {49},
	issn = {03257541},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0325754117300044},
	doi = {10.1016/j.ram.2016.12.003},
	pages = {227--234},
	number = {3},
	journaltitle = {Revista Argentina de Microbiología},
	shortjournal = {Revista Argentina de Microbiología},
	author = {Rojas, María Del C. and Rodríguez Fermepín, Marcelo and Gracia Martínez, Florencia and Costamagna, Sixto R.},
	urldate = {2023-06-10},
	date = {2017-07},
	langid = {spanish},
	file = {全文:files/209914/Rojas 等 - 2017 - Presencia de Acanthamoeba spp. en agua para consum.pdf:application/pdf},
}

@article{herath_evaluation_2018,
	title = {Evaluation of green infrastructure effects on tropical Sri Lankan urban context as an urban heat island adaptation strategy},
	volume = {29},
	issn = {16188667},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1618866717301826},
	doi = {10.1016/j.ufug.2017.11.013},
	pages = {212--222},
	journaltitle = {Urban Forestry \& Urban Greening},
	shortjournal = {Urban Forestry \& Urban Greening},
	author = {Herath, H.M.P.I.K. and Halwatura, R.U. and Jayasinghe, G.Y.},
	urldate = {2023-06-10},
	date = {2018-01},
	langid = {english},
}

@article{badia_aranda_evolucion_2022,
	title = {Evolución de los pacientes con infección crónica por hepatitis C con fibrosis avanzada o cirrosis curados con antivirales de acción directa. Seguimiento a largo plazo},
	volume = {45},
	issn = {02105705},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0210570522000644},
	doi = {10.1016/j.gastrohep.2022.02.002},
	pages = {767--779},
	number = {10},
	journaltitle = {Gastroenterología y Hepatología},
	shortjournal = {Gastroenterología y Hepatología},
	author = {Badia Aranda, Ester and Fernández Marcos, Cristina and Puebla Maestu, Aida and Gozalo Marín, Visitación and Vinuesa Campo, Raquel and Calvo Simal, Sara and Gómez Camarero, Judith},
	urldate = {2023-06-10},
	date = {2022-12},
	langid = {spanish},
}

@article{martinez-solano_lactate_2022,
	title = {Lactate levels as a prognostic predict in cardiogenic shock under venoarterial extracorporeal membrane oxygenation support},
	volume = {75},
	issn = {18855857},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1885585721003194},
	doi = {10.1016/j.rec.2021.08.020},
	pages = {595--603},
	number = {7},
	journaltitle = {Revista Española de Cardiología (English Edition)},
	shortjournal = {Revista Española de Cardiología (English Edition)},
	author = {Martínez-Solano, Jorge and Sousa-Casasnovas, Iago and Bellón-Cano, José María and García-Carreño, Jorge and Juárez-Fernández, Miriam and Díez-Delhoyo, Felipe and Sanz-Ruiz, Ricardo and Devesa-Cordero, Carolina and Elízaga-Corrales, Jaime and Fernández-Avilés, Francisco and Martínez-Sellés, Manuel},
	urldate = {2023-06-10},
	date = {2022-07},
	langid = {english},
}

@article{atuesta_rodriguez_factors_2022,
	title = {Factors associated with asymptomatic vertebral fractures in patients with rheumatoid arthritis in a rheumatology service in Bogotá (Colombia)},
	volume = {29},
	issn = {24444405},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S244444052300002X},
	doi = {10.1016/j.rcreue.2023.01.002},
	pages = {274--282},
	number = {4},
	journaltitle = {Revista Colombiana de Reumatología (English Edition)},
	shortjournal = {Revista Colombiana de Reumatología (English Edition)},
	author = {Atuesta Rodríguez, Alexis Javier and Rondón Herrera, Federico and Calvo Páramo, Enrique and Motta Quimbaya, Orfa Yaneth and Caicedo Gélvez, Yazmín Adriana},
	urldate = {2023-06-10},
	date = {2022-10},
	langid = {english},
}

@incollection{gerardi_global_2023,
	title = {Global Food Safety Initiative ({GFSI}): underpinning the safety of the global food chain, facilitating regulatory compliance, trade, and consumer trust},
	isbn = {978-0-12-819470-6},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128194706000585},
	shorttitle = {Global Food Safety Initiative ({GFSI})},
	pages = {1089--1098},
	booktitle = {Present Knowledge in Food Safety},
	publisher = {Elsevier},
	author = {Gerardi, Anne},
	urldate = {2023-06-10},
	date = {2023},
	langid = {english},
	doi = {10.1016/B978-0-12-819470-6.00058-5},
}

@article{hu_linking_2017,
	title = {The linking of the upper-middle and lower reaches of the Yellow River as a result of fluvial entrenchment},
	volume = {166},
	issn = {02773791},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0277379117301737},
	doi = {10.1016/j.quascirev.2017.02.026},
	pages = {324--338},
	journaltitle = {Quaternary Science Reviews},
	shortjournal = {Quaternary Science Reviews},
	author = {Hu, {ZhenBo} and Pan, {BaoTian} and Bridgland, David and Vandenberghe, Jef and Guo, {LianYong} and Fan, {YunLong} and Westaway, Rob},
	urldate = {2023-06-10},
	date = {2017-06},
	langid = {english},
	file = {已接受版本:files/209913/Hu 等 - 2017 - The linking of the upper-middle and lower reaches .pdf:application/pdf},
}

@article{boixeda_consensus_2019,
	title = {Consensus for managing patients with chronic obstructive pulmonary disease according to the {CODEX} index},
	volume = {219},
	issn = {22548874},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2254887419301110},
	doi = {10.1016/j.rceng.2019.03.022},
	pages = {494--504},
	number = {9},
	journaltitle = {Revista Clínica Española (English Edition)},
	shortjournal = {Revista Clínica Española (English Edition)},
	author = {Boixeda, R. and Díez-Manglano, J. and Gómez-Antúnez, M. and López-García, F. and Recio, J. and Almagro, P.},
	urldate = {2023-06-10},
	date = {2019-12},
	langid = {english},
}

@article{guardiola_arevalo_characteristics_2017,
	title = {Characteristics and course of chronic hepatitis B e antigen-negative infection},
	volume = {40},
	issn = {24443824},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2444382417300275},
	doi = {10.1016/j.gastre.2016.11.009},
	pages = {59--69},
	number = {2},
	journaltitle = {Gastroenterología y Hepatología (English Edition)},
	shortjournal = {Gastroenterología y Hepatología (English Edition)},
	author = {Guardiola Arévalo, Antonio and Gómez Rodríguez, Rafael and Romero Gutiérrez, Marta and Gómez Moreno, Ana Zaida and García Vela, Almudena and Sánchez Simón, Raquel and Gómez Hernando, Cesar and Andrés Esteban, Eva María},
	urldate = {2023-06-10},
	date = {2017-02},
	langid = {english},
}

@article{guardiola_arevalo_caracteristicas_2017,
	title = {Características y evolución de la infección crónica por virus de la hepatitis B antígeno e negativo},
	volume = {40},
	issn = {02105705},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0210570516301637},
	doi = {10.1016/j.gastrohep.2016.11.002},
	pages = {59--69},
	number = {2},
	journaltitle = {Gastroenterología y Hepatología},
	shortjournal = {Gastroenterología y Hepatología},
	author = {Guardiola Arévalo, Antonio and Gómez Rodríguez, Rafael and Romero Gutiérrez, Marta and Gómez Moreno, Ana Zaida and García Vela, Almudena and Sánchez Simón, Raquel and Gómez Hernando, Cesar and Andrés Esteban, Eva María},
	urldate = {2023-06-10},
	date = {2017-02},
	langid = {spanish},
}

@article{pinto-teixeira_development_2018,
	title = {Development of Concurrent Retinotopic Maps in the Fly Motion Detection Circuit},
	volume = {173},
	issn = {00928674},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418302307},
	doi = {10.1016/j.cell.2018.02.053},
	pages = {485--498.e11},
	number = {2},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Pinto-Teixeira, Filipe and Koo, Clara and Rossi, Anthony Michael and Neriec, Nathalie and Bertet, Claire and Li, Xin and Del-Valle-Rodriguez, Alberto and Desplan, Claude},
	urldate = {2023-06-10},
	date = {2018-04},
	langid = {english},
	file = {全文:files/209897/Pinto-Teixeira 等 - 2018 - Development of Concurrent Retinotopic Maps in the .pdf:application/pdf},
}

@article{boixeda_consenso_2019,
	title = {Consenso para el manejo de pacientes con {EPOC} según el índice {CODEX}},
	volume = {219},
	issn = {00142565},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0014256519300967},
	doi = {10.1016/j.rce.2019.03.006},
	pages = {494--504},
	number = {9},
	journaltitle = {Revista Clínica Española},
	shortjournal = {Revista Clínica Española},
	author = {Boixeda, R. and Díez-Manglano, J. and Gómez-Antúnez, M. and López-García, F. and Recio, J. and Almagro, P.},
	urldate = {2023-06-10},
	date = {2019-12},
	langid = {spanish},
}

@article{barrie_investigation_2021,
	title = {An investigation into the effect of potential confounding patient and treatment parameters on human embryo morphokinetics},
	volume = {115},
	issn = {00150282},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0015028220325516},
	doi = {10.1016/j.fertnstert.2020.10.037},
	pages = {1014--1022},
	number = {4},
	journaltitle = {Fertility and Sterility},
	shortjournal = {Fertility and Sterility},
	author = {Barrie, Amy and {McDowell}, Garry and Troup, Stephen},
	urldate = {2023-06-10},
	date = {2021-04},
	langid = {english},
	file = {全文:files/209910/Barrie 等 - 2021 - An investigation into the effect of potential conf.pdf:application/pdf},
}

@article{dewaal_regional_2022,
	title = {Regional Codex Guidelines and Their Potential To Impact Food Safety in Traditional Food Markets},
	volume = {85},
	issn = {0362028X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0362028X22055740},
	doi = {10.4315/JFP-22-052},
	pages = {1148--1156},
	number = {8},
	journaltitle = {Journal of Food Protection},
	shortjournal = {Journal of Food Protection},
	author = {{DeWaal}, Caroline Smith and Okoruwa, Augustine and Yalch, Teale and {McClafferty}, Bonnie},
	urldate = {2023-06-10},
	date = {2022-08},
	langid = {english},
	file = {全文:files/209909/DeWaal 等 - 2022 - Regional Codex Guidelines and Their Potential To I.pdf:application/pdf},
}

@article{guardiola-arevalo_hepatitis_2018,
	title = {Hepatitis B virus e antigen-negative chronic infection. Treatment based on glutamic pyruvic transaminase and hepatitis B virus deoxyribonucleic acid cut-off values},
	volume = {41},
	issn = {24443824},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2444382418300373},
	doi = {10.1016/j.gastre.2018.03.003},
	pages = {153--162},
	number = {3},
	journaltitle = {Gastroenterología y Hepatología (English Edition)},
	shortjournal = {Gastroenterología y Hepatología (English Edition)},
	author = {Guardiola-Arévalo, Antonio and Gómez Rodríguez, Rafael and Romero Gutiérrez, Marta and Gómez Moreno, Ana Zaida and García Vela, Almudena and Sánchez Simón, Raquel and Gómez Hernando, Cesar and Andrés Esteban, Eva María},
	urldate = {2023-06-10},
	date = {2018-03},
	langid = {english},
}

@article{trivino-ibanez_biomarkers_2022,
	title = {Biomarkers associated with survival and favourable outcome of radioembolization with yttrium-90 glass microspheres for colon cancer liver metastases: Single centre experience},
	volume = {41},
	issn = {22538089},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2253808921001282},
	doi = {10.1016/j.remnie.2021.08.001},
	shorttitle = {Biomarkers associated with survival and favourable outcome of radioembolization with yttrium-90 glass microspheres for colon cancer liver metastases},
	pages = {231--238},
	number = {4},
	journaltitle = {Revista Española de Medicina Nuclear e Imagen Molecular (English Edition)},
	shortjournal = {Revista Española de Medicina Nuclear e Imagen Molecular (English Edition)},
	author = {Triviño-Ibáñez, E.M. and Pardo Moreno, P. and Ciampi Dopazo, J.J. and Ramos-Font, C. and Ruiz Villaverde, G. and González-Flores, E. and Navarro Vergara, P.F. and Rashki, M. and Gómez-Río, M. and Rodríguez-Fernández, A.},
	urldate = {2023-06-10},
	date = {2022-07},
	langid = {english},
}

@article{piluso_ultimate_2019,
	title = {Ultimate resistance and rotation capacity of low yielding high hardening aluminium alloy beams under non-uniform bending},
	volume = {135},
	issn = {02638231},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263823118308450},
	doi = {10.1016/j.tws.2018.11.006},
	pages = {123--136},
	journaltitle = {Thin-Walled Structures},
	shortjournal = {Thin-Walled Structures},
	author = {Piluso, Vincenzo and Pisapia, Alessandro and Nastri, Elide and Montuori, Rosario},
	urldate = {2023-06-10},
	date = {2019-02},
	langid = {english},
}

@article{trivino-ibanez_biomarcadores_2022,
	title = {Biomarcadores asociados con la supervivencia y la respuesta terapéutica a la radioembolización con esferas cargadas de itrio-90 en las metástasis hepáticas del carcinoma colorrectal: nuestra experiencia},
	volume = {41},
	issn = {2253654X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2253654X21001293},
	doi = {10.1016/j.remn.2021.05.004},
	shorttitle = {Biomarcadores asociados con la supervivencia y la respuesta terapéutica a la radioembolización con esferas cargadas de itrio-90 en las metástasis hepáticas del carcinoma colorrectal},
	pages = {231--238},
	number = {4},
	journaltitle = {Revista Española de Medicina Nuclear e Imagen Molecular},
	shortjournal = {Revista Española de Medicina Nuclear e Imagen Molecular},
	author = {Triviño-Ibáñez, E.M. and Pardo Moreno, P. and Ciampi Dopazo, J.J. and Ramos-Font, C. and Ruiz Villaverde, G. and González-Flores, E. and Navarro Vergara, P.F. and Rashki, M. and Gómez-Río, M. and Rodríguez-Fernández, A.},
	urldate = {2023-06-10},
	date = {2022-07},
	langid = {spanish},
}

@article{guardiola-arevalo_infeccion_2018,
	title = {Infección crónica por virus de la hepatitis B antígeno e negativo. Manejo en función de puntos de corte de glutámico-pirúvica transaminasa y ácido desoxirribonucleico del virus de la hepatitis B},
	volume = {41},
	issn = {02105705},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0210570517302431},
	doi = {10.1016/j.gastrohep.2017.11.003},
	pages = {153--162},
	number = {3},
	journaltitle = {Gastroenterología y Hepatología},
	shortjournal = {Gastroenterología y Hepatología},
	author = {Guardiola-Arévalo, Antonio and Gómez Rodríguez, Rafael and Romero Gutiérrez, Marta and Gómez Moreno, Ana Zaida and García Vela, Almudena and Sánchez Simón, Raquel and Gómez Hernando, Cesar and Andrés Esteban, Eva María},
	urldate = {2023-06-10},
	date = {2018-03},
	langid = {spanish},
}

@article{lierman_low_2021,
	title = {Low feasibility of in vitro matured oocytes originating from cumulus complexes found during ovarian tissue preparation at the moment of gender confirmation surgery and during testosterone treatment for fertility preservation in transgender men},
	volume = {116},
	issn = {00150282},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0015028221002144},
	doi = {10.1016/j.fertnstert.2021.03.009},
	pages = {1068--1076},
	number = {4},
	journaltitle = {Fertility and Sterility},
	shortjournal = {Fertility and Sterility},
	author = {Lierman, Sylvie and Tolpe, Annelies and De Croo, Ilse and De Gheselle, Stefanie and Defreyne, Justine and Baetens, Machteld and Dheedene, Annelies and Colman, Roos and Menten, Björn and T’Sjoen, Guy and De Sutter, Petra and Tilleman, Kelly},
	urldate = {2023-06-10},
	date = {2021-10},
	langid = {english},
	file = {全文:files/209911/Lierman 等 - 2021 - Low feasibility of in vitro matured oocytes origin.pdf:application/pdf},
}

@article{sala-icardo_impacto_2017,
	title = {Impacto de variantes genéticas del transportador de membrana que une {ATP} B1, la aicar transformilasa/{IMP} ciclohidrolasa, la folilpoliglutamatosintetasa y la metilen-tetrahidrofolatorreductasa en la toxicidad de metotrexato},
	volume = {13},
	issn = {1699258X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1699258X16301012},
	doi = {10.1016/j.reuma.2016.08.006},
	pages = {318--325},
	number = {6},
	journaltitle = {Reumatología Clínica},
	shortjournal = {Reumatología Clínica},
	author = {Sala-Icardo, Luis and Lamana, Amalia and Ortiz, Ana María and García Lorenzo, Elena and Moreno Fresneda, Pablo and García-Vicuña, Rosario and González-Álvaro, Isidoro},
	urldate = {2023-06-10},
	date = {2017-11},
	langid = {spanish},
}

@article{martinez-solano_cinetica_2022,
	title = {Cinética del lactato para el pronóstico en el shock cardiogénico asistido con oxigenador extracorpóreo de membrana venoarterial},
	volume = {75},
	issn = {03008932},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0300893221003651},
	doi = {10.1016/j.recesp.2021.08.007},
	pages = {595--603},
	number = {7},
	journaltitle = {Revista Española de Cardiología},
	shortjournal = {Revista Española de Cardiología},
	author = {Martínez-Solano, Jorge and Sousa-Casasnovas, Iago and Bellón-Cano, José María and García-Carreño, Jorge and Juárez-Fernández, Miriam and Díez-Delhoyo, Felipe and Sanz-Ruiz, Ricardo and Devesa-Cordero, Carolina and Elízaga-Corrales, Jaime and Fernández-Avilés, Francisco and Martínez-Sellés, Manuel},
	urldate = {2023-06-10},
	date = {2022-07},
	langid = {spanish},
}

@article{sala-icardo_impact_2017,
	title = {Impact of Genetic Variants of {ATP} Binding Cassette B1, {AICAR} Transformylase/{IMP} Cyclohydrolase, Folyl-polyglutamate Synthetase, and Methylenetetrahydrofolate Reductase on Methotrexate Toxicity},
	volume = {13},
	issn = {21735743},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S217357431730120X},
	doi = {10.1016/j.reumae.2016.08.004},
	pages = {318--325},
	number = {6},
	journaltitle = {Reumatología Clínica (English Edition)},
	shortjournal = {Reumatología Clínica (English Edition)},
	author = {Sala-Icardo, Luis and Lamana, Amalia and Ortiz, Ana María and García Lorenzo, Elena and Moreno Fresneda, Pablo and García-Vicuña, Rosario and González-Álvaro, Isidoro},
	urldate = {2023-06-10},
	date = {2017-11},
	langid = {english},
}

@article{valipakka_improving_2020,
	title = {Improving Copy Number Variant Detection from Sequencing Data with a Combination of Programs and a Predictive Model},
	volume = {22},
	issn = {15251578},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1525157819304076},
	doi = {10.1016/j.jmoldx.2019.08.009},
	pages = {40--49},
	number = {1},
	journaltitle = {The Journal of Molecular Diagnostics},
	shortjournal = {The Journal of Molecular Diagnostics},
	author = {Välipakka, Salla and Savarese, Marco and Sagath, Lydia and Arumilli, Meharji and Giugliano, Teresa and Udd, Bjarne and Hackman, Peter},
	urldate = {2023-06-10},
	date = {2020-01},
	langid = {english},
}

@article{sartori_efficacy_2017,
	title = {Efficacy of epiphytic bacteria to prevent northern leaf blight caused by Exserohilum turcicum in maize},
	volume = {49},
	issn = {03257541},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0325754116300980},
	doi = {10.1016/j.ram.2016.09.008},
	pages = {75--82},
	number = {1},
	journaltitle = {Revista Argentina de Microbiología},
	shortjournal = {Revista Argentina de Microbiología},
	author = {Sartori, Melina and Nesci, Andrea and García, Julián and Passone, María A. and Montemarani, Analía and Etcheverry, Miriam},
	urldate = {2023-06-10},
	date = {2017-01},
	langid = {english},
	file = {全文:files/209912/Sartori 等 - 2017 - Efficacy of epiphytic bacteria to prevent northern.pdf:application/pdf},
}

@article{shepherd_post-depositional_2018,
	title = {Post-depositional behaviour of mercury and arsenic in submarine mine tailings deposited in Buyat Bay, North Sulawesi, Indonesia},
	volume = {137},
	issn = {01411136},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141113617307705},
	doi = {10.1016/j.marenvres.2018.02.028},
	pages = {88--97},
	journaltitle = {Marine Environmental Research},
	shortjournal = {Marine Environmental Research},
	author = {Shepherd, Thomas and Rumengan, Inneke and Sahami, Ali},
	urldate = {2023-06-10},
	date = {2018-06},
	langid = {english},
}

@inproceedings{akimova_pytracebugs_2021-1,
	location = {Taipei, Taiwan},
	title = {{PyTraceBugs}: A Large Python Code Dataset for Supervised Machine Learning in Software Defect Prediction},
	isbn = {978-1-66543-784-4},
	url = {https://ieeexplore.ieee.org/document/9712116/},
	doi = {10.1109/APSEC53868.2021.00022},
	shorttitle = {{PyTraceBugs}},
	eventtitle = {2021 28th Asia-Pacific Software Engineering Conference ({APSEC})},
	pages = {141--151},
	booktitle = {2021 28th Asia-Pacific Software Engineering Conference ({APSEC})},
	publisher = {{IEEE}},
	author = {Akimova, Elena N. and Bersenev, Alexander Yu. and Deikov, Artem A. and Kobylkin, Konstantin S. and Konygin, Anton V. and Mezentsev, Ilya P. and Misilov, Vladimir E.},
	urldate = {2023-06-10},
	date = {2021-12},
}

@inproceedings{ciniselli_empirical_2021-3,
	location = {Madrid, Spain},
	title = {An Empirical Study on the Usage of {BERT} Models for Code Completion},
	isbn = {978-1-72818-710-5},
	url = {https://ieeexplore.ieee.org/document/9463129/},
	doi = {10.1109/MSR52588.2021.00024},
	eventtitle = {2021 {IEEE}/{ACM} 18th International Conference on Mining Software Repositories ({MSR})},
	pages = {108--119},
	booktitle = {2021 {IEEE}/{ACM} 18th International Conference on Mining Software Repositories ({MSR})},
	publisher = {{IEEE}},
	author = {Ciniselli, Matteo and Cooper, Nathan and Pascarella, Luca and Poshyvanyk, Denys and Di Penta, Massimiliano and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2021-05},
	file = {已提交版本:files/209966/Ciniselli 等 - 2021 - An Empirical Study on the Usage of BERT Models for.pdf:application/pdf},
}

@article{salza_effectiveness_2023-3,
	title = {On the Effectiveness of Transfer Learning for Code Search},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9835142/},
	doi = {10.1109/TSE.2022.3192755},
	pages = {1804--1822},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Salza, Pasquale and Schwizer, Christoph and Gu, Jian and Gall, Harald C.},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/209968/Salza 等 - 2023 - On the Effectiveness of Transfer Learning for Code.pdf:application/pdf},
}

@inproceedings{lajko_towards_2022-2,
	location = {Pittsburgh Pennsylvania},
	title = {Towards {JavaScript} program repair with generative pre-trained transformer ({GPT}-2)},
	isbn = {978-1-4503-9285-3},
	url = {https://dl.acm.org/doi/10.1145/3524459.3527350},
	doi = {10.1145/3524459.3527350},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {61--68},
	booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
	publisher = {{ACM}},
	author = {Lajkó, Márk and Csuvik, Viktor and Vidács, László},
	urldate = {2023-06-10},
	date = {2022-05-19},
	langid = {english},
	file = {全文:files/209971/Lajkó 等 - 2022 - Towards JavaScript program repair with generative .pdf:application/pdf},
}

@article{wang_fret_2020-1,
	title = {Fret: Functional Reinforced Transformer With {BERT} for Code Summarization},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9146834/},
	doi = {10.1109/ACCESS.2020.3011744},
	shorttitle = {Fret},
	pages = {135591--135604},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Wang, Ruyun and Zhang, Hanwen and Lu, Guoliang and Lyu, Lei and Lyu, Chen},
	urldate = {2023-06-10},
	date = {2020},
}

@inproceedings{ochs_evaluating_2023,
	location = {Taipa, Macao},
	title = {Evaluating and improving transformers pre-trained on {ASTs} for Code Completion},
	isbn = {978-1-66545-278-6},
	url = {https://ieeexplore.ieee.org/document/10123540/},
	doi = {10.1109/SANER56733.2023.00096},
	eventtitle = {2023 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {834--844},
	booktitle = {2023 {IEEE} International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	publisher = {{IEEE}},
	author = {Ochs, Marcel and Narasimhan, Krishna and Mezini, Mira},
	urldate = {2023-06-10},
	date = {2023-03},
}

@article{von_der_mosel_validity_2023-2,
	title = {On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9785808/},
	doi = {10.1109/TSE.2022.3178469},
	pages = {1487--1507},
	number = {4},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	shortjournal = {{IIEEE} Trans. Software Eng.},
	author = {Von Der Mosel, Julian and Trautsch, Alexander and Herbold, Steffen},
	urldate = {2023-06-10},
	date = {2023-04-01},
	file = {已提交版本:files/209965/Von Der Mosel 等 - 2023 - On the Validity of Pre-Trained Transformers for Na.pdf:application/pdf},
}

@inproceedings{alhamed_evaluation_2022,
	location = {Limassol, Cyprus},
	title = {Evaluation of Context-Aware Language Models and Experts for Effort Estimation of Software Maintenance Issues},
	isbn = {978-1-66547-956-1},
	url = {https://ieeexplore.ieee.org/document/9978209/},
	doi = {10.1109/ICSME55016.2022.00020},
	eventtitle = {2022 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {129--138},
	booktitle = {2022 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Alhamed, Mohammed and Storer, Tim},
	urldate = {2023-06-10},
	date = {2022-10},
}

@inproceedings{chen_varclr_2022-1,
	location = {Pittsburgh Pennsylvania},
	title = {{VarCLR}: variable semantic representation pre-training via contrastive learning},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510162},
	doi = {10.1145/3510003.3510162},
	shorttitle = {{VarCLR}},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2327--2339},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J. and Neubig, Graham and Vasilescu, Bogdan and Goues, Claire Le},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
}

@inproceedings{tufano_using_2022-2,
	location = {Pittsburgh Pennsylvania},
	title = {Using pre-trained models to boost code review automation},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510621},
	doi = {10.1145/3510003.3510621},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {2291--2302},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Tufano, Rosalia and Masiero, Simone and Mastropaolo, Antonio and Pascarella, Luca and Poshyvanyk, Denys and Bavota, Gabriele},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {已提交版本:files/209970/Tufano 等 - 2022 - Using pre-trained models to boost code review auto.pdf:application/pdf},
}

@inproceedings{jain_jigsaw_2022-2,
	location = {Pittsburgh Pennsylvania},
	title = {Jigsaw: large language models meet program synthesis},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510203},
	doi = {10.1145/3510003.3510203},
	shorttitle = {Jigsaw},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {1219--1231},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
}

@inproceedings{ciborowska_fast_2022-3,
	location = {Pittsburgh Pennsylvania},
	title = {Fast changeset-based bug localization with {BERT}},
	isbn = {978-1-4503-9221-1},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510042},
	doi = {10.1145/3510003.3510042},
	eventtitle = {{ICSE} '22: 44th International Conference on Software Engineering},
	pages = {946--957},
	booktitle = {Proceedings of the 44th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Ciborowska, Agnieszka and Damevski, Kostadin},
	urldate = {2023-06-10},
	date = {2022-05-21},
	langid = {english},
	file = {已提交版本:files/209969/Ciborowska 和 Damevski - 2022 - Fast changeset-based bug localization with BERT.pdf:application/pdf},
}

@inproceedings{zhang_sentiment_2020-1,
	location = {Adelaide, Australia},
	title = {Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?},
	isbn = {978-1-72815-619-4},
	url = {https://ieeexplore.ieee.org/document/9240704/},
	doi = {10.1109/ICSME46990.2020.00017},
	shorttitle = {Sentiment Analysis for Software Engineering},
	eventtitle = {2020 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {70--80},
	booktitle = {2020 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	publisher = {{IEEE}},
	author = {Zhang, Ting and Xu, Bowen and Thung, Ferdian and Haryono, Stefanus Agus and Lo, David and Jiang, Lingxiao},
	urldate = {2023-06-10},
	date = {2020-09},
}

@article{messaoud_duplicate_2023,
	title = {Duplicate Bug Report Detection Using an Attention-Based Neural Language Model},
	volume = {72},
	issn = {0018-9529, 1558-1721},
	url = {https://ieeexplore.ieee.org/document/9852720/},
	doi = {10.1109/TR.2022.3193645},
	pages = {846--858},
	number = {2},
	journaltitle = {{IEEE} Transactions on Reliability},
	shortjournal = {{IEEE} Trans. Rel.},
	author = {Messaoud, Montassar Ben and Miladi, Asma and Jenhani, Ilyes and Mkaouer, Mohamed Wiem and Ghadhab, Lobna},
	urldate = {2023-06-10},
	date = {2023-06},
}

@inproceedings{liu_multi-task_2020-3,
	title = {Multi-task Learning based Pre-trained Language Model for Code Completion},
	abstract = {Code completion is one of the most useful features in the Integrated Development Environments ({IDEs}), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.},
	pages = {473--485},
	booktitle = {2020 35th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
	date = {2020-09},
	note = {{ISSN}: 2643-1572},
	keywords = {Context modeling, multi-task learning, Software engineering, Predictive models, Software, Task analysis, Tools, code completion, Real-time systems, pre-trained language model, transformer networks},
}
