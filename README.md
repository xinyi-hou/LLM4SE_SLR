# LLM4SE_SLR

This repository extends from our recent work, "[Large Language Models for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2308.10620)". It includes necessary information for our research and a curated collection of LLM4SE papers and other resources (datasets, tutorials, etc.). The focus is primarily on papers that use Large Language Models (LLM) in Software Engineering (SE) research.

> Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We collect and analyze 395 research papers from 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study.

Please feel free to send a pull request to add papers and relevant content that are not listed here.  We uploaded our completed paper lists and four RQs to [Google Drive](https://docs.google.com/spreadsheets/d/1yyixATa5tVQXo-P4iS6Fs_FsAUIJ_E_YVtS4DNfgjpQ/edit?usp=sharing) with detailed reviewed information. 

## Papers
### *Requirements engineering*
- **A Deep Context-wise Method for Coreference Detection in Natural Language Requirements(detecting coreferent entities in natural language requirements)** (2020), RE, Wang, Yawen; Shi, Lin; Li, Mingyang; Wang, Qing; Yang, Yun.
- **Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-Solution Study** (2022), ICSE, Ezzini S,Abualhaija S,Arora C,Sabetzadeh M.
- **ChatGPT as a tool for User Story Quality Evaluation: Trustworthy Out of the Box?** (2023), arXiv, Ronanki, Krishna and Cabrero-Daniel, Beatriz and Berger, Christian.
- **chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design** (2023), arXiv, White, Jules; Hays, Sam; Fu, Quchen; Spencer-Smith, Jesse; Schmidt, Douglas C..
- **ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks** (2023), arXiv, Sridhara, Giriprasad; G., Ranjani H.; Mazumdar, Sourav.
- **Experimenting a New Programming Practice with LLMs** (2024), arXiv, Zhang, Simiao; Wang, Jiaping; Dong, Guoliang; Sun, Jun; Zhang, Yueling; Pu, Geguang.
- **Few-shot learning for sentence pair classification and its applications in software engineering** (2023), arXiv, Helmeczi, Robert Kraig; Cevik, Mucahit; Yıldırım, Savas.
- **Formalizing Natural Language Intent into Program Specifications via Large Language Models** (2023), arXiv, Endres, Madeline; Fakhoury, Sarah; Chakraborty, Saikat; Lahiri, Shuvendu K..
- **Identification of intra-domain ambiguity using transformer-based machine learning** (2022), ICSE, Moharil, Ambarish; Sharma, Arpit.
- **Impact of Large Language Models on Generating Software Specifications** (2023), arXiv, Xie, Danning; Yoo, Byungwoo; Jiang, Nan; Kim, Mijung; Tan, Lin; Zhang, Xiangyu; Lee, Judy S..
- **Leveraging Transformer-based Language Models to Automate Requirements Satisfaction Assessment** (2023), arXiv, Poudel, Amrit; Lin, Jinfeng; Cleland-Huang, Jane.
- **NoRBERT: Transfer Learning for Requirements Classification** (2020), RE, Hey, Tobias; Keim, Jan; Koziolek, Anne; Tichy, Walter F..
- **PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models** (2022), ASE, Luo, Xianchang; Xue, Yinxing; Xing, Zhenchang; Sun, Jiamou.
- **SpecGen: Automated Generation of Formal Program Specifications via Large Language Models** (2024), arXiv, Ma, Lezhi; Liu, Shangqing; Li, Yi; Xie, Xiaofei; Bu, Lei.
- **TABASCO: A transformer based contextualization toolkit** (2023), SCP, Moharil, Ambarish; Sharma, Arpit.
- **Traceability transformed: Generating more accurate links with pre-trained bert models** (2021), ICSE, Lin, Jinfeng; Liu, Yalin; Zeng, Qingkai; Jiang, Meng; Cleland-Huang, Jane.
- **Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT** (2023), arXiv, El-Hajjami, Abdelkarim; Fafin, Nicolas; Salinesi, Camille.


## Cites
If you find this repository useful, please cite our survey paper:
```
@article{hou2023large,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={arXiv preprint arXiv:2308.10620},
  year={2023}
}
```

